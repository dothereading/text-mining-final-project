url,title,subtitle,n_sections,n_paragraphs,section_titles,story_text,claps,responses,reading_time,publication,date,year
https://towardsdatascience.com/optimal-threshold-for-imbalanced-classification-5884e870c293,Optimal Threshold for Imbalanced Classification,How to choose the…,1,28,['Optimal Threshold for Imbalanced Classification'],"Classification is one of the supervised learning techniques to conduct predictive analytics with the categorical outcome, it might be a binary class or multiclass. Nowadays, there is a lot of research and cases about classification using several algorithms, from basic to advanced like logistic regression, discriminant analysis, Naïve Bayes, decision tree, random forest, support vector machine, neural network, etc. They have been well developed and successfully applied to many application domains. However, the imbalanced class distribution of a data set has a problem because the majority of supervised learning techniques developed are for balanced class distribution. The imbalanced class distribution usually happens when we are studying a rare phenomenon such as medical diagnosis, risk management, hoax detection, and many more. Before talking intensively about imbalanced classification and how to handle this case, it will be good if we have a good foundation with a confusion matrix. A confusion matrix (also well-known as an error matrix) contains information about actual and predicted classifications done by a classification algorithm. The performance of such algorithms is commonly evaluated using the data in the matrix. The following table shows the confusion matrix for a two-class classifier. The classification with the two-class classifier will have four possible outcomes as follows. Read more about Type I Error and Type II Error HERE Furthermore, in order to evaluate our machine learning model or algorithm in classification case, there are a few evaluation metrics to explore but it’s tricky if we meet the imbalanced class. For imbalanced classification, we must choose the correct evaluation metrics to use with the condition they are valid and unbiased. It means that the value of these evaluation metrics will have to represent the actual condition of the data. For instance, accuracy will be actually biased in imbalanced classification because of the different distribution of classes. Take a look at the following study case to understand the statement above. Balanced classificationSuppose we are a Data Scientist in a tech company and asked for developing a machine learning model to predict whether our customer will be a churn or not. We have 165 customers where the 105 customers are categorized as not churn and the rest as churn customer. The model produces a given outcome as follows. As a balanced classification, accuracy may be the unbiased metric for evaluation. It represents the model performance correctly over the balanced class distribution. The accuracy, in this case, has a high correlation to the recall, specificity, precision, etc. According to the confusion matrix, that’s easier to conclude that our research has been produced as an optimal algorithm or model. Imbalanced classificationSimilar to the previous case but we modified the number of customers for constructing the imbalanced classification. Now, there are 450 customers in total where 15 customers are categorized as churn and the rest, 435 customers as not churn. The model produces a given outcome as follows. Looking at the accuracy in the confusion matrix above, the conclusion may be misleading because of the imbalanced class distribution. What does happen to the algorithm when it produces an accuracy of 0.98? The accuracy will be biased in this case. It doesn't represent the model performance as well. The accuracy is high enough but the recall is very bad. Furthermore, the specificity and precision equal to 1.0 because the model or algorithm doesn’t produce the False Positive. That is one of the consequences of imbalanced classification. However, F1-score will be the real representation of model performance cause it considers the recall and precision in its calculation. Note: to classify the data into positive and negative, there is still no a rigid policy In addition to some of the evaluation metrics that have been mentioned above, there are two important metrics to understand as follows. To compare the uses of evaluation metrics and determine the probability threshold for imbalanced classification, the real data simulation is proposed. The simulation generates the 10,000 samples with two variables, dependent and independent, with the ratio between major and minor classes is about 99:1. It belongs to the imbalanced classification, no doubt. To deal with the imbalanced class, threshold moving is proposed as the alternative to handling the imbalanced. Generating the synthetic observation or resample a certain data, theoretically, has its own risk, like create a new observation actually doesn’t appear in the data, decrease the valuable information of the data itself or create a flood of information. The X-axis or independent variable is the false positive rate for the predictive test. The Y-axis or dependent variable is the true positive rate for the predictive test. A perfect result would be the point (0, 1) indicating 0% false positives and 100% true positives. The geometric mean or known as G-mean is the geometric mean of sensitivity (known as recall) and specificity. So, it will be one of the unbiased evaluation metrics for imbalanced classification. Using the G-mean as the unbiased evaluation metrics and the main focus of threshold moving, it produces the optimal threshold for the binary classification in the 0.0131. Theoretically, the observation will be categorized as a minor class when its probability is lower than 0.0131, vice versa. One of the metrics to be discussed is Youden’s J statistics. Optimizing Youden’s J statistics will determine the best threshold for the classification. Youden’s J index gives a equals result of the threshold as using G-mean. It produces the optimal threshold for the binary classification in 0.0131. A precision-recall curve is a graph that represents the relationship between precision and recall. There are several evaluation metrics that are ready to use as the main focus for calculation. They are G-mean, F1-score, etc. As long as they are unbiased metrics for imbalanced classification, they can be applied in the calculation. Using the Precision-Recall curve and F1-score, it produces a threshold of 0.3503 for determining whether a given observation belongs to the major or minor class. It differs too much from the previous technique using the ROC curve because of the approaches. Threshold tuning is a common technique to determine an optimal threshold for imbalanced classification. The sequence of the threshold is generated by the researcher need while the previous techniques using the ROC and Precision & Recall to create a sequence of those thresholds. The advantages are the customization of the threshold sequence as the need but it will have a higher cost of computation. The syntax np.arrange(0.0, 1.0, 0.0001) means that there are 10,000 candidates of a threshold. Using a looping mechanism, it tries to find out the optimal threshold with the subject to maximize the F1-score as an unbiased metric. Finally, the looping mechanism was stopped and printed out the optimal threshold of 0.3227. Big thanks to Jason Brownlee who has been giving me the motivation to learn and work harder related to Statistics and machine learning implementation especially in threshold moving technique with a clear and proper article. Thanks! The machine learning algorithm mainly works well on the balanced classification because of their algorithm assumption using the balanced distribution of the target variable. Further, accuracy is no longer relevant to the imbalanced case, it’s biased. So, the main focus must be switched to those unbiased like G-mean, F1-score, etc. Threshold moving using ROC curve, Precision-Recall curve, threshold tuning curve can be the alternative solution to handling the imbalanced distribution since the resampling technique seems like it doesn’t make sense to the business logic. However, the options are open and the implementation must keep consideration of the business needs. [1] J. Brownlee. A Gentle Introduction to Threshold-Moving for Imbalanced Classification (2020). https://machinelearningmastery.com/.",,0,7,Towards Data Science,2021-01-01,2021
https://towardsdatascience.com/implementing-visualttransformer-in-pytorch-184f9f16f632,Implementing VisualTtransformer in PyTorch,"Hi guys, happy new year! Today we are going to implement…",6,59,"['Implementing Vision Transformer (ViT) in PyTorch', 'CLS Token', 'Position Embedding', 'Attention', 'Residuals', 'Transformer']","I am on LinkedIn, come and say hi 👋 Hi guys, happy new year! Today we are going to implement the famous Vi(sion) T(ransformer) proposed in AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE. Code is here, an interactive version of this article can be downloaded from here. ViT is available on my new computer vision library called glasses This is a technical tutorial, not your normal medium post where you find out about the top 5 secret pandas functions to make you rich. So, before beginning, I highly recommend you to: - have a look at the amazing The Illustrated Transformer website- watch Yannic Kilcher video about ViT- read Einops doc So, ViT uses a normal transformer (the one proposed in Attention is All You Need) that works on images. But, how? The following picture shows ViT’s architecture The input image is decomposed into 16x16 flatten patches (the image is not in scale). Then they are embedded using a normal fully connected layer, a special cls token is added in front of them and the positional encoding is summed. The resulting tensor is passed first into a standard Transformer and then to a classification head. That's it. The article is structure into the following sections: We are going to implement the model block by block with a bottom-up approach. We can start by importing all the required packages Nothing fancy here, just PyTorch + stuff First of all, we need a picture, a cute cat works just fine :) Then, we need to preprocess it The first step is to break-down the image in multiple patches and flatten them. Quoting from the paper: This can be easily done using einops. Now, we need to project them using a normal linear layer We can create a PatchEmbedding class to keep our code nice and clean Note After checking out the original implementation, I found out that the authors are using a Conv2d layer instead of a Linear one for performance gain. This is obtained by using a kernel_size and stride equal to the `patch_size`. Intuitively, the convolution operation is applied to each patch individually. So, we have to first apply the conv layer and then flat the resulting images. Next step is to add the cls token and the position embedding. The cls token is just a number placed in from of each sequence (of projected patches) cls_token is a torch Parameter randomly initialized, in the forward the method it is copied b (batch) times and prepended before the projected patches using torch.cat So far, the model has no idea about the original position of the patches. We need to pass this spatial information. This can be done in different ways, in ViT we let the model learn it. The position embedding is just a tensor of shape N_PATCHES + 1 (token), EMBED_SIZE that is added to the projected patches. We added the position embedding in the .positions field and sum it to the patches in the .forward function Now we need the implement Transformer. In ViT only the Encoder is used, the architecture is visualized in the following picture. Let’s start with the Attention part So, the attention takes three inputs, the famous queries, keys, and values, and computes the attention matrix using queries and values and use it to “attend” to the values. In this case, we are using multi-head attention meaning that the computation is split across n heads with smaller input size. We can use nn.MultiHadAttention from PyTorch or implement our own. For completeness I will show how it looks like: So, step by step. We have 4 fully connected layers, one for queries, keys, values, and a final one dropout. Okay, the idea (really go and read The Illustrated Transformer ) is to use the product between the queries and the keys to knowing “how much” each element is the sequence in important with the rest. Then, we use this information to scale the values. The forward method takes as input the queries, keys, and values from the previous layer and projects them using the three linear layers. Since we implementing multi heads attention, we have to rearrange the result in multiple heads. This is done by using rearrange from einops. Queries, Keys and Values are always the same, so for simplicity, I have only one input ( x). The resulting keys, queries, and values have a shape of BATCH, HEADS, SEQUENCE_LEN, EMBEDDING_SIZE. To compute the attention matrix we first have to perform matrix multiplication between queries and keys, a.k.a sum up over the last axis. This can be easily done using torch.einsum The resulting vector has the shape BATCH, HEADS, QUERY_LEN, KEY_LEN. Then the attention is finally the softmax of the resulting vector divided by a scaling factor based on the size of the embedding. Lastly, we use the attention to scale the values and we obtain a vector of size BATCH HEADS VALUES_LEN, EMBEDDING_SIZE. We concat the heads together and we finally return the results. Note we can use a single matrix to compute in one shot queries, keys and values. The transformer block has residuals connection We can create a nice wrapper to perform the residual addition, it will be handy later on The attention’s output is passed to a fully connected layer composed of two layers that upsample by a factor of expansion the input Just a quick side note. I don’t know why but I’ve never seen people subclassing nn.Sequential to avoid writing the forward method. Start doing it, this is how object programming works! Finally, we can create the Transformer Encoder Block ResidualAdd allows us to define this block in an elegant way Let’s test it you can also PyTorch build-in multi-head attention but it will expect 3 inputs: queries, keys, and values. You can subclass it and pass the same input. In ViT only the Encoder part of the original transformer is used. Easily, the encoder is L blocks of TransformerBlock. Easy peasy! The last layer is a normal fully connect that gives the class probability. It first performs a basic mean over the whole sequence. We can compose PatchEmbedding, TransformerEncoder and ClassificationHead to create the final ViT architecture. We can use torchsummary to check the number of parameters et voilà I checked the parameters with other implementations and they are the same! In this article, we have seen how to implement ViT in a nice, scalable, and customizable way. I hope it was useful. By the way, I am working on a new computer vision library called glasses, check it out if you like Take care :) Francesco",477,7,6,Towards Data Science,2021-01-01,2021
https://towardsdatascience.com/the-ultimate-guide-to-acing-coding-interviews-for-data-scientists-d45c99d6bddc,The Ultimate Guide to Acing Coding Interviews for Data Scientists,,12,43,"['The Ultimate Guide to Acing Coding Interviews for Data Scientists', 'Introduction', 'Table of Contents', 'Why are Coding Questions Asked in DS Interviews?', 'Roles That are Likely to Have Coding Interviews', 'When to Expect a Coding Interview?', 'Different Categories of Coding Interviews', 'How to Prepare?', 'How You Are Evaluated?', 'Tips to Ace Coding Interviews', 'Final Thoughts', 'Thanks for Reading!']","Written by Emma Ding and Rob Wang Data science (DS) is a relatively new profession compared to other types of roles in the tech industry, such as software engineering and product management. Initially, DS interviews had a limited coding component, including only SQL or applied data manipulation sessions using Python or R. In recent years, however, DS interviews have shown an increased emphasis on computer science (CS) fundamentals (data structures, algorithms, and programming best practices). For someone looking to enter the data science profession, this trend towards more CS in interviews can be daunting. In this post, we hope to increase your understanding of the coding interview and teach you how to prepare for it. We will categorize different coding questions and provide tips to crack them so that you can have a stellar interview. You can reach out to us here if you think we might be able to make your journey easier in any way! Before you start reading, if you are a video person, feel free to check out this YouTube video for an abbreviated version of this post. What exactly is a coding interview? We use the phrase “coding interview” to refer to any technical session that involves coding in any programming language other than a query language like SQL. In today’s market, you can expect a coding interview with just about any data science job. Why? Coding is an essential part of your DS careers. Here are three reasons: To sum up, strong coding skills are necessary to perform well in many data science positions. If you cannot show that you possess those skills in the coding interview, you will not get the job. Of course, the level of coding required does differ depending on the position. Check this YouTube video if you’re interested in learning the differences between various DS roles. If you are looking for a data scientist role that falls into any of the categories below, the chances of encountering a coding interview are very high: In contrast, if you are interviewing for a DS role with a Product Analytics emphasis, there is a lower likelihood of encountering coding questions. Interviews for these roles do not often go beyond evaluating SQL proficiency, but general programming may still be tested from time to time. Candidates who do not possess a basic level of coding knowledge can be easily caught off guard during the interview and may fail to move forward in the process. Do not let that be you! Make sure you are prepared. You can start your preparation by learning what to expect with a coding interview. A coding interview can appear during the technical phone screen (TPS), onsite, or both. There could even be multiple rounds of coding interviews during the onsite portion, depending on the coding proficiency expected. In general, you should expect coding interviews in at least one stage of an overall DS interview loop. During the TPS, the delivery of the coding interview will typically be through online integrated development environments (IDEs) such as CoderPad, HackerRank, and CodeSignal. During onsite sessions, either an online IDE or a whiteboard can be used. In the current remote interview environment, the former is used by default. The length of a coding session ranges from 45 minutes to 1 hour and it usually involves one or more questions. The choice of language is typically flexible, but most candidates will choose Python for its simplicity. Based on our experiences interviewing with dozens of large and medium-sized companies, such as Airbnb, Amazon, Facebook, Intuit, Lyft, Robinhood, Slack, Snapchat, Square, Stitch Fix, Twitter, Upstart, and more, we have categorized coding questions into the following four types. This type of question aims at evaluating candidates’ proficiency in introductory CS fundamentals. These fundamental topics can include, but are not limited to: Some additional topics such as Linked Lists and Graphs (Depth First Search or Breadth-First Search) are less likely to occur during this type of interview. Typically, multiple questions will be asked about a single scenario, ranging from simple to hard. Each question may cover a unique data structure or algorithm. Here is an example of a classic problem that revolves around finding the median of a list of numbers: This type of question may also appear as an applied business problem. For such questions, the candidate is expected to code up a solution to a hypothetical applied problem, which is usually related to the company’s business model. These questions are easy to medium in the level of difficulty (based on the categorization of Leetcode). The key here is to understand the business scenario and exact requirements before coding. These questions will require undergraduate-level mathematics and statistics knowledge in addition to coding capability. A few most commonly asked concepts include: Some common questions include: This type of question involves coding up a basic ML algorithm from scratch. Besides general coding ability, the interviewer will also be able to evaluate candidates’ applied machine learning knowledge. You will need to be familiar with common families of machine learning models to answer these questions. Here is a list of the most common model families that appear frequently during coding interviews: Other model families, such as Support Vector Machines, Gradient Boosting Trees, and Naive Bayes are less likely to occur. You also are not likely to be required to code up a deep learning algorithm from scratch. This type of question is not as common as the other types. They ask candidates to carry out data processing and transformations without using SQL or any data analysis library such as pandas. Instead, candidates are only allowed to use a programming language of choice to solve the problems. Some common examples include: Knowing that you can expect these four types of questions will help you to prepare systematically. In the next section, we will share some tips on how exactly to do that. This list of types of questions may appear daunting at the first glance, but don’t be discouraged or overwhelmed! If you have a good grasp of basic CS knowledge and machine learning algorithms, and you take the time to prepare (which we will show you how to do in this section), then you will be able to ace the coding interview. To prepare for different categories of coding questions, we recommend the following strategies: For each of the four major question themes outlined above, begin by reviewing the fundamentals. These descriptions can be found in various online sources as well as books. Specifically: Once you feel relatively at home with the basics, expand the scope of your review to include a larger set of commonly encountered problems. You can find these on Leetcode, GeeksForGeeks, and GlassDoor. You can save the problem statements in an organized manner, ideally grouped by theme using tools such as Notion or Jupyter notebooks. For each of the topics, practice a lot of easy questions and a few medium ones. Taking the time to create a categorized collection of coding problems will not only benefit your current job search, but it will also prove helpful for future job searches. Relying on rote memorization will not be sufficient for acing the interview. To achieve a more comprehensive understanding, we recommend coming up with multiple solutions to the same problem and comparing the strengths and weaknesses (e.g. run-time/storage complexities) of the different approaches. To reinforce understanding, explain your solutions/approaches to a non-technical person using plain English. A higher-level understanding of the common problem approaches often has greater value than detailed implementation and can be especially helpful for adapting existing knowledge to new and unfamiliar settings. Work with a peer to do a mock interview, or conduct it by yourself. You can use an online coding platform, such as Leetcode, to solve real interview questions in a limited time window. Employ these preparation techniques, and you will go into your interview not only with more knowledge but also with more confidence! There are 4 major qualities you want to convey during your interview. The interviewer wishes to see candidates make logical connections between the information provided and the ultimate answer. You should therefore describe clearly what is needed for the computation and how you would write the code to solve the problem, before diving into the actual coding. The effectiveness of your communication matters significantly. Before coding, clearly communicate your thought process. If the interviewer asks questions at any point during the interview, you need to be able to explain the reasoning of your assumptions and choices. The interviewer will also evaluate your overall code quality. While the standard expectations in a DS interview would not be as high as those in a software engineering interview, candidates should still focus on several aspects: Just as with software engineering coding interviews, for DS coding interviews, it is reasonable to expect multi-part questions and sometimes multiple questions. In other words, speed is also important. Being able to solve more questions within a limited amount of time is a signal of overall proficiency. Before the interview, it is worth clarifying with recruiters what kinds of coding questions will be asked, as well as the approximate difficulty level. Lots of data science interviews do not require heavy programming, but that does not mean interviewers will not expect basic coding proficiency at your fingertips. Always ask your recruiter what to expect. If you make incorrect assumptions on the types of questions that can appear during interviews, you may end up preparing inadequately. During the interview, use these tips to answer coding questions effectively. Coding interviews, like other technical interviews, require systematic and effective preparation. Hopefully, our article has given you some insights into both what to expect in a coding interview for DS related positions and how to prepare for them. Remember: Enhancing your coding skills will be extremely rewarding not only for landing your dream job, but also for excelling in the job! If you like this post and want to support me… pub.towardsai.net towardsdatascience.com",,0,11,Towards Data Science,2021-01-01,2021
https://towardsdatascience.com/customer-segmentation-in-online-retail-1fc707a6f9e6,Customer Segmentation in Online Retail,A detailed step-by-step explanation on performing Customer…,15,91,"['Customer Segmentation in Online Retail', 'Understanding Customer Segmentation', 'Why segment your customers?', 'How to segment your customers?', 'Getting started', 'Data Snapshot', 'Data Attributes', 'Exploring the data', 'Understanding Cohort Analysis', 'Diving into Cohort Analysis', 'RFM Segmentation', 'Preprocessing data for K-means clustering', 'Clustering with K-means', 'Final Thoughts', 'Further analysis']","In this article, I am going to write about how to carry out customer segmentation and other related analysis on online retail data using python. This is going to get a bit long, so feel free to go through some sections at a time and come back again. Before going into the definition of customer segmentation, let us take a look at how online retail works and how the associated data would look like. When a person goes into a retail store and purchases a few items, the following basic data points should be generated: Now that we have developed a basic idea about how retail data looks like, let us think about how a company should think in order to make effective marketing policies. For a small company, the customer base is usually quite small and individually targetable. But, as a business grows in size, it will not be possible for the business to have an intuition about each and every customer. At such a stage, human judgments about which customers to pursue will not work and the business will have to use a data-driven approach to build a proper strategy. For a medium to large size retail store, it is also imperative that they invest not only in acquiring new customers but also in customer retention. Many businesses get most of their revenue from their ‘best’ or high-valued customers. Since the resources that a company has, are limited, it is crucial to find these customers and target them. It is equally important to find the customers who are dormant/are at high risk of churning to address their concerns. For this purpose, companies use the technique of customer segmentation. One axiom frequently used in business and economics is the Pareto principle. This can be applied to understanding the revenue stream of a company as well. As per the Pareto Principle, 80% of outcomes result from 20% of all the causes of any given event. In business terms, we can say that 20% of customers contribute 80% share of the total revenue of a company. That’s why finding this set of people is important. I will explain the importance of customer segmentation in a detailed manner later in this article itself. Let us now try to understand what customer segmentation is and why is it such an effective tool for developing an effective strategy. Then, we will work on how to perform segmentation. Customer segmentation is the process of separating customers into groups on the basis of their shared behavior or other attributes. The groups should be homogeneous within themselves and should also be heterogeneous to each other. The overall aim of this process is to identify high-value customer base i.e. customers that have the highest growth potential or are the most profitable. Insights from customer segmentation are used to develop tailor-made marketing campaigns and for designing overall marketing strategy and planning. A key consideration for a company would be whether or not to segment its customers and how to do the process of segmentation. This would depend upon the company philosophy and the type of product or services it offers. The type of segmentation criterion followed would create a big difference in the way the business operates and formulates its strategy. This is elucidated below. Once the company has identified its customer base and the number of segments it aims to focus upon, it needs to decide the factors on whose basis it will decide to segment its customers. Factors for segmentation for a business to business marketing company: Factors for segmentation for a business to consumer marketing company: Customer segmentation has a lot of potential benefits. It helps a company to develop an effective strategy for targeting its customers. This has a direct impact on the entire product development cycle, the budget management practices, and the plan for delivering targeted promotional content to customers. For example, a company can make a high-end product, a budget product, or a cheap alternative product, depending upon whether the product is intended for its most high yield customers, frequent purchasers or for the low-value customer segment. It may also fine-tune the features of the product for fulfilling the specific needs of its customers. Customer segmentation can also help a company to understand how its customers are alike, what is important to them, and what is not. Often such information can be used to develop personalized relevant content for different customer bases. Many studies have found that customers appreciate such individual attention and are more likely to respond and buy the product. They also come to respect the brand and feel connected with it. This is likely to give the company a big advantage over its competitors. In a world where everyone has hundreds of emails, push notifications, messages, and ads dropping into their content stream, no one has time for irrelevant content. Finally, this technique can also be used by companies to test the pricing of their different products, improve customer service, and upsell and cross-sell other products or services. To start with customer segmentation, a company needs to have a clear vision and a goal in mind. The following steps can be undertaken to find segments in the customer base on a broad level. In the following analysis, I am going to use the Online Retail Data Set, which was obtained from the UCI Machine Learning repository. The data contains information about transnational transactions for a UK-based and registered non-store online retail. The link to the data can be found here. Before diving into insights from the data, duplicate entries were removed from the data. The data contained 5268 duplicate entries (about ~1%). Let us now look at the total number of products, transactions, and customers in the data, which correspond to the total unique stock codes, invoice number, and customer IDs present in the data. Thus, for 4070 products, there are 25900 transactions in the data. This means that each product is likely to have multiple transactions in the data. There are almost as many products as customers in the data as well. Since the data, taken from the UCI Machine Learning repository describes the data to based on transactions for a UK-based and registered non-store online retail, let us check the percentage of orders from each country in the data. The above graph shows the percentage of orders from the top 10 countries, sorted by the number of orders. This shows that more than 90% of orders are coming from United Kingdom and no other country even makes up 3% of the orders in the data. Therefore, for the purpose of this analysis, I will be taking data corresponding to orders from the United Kingdom. This subset will be made in one of the next steps and will be mentioned as required. Let us now look at the number of canceled orders in the data. As per the data, if the invoice number code starts with the letter ‘c’, it indicates a canceled order. A flag column was created to indicate whether the order corresponds to a canceled order. All the canceled orders contain negative quantities (since it is a cancellation) and hence were removed from the data. Finally, I ran a check to confirm whether there were any orders with negative quantities in the orders that were not canceled. There were 1336 such cases. As we can see from the above figure, these cases are the ones where CustomerID values are NaNs. These cases were also removed from the data. Now, the data was filtered to contain orders only from the United Kingdom and finally, the structure of the data was checked by calling the .info() method: There were no nulls in any of the columns in the data, and there were a total of 349227 rows in the data. Let us now check the number of products, transactions, and customers in our cleaned data: Let us now try to understand cohort analysis so that we can perform it on our data. But, what is a Cohort? A cohort is a set of users who share similar characteristics over time. Cohort analysis groups the users into mutually exclusive groups and their behavior is measured over time. It can provide information about the product and customer lifecycle. There are three types of cohort analysis: Understanding the needs of various cohorts can help a company design custom-made services or products for particular segments. In the following analysis, we will create Time cohorts and look at customers who remain active during particular cohorts over a period of time that they transact over. Checking the date range of our data, we find that it ranges from the start date: 2010–12–01 to the end date: 2011–12–09. Next, a column called InvoiceMonth was created to indicate the month of the transaction by taking the first date of the month of InvoiceDate for each transaction. Then, information about the first month of the transaction was extracted, grouped by the CustomerID. Next, we need to find the difference between the InvoiceMonth and the CohortMonth column in terms of the number of months. The following code was used: After obtaining the above information, we obtain the cohort analysis matrix by grouping the data by CohortMonth and CohortIndex and aggregating on the CustomerID column by applying the pd.Series.nunique function. Here are the cohort counts obtained: What does the above table tell us? Consider CohortMonth 2010–12–01: For CohortIndex 0, this tells us that 815 unique customers made transactions during CohortMonth 2010–12–01. For CohortIndex 1, this tells that there are 289 customers out of 815 who made their first transaction during CohortMonth 2010–12–01 and they also made transactions during the next month. That is, they remained active. For CohortIndex 2, this tells that there are 263 customers out of 815 who made their first transaction during CohortMonth 2010–12–01 and they also made transactions during the second-next month. And so on for higher CohortIndices. Let us now calculate the Retention Rate. It is defined as the percentage of active customers out of total customers. Since the number of active customers in each cohort corresponds to the CohortIndex 0 values, we take the first column of the data as the cohort sizes. From the above retention rate heatmap, we can see that there is an average retention of ~35% for the CohortMonth 2010–12–01, with the highest retention rate occurring after 11 months (50%). For all the other CohortMonths, the average retention rates are around 18–25%. Only this percentage of users are making transactions again in the given CohortIndex ranges. From this analysis, a company can understand and create strategies to increase customer retention by providing more attractive discounts or by doing more effective marketing, etc. RFM stands for Recency, Frequency, and Monetary. RFM analysis is a commonly used technique to generate and assign a score to each customer based on how recent their last transaction was (Recency), how many transactions they have made in the last year (Frequency), and what the monetary value of their transaction was (Monetary). RFM analysis helps to answer the following questions: Who was our most recent customer? How many times has he purchased items from our shop? And what is the total value of his trade? All this information can be critical to understanding how good or bad a customer is to the company. After getting the RFM values, a common practice is to create ‘quartiles’ on each of the metrics and assigning the required order. For example, suppose that we divide each metric into 4 cuts. For the recency metric, the highest value, 4, will be assigned to the customers with the least recency value (since they are the most recent customers). For the frequency and monetary metric, the highest value, 4, will be assigned to the customers with the Top 25% frequency and monetary values, respectively. After dividing the metrics into quartiles, we can collate the metrics into a single column (like a string of characters {like ‘213’}) to create classes of RFM values for our customers. We can divide the RFM metrics into lesser or more cuts depending on our requirements. Let’s get down to RFM analysis on our data now. Firstly, we need to create a column to get the monetary value of each transaction. This can be done by multiplying the UnitValue column with the Quantity column. Let’s call this the TotalSum. Calling the .describe() method on this column, we get: This gives us an idea of how consumer spending is distributed in our data. We can see that the mean value is 20.86 and the standard deviation is 328.40. But the maximum value is 168,469. This is a very large value. Therefore, the TotalSum values in the Top 25% of our data increase very rapidly from 17.85 to 168,469. Now, for RFM analysis, we need to define a ‘snapshot date’, which is the day on which we are conducting this analysis. Here, I have taken the snapshot date as the highest date in the data + 1 (The next day after the date till which the data was updated). This is equal to the date 2011–12–10. (YYYY-MM-DD) Next, we confine the data to a period of one year to limit the recency value to a maximum of 365 and aggregate the data on a customer level and calculate the RFM metrics for each customer. As the next step, we create quartiles on this data as described above and collate these scores into an RFM_Segment column. The RFM_Score is calculated by summing up the RFM quartile metrics. We are now in a position to analyze our results. The RFM_Score values will range from 3 (1+1+1) to 12 (4+4+4). So, we can group by the RFM scores and check the mean values of recency, frequency, and monetary corresponding to each score. As expected, customers with the lowest RFM scores have the highest recency value and the lowest frequency and monetary value, and the vice-versa is true as well. Finally, we can create segments within this score range of RFM_Score 3–12, by manually creating categories in our data: Customers with an RFM_Score greater than or equal to 9 can be put in the ‘Top’ category. Similarly, customers with an RFM_Score between 5 to 9 can be put in the ‘Middle’ category, and the rest can be put in the ‘Low’ category. Let us call our categories the ‘General_Segment’. Analyzing the mean values of recency, frequency, and monetary, we get: Note that we had to create the logic for distributing customers into the ‘Top’, ‘Middle’, and ‘Low’ category manually. In many scenarios, this would be okay. But, if we want to properly find out segments on our RFM values, we can use a clustering algorithm like K-means. In the next section, we are going to preprocess the data for K-means clustering. K-means is a well-known clustering algorithm that is frequently used for unsupervised learning tasks. I am not going into details regarding how the algorithm works here, as there are plenty of resources online. For our purpose, we need to understand that the algorithm makes certain assumptions about the data. Therefore, we need to preprocess the data so that it can meet the key assumptions of the algorithm, which are: Let us check the first assumption by building histograms of Recency, Frequency, and MonetaryValue variables using the seaborn library: From the above figure, all the variables do not have a symmetrical distribution. All of them are skewed to the right. To remove the skewness, we can try the following transformations: 1. Log transformations2. Box-Cox transformations3. Cube root transformations I will use the Log transformation here. Since the log transformation cannot be used for negative values, we need to remove them, if they exist. One common practice one can use here is to add a constant value to get a positive value and this is generally taken as the absolute of the least negative value of the variable to each observation. However, in our data, we do not have any negative values since we are dealing with customer transactions dataset. Checking the distribution of the recency, frequency, and monetary variables, we get this by called the .describe() method: From the above description, we can see that the minimum MonetaryValue for a particular customerID is 0. This transaction therefore does not make any sense and needs to be removed. Checking the occurrence: This customer was removed from the data. We also see that we do not get a constant mean and standard deviation values. To check that, we will standardize the data. Applying the log transformation on the data first and passing it through the StandardScaler() method from the sklearn library, we obtained the preprocessed data. Checking the distribution of RFM variables for symmetrical distribution now: As we can see from the above plots, skewness has been removed from the data. In this section, we will build multiple clusters upon our normalized RFM data and will try to find out the optimal number of clusters in our data using the elbow method. Attached below is the code for this purpose. For each cluster, I have also extracted information about the average of the intracluster sum of squares through which we can build the elbow plot to find the desired number of clusters in our data. One can also use silhouette analysis to find the optimal number of clusters. You can read more about it in my previous article here. For the purpose of this analysis, I have only used the elbow plot method. From the above plot, we can see that the optimal number of clusters is 3 or 4. Let us now compare the clustering performance. For this purpose, I calculated the mean values of recency, frequency, and monetary metrics to get the following result: From the above table, we can compare the distribution of mean values of recency, frequency, and monetary metrics across 3 and 4 cluster data. It seems that we get a more detailed distribution of our customer base using k=4. However, this may not be a very visually appealing method to extract insights. Another commonly used method to compare the cluster segments is Snakeplots. They are commonly used in marketing research to understand customer perceptions. Let us build a snake plot for our data with 4 clusters below. Before building snake plots, we need to melt the data into along format so RFM values and metric names are stored in 1 column each. Link to understanding the pd.melt method: link. From the above snake plot, we can see the distribution of recency, frequency, and monetary metric values across the four clusters. The four clusters seem to be separate from each other, which indicates a good heterogeneous mix of clusters. As the final step in this analysis, we can extract this information now for each customer that can be used to map the customer with thei relative importance by the company: From the above analysis, we can see that there should be 4 clusters in our data. To understand what these 4 clusters mean in a business scenario, we should look back the table comparing the clustering performance of 3 and 4 clusters for the mean values of recency, frequency, and monetary metric. On this basis, let us label the clusters as ‘New customers’, ‘Lost customers’, ‘Best customers’, and ‘At risk customers’. Below is the table giving the RFM interpretation of each segment and the points that a company is recommended to keep in mind while designing the marketing strategy for that segment of customers. If you liked the article and found it informative, please share it to spread knowledge :). Thank you! Link to the Github repository for this project: link [1] https://www.shopify.in/encyclopedia/customer-segmentation [2] https://learn.datacamp.com/courses/customer-segmentation-in-python [3] https://looker.com/blog/creating-actionable-customer-segmentation-models [4] https://www.business2community.com/customer-experience/4-types-of-customer-segmentation-all-marketers-should-know-02120397 [5] https://www.intercom.com/blog/customer-segmentation/",236,1,19,Towards Data Science,2021-01-01,2021
https://towardsdatascience.com/7-most-recommended-data-science-skills-to-learn-in-2021-ac26933f0e8a,7 Most Recommended Skills to Learn in 2021 to be a Data Scientist,Recommended by some of the largest…,11,34,"['7 Most Recommended Skills to Learn in 2021 to be a Data Scientist', 'Introduction', '1) SQL', '2) Data Visualizations & Storytelling', '3) Python', '4) Pandas', '5) Git/Version Control', '6) Docker', '7) Airflow', 'Thanks for Reading!', 'Terence Shin']","Happy New Year! To kick off 2021, I wanted to share the seven most recommended data science skills from dozens of interactions and discussions with some of the largest data leaders in the world, including the Head of Data & Analytics @ Google, the Senior Director of Engineering @ NVIDIA, and the VP of Data Science and Engineering @ Wealthsimple. While this article may be more anecdotal, I feel like this article shares a valuable perspective. I’m specifically not referring to data from scraped job postings because from my experiences, there seems to be quite a disconnect between job descriptions and what’s actually done on the job. You might notice that none of the seven skills have anything to do with machine learning or deep learning, and this is not a mistake. Currently, there is a much higher demand for skills that are used in the pre-modeling phases and post-modeling phases. And so, the seven most recommended skills to learn actually overlap with the skills of a data analyst, a software engineer, and a data engineer. I wrote an article specifically on why you shouldn’t learn machine learning first — you can check it out below: towardsdatascience.com With that said, let’s dive into the seven most recommended data science skills to learn in 2021: SQL is the universal language in the world of data. Whether you’re a data scientist, a data engineer, or a data analyst, you’ll need to know SQL. SQL is used to extract data from a database, manipulate data, and create data pipelines — essentially, it’s important for almost every pre-analysis/pre-modeling stage in the data lifecycle. Developing strong SQL skills will allow you to take your analyses, visualizations, and modeling to the next level because you will be able to extract and manipulate the data in advanced ways. Also, writing efficient and scalable queries is becoming more and more important for companies that work with petabytes of data. If you think creating data visualizations and storytelling are specific to the role of a data analyst, think again. Data visualizations simply refer to data that is presented visually — it can be in the form of graphs, but it can also be presented in unconventional ways. Data storytelling takes data visualizations to the next level — data storytelling refers to “how” you communicate your insights. Think of it like a picture book. A good picture book has good visuals, but it also has an engaging and powerful narrative that connects the visuals. Developing your data visualization and storytelling skills are essential because you’re always selling your ideas and your models as a data scientist. And it’s especially important when communicating with others who are not as technologically savvy. From my interactions, Python seems to be the go-to programming language to learn over R. That doesn’t mean that you can’t be a data scientist if you use R, but it just means that you’ll be working in a language that is different from what the majority of people use. Learning Python syntax is easy, but you should be able to write efficient scripts and leverage the wide-range of libraries and packages that Python has to offer. Python programming is a building block for applications like manipulating data, building machine learning models, writing DAG files, and more… Arguably the most important library to know in Python is Pandas, which a package for data manipulation and analysis. As a data scientist, you’ll be using this package all the time, whether you’re cleaning data, exploring data, or manipulating the data. Pandas has become such a prevalent package, not only because of it’s functionality, but also because DataFrames have become a standard data structure for machine learning models. Git is the main version control system that is used in the tech community. If that doesn’t make sense, consider this example. In high school or university, if you ever had to write an essay, you might have saved different versions of your essay as you progressed through it. For example: 📂Final Essay └📁Essay_v1  └📁Essay_v2 └📁Essay_final └📁Essay_finalfinal └📁Essay_OFFICIALFINAL All jokes aside, Git is a tool that serves the same purpose, except that it’s a distributed system. This means that files (or repositories) are stored both locally and in a central server. Git is extremely important for several reasons, with a few being that: Docker is a containerization platform that allows you to deploy and run applications, like machine learning models. It’s becoming increasingly important that data scientists not only know how to build models but how to deploy them as well. In fact, a lot of job postings are now requiring some experience in model deployment. The reason that it’s so important to learn how to deploy models is that a model delivers no business value until it is actually integrated with the process/product that it is associated with. Airflow is a workflow management tool that allows you to automate… well workflows. More specifically Airflow allows you to create automated workflows for data pipelines and machine learning pipelines. Airflow is powerful because it allows you productionalize tables that you may want to use for further analysis or modeling, and it’s also a tool that you can use to deploy machine learning models. I hope that this helps guide your learnings and gives you some direction for the new year. This is a lot to learn so I would definitely choose a couple of skills that sound most interesting to you and go from there. Do keep in mind that this more of an opinionated article that is backed by anecdotal experience, so take what you want from this article. But as always, I wish you the best in your learning endeavors! Not sure what to read next? I’ve picked another article for you: towardsdatascience.com and another! towardsdatascience.com",1000,10,6,Towards Data Science,2021-01-01,2021
https://towardsdatascience.com/understanding-the-confusion-matrix-from-scikit-learn-c51d88929c79,Understanding the Confusion Matrix from Scikit learn,Clear representation of output of confusion…,5,42,"['Understanding the Confusion Matrix from Scikit learn', 'INTRODUCTION', 'UNDERSTANDING THE STRUCTURE OF CONFUSION MATRIX', 'CONCLUSION:', 'Thanks for reading!']","In one of my recent projects — a transaction monitoring system generates a lot of False Positive alerts (these alerts are then manually investigated by the investigation team). We were required to use machine learning to auto close those false alerts. Evaluation criteria for the machine learning model was a metric Negative Predicted Value that means out of total negative predictions by the model how many cases it has identified correctly. NPV = True Negative / (True Negative + False Negative) The cost of false-negative is extremely high because these are the cases where our model is saying they are not-fraudulent but in reality, they are fraudulent transactions. To get into action I would quickly display the confusion_matrix and below is the output from the jupyter notebook. My binary classification model is built with target = 1 (for fraud transactions) so target= 0 (for non fraud). Depending upon how you interpret the confusion matrix, you can either get an NPV of 90% or 76%. Because — TN = cm[0][0] or cm[1][1] ie. 230 or 74 FN = cm[1][0] ie. 24 I referred to confusion matrix representation from Wikipedia. This image from Wikipedia shows that predicted labels are on the horizontal levels and actual labels are on the verticals levels. This implies, TN = cm[1][1] ie. 76 FN = cm[1][0] ie. 24 NPV = 76% Scikit learn documentation says — Wikipedia and other references may use a different convention for axes. Oh Wait! documentation doesn’t mention anything clear, isn’t it? They say Wikipedia and other references may use a different convention for axes. What do you mean by “may use a different convention for axes”? We have seen that if you use the wrong convention for axes your model evaluation metric may completely go off the track. If you read through the documentation and towards the bottom you will find this example Here, they have flattened the matrix output. On our example this implies that, TN = cm[0][0] ie. 230 FN = cm[1][0] ie. 24 NPV = 90% Clearly understanding the structure of the confusion matrix is of utmost importance. Even though you can directly use the formula for most of the standard metrics like accuracy, precision, recall, etc. Many times you are required to compute the metrics like negative predictive value, false-positive rate, false-negative rate which are not available in the package out of the box. Now, if I ask you to pick the correct option for the confusion matrix that is the output of confusion_matrix. Which one would you pick? Would your answer be “A” because that’s what Wikipedia says or would it be “C” because sklearn documentation says so? Consider these are your y_true and y_pred values. By looking at the given lists, we can calculate the following: TP (True Positive) = 1 FP (False Positive) = 4 TN (True Negative) = 0 FN (False Negative) = 2 For your classic Machine Learning Model for binary classification, mostly you would run the following code to get the confusion matrix. If we fill it back to the confusion matrix, we get the confusion matrix as below TN (True Negative) = cm[0][0] = 0 FN (False Negative) = cm[1][0] = 2 TP (True Positive) = cm[1][1] = 1 FP (False Positive) = cm[0][1] = 4 However, if you were to add a simple parameter “labels”. TP (True Positive) = cm[0][0] = 1 FP (False Positive) = cm[1][0] = 4 TN (True Negative) = cm[1][1] = 0 FN (False Negative) = cm[0][1] = 2 The correct representation of the default output of the confusion matrix from sklearn is below. Actual labels on the horizontal axes and Predicted labels on the vertical axes. 2. By adding the labels parameter, you can get the following output",359,6,5,Towards Data Science,2021-01-01,2021
https://towardsdatascience.com/what-is-p-value-370056b8244d,What is p-value?,Detailed explanation of p-value,9,87,"['What is the p-value?', 'Santa Claus’s Cookie Shop', 'Can We Believe in Santa’s Words?', 'Core Concept of Inferential Statistics', 'Sampling Distribution Review', 'Testing Hypothesis Statements', 'Recap', 'Icon Attribution', 'Reference']","If you google “what is p-value”, the first result shown on the page is the definition from Wikipedia: “ The p-value is the probability of obtaining test results at least as extreme as the results actually observed, under the assumption that the null hypothesis is correct.” — Wikipedia Hmm… the good thing is we know this definition is correct; the bad thing is this definition is too correct to understand. So…today’s game is trying to break down this sentence! Since now is the holiday season! (Hooray~), let’s invite Gingerbread Man to join us for fun! Santa Claus’s cookie shop is selling their famous product — gingerbread cookie! Santa is very proud of his cookies. He believes his product is the most delicious one in the world. Also, Santa said that the average weight (μ) of each product (a bag of gingerbread cookies) is 500g. Now is the 21st century. Santa already has his own factories and automated machines to help him make cookies. As you know, there’s no perfect machine and production process, so there is a variance between different bags of cookies. Assume that we know the bags of cookie is normally distributed with a standard deviation (σ) equals 30g. So, if Satna’s claim is true (the average weight of one bag of cookies = 500g), we could expect the distribution of one bag of cookies looks like below: But, as a curious customer who really loves gingerbread cookies, I am wondering…does the average weight of a bag of cookies really equal 500g? What if Santa deceives customers and gives us less than 500g cookies? How do we validate Santa’s words? Here, is where “hypothesis testing” comes in. To implement hypothesis testing, firstly, let’s set up our null hypothesis (H0) and the alternative hypothesis (H1). As a reasonable person, we should not suspect others without having any evidence. So, we assume Santa is honest about his business (H0). If we want to check whether his cookies is less than 500g, we need to collect data and have enough evidence to support our guess (H1). So…we have the hypothesis statement set up as follows: H0: Average weight of one bag of cookies (μ) = 500gH1: Average weight of one bag of cookies (μ) < 500g Since we are unsure about how our population distribution looks like, I use the dashed line to represent possible distributions. If Santa’s claim is true, we could expect one bag of cookies has a distribution with a mean weight equals to 500g (left picture). However, if Santa's claim is not true and the mean weight of cookies is less than 500g, the population distribution should look differently (any of right picture). Cool! The problem statement is set. So now, the next question is: how to test our hypothesis statement? Maybe just weigh all bags of cookies so that we could know the exact population distribution? Well…obviously, it is IMPOSSIBLE for us to collect ALL the cookies (population) produced from Santa Claus’s cookie shop!!! So…what should we do? Here, “inferential statistic” comes in handy! In inferential statistics, what we are interested in is the population parameters (attributes of the population). However, it is almost impossible to collect all the data of the whole population to calculate the parameters. As a result, we sampling from the whole population to get the sample data. Then, we calculate the statistic (attributes of the sample) from the sample data as our estimator to help us infer the unknown population parameters. (As the picture below) Examples of parameters and statistics: - parameters: population mean (μ), population standard deviation (σ) …- statistics: sample mean (x̄), sample standard deviation (s) … Testing our hypothesis statement is also an inferential statistic work. The process is the same as above. But now, we are not interested in a single unknown parameter; instead, we are interested in “whether we can reject the null hypothesis?”. How to answer this question? The same method is used — we calculate the statistics from our sample data for inferring the answer to this question. The statistics used here called Test Statistics. Great! So now, we know we should collect sample data and calculate the test statistic in order to test the hypothesis statement. But…let’s pause for a second. Before jumping into the testing part, let me quickly review the concept of sampling distribution to make sure we are on the same page. Sampling Distribution is the distribution of the sample statistic. Let’s take one of the statistics — sample mean (x̄) — as an example. If we sampling from the population many times, we could get many sample datasets (sample 1 to sample m). Then, if we calculate the sample mean (x̄) from each sample dataset, we could get m data points of the sample mean (x̄). Use these data points, we could draw a distribution of sample mean (x̄). Since this distribution is from the sample statistic, we called the distribution Sampling Distribution of sample mean (x̄). The same idea applies to other statistics. For example, if we calculate the test statistic from each sample dataset, we could get the sampling distribution of the test statistic. A sampling distribution is similar to all the other distributions, it shows how likely (probability) the statistic value might appear if we sampling from the population many times. I’ll use brown color to represent the sampling distribution curve in the following sections. Nice! Now, it’s time to jump into the testing part! The first thing we need to do is to have a sample dataset. So, I go to Santa Clause’s cookie shop and randomly pick up 25 bags of cookies (n) as our sample data. Also, I calculate the mean weight (x̄) of this sample is 485g. The first part of testing is to compare our sample statistic to the null hypothesis so that we can know how far away our sample statistic is from the expected value. To do so, we first assume the null hypothesis is true. What does this mean? This means, in our case, we assume the population distribution of one bag of cookies is really equals to 500g. And if this statement is true, according to Central Limit Theorem, we could have a sampling distribution of sample mean (x̄) looks like the below picture (mean value of the sample mean = 500g) if we sampling from this population many times. p-value definition:“ The p-value is the probability of obtaining test results at least as extreme as the results actually observed, under the assumption that the null hypothesis is correct.” — Wikipedia So now, if the null hypothesis is true, we could easily see that our sample mean is 15g below (485–500 = -15) the expected mean value (500g). Hmm… but “15g” is only a number, which is not very helpful for us to explain the meaning. Also, if we want to calculate the probability under the curve, it is inefficient to calculate it case by case (imagine there are numerous distributions, each of them has its own mean and standard deviation…you really don’t want to calculate the probability for many many times…) So, what should we do? We standardize our value so that the mean value of distribution always equals zero. The benefit of standardization is that statisticians already generate a table that includes the area under each standardized value. So that we don’t need to calculate the area case by case. All we need to do is to standardize our data. How to standardize? In our case, we use the z-score to transform our data. And z-score is the Test Statistic in our case. The below picture shows the sampling distribution of the test statistic (z-score). We can see that if our sample data exactly equal to the null hypothesis (the population mean =500g, the sample mean = 500g), we should have the test statistic equals to 0. In our case, our sample mean equals 485g, which gives us the test statistic equals to -2.5 (-2.5 is the test result we observed from our sample data). This indicates that our sample data has 2.5 standard errors below the expected value. p-value definition:“ The p-value is the probability of obtaining test results at least as extreme as the results actually observed, under the assumption that the null hypothesis is correct.” — Wikipedia Here, I want to mention that the test statistic is chosen based on different cases. You might hear different kinds of statistical tests, such as z-test, t-test, chi-square test…Why we need different kinds of tests? Because we might need to test different types of data (categorical? quantitative?), we might have different purpose of testing (testing for mean? proportion?), the data we have might have a different distribution, we might only have limited attributes of our data……Hence, how to choose a suitable testing method is another crucial work. In this case, since we are interested in testing the mean value, also, I assume our population data is normally distributed with known population standard deviation (σ). Based on our condition, we choose the z-test for this case. ** Please refer to the assumptions of different kinds of statistical test if you are interested in when to use each statistical test.** Okay, so now we have our test statistic. we know how far away our test statistic is from the expected value when the null hypothesis is true. Then, what we really want to know is: how likely (probability) we get this sample data if the null hypothesis is true? To answer this question, we need to calculate the probability. As you know, the probability between one point to the other point is the area under our sampling distribution curve between these two points. So here, we do not calculate the probability of a specific point; instead, we calculate the probability from our test statistic point to infinite — indicates the cumulative probability of all the points which farther away from our test statistic (also farther away from the expected test statistic). This cumulative probability is our p-value. You might wonder why we don’t calculate the probability of the specific test statistic (one point). Here are two possible explanations I found in this post: (1) Mathematically, the probability of a specific point on the probability curve is zero. To calculate the probability, we need to calculate the area under the curve. (2) To decide whether we should reject the null hypothesis, we use p-value compare to the significant level. since the significant level is the cumulative probability, we need to use the same format to compare two of them. Hence, the p-value should also be a cumulative probability. (I’ll introduce siginificant level later) p-value definition:“ The p-value is the probability of obtaining test results at least as extreme as the results actually observed, under the assumption that the null hypothesis is correct.” — Wikipedia Wonderful! We just explained all the parts of the p-value definition! Let’s calculate the p-value in our case. As I mentioned before, the best thing to use the test statistic is that statisticians already understand the attributes of the sampling distribution. So we could just look up the z-table, or use any statistical software to help us get the p-value. In our case, we have p-value equals 0.0062 (0.62%). Please note that we are doing the one-tail test in our case. That is, we only consider one direction for calculating “extreme” probability in the distribution. Since our alternative hypothesis (H1) is set up as “mean value less than 500g”, we only care about the value that less than our test statistics (left-hand side). We can identify which tail we should focus on based on our alternative hypothesis. If the alternative hypothesis includes:(1) interested attribute less than (<) expected value: focus on left-tailed(2) interested attribute greater than (>) expected value: focus on right-tailed(3) interested attribute not equal to (≠) expected value: focus on two-tailed Notes:interested attribute- mean, proportion…(In our case is mean value)expected value- specific number…(In our case is 500g) Now, we have p-value = 0.0062. Hmm… it is a small number…but what does this mean? This means, under the condition that our null hypothesis is true (population mean really equals 500g), if we sampling from this population distribution 1000 times, we will have 6.2 times chance to get this sample data (sample mean = 485g) or other samples with sample mean less than 485g. In other words, if we get sample data with a sample mean equals to 485g, there are two possible explanations: Or… 2. The assumption of the “null hypothesis is true” is incorrect. This sample data (sample mean equals 485g) actually comes from other population distribution where the sample mean = 485g more likely to happen. Cool! So now we know that if our p-value is very small, that means either we get a very rare sample data or our assumption (null hypothesis is true) is incorrect. Then, the next question is: we only have the p-value now, but how to use it to judge when to reject the null hypothesis? In other words, how small the p-value is, we are willing to say that this sample comes from another population? Here, let’s introduce the judgment standard — significant level (α). The significant level is a pre-defined value that needs to be set before implementing the hypothesis testing. You can seem significant level as a threshold, which gives us a criterion of when to reject the null hypothesis. This criterion is set as below: if p-value ≤ significant level (α), we reject the null hypothesis (H0).if p-value > significant level (α), we fail to reject the null hypothesis (H0). Say, I set my significant level as 0.05. We can see the below picture, the red area is the significant level (In our case, it equals 0.05). We use the significant level as our criterion, if the p-value within (less than or equal to) the red area, we reject H0; if the p-value exceeds (greater than) the red area, we fail to reject H0. Here, I want to mention that the significant level (α) also indicates the maximum risk we are acceptable for type I error (type I error means we reject H0 when H0 is actually true). It is easy to see why through the below picture — The distribution curve below indicates the null hypothesis (H0) is true. And the red area is the probability we decide to reject the null hypothesis when it is true. If the p-value equals the significant level (our case is 0.05), then it would be the maximum probability we incorrectly reject H0 when H0 is true. In our case, we have p-value = 0.0062, which smaller than 0.05, as a result, we can reject our null hypothesis. In other words, based on our testing, we are sad to say that we have enough evidence to support our alternative hypothesis (the mean value of one bag of cookies is less than 500g). And that means… we have enough evidence to say that Santa cheats to us…. Well…what happens if we change the significant level to 0.005? The result will be different. Since 0.0062 > 0.005, we then fail to reject H0. So here is the tricky part, since the significant level is subjective, we need to determine it before the testing. Otherwise, we might very likely to cheat ourselves after knowing the p-value. Thank you for reading to this point. Let’s have a quick recap to close today’s game! Part 1: To test whether our sample data support the alternative hypothesis or not, we first assume the null hypothesis is true. So that we can know how far away our sample data from the expected value given by the null hypothesis. p-value definition:“ The p-value is the probability of obtaining test results at least as extreme as the results actually observed, under the assumption that the null hypothesis is correct.” — Wikipedia Part 2: Based on the distribution, data types, purpose, known attributes of our data, choose an appropriate test statistic. And calculate the test statistic of our sample data. (Test statistic shows how far away our sample data from the expected value) p-value definition:“ The p-value is the probability of obtaining test results at least as extreme as the results actually observed, under the assumption that the null hypothesis is correct.” — Wikipedia Part 3: Calculate the probability (area under the sampling distribution curve) from the test statistic point to infinite (indicates more extreme) at the direction represent your alternative hypothesis(left-tailed, right-tailed, two-tailed). p-value definition:“ The p-value is the probability of obtaining test results at least as extreme as the results actually observed, under the assumption that the null hypothesis is correct.” — Wikipedia This cumulative probability is the p-value. If we have a very small p-value, it might indicate two possible meaning:(1) we are so “lucky” to get this very rare sample data!(2) This sample data is not from our null hypothesis distribution; instead, it is from other population distribution. (So that we consider to reject the null hypothesis) To determine whether we could reject the null hypothesis, we compare the p-value to the pre-defined significant level (threshold). if p-value ≤ significant level (α), we reject the null hypothesis (H0).if p-value > significant level (α), we fail to reject the null hypothesis (H0). ** Thank you for reading! Welcome to any feedback, suggestions, and comments. I would be grateful if you could let me know what you think and the possible errors within my article. ** Gingerbread Man icon made by iconixar from Flaticon Paper Bag icon made by catkuro from Flaticon [1] Everything you Should Know about p-value from Scratch for Data Science[2] (video) What is a Hypothesis Test and a P-Value? | Puppet Master of Statistics[3] (video) Hypothesis Testing: Test Statistic (one-sample t-test) I Statistics 101 #3 | MarinStatsLectures[4] (video) What is a p-value? (Updated and extended version)[5] How t-Tests Work: t-Values, t-Distributions, and Probabilities[6] Interpreting P values[7] P-values Explained By Data Scientist[8] Test statistics explained[9] What is a test statistic?[10] Test Statistic: What is it? Types of Test Statistic[11] Why do p-values include the probability of obtaining more extreme values than the test statistic?[12] Wikipedia - p-value [13] Bluman, A. G. (2018). Elementary statistics: A step by step approach. New York, NY: McGraw-Hill Higher Education.[14] 沈明來. (2016). 生物統計學入門第六版, 九州圖書文物有限公司",116,3,14,Towards Data Science,2021-01-01,2021
https://towardsdatascience.com/natural-language-generation-part-2-gpt-2-and-huggingface-f3acb35bc86a,Natural Language Generation Part 2: GPT2 and Huggingface,Learn to use Huggingface and GPT-2 to train a…,1,24,['Natural Language Generation Part 2: GPT2 and Huggingface'],"So it’s been a while since my last article, apologies for that. Work and then the pandemic threw a wrench in a lot of things so I thought I would come back with a little tutorial on text generation with GPT-2 using the Huggingface framework. This will be a Tensorflow focused tutorial since most I have found on google tend to be Pytorch focused, or light on details around using it with Tensorflow. If you don’t want to read my whole post and just see how it works, I have the following Colab notebook as an outline for people to reference here. This post will be basically going over whats in the notebook so should be easy to reference back and forth. In my last tutorial, I used Markov chains to learn n-gram probabilities from presidential speeches and used those probabilities to generate similar text output given new starting input. Now we will go a step further and utilize a more state of the art architecture to create text output that should be more accurate and realistic. If you haven't already heard about GPT-2, its a language model from OpenAI trained on a mass amount of data from the web using an architecture called the Transformer. Here is a good visual overview of the transformer architecture used by GPT-2 that should help give you intuition on how it works. GPT-2 is not the most advanced version of the language model from Open AI, but its one that has many reference implementations and frameworks to use compared to the newer GPT-3 model. As well its a version of the model that can run on Colab and is fairly straight forward to setup and hopefully even easier after this tutorial :) Let's talk about the data For our task we will create a model to generate financial article titles. If we started the task of training the language model from scratch we would need lots and lots of examples (GPT-2 was trained on 8 million web pages). Fine tuning from the pre-trained model means we don’t need to use nearly the same amount to get decent results on our specific task. The plan is to get a decent amount of examples, couple hundred thousand, and then split them into train and eval sets. I decided to grab data from reddit titles in the /r/investing subreddit and titles extracted from US Financial News Articles dataset from Kaggle. Some of the examples from the joined dataset are not just finance related, since many financial news sites also report on non-financial events and the subreddit data has a mix of investing advice and questions. The titles pulled from reddit submissions are about 100k and the titles extracted from the Kaggle dataset are about another 179k. That should be enough examples so as to not over fit on our task and give us a rich set of possible text to generate from within the “financial” domain. Data format The format of the data seems to make or break the training and output of these models I have found. For GPT-2 if you want to just generate a whole bunch of text, say a book or articles, you can throw all the examples into a single document with no special tokens between examples. However if you want to generate output that follows a certain pattern or prompt, you should add special tokens into the dataset to make it more clear what pattern GPT-2 should attempt to learn to output. Below is the basic format for an example in the dataset for our title generation task. Each example is then concatenated together as one long string. We don’t have to add a start token for training since GPT-2 only needs the ‘<|endoftext|>’ token to split examples, but with this leading token we can then have the model generate new random output on each run when we prompt it with “<|title|>” first. You can set the start token to be whatever you want really, or have none at all, but I have found that setting these tokens to something that wont be likely to show up in the vocab of the data makes it easier to generate coherent text and you won’t be as likely to fall into a repetitive cycle. The gist above shows the cell step that is used to create our train and eval sets. As you can see when we read in the dataset line by line, then append the <|title|> token to the input then rejoin with <|endoftext|> and write back out to their respective file. Now that we have these two files written back out to the Colab environment, we can use the Huggingface training script to fine tune the model for our task. How to fine tune GPT-2 For fine tuning GPT-2 we will be using Huggingface and will use the provided script run_clm.py found here. I tried to find a way to fine tune the model via TF model calls directly, but had trouble getting it to work easily so defaulted to using the scripts provided. Some things like classifiers can be trained directly via standard TF api calls, but the language models seem to not be fully supported when I started this work. Its possible newer versions of Huggingface will support this. The script above will run the fine tuning process using the medium sized GPT-2 model, though if you are using standard Colab you might only be able to run the small GPT-2 model due to resource limits on the vm. For myself I am using Colab Pro which gives me access to more powerful base machines and GPU’s. Depending on your use case regular Colab may be sufficient or you can use GCP if you really need access to more powerful GPU instances for longer times. Transformer models are very computationally expensive due to their architecture, so when training on a GPU it can easily take hours or days with a large enough dataset. For the investing title dataset, 5 epochs on a p100 took over 3–4 hours while on a v100 it only took 1.5 to 2 hours depending on the settings I used. Its up to some luck it seems on which GPU you get when starting up your Colab instance. I found I was usually able to get a v100 every other day after a multi hour training session. One thing to call out in the above script call is that I am using mixed precision in the model training with the — fp16 argument. Using mixed precision shaved off about 30 mins of training time with no noticeable drop in model performance when compared to a single precision trained model on our data. At the end of the model training there is an eval step that happens which tells us our models perplexity. As you can see our title generation GPT-2 model gets us a perplexity score of around 10.6 which isn't bad considering it only ran for 5 epochs. So now that we have trained our new language model to generate financial news titles, lets give it a try! We will want to use the path to the directory that the script outputs the model file to, and load it up to see if it will output some great new finance article / reddit titles for us! To load into TF we will want to import the TFGPT2LMHeadModel and then call from_pretrained, making sure to set the from_pt flag to True. This way it will load the Pytorch model into TF compatible tensors. We will also use the pre-trained GPT-2 tokenizer for creating our input sequence to the model. The pre-trained tokenizer will take the input string and encode it for our model. When using the tokenizer also be sure to set return_tensors=”tf”. If we were using the default Pytorch we would not need to set this. With these two things loaded up we can set up our input to the model and start getting text output. After creating the input we call the models generate function. Huggingface has a great blog that goes over the different parameters for generating text and how they work together here. I suggest reading through that for a more in depth understanding. The below parameters are ones that I found to work well given the dataset, and from trial and error on many rounds of generating output. The one thing with language models is that you have to try a number of different parameter options to start to see some good output, and even then sometimes it takes many runs to get output that fits your task so do not be surprised if initial results are less than stellar. Below is some of the output that was generated by our investing title model given the “<|title|>” token as the prompt. From the generated examples above they look like believable article and reddit titles. Still sometimes when running you can get some funny output like the one below. Well, that was maybe a bit long of a post, but hopefully, you found it useful for learning how to use Huggingface to fine tune a language model and generate some text using a Tensorflow back end. Now with these techniques you can start to come up with different tasks and models for your own work/interests. For instance, after building this title model I decided to see if I can generate a title and use that title to generate some sort of article, to varying degrees of success. Try experimenting for your self and see what you can come up with! Thanks for reading! Link to colab gist: https://gist.github.com/GeorgeDittmar/5c57a35332b2b5818e51618af7953351",144,0,8,Towards Data Science,2021-01-01,2021
https://towardsdatascience.com/shakespeare-versus-eminem-1de54e479279,Shakespeare versus Eminem— who’s the better lyricist,"He is known for his poetry, his writings on life…",13,62,"['Shakespeare versus Eminem—who’s the better lyricist?', 'The Weigh-In', 'Rhythm And Poetry (r.a.p.)', 'Round 1: Averages, Shaky at (his) best…', 'Round 2: Normalized Averages…', 'Round 3: Your own worse Eminemeny…', 'Round 4: Shakespeareheading the conversation…', 'Round 5: Double entendres', 'Round 6: Elements of Eloquence', 'Round 7: Multi-syllable rhyme schemes', 'Round 8: Fastest Verses', 'Round 9: T.K.O.', 'The Aftermath']","Data science has crept into every conceivable nook and cranny of modern life. Apple uses a dedicated chip in your phone to inform Siri what to suggest to you. Even grocery store tags can update product prices in real-time. But in the meantime the real questions of life remain unanswered; we are no closer to answering what the meaning of life is, how to obtain true happiness, or whether Shakespeare is a better lyricist than Eminem. (Disclaimer: this analysis is heavily biased towards showing that b-rabbit is the better lyricist. I will take no measures to account for this bias. In fact, I’ll try everything in my power to show it is so. Because it is the right thing to do. “If I could only use this power for good I wouldn’t, not even if I could.”) Sorry if I’m being graphicBut I’m stiff as a statue, you sat on a shelfI feel like I’m a bust, maybe I’m just ahead of myself — Eminem Both men are famous for being brilliant with word-play. Shakespeare invented more than 1700 words that we use nowadays. He produced 37 plays and a bunch of poetry. His longest work is Hamlet, which has 4,042 lines, (“bars” in rap parlance) and his complete works contain 884,647 words. Our data in this analysis of Shakespeare contains a selection of 36 works and 857,648 words. Contrary to common belief Shakespeare wasn’t impressive in his own lifetime, he was popular, sure, but other playwrights were more acclaimed. His rise to fame had to wait until long after he died. 100 years post mortem, in 1730, the Shakespeare Ladies’ Club started promoting his works through local theatres which thrust him into the limelight and the fame that he enjoys nowadays. On the other side, Marshall Mathers (M&M: Eminem for the uninitiated) has been a legend ever since he hit the scene. To date, he has produced 12 studio albums. Eminem has a multitude of records (pun intended) to his name; his often lauded ‘Rap God’ holds a Guinness record for most words in a hit song — 1,560 — and, when it came out was the fasted delivery of syllables in a certain amount of one — 157 syllables in 16.3, a tongue-twisting 9.6s syllables per second —a record he broke with approximately .7 syllables more per second on ‘Majesty.’ Then, in 2018 he broke his own record again with ‘Godzilla,’ but now “maybe I’m just ahead of myself.” Oh, and Eminem holds an oscar. He was also the only person ever to have a number one song, album and movie in the same week! Our selection of Eminem works contains 17 different albums, 204 songs and a combined total of 167,080 words (and doesn’t include his many singles or mixtapes, freestyles, collaborative tapes, or ‘features where he stole the show’). Slim Shady doesn’t shy away from a challenge, on Jay-Z’s Renegade he straight up calls out Shakespeare. Has Shakespeare ever done that to Eminem? I think not! Shall I compare thee to a summer’s day? When in eternal lines to time thou grow’st: So long as men can breathe or eyes can see, So long lives this, and this gives life to thee. — Shakespeare In his book Dataclysm, Christian Rudder shows that the use of words on Twitter is of a higher calibre than typically found in books, even those whom we consider ‘the great.’ He argues, that the reason is that Twitter’s 140 character limit forces users to think hard and long about our words used. Twitterers will choose weightier words to convey more meaning in the limited space presented. There are many different indicators of text ‘maturity,’ such as the Dale Chall readability test. Such a test does not capture the full brilliance of our Royal Rhymliness’ but it’ll do. Let’s see what happens if our protagonist and antagonist go toe-to-toe in an exchange of fisticuffs. Time for Round 1. Ding-ding. “Averages are where you hide the truth.” That wisdom is equally true here. Averages say little about the absolute lyrical literacy and the lavish lexicons of our heroes. In some respects, the works of Em and Shaky are very different; their lengths for instance. Eminem clocks in at an average of 827 words per song with on average 110 bars, while Shakespeare clocks an impressive 23,823 words per play over an average of 3,949 lines. Neither Em nor Shaky is at an advantage at this point. They occupy slightly different places in society. This round is therefore a little underwhelming. Neither of our legends truly packs the punch, but then again “you gotta be diamond to even climb in the ring.” This is where the fight gets a bit more interesting. This is where we see who punches above their weight in their verbal assaults. Eminem is not phased: “just go hard, do the rope-a-dope Ali shuffle, and dance around his opponents.” For this, we want to see who uses more complicated words. We have used a list of 3,000 simple words. These words are used in everyday parlance and should be avoided as they are mainly filler. Eminem and Shakespeare are on par. Both average mid-20% ‘hard words’ in their works. This difference isn’t much, to be honest, but the advantage is for Shakespeare. However, when it comes to unique words per work Eminem outperforms Shakespeare. On average, Em’s works have 42% of unique words whereas Shakespeare only has about 15% unique words. This isn’t strange —Shakespeare needs a lot of filler words to fill to long-winding works of his. Eminem swings hard, Shakespeare dodges. That guy’s “never even been charged in connection with battery. Bitch, he ain’t plugged into nothin.” Eminem often refers to being his own worse enemy. And he’s proven that he’s quite the “spectacular archer.” His Rap God is an outlier, even by his own standards (see featured diagram below, top right-hand corner). As said, the song packs a whopping 1547 words, of which 617 words are unique. One song that is dramatically under-rated is the song shadyxv from the album with the same name. The song has ‘only’ 1158 words but packs a whopping 611 unique words (almost as many as Rap God). For a full breakdown of songs in our analysis, see the bubble-plot below. Hover over to see which work is under consideration. Shakespeare also would like to get a word in edgewise. Although his works are long as they are boring there is much less variation in his works. Have a look for yourself, below: Both authors have a tendency to play with the language. They bend words in place and language becomes almost liquid in their hands. In Twelfth Night Shakespeare whips out his naughtier side: By my life, this is my lady’s hand: these be her very C’s, her U’s, and (read: ‘N’) her T’s; and thus makes she her great P’s — Shakespeare However, on Marsh Eminem counters with his triple (?) entendre: S on my chest (Superman) like it’s plural — Eminem S on my chest; placing an ‘s’ on ‘chest’ will make it plural. However pleural is also also the region in your chest that encapsulates your lungs. All fun and games, but also, plural ‘s’ (read: S.S.) makes for the abbreviation of Slim Shady. Or, consider the song transcending: One-two-three, chk-chk, one-two-threeChk-chk, one-two-three — Eminem On My Darling, where the 1+2+3, 1+2+3, 1+2+3 = 666, the sign of the devil — who he’s arguably battling within the song. But he brings these bars at exactly 1:23 into the song. There are too many to list here, it’s worth checking out Genius website to see how many you’ve actually missed. Still light on his feet. Eminem moves quickly. Shakespeare looks old and tired. Mark Forsyth has written his book Elements of Eloquence in praise of Shakespeare's way with words, no one does it better than The Goat (“for those who you who don’t know what a Goat is, it means the Greatest Of All Time, and I consider him one of those.”). Mark gives us a lot to work with (39 chapters in which Eminem could have bested Shakespeare). In his first and 25th chapter, he talks about alliteration (or assonance), words with similar letters or sounds in quick progression which satisfies the listeners ‘need’ for rhyme. Mark gives us Shakespeare’s Midsummer Night’s Dream: Whereat, with blade, with bloody blameful blade,He bravely broached his boiling bloody breast; But I’m happy to raise him Eminem on Cleaning Out My Closet: Take a second to listen ‘fore you think this record is dissin’But put yourself in my position, just try to envisionWitnessin’ your mama poppin’ prescription pills in the kitchenBitchin’ that someone’s always goin’ through hear purse and shit’s missin’ Or consider our hero’s rhyme schemes; Shakespeare’s The Tempest goes: Full fathom five thy father lies;Of his bones are coral made;Those are pearls that were his eyes;Nothing of him that does fade,But doth suffer a sea-changeInto something rich and strange — Shakespeare Shakespeare’s rhyme scheme is a simple A; B; A; B; C; C (where the ‘;’ indicates the end of a sentence. Compare this to Eminem’s Wide Awake verse: But they can’t see what I can see, there’s a vacancy in my tummyIt’s making me play hide-and-seek, like Jason I’m so hungryShe’s naked, see, no privacy but I can see she wants meSo patient, see, I try to be but gee, why does she taunt me?(etc.) — Eminem The rhyme scheme here is ABCD; ABCD; ABCD; ABCD; etc for the next 14 bars! It’s clear that Shakespeare might have started it, Em is definitely here to finish it. There are five verses worth considering here. They are all impossibly fast. Now, go and try to rap along to any of these verses and you’ll find ‘what you see is a genius at work.’ Godzilla is incidentally the third time in a row that Eminem has beaten his own record for fastest verse. (Okay, TKO was a JT song, my bad.) Shakespeare is hardly standing. One final Nail in the Coffin, Eminem’s way with music. It’s clear that “papers are hand grenades soon as I pull the pen out” On the amazing (bonus) track on Recovery Untitled, he raps a 4/4 time signature over a 3/4 beat. The result is virtuous ‘head bobbing’ bars over a mesmerizing smooth waltzy flow. He’s against the ropes. He swings. He hits. Shakespeare goes down… He’s the greatest of all time: Eminem. Although Shakespeare put up a formidable fight, all that matters is that he’s clearly no match for Marshall Mathers. So what does someone like that have on Shakespeare? Everything. So, where does it leave our heroes? In the Aftermath (get it?) there’s only one left standing: Shady. And although ‘that 5–0’s startin’ to creep up on him’ and I’m no kid anymore either. I guess when Em says: “Somewhere some kid is bumpin’ this while he lip-syncs in the mirror, that’s who I’m doin’ it for, the rest I don’t really even care” — I’m that kid. The complete analysis and datasets can be found on my GitHub page, here. For the Slim Shady lyrics, I’ve used the database on AZLyrics. For Shakespeare’s work, I’ve used the Complete Works, 2nd Edition (1905, Oxford University Press). Thanks to Elizabeth Ter Sahakyan for her Plotly Visualization embedding in Medium, here.",298,5,9,Towards Data Science,2021-01-01,2021
https://towardsdatascience.com/cifar-100-pre-processing-for-image-recognition-task-68015b43d658,CIFAR-100: Pre-processing for image recognition task,Pre-processing or data preparation of a popular…,5,53,"['CIFAR-100: Pre-processing for image recognition task', 'What is CIFAR-100? 🤔', 'How can one obtain the CIFAR-100 dataset? 🙋', 'How to load this dataset? 🚛', 'Visualization:']","Recognition of images is a simple task for humans as it is easy for us to distinguish between different features. Somehow our brains are trained unconsciously with different or similar types of images that have helped us distinguish between features (images) without putting much effort into the task. For instance, after seeing a few cats, we can recognize almost every different type of cat we encounter in our life. 🐱 However, machines need a lot of training for feature extraction which becomes a challenge due to high computation cost, memory requirement, and processing power. In this article, we will discuss the pre-processing of one such use case. So, let’s dive deeper and understand how we can pre-process an image dataset to build a convolutional neural network model. 🏊🏼 Note: Convolutional Neural Network (CNN) is a class of deep neural networks commonly used to analyze images. A convolutional neural network model can be built to correctly recognize and classify colored images of objects into one of the 100 available classes of the CIFAR-100 dataset. So, let’s get started. 🏃🏻 CIFAR-100 is a labeled subset of 80 million tiny images dataset where CIFAR stands for Canadian Institute For Advanced Research. The images were collected by Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. The dataset consists of 60000 colored images (50000 training and 10000 test) of 32 × 32 pixels in 100 classes grouped into 20 superclasses. Each image has a fine label (class) and a coarse label (superclass). The Python version of this dataset can be downloaded from the website of the University of Toronto Computer Science. The downloaded files are Python pickled objects produced using cPickle. Do not worry about this for now. We will go through each step together to use this dataset. After downloading the dataset from the website, we need to load it into our jupyter notebook. The files obtained from the website are Python pickled objects. The folder structure after unzipping looks like this: We can see that we have separate train and test files, and a meta file. Python Pickle or cPickle module can be used to serialize or deserialize objects in Python. Here, I am using Pickle. The load() method of Pickle can be used to read these files and analyze their structure. Read this to know more about pickling. Pickle needs binary data so we will open files as ‘rb’ and load it using the pickle load() method with ‘latin1’ encoding. Let us first import the libraries which we will use in pre-processing. Here is the code to read these files. Read this to know why we mostly use ‘latin1’ as the encoding. Let us now load our training set. The output looks like this: The training file has the above items in it. The coarse_labels and fine_labels are labels of the images (20, 100 respectively), the data file has the image data in the form of a NumPy array, filenames is a list stating the names of the files, and batch_label is the label of the batch. Let us check the length of the dataset. The output looks like this: So, there are 50,000 images in the training dataset and each image is a 3 channel 32 × 32 pixel image (32 × 32 × 3 = 3072). Let us have a look at the unique fine labels. The output looks like this: So, there are 100 different fine labels for the images ranging from 0 to 99. Let us now have a look at the unique coarse labels. The output looks like this: So, there are 20 different coarse labels for the images ranging from 0 to 19. Let us check what is there in the batch_label file. The output looks like this: Here we have only one batch, so batch_label is a string stating that. As we are done with exploring the different files in the training dataset except for the data file itself, let us first unpickle our test dataset and meta file. Meta file has a dictionary of fine labels and coarse labels. For clarity, I have printed them separately. Here, is the output. Our task will be to recognize images and provide them fine labels. Let us now create dataframes using the labels, which will help us in visualization. A glimpse of the two dataframes: Let us now look at our data. The output is a NumPy array. In order to perform the task of image recognition and classification, a convolutional neural network has to be built which requires a 4D array as the input. So, the data has to be transformed to acquire that shape. For instance, the training dataset had 50000 images with the shape (50000, 3072), so we need to transform these images to acquire the following shape using the reshape and transpose operation of NumPy array: (Number of instances × Width × Height × Depth) The width, height, and depth are the dimensions of the image where depth is nothing but the number of color channels in the image which is 3 in our case as the images are RGB. The following diagram illustrates the form of 4D input for the convolutional neural network model. Let us write code for this transformation of images. We are now done with our transformation. Let us create visualizations to see these images. The output looks like this: Let us display some more images. We can see from the visualization that the quality of images is low and the position of the object in the image varies a lot. It would be difficult to train a model to recognize and classify such images. 🙆🏻 Let us now work on the test dataset. In order to make predictions, the labels of the images have been converted to categorical matrix structure from the existing 1D NumPy array structure. We are now done with our pre-processing and we will look at how to build a convolutional neural network model for this dataset in another article. Here is the link to the GitHub repository which has all this code. Please feel free to use this in your work to train a classic CNN model that can classify images. 😊 Related article: towardsdatascience.com Thank you, everyone, for reading this. Do share your valuable feedback or suggestion regarding this post! Happy reading! 📗 🖌 LinkedIn",95,1,8,Towards Data Science,2021-01-01,2021
https://towardsdatascience.com/how-the-lstm-improves-the-rnn-1ef156b75121,How the LSTM improves the RNN,,1,51,['How the LSTM improves the RNN'],"The advantage of the Long Short-Term Memory (LSTM) network over other recurrent networks back in 1997 came from an improved method of back propagating the error. Hochreiter and Schmidhuber called it “constant error back propagation” [1]. But what does it mean to be “constant”? We’ll go through the architecture of the LSTM and understand how it forward and back propagates to answer the question. We will make some comparisons to the Recurrent Neural Network (RNN) along the way. If you are not familiar with the RNN, you may want to read about it here. However, we should first understand what is the issue of the RNN that demanded for the solution the LSTM presents. That issue is the exploding and vanishing of the gradients that comes from the backward propagation step. Back propagation is the propagation of the error from its prediction up until the weights and biases. In recurrent networks like the RNN and the LSTM this term was also coined Back Propagation Through Time (BPTT) since it propagates through all time steps even though the weight and bias matrices are always the same. The figure above depicts a portion of a typical RNN with two inputs. The green rectangle represents the feed forward calculation of net inputs and their hidden state activations using an hyperbolic tangent (tanh) function. The feed forward calculations use the same set of parameters (weight and bias) in all time steps. In red we see the BPTT path. For large sequences one can see that the calculations stack. This is important because it creates an exponential factor that depends greatly on the values of our weights. Everytime we go back a time step, we need to make an inner product between our current gradient and the weight matrix. We can imagine our weight matrix to be a scalar and let’s say that the absolute scalar is either around 0.9 or 1.1. Also, we have a sequence as big as 100 time steps. The exponential factor created by multiplying these values one hundred times would raise a vanishing gradient issue for 0.9: 0.9¹⁰⁰ = 0.000017(…) and an exploding gradient issue for 1.1: 1.1¹⁰⁰ = 13780.61(…) Essencially, the BPTT calculation at the last time step would be similar to the following: Note that although the representation is not completely accurate, it gives a good idea of the exponential stacking of the weight matrices in the BPTT of an RNN with n inputs. W_h is the weight matrix of the last linear layer of the RNN. Next, we would be adding a portion of these values to the weight and bias matrices. You can see that we either barely improve the parameters, or try to improve so much that it backfires. Now that we understand these concepts of vanishing and exploding gradients, we can move on to learn the LSTM. Let’s start by its forward pass. Despite the differences that make the LSTM a more powerful network than RNN, there are still some similarities. It mantains the input and output configurations of one-to-one, many-to-one, one-to-many and many-to many. Also, one may choose to use a stacked configuration. Above we can see the forward propagation inside an LSTM cell. It is considerably more complicated than the simple RNN. It contains four networks activated by either the sigmoid function (σ) or the tanh function, all with their own different set of parameters. Each of these networks, also refered to as gates, have a different purpose. They will transform the cell state for time step t (c^t) with the relevant information that should be passed to the next time step. The orange circles/elipse are element-wise transformations of the matrices that preceed them. Here’s what the gates do: Forget gate layer (f): Decides which information to forget from the cell state using a σ function that modulates the information between 0 and 1. It forgets everything that is 0, remembers all that is 1 and everything in the middle are possible candidates. Input gate layer (i): This could also be a remember gate. It decides which of the new candidates are relevant for this time step also with the help of a σ function. New candidate gate layer (n): Creates a new set of candidates to be stored in the cell state. The relevancy of these new candidates will be modulated by the element-wise multiplication with the input gate layer. Output gate layer (o): Determines which parts of the cell state are output. The cell state is normalized through a tanh function and is multiplied element-wise by the output gate that decides which relevant new candidate should be output by the hidden state. On the left you can see the calculations performed inside an LSTM cell. The last two calculations are an external feed forward layer to obtain a prediction and some loss function that takes the prediction and the true value. The entire LSTM network’s architecture is built to deal with a three time step input sequence and to forecast a time step into the future like shown in the following figure: Putting the inputs and parameters into vector and matrix form may help understand the dimansionality of the calculations. Note that we are using four weight and bias matrices with their own values. This is the forward propagation of the LSTM. Now it is time to understand how the network back propagates and how it shines compared to the RNN. The improved learning of the LSTM allows the user to train models using sequences with several hundreds of time steps, something the RNN struggles to do. Something that wasn’t mentioned when explaining the gates is that it is their job to decide the relevancy of information that is stored in the cell and hidden states so that, when back propagating from cell to cell, the passed error is as close to 1 as possible. This ensures that there is no vanishing or exploding of gradients. Another simpler way of understanding the process is that the cell state connects the layers inside the cell with information that stabilizes the propagation of error somewhat like a ResNet does. Let’s see how the error is kept constant by going through the back propagation calculations. We’ll start with the linear output layer. Now we’ll go about the LSTM cell’s back propagation. But first let’s get a visual of the path we must take within a cell. As you can see, the path is quite complicated, which makes for computationally heavier operations than the RNN. Bellow you can see the back propagation of both outputs of an LSTM cell, the cell state and the hidden state. You can refer to the equations I showed above for the forward pass to get a better understanding of which equations we are going through. You can see that the information that travelled forward through the cell state is now going backwards modulated by the tanh’. Note that the prime (‘) in σ’ and tanh’ represent the first derivative of both these functions. In the next steps, we are going back to the parameters in each gate. Output gate: New candidate gate: Input gate: Forget gate: We have calculated the gradient for all the parameters inside the cell. However, we need to keep back propagating until the last cell. Let’s see the last steps: You may see that the information that travels from cell state c³ to c² largelly depends on the outputs of the output gate and the forget gate. At the same time, the output and forget gradients depend on the information that was previously stored in the cell states. These interactions should provide the constant error back propagation. Going further back into the global input (X³), we add what is coming from all four gates together. Finally we deconcatenate the hidden state from the global input vector, go through the remaining cells and add all the gradients with respect to the parameters from all cells together. This story’s goal was to understand why the LSTM is capable of dealing with more complex problems than the RNN by keeping a constant flow of error throughout the backpropagation from cell to cell. We explored the resulting issues from the poor handling of complex sequences from the RNN giving raise to the exploding and vanishing gradients. Then we saw how these issues come to happen by exploring the flow of gradients in the RNN. Finally we introduced the LSTM, its forward pass and, by deconstructing its backward pass, we understood that the cell state is influenced by two gate units that are responsible for ensuring a constant back flow of the error. It is important to mention that as more experiments were performed with the LSTM there is a certain degree of complexity where this network stops being able to learn. Generally it goes to the thousand time steps before it happens which is already pretty good. This is leading to a gradual phase out of the LSTM as problems become more ambitious in favour of a newer network called the Transformer or BERT. You may have also heard of the GTP-3 for Natural Language Processing. These are very powerful networks with a great potential. However, the LSTM sure had its impact and was created with ingenuity and still is usefull today. Thanks for reading! [1] Sepp Hochreiter and Jürgen Schmidhuber, “Long Short-Term Memory” in Neural Computation, 1997, DOI: 10.1162/neco.1997.9.8.1735.",,0,8,Towards Data Science,2021-01-01,2021
https://towardsdatascience.com/creating-abstract-art-with-stylegan2-ada-ea3676396ffb,Creating Abstract Art with StyleGAN2 ADA,How I used Adaptive Discriminator Augmentation and Learning…,9,50,"['Creating Abstract Art with StyleGAN2 ADA', 'MachineRay 2', 'Transfer Learning', 'Create your Own Painting', 'Next Steps', 'Acknowledgments', 'Source Code', 'References', 'Appendix A — Gallery of MachineRay 2 Results']","Back in August 2020, I created a project called MachineRay that uses Nvidia’s StyleGAN2 to create new abstract artwork based on early 20th century paintings that are in the public domain. Since then, Nvidia has released a new version of their AI model, StyleGAN2 ADA, that is designed to yield better results when generating images from a limited dataset [1]. (I’m not sure why they didn't call it StyleGAN3, but I’ll refer to the new model as SG2A to save a few characters). In this article, I’ll show you how I used SG2A to create better looking abstract paintings. Similar to the original Machine Ray, I am using abstract paintings that are in the public domain as my source images to train the SG2A system. I then change the aspect ratio as part of the post-processing. This diagram shows the flow through the system. Everything starts with the 850 images I scraped from WikiArt.org using a custom script. The images are pre-processed and fed into the Discriminator Network as the “real” images. A set of 512 random numbers are chosen and fed into the Style Mapper and Generator Networks to create the “fake” images. Both the real and fake images are modified with Adaptive Discriminator Augmentation, the key innovation in SG2A. I’ll discuss this further down in the article. The job of the Discriminator Network is to determine if the input is real or fake. The result is fed back into the three networks to train them. When the training is done, I post-process the output of the Generator Network to get the final images. I’ll go through each step in more detail in the sections below. I use a Python script to gather images from WikiArt.org that are labeled as being “abstract” and are in the public domain, i.e. created before 1925. The images are from the likes of Mondrian, Kandinsky, Klee, etc. The source code to gather the images is here. Here is a sample of the source paintings. Here are the steps I use for pre-processing the images: Here is a sample of pre-processed images. One of the major improvements in SG2A is dynamically changing the amount of image augmentation during training. Image augmentation has been around for a while. The concept is fairly simple. If you don’t have enough images to train a GAN, it can lead to poor performance, like overfitting, underfitting, or the dreaded “model collapse”, where the generator repeats the same output image. A remedy for these problems is image augmentation, where you can apply transformations like rotation, scaling, translation, color adjustments, etc., to create additional images for the training set. A downside to image augmentation is that the transformations can “leak” into the generated images which may not be desirable. For example, if you are generating human faces, you could use 90° rotation to augment the training data, but you may not want the generated faces to be rotated. Nvidia found that augmentations can be designed to be non-leaking on the condition that they are skipped with a non-zero probability. So if most of the images being fed into the discriminator are not rotated for augmentation, the generator will learn to not create images that are rotated [1]. …the training implicitly undoes the corruptions and finds the correct distribution, as long as the corruption process is represented by an invertible transformation of probability distributions over the data space. We call such augmentation operators non-leaking. — Tero Karras, et al. The new version of StyleGAN has a feature called Adaptive Discriminator Augmentation (ADA) that performs non-leaking image augmentations during training. A new hyperparameter, p, in the range of 0 to 1, determines how much and how often augmentations are to be applied to both the real images and the fake images during training. Here is a sample of augmentations with different values for p. You can see how the sample images show more variety with spatial and color changes as p increases from 0.0 to 1.0. (Note that SG2A does not show the augmentation done to the real images during training. So I added the functionality to do this in my fork of the code, here. You can run a test in the Google Colab, here.) The value p starts at 0 when training begins, and then increases or decreases if the system senses that there is overfitting or underfitting during training. The heuristic used in the system is based on the sign (positive or negative) of the output of the discriminator during the processing of a batch of generated images. If there are more positive output values than negative, then the trend is towards real images, which is an indication of overfitting. If there are more negative values than positive then the trend is towards fake images, which is an indication of underfitting. The value p is adjusted up or down accordingly after the batch — up for overfitting and down for underfitting. A target value for p can be set, i.e. 0.7, so there is always a non-zero probability that the augmentations can be skipped, which avoids leaking. You can read more about Adaptive Discriminator Augmentation in Mayank Agarwal’s post here. I trained the system using Google Colab. It took about four days to run. The system has an Nvidia Tesla V100 GPU which can run up to 14 teraFLOPS (14 trillion floating-point operations per second). So it took about 4.6 exaFLOPS to train (about 4.6 quintillion floating-point operations). That’s a lot of math. Here’s the shell command I used to kick things off. Setting the aug parameter to ada enables the Adaptive Discriminator Augmentation to kick in when needed. Setting the target parameter to 0.7 prevents p from going over 0.7, which maximizes the dynamic augmentation without leaking any further augmentation into the final images. Here’s a graph of the training results over time. I am using the Fréchet Inception Distance (FID) as a metric for image quality and diversity, where a lower score is better [2]. You can read about the FID score in Cecelia Shao’s post here. You can see how StyleGAN2 ADA outperforms the original StyleGAN2 for the same number of iterations. The FID score for SG2A bottomed out at just over 100 after about 300 iterations. Here are some samples of the results. The GAN seems to produce a nice variety of abstract paintings with interesting compositions with varied color schemes. But there is room for improvement. It may not be intuitive, but it’s possible to improve the quality of the paintings by first training the GAN on a different, larger set of images, and then further train the model using the abstract paintings. This technique is called Transfer Learning. It was first described as a technique for training NNs in 1976 [3]. Transfer Learning is technique that uses a pre-trained neural network trained for Task 1 for achieving shorter training time in learning Task 2. [4] — Stevo Bozinovski I remember learning in my Art History 101 class that many abstract painters started by painting figurative subjects, like people and landscapes. For example here are sequences of paintings by Mondrian, Klee, and Kandinsky that show their progressions from landscape to abstract art. I wanted to see if I could transfer the learning from creating landscape photos to perhaps improve its ability to create abstract paintings. As an experiment, I trained SG2A on 4,800 photographs of landscape photos from Flickr that are in the public domain. The dataset, by Arnaud Rougetet, is available on Kaggle, here. Here is a sample of the photos from Flickr. Note that I resized the photos to be 1,024 x 1,024 pixels each. I trained the SG2A system using another Google Colab to create new landscape images. It also took about four days to run the training. Here’s the shell command I used. The command is almost the same as the one I used to train using the abstract paintings. The only differences are the paths to the landscape folders on Google Drive. Here’s a graph that shows the FID scores of the training of landscapes (the green line). The score for the generated landscapes is much better than the abstract paintings. You can see that it bottomed out around 25 after about 150 iterations. The reason the landscapes have a better score than the abstract paintings is probably due to the larger training set — 4,800 landscapes and only 850 abstract paintings. Here are some generated landscape photos that were created after four days of training SG2A. These look pretty good. The horizons are a bit distorted and the clouds seem somewhat fantastical, but they could otherwise pass as real landscape photos. Now let’s see if this learning can help with creating abstract art. The next step of the experiment was retraining SG2A to create abstract art starting with the model previously trained on landscapes. Here’s a flow diagram that shows the process. After the GAN was trained to create landscapes, it was further trained to create abstract paintings using prior knowledge of the landscapes. I trained the system using a third Google Colab. Here’s the shell command I used. I am training using abstract paintings, but the resume option starts the training with the GAN trained with landscapes. It took about four days to run the final training. Here is a graph of the FID scores. The new results are in blue. You can see that Transfer Learning helped improve the scores. The FID numbers for the latest training run settle in around 85, which is better than the previous run with a score of about 105. Here are some sample images that benefitted from the Transfer Learning. It may be subtle, but the images created with the model with Transfer Learning appear to be more refined, pleasing compositions. Please check out the appendix below for a gallery of samples. Once the GAN is trained, images can be generated using this command line. This will generate four images using random seeds 1, 2, 3, and 4. The truncation parameter, trunc, will determine how much variation there will be in the images. I found that the default of 0.5 is too low, and 1.5 gives much more variety. A Google Colab for generating images with a variety of aspect ratios is available here. It generates 21 images and lets you choose one to see at high resolution. Here is a sample of images. Additional work might include training on paintings of landscapes instead of photos for the Transfer Learning. This may help the GAN pick up on a painterly look to apply to abstract paintings. I would like to thank Jennifer Lim, Oliver Strimpel, Mahsa Mesgaran, and Vahid Khorasani for their help and feedback on this project. The 850 abstract paintings I collected can be found on Kaggle here. All source code for this project is available on GitHub. The sources are released under the CC BY-NC-SA license. [1] Karras, T., Aittala, M., Hellsten, J., Laine, S., Lehtinen, J., and Aila., T., “Training Generative Adversarial Networks with Limited Data.”, October 7, 2020, https://arxiv.org/pdf/2006.06676.pdf [2] Eiter, T. and Mannila, H., “Computing the Discrete Fréchet Distance”, Christian Doppler Labor für Expertensyteme, April 25, 1994, http://www.kr.tuwien.ac.at/staff/eiter/et-archive/cdtr9464.pdf [3] Bozinovskim S., and Fulgosi, A., “The influence of pattern similarity and transfer learning upon training of a base perceptron B2.” Proceedings of Symposium Informatica, 3–121–5, 1976 [4] Bozinovski, S., “Reminder of the first paper on transfer learning in neural networks, 1976”. Informatica 44: 291–302, 2020, http://www.informatica.si/index.php/informatica/article/view/2828",167,4,11,Towards Data Science,2021-01-01,2021
https://towardsdatascience.com/a-guide-to-expected-goals-63925ee71064,A Guide to Expected Goals,Part 1: Data Exploration and Visualization,3,32,"['An Exploration of Expected Goals', 'What is xG?', 'Data Exploration']","Here, I will introduce the concept of expected goals (xG) and conduct an exploration of event data. This will represent the first part of a three part series on expected goals. Part II will be centered around constructing a machine-learning model from this event data, while Part III will explore the applications, strengths and deficiencies of this model. Results in football, more so than any other sport, can be greatly influenced by random moments and “luck.” Near misses, deflected shots, goalkeeping errors, and controversial refereeing decisions alone can dictate the final result. Football is a game of inches. These effects are amplified by the fact that goals are rare events; a match produces 2.5 goals on average. Furthermore, a large majority of matches end in a draw or are decided by just a couple of goals, meaning a single goal can be largely significant to the result of a match. Luck and randomness can therefore have a notable effect when so many matches are defined by fine margins. This also makes performances difficult to evaluate; is a dogged 1–0 win a product of a deserving performance or a series of fortunate events? Sometimes, this is difficult to evaluate with the naked eye. It is our hope to quantify and qualify performances by eliminating as much randomness as possible when examining a match. In order to score a goal, you must first attempt a shot at goal. Assessing a performance ten or so years ago would simply entail taking a look at the total shots and shots on target. While these are useful tools for assessing chance creation, they do not tell the whole story as not all shots are created equal. There are many factors that influence the the likelihood of a shot resulting in a goal. This is where xG comes into play. xG measures the probability that a shot will result in a goal based on a number of factors. Such factors include the distance from where the shot was taken, angle with respect to the goal line, the game state (what is the score), if it was a header, if the shot came during a counter attack and other factors. For the purpose of simplicity, our exploration will focus on just three of these factors. We can use this metric to sum over all the chances in a match to determine how many goals a team should have scored based on the factors we aggregated in our model. We can go even further to apply this to a stretch of games, a season or even a manager’s tenure. xG therefore can serve as a gauge of how potent a team is in attack and how solid they are on the back. It can also be used to analyze a players ability to create shooting opportunities in dangerous areas and how well he takes his chances. In summary, the xG model helps us eliminate a portion of the random factors associated with scoring opportunities when we attempt to quantify a team’s ability to score goals, which in the end is the ultimate goal of football. We will see later that we can use xG to predict future results, guide decisions on player recruitment and evaluate coaching instructions, but first let’s try to explore some data. Before we get into building our xG model, we need to consider what sort of data we are interested in. Obviously, we need a large collection of shot data but more importantly we need the data to describe the type of shots that result in goals. We can deduce that the most important factors we need would be the distance from goal when the shot was taken, the angle with respect to the goal and what part of the body the shot was taken with. Football data is normally split into two forms: event data and tracking data. Event data records all on-ball events and where on the pitch they happened (such as shots, passes, tackles, dribbles), whereas tracking data records the positions of players and the ball through-out the game at regular intervals. The event data that I will use today comes from Wyscout. It covers all events from all matches across the top 5 domestics leagues in Europe (English Premier League, Ligue 1, Bundesliga, La Liga, Seria a) from the 2017/2018 season. While some of the findings in this section may seem elementary to those who have an extensive understanding of football, I always believe it is important to test our assumptions, as they can be misleading at times. I think the best place to start is to ask, where do most shots happen on the pitch? Right away, there are some conclusions we can draw. The distributions suggests that: The angle distribution agrees with the distance distribution in that shots taken from closer (larger angles) are much more difficult to produce. Just with a simple distribution chart, we can conclude that it is quite difficult to produce shots that are close and central to goal. While we now know how shots are distributed by distance and angle, we have yet to address how shots that result in goals differ from those that do not. The violin plot above plays a similar role to a box and whiskers plot but also provides use with the kernel distribution estimate of the data (essentially a smoothing of the distribution). In splitting up the data by the result of the shot, we can see that on average, shots that result in goals are taken from much closer to goal than shots that do not result in goals. The mean of shots resulting in goals is about 12 meters compared to about 18 meters for those that don’t bulge the net. Similarly, goals are typically scored from angles of 20 degrees to about 50 degrees. So while it is difficult to produce shooting chances close to goal, the violin plot suggests that those that are close and central tend to result in goals. Let’s see how headers impact the mean and the distributions. Headers, as we might expect, are normally taken within the 18 yard box (16.5 m). Interestingly the means and distributions of the results do not differ by much, so that is something we should consider down the line. We have gained some fabulous insight through some basic distribution plots but we can take this a step further. We can better visualize how these variables impact the result by plotting the density of shots on a pitch. That is, we would like to split the pitch up into bins, calculate the number of shots taken within each bin and then use a color gradient to visualize how the density differs from bin to bin. These density plots serve a similar function to the violin plots above but give us a much better visual understanding of which areas of the pitch normally produce shots and goals. This is because we can see how both distance and angle impact the distribution of shots on the same plot. As we learned with the violin plots: Before we delve deeper, we need to address why there is a sharp decrease in the number of shots on the edge of the box. This could be due to a number of reasons. There are other possibilities and it is difficult to make any sort of conclusion regarding the trough but for now we will have to live with it. Now to return to some more data visualization. As you might guess, we can plot a probability density to assess which areas of the pitch have a high probability of a shot resulting in the goal and which do not. As expected, the closer you shoot from the goal, the more likely the shot is to result in a goal. Notice that there are certain outliers in which the probability density in those bin are very high. This is because of the few shots taken from those areas, they resulted in goals. If we had say 10 seasons of data, we would see a much more homogenous probability density. One of the things that is most surprising to those who have never used xG is that the probability of scoring from beyond 11 meters out is in fact lower than maybe we appreciate when we see a game live. It is for this reason that this sort of analysis is important. We tend to over-estimate the quality of chance, such as shots from outside the box, when in fact there shots are quite difficult and inefficient. So, the next time your favorite player misses from 11 meters out, remember that he only really had a 3 in 10 chance of scoring anyway. Now what about headers? While headers exhibit a similar trend to regular shots, they have lower probability values overall. This seems to suggest that while headers happen closer to goal on average, they also represent a much more difficult chance to put away. As we will see down the line, this is an important discovery and one that impacts our interpretation of chance evaluation. Before we progress into some machine learning, I want to close with us trying to gain some insight into these trends. We have hypothesized that both the distance and the angle with the goal line affect the probability of a shot resulting in a goal and we have seen that with the graphs above. But what is the nature of this relationship? As we move away from the goal, how does the probability of scoring change? We can address these question as we have done before, with some well designed graphs. The first thing that pops out and is quite intriguing is that as we move further away from goal, the probability of scoring becomes exponentially more difficult. Now that is profound because it vastly diminishes the value of shots from distance. So why would this be? Up until now, we have ignored the fact that the angle with the goal decreases as we move away from the goal. So we have this sort of ‘doubling factor’ for the distance. We can hypothesize that this is because as we increase the distance a shot is taken from, it not only has a longer distance to travel but the target also becomes smaller. Fantastic! Now we have seen the power of data visualization and how even the simplest graphs can help us discern the information locked behind large datasets. It is for this reason that spending time and effort on data exploration is so important. This will serve as a good foundation when we move onto some machine learning in the next part. See you soon! For all the code associated with the analysis and visualizations here, please visit my github.",116,0,10,Towards Data Science,2021-01-01,2021
https://towardsdatascience.com/visualize-bert-sequence-embeddings-an-unseen-way-1d6a351e4568,Visualize BERT sequence embeddings: An unseen way,,10,30,"['Visualize BERT sequence embeddings: An unseen way', 'About', 'Why so many layers?', 'Hands on💪', 'The companion Jupyter notebook', '⚠️ What we won’t be covering', 'Let’s get started', 'The Visualizations🔬👨\u200d⚕️', 'Going further🙇\u200d♂️', 'References']","Transformer-encoder based language models like BERT [1] have taken the NLP community by storm, with both research and development strata utilising these architectures heavily to solve their tasks. They have become ubiquitous by displaying state-of-the-art results on a wide range of language tasks like text classification, next-sentence prediction, etc. The BERT-base architecture stacks 12 encoder-layers, which brings it up to a whopping 100 million tuneable parameters! The BERT-large architecture takes it yet a notch higher with 24 encoder-layers and ~350 million parameters! 🤯 In the forward pass for an input text sequence, the output from each of these encoder blocks can be seen as a sequence of contextualised embeddings. Each contextualised embedding sequence is then fed as an input to the next layer. This repetitive application of encoder layers enables: It is a known fact that when using these deep architectures, it is very easy to fall into the pits of overfitting. Looking at how each encoder layer offers its own embeddings for the input, an interesting question may arise in one’s head:“As I train my model, how effectively do each layer’s embeddings generalize on unseen data?”In simpler words, it is of interest to see to what extent each layer of BERT is able to find patterns in data that hold on unseen data. In this tutorial we will be talking about a really cool way to visualize how effective each layer is in finding patterns for a classification task. With the motivation set, let’s look at what we will be doing. Train a BERT model for multiple epochs, and visualize how well each layer separates out the data over these epochs. We will be training the BERT for a sequence classification task (using the BertForSequenceClassification class). The same exercise can be extended to other tasks with some tweaks in implementation details. For example, language modeling (using the BertForMaskedLM class). Re-train a language model on your own dataset, and inspect the characteristics of each cluster or the distribution of embeddings! Training: Visualization: Dataset: HatEval [2], a dataset with tweets labeled as Hateful/Neutral. However, feel free to load your own dataset in the companion Jupyter notebook. I am placing the complete code for data loading, model training and embedding visualization in this notebook. The code present in this tutorial is intended only for explanation purposes. Please refer the notebook for complete working code. Since this article focuses only on the visualization of layer embeddings, we will be walking through only relevant parts of the code. Rest of the code lies outside the scope of this tutorial.I assume a prior knowledge of the 🤗Transformers BERT basic workflow (data preparation, training/eval loops, etc). Extract Hidden States of each BERT encoder layer: Next, we define a function that can plot the layers’ embeddings for a split of our dataset (eg- train/val/test) after an epoch: These computed values are finally plotted on a new plot using the Seaborn library. Finally, putting together what we’ve seen till now inside a training loop: Here we call the visualize_layerwise_embeddings function once per epoch for every split of the dataset we want to visualize separately. I choose to visualize embeddings from the first 4 and last 4 layers. We have our visualizations ready!I took a step further to make things more convenient by stitching the different images into a gif! Once again, code is present in the notebook.Pretty aesthetic, right?🤩 Spend a moment on each layer’s output. Try to draw some interesting inferences out of them! I’ll give some examples:- can you comment on how each layer is performing with each successive epoch?- the train accuracy of the classifier dropped from epoch 4 to epoch 5! Can you verify this fact from the above gif? ** Finally, we are more interested in knowing if our embeddings are helping us generalize. We can judge that by the validation split visualizations above. Some interesting questions from the top of my head are:- Which layer are generalizing better that the others?- How well is the last layer able to separate out the classes?- Do you see any difference in the separability between the embeddings for train and validation splits?- Does taking an average of the embeddings across all non-masked tokens of the sequence produce better results that taking embedding only for ‘[CLS]’ token? (You might have to tweak the notebook a little to answer this one😉) Don’t stop just here, yet!Go and play around in the notebook provided, and try to mix and match different layers’ output embeddings to see which combination helps you produce the best downstream performance! ** Answer: the embeddings for the 12th layer are more neatly clustered for epoch 4 as compared to epoch 5! It is a clear indicator of the classifier having hit and then over-shot a minima in the loss-function space. [1]: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, Delvin et al., 2019[2]: SemEval-2019 Task 5: Multilingual Detection of Hate Speech Against Immigrants and Women in Twitter, Basile et al., 2019",142,1,6,Towards Data Science,2021-01-01,2021
https://towardsdatascience.com/project-modeling-predicting-of-churning-customers-in-r-cb0a846ba94a,"<strong class=""markup--strong markup--h3-strong"">Project: Modeling &amp; Predicting of Churning Customers (in R)</strong>",You have just been hired as a Data Scientist,8,40,"['Project: Modeling & Predicting of Churning Customers (in R)', '1. Introduction', 'Scenario:', 'Goal:', 'Features & Predictor:', '2. Data Wrangling', '3. Exploratory Data Analysis (EDA)', '4. Modeling + Confidence Intervals']","Table of Contents 1. Introduction 2. Data Wrangling 3. Exploratory Data Analysis 4. Modeling 5. References You have just been hired as a Data Scientist. A manager at the bank is disturbed with an alarming number of customers leaving their credit card services. You have been hired as a data scientist to predict who is gonna get leave their company so they can proactively go to the customer to provide them better services and turn customers’ decisions in the opposite direction. A data collector hands you this data to perform Data Analysis and wants you to examine trends & correlations within our data. We would like to do model & predict who will leave our companyNote: Exit Flag is the discrete target variable here for which we have to create the predictive model, but we will also use Credit Limit for another modeling example walkthrough for a continuous predictor. In Part 1 of 3 of Data Wrangling, we read in our data file & install all required libraries/packages for our project. We also examine if there are any problems with our dataset, & hence see that there are no issues. In Part 2 of 3 of Data Wrangling, we manipulate the data to get only the columns we want & to remove NA & Unknown values in our data. We also examine the dimensions & unique values for our discrete variables. 6 distinct discrete types for Income_Category :$60K — $80K, Less than $40K ,$80K — $120K ,$40K — $60K ,$120K + , Unknown  4 distinct discrete types for Marital_Status: Married, Single, Divorced, Unknown 4 distinct discrete types for Card_Category: Blue, Gold, Siler, Platinum Note: We will also remove any rows/entries with a “Unknown”/NA value. We see here we initally have 10,127 rows & 23 columns, but we truncate that too 8348 rows by 9 columns. In Part 3 of 3 Data Wrangling, we rename our predictor Column Attrition_Flag to Exited_Flag. We also rename the binary output values for this predictor from Existing Customer/Attrited Customer to Current/Exited, respectivley. We lastly, also see the cout of each discrete feature with our discrete predictor. Above, we can evidently see that Current Customers had higher mean credit limits than did churning customers. In part 1 of 2 in EDA, we reorder the Factor levels of certain discrete variables, because we want them to display in order in our graphs. We have 2 discrete variables plotted against each other, with 1 of the variables being a fill. We examine the count of each visually. In part 2 of 2 in EDA, we explore 2 violin plots with quantiles, a visualization for 2 discrete variables, & area distribution plot. Quantiles can tell us a wide array of information. The 1st horizontal line tells us the 1st quantile, or the 25th percentile- the number that separates the lowest 25% of the group from the highest 75% of the credit limit. Next, the 2nd line is the Median, or 50th percentile — the number in the middle of the credit limit. Lastly, the 3rd line is the 3rd quartile, Q3, or 75th percentile — the number that separates the lowest 75% of the group from the highest 25% of the group. Quantiles are shown both in the violin plots above & below. We have only 16.07% of customers who have churned. Hence, it’s a bit difficult to train our model to predict churning customers. We will do our best with the data given, as with any model. Our goal in modeling is to provide a simple low-dimensional summary of a dataset.We use models to partition data into patterns and residuals. Fitted model is just the closest model from a family of models. Classification models are models that predict a categorical label. It will be interesting to study which characteristic(s) discriminates each category and to what extent. Predicting whether a customer will a customer will exit or stay with the company. Below we will use a logistic regression algorithm. First we must convert the non numeric variables (discrete variables) into factors. Factors are the data objects which are used to categorize the data and store it as levels. We fit the logistic regression model. The first step is to instantiate the algorithm. We declare binomial because 2 possible outcomes: exit/current. The significance code ‘***’ in the above below output shows the relative importance of the feature variablesAIC estimates the relative amount of information lost by a given model:the less information a model loses, the higher the quality of that model. Lower AIC values indicate a better-fit model, You have learned techniques of building a classification model in R using the powerful logistic regression algorithm. The baseline accuracy for the training & test data was 84 percent. Overall, the logistic regression model is beating the baseline accuracy by a big margin on both the train and test datasets, and the results are very good. Below I will show you another way to model this, using SVM (Support Vector Machine). Confidence intervals are of interest in modeling because they are often used in model validation. Next, we consider the 95% confidence interval of Credit Limit.As the Credit Limit is greater than 0, we narrow the confidence interval.There are 91.75% data locates within the confidence interval. We will keep the corresponding records and store the rest in another variable rest.data for latter analysis. #The following aims to build a model to predict the price. ## Evaluation We will use MSE as the criteria to measure the model performance. We Split 20% as test dataset and 80% as training dataset. To recal: AIC estimates the relative amount of information lost by a given model: The less information a model loses, the higher the quality of that model. Thus, the lower AIC values indicate a better-fit model, The AIC table shows the best model is Credit_Limit ~ Customer_Age + Gender + Dependent_count + Education_Level + Marital_Status + Income_Category + Card_CategoryWith a AIC of 72870.02, we want the lowest AIC. Gender Male, Income Categories, and Silver Card Card Categories were highly significant with respect to our predictor of Credit Limit. Here is access to the data set & code from my GitHub page: https://github.com/jararzaidi/ModelingChurningCustomersR https://www.kaggle.com/sakshigoyal7/credit-card-customers Recommendations & comments are welcomed!",94,0,14,Towards Data Science,2021-01-01,2021
https://towardsdatascience.com/stock-price-analysis-with-pandas-and-altair-ef1e178cc744,Stock Price Analysis with Pandas and Altair,Practical guide for Pandas and Altair,1,27,['Stock Price Analysis with Pandas and Altair'],"Stock price analysis is an example of time series analysis which is one of the primary areas in predictive analytics. Time series data contains measurements or observations attached to sequential time stamps. In case of stock prices, the time stamps can be seconds, minutes, or days depending on the task. This article is a practical guide that covers some basic and essential operations in stock price analysis. We will be using Pandas for data manipulation and Altair for data visualization. Both are Python libraries that are commonly used in data science. There are many resources to get stock price data. We will use the data reader API of Pandas. We first import the dependencies. We can now use the data reader to create Pandas dataframes that contain stock price data. We use the DataReader function to get the stock prices. The stock name, start and end dates, and the source are the required parameters. This function returns a dataframe as below: We have a separate dataframe for Apple, IBM, and Microsoft. It is better to combine them but we need a column to specify the name of the stock. The symbol column in each dataframe indicates the name. We can now combine them using the concat function of Pandas. I have only included the date, close, volume, and symbol columns. Let’s first create a basic line plot of Apple stock prices. The first line in the code is a top-level Chart object. We pass the data to be plotted to this object. A filter is applied using the symbol column to only include data that belongs to Apple stocks. In the next line, the type of the plot is specified. The encode function indicates the columns to be plotted. Whatever we write as an argument in the encode function must be linked to the data passed to the Chart object. The stock price of Apple seems to be continuously increasing except for a few occasional drops. The one in the April 2020 might be related to the global pandemic due to corona virus. We can show the stock prices of all three companies in one visualization. What distinguishes the companies is the symbol column. It is passed as an argument to the color parameter of the encode function. Apple and Microsoft stock prices follow a similar trend. IBM seems to suffer more from the pandemic. In the last line, we use the properties function to change the size of the figure. We can smooth the line by resampling the data. Resampling basically means representing the data with a different frequency. One option is to use the resample function Pandas. It aggregates data based on specified frequency and aggregation function. The code above resamples the Microsoft stock prices based on the average of 7-day periods. The only difference is that we pass the resampled data to the Chart object. The line seems much more smooth now. Altair makes it quite simple to have multiple plots in one visualization. For instance, we can plot the closing price and volume for Microsoft stocks as below. We assign each plot to a variable. The variables can easily be combined by logical operators. For instance, “|” puts them side-by-side whereas “&” puts the second plot under the first one. Altair provides many options to enrich the visualizations by conveying more information. For instance, we can add a line that indicates the average value. The first plot is a line plot just like the ones created earlier. The second plot is a rule that calculates the average value for each line. It is pretty straightforward to transform data with Altair. For instance, in the encode function of the second plot, the aggregation is indicated with a string (‘average(Close)’). In our case, we can visually compare the mean values because there is a significant gap between values. However, it might be useful to indicate the mean when the values are close. There are many definitions of time series data, all of which indicate the same meaning in a different way. A straightforward definition is that time series data includes data points attached to sequential time stamps. We have covered some basic operations with Pandas and Altair. There is much more to cover in time series analysis. In fact, it is a highly complex topic and there is a great amount of research being done in this field. With that being said, it is almost beneficial to comprehend the basics and then start on the more advanced topics. Thank you for reading. Please let me know if you have any feedback.",94,1,5,Towards Data Science,2021-01-01,2021
https://towardsdatascience.com/cohort-analysis-with-python-2bdf05e36f57,Cohort Analysis with Python,A data clustering skill that every e-commerce data analyst must master,2,16,"['Cohort Analysis with Python', 'Introduction']","If you are a data analyst working for an e-commerce company, there is a high chance that one of your tasks at work is to discover insights from customer data to improve customer retention. However, the customer data is massive and each customer behaves differently. Customer A who was acquired in March 2020 displays a different behaviour from customer B who was acquired in May 2020. Therefore, it is essential to group customers into different clusters and then investigate the behaviour of each cluster over time. This is called cohort analysis. Cohort Analysis is the data analytic technique in understanding the behaviour of a special group of customers over a period of time. In this article, I will not go into details the theory of cohort analysis. If you do not know what cohort analysis is all about, I would strongly recommend you read this blog first. This article is more to show you how to segment customers into different cohorts and observe the retention rate for each cohort over a period of time. Let’s get into it! You can download the data here. Here is it not wise to just simply select df.loc[df['customer_type']]. Let me explain why. In this data, under the customer_type column, First_time refers to new customer, whereas Returning refers to returning customer. So if I first bought on 31 Dec 2019, the data will show me as new customer on 31 Dec 2019 but as returning customer on my 2nd, 3rd, … time. The cohort analysis looks at new customers and their subsequent purchases. Therefore, if we simply use df.loc[df['customer_type']=='First-time',] we would be ignoring the subsequent purchases of the new customers, which is not a correct way to analyse cohort behaviour. Thus, what I did here was that, first create a list of all the first-time customers and store it as first_time. Then from the original customer dataframe df select only those customers whose IDs fall within the first_time customers group. By doing this, we can make sure that we obtain the data that has only first-time customers and their subsequent purchases. Now, let’s drop the customer_type column because it is not necessary anymore. Also, convert the day column into correct date-time format purchase_rate function will determine whether that is a 2nd, 3rd, 4th purchase of each customer. join_date function allows us to identify the date the customer joins. age_by_month function gives us how many months from the current purchase of a customer to the first time purchase. Now the input is ready. Let’s create cohorts. How to interpret this tableTake cohort 2018–01 as an example. In Jan 2018, there were 462 new customers. Out of these 462, 121 customers came back and purchased in Feb 2018, 125 in Mar 2018 and so on and so forth. That is it. Hope you guys enjoyed and picked up something from this article. If you have any questions, feel free to put them down in the comment section below. Thank you for your read. Have a great day and happy new year 🎉🎉🎉",196,3,5,Towards Data Science,2021-01-01,2021
https://towardsdatascience.com/a-skill-to-master-in-python-d6054394e073,How to Slice Sequences in Python,Learn how to slice lists and strings in Python,6,74,"['How to Slice Sequences in Python', 'Indexing a Sequence', 'Slicing a Sequence', 'Slicing Strings', 'Palindrome Example', 'Slice Assignment']","Being able to efficiently slice sequences in Python (such as lists, strings, and tuples) is one of the most crucial skills to have when programming. Luckily, python provides indexing syntax that greatly facilitates the slicing process and makes it much more intuitive. In this tutorial, we will first review how to index a sequence and then move on to slicing sequences, specifically lists and strings. Furthermore, we will cover some important differences between slicing lists and slicing strings. We will then review slice assignment in lists. And lastly, we will look at what exactly is happening when we use the indexing syntax in Python. Before we start with slicing, let’s briefly review how to index elements in a sequence (specifically a list). Remember that we can access individual elements in a list by using their index within square brackets. Let’s look at a list of numbers below: The index of an element in a sequence is its location in that sequence. In the example above, we have a list of numbers, num_list, and the numbers below that list represent the indices of the corresponding elements. As we may recall, we can either index our sequence starting from the beginning (from the left) with positive indexing starting with index 0, or starting from the end of the sequence (from the right) with negative indexing, starting with index -1. In other words, if we want to retrieve the number 10 (or third element) from num_list, we can either use its positive index of 2, or negative index of -7: If we want to obtain the last number in our list, which is 40, we can do so using the index 8 or -1: Or we can just use the len() function as follows: If we use an index value that’s not present in our list or out of range, we will receive an IndexError: Now that we’ve reviewed how to index a sequence using positive and negative indexing, let’s look at slicing. towardsdatascience.com We just saw how indexing can be used to retrieve individual elements from a list. Slicing, on the other hand, allows us to obtain a portion from our sequence, such as of a list or string. Sometimes to understand slicing it can be useful to imagine that the indices are pointing between the elements, rather than to the elements themselves. Although this is only useful when the step value is positive, meaning when we are slicing from left to right. More on step values later. The syntax for slicing a sequence is as follows: variable[start:stop:step] In order to slice a sequence, we need to use a colon within square brackets. In other words, the colon (:) in subscript notation [square brackets] make slice notation. Even though there are three possible values that we can provide within the brackets (the start value, stop value, and step/stride value), we don’t actually have to provide all three unless we need to, as we will see below. Let’s look at some examples. One way we can slice a sequence, such as a list, is by specifying the start and stop values. In other words, if we want all the elements between two specific points in our list, we can use the following format: variable[start:stop] variable[start:stop] returns the portion of the variable that starts with position start, and up to but not including position stop. For example, if we want to obtain all the elements from index 2 up to and including index 6, we can do so as follows: Notice how the start value is inclusive, but the stop value is exclusive. Thus, we start with index 2, which is the number 10, and go all the way to but not including the index 7, which would be the number 30 at index 6. If we imagine the indices as between the elements (as seen above), then that is further illustrated, since the index 7 is before the number 35. Since we did not provide a step value, the default step value is 1. Thus, we start with index 2, then take 1 step to index 3, then 1 step to index 4, and so on. In other words, because the step value is positive, we are increasing the index by 1 (moving to the right) as we are slicing our list. If we want to start at a specific number and go through the entire list, then we would only need to provide the start value. variable[start:] variable[start:] returns the portion of the variable that starts with position start, through the end of the sequence. For example, if we want to retrieve all the elements from the second index through the entire list, we can use the following code: As we can see, if we only provide an index before the colon, then that will be our start index, and we will obtain the rest of the elements in the list (since the step value is still 1). If we want to start from the beginning of the list and go up to a specific index, then we would only need to provide the stop value. variable[:stop] variable[:stop] returns the portion of the variable that starts at the beginning of the sequence, up to but not including position stop. For example, if we want to retrieve all the elements from the start of the list up to and including index 7, we can do so as follows: Thus, if no number is provided for the start value, then it assumes that we want to start from index 0. And since we want to retrieve all elements up until index 7, we would use the stop value of 8 since it is exclusive. We can also use -1 as the stop value. We can also mix and match the positive and negative indices. For example, if we want to retrieve all the elements between index 2 up to and including index 7, we can do so as follows: Note that in all instances, the stop value is to the right of the start value, since we are using a positive step value. In other words, the stop value must be in the direction of the step value, in relation to the start value. If the step value is positive, then the stop value must be right of the start value. If the step value is negative, then the stop value must be left of the start value. More on that later. We can also retrieve the entire list by just using a colon with no start or stop values. variable[:] variable[:] returns the entire sequence. towardsdatascience.com So far we’ve only specified start and/or stop values, where we start at the start value, and end right before the stop value (since it is exclusive). But what if we don’t want all the elements between those two points? What if we want every other element? That’s where the step value comes in. Let’s say that we want every other value in our list, starting from index 0. Or perhaps we only want the elements at even indices. We can do so using the step value: variable[::step] Since we did not specify a start or stop value, it assumes that we want to start at the beginning of the sequence and go through the entire list. So it starts at index 0, then goes to index 2 (since the step is 2), then to index 4, and so on. Previously, we mentioned that the stop value must be in the same direction as the step value relative to the start value. In other words, if the step value is positive, which means we are moving to the right, the stop value must be to the right of the start value. If the step value is negative, then the stop value must be to the left of the start value. Otherwise, an empty list is returned: As we can see, in both examples, the start value is 8, and the stop value is 5, so the stop value is to the left of the start value. In the first example, the step value is +1. Since the stop value is to the left of the start value, and our step value is positive, an empty list is returned, since we cannot move in the direction of the stop value. However, in the second example, we changed the step value to -1. Thus, we start at index 8, which is 40, move 1 index in the negative or left direction to index 7, which is 35, then to index 6, which is 30. We do not go to index 5 because the stop value is exclusive. Perhaps the most important practical application of the step value is to reverse a sequence. For example, if we want to retrieve the entire list in reverse order, we can do so by using a -1 for the step value: Since we did not specify a start or stop value, then the entire sequence will be retrieved. However, since our step value is -1, it obtains the elements in reverse order. What if our stop value is greater than the highest index available in our sequence? Or if our start and/or stop values are out of range? In other words, what happens if we ask for more items than are present? For example, if we try the following: As we can see, even if we ask for more items than present in our sequence, it just returns whatever elements are present and does not give us an error. In contrast, if we try to index a single element that is out of range (instead of slicing), then we would get an IndexError as we saw earlier. Indexing and slicing work the same way for strings as well. Again, we can imagine the indices between the characters if we are using a positive step value as follows: Thus, to obtain the substring ‘yt’ via slicing, we can do so as follows: To reverse a string, we can do so by using a step value of -1: Let’s use what we learned to solve a very commonly asked python coding question. We want to write a function that takes in a string, and returns whether or not that string is a palindrome. A string is a palindrome if the reverse of the string is identical to the original string. For example, ‘civic’ is a palindrome, but ‘radio’ is not, since the reverse of ‘radio’ is ‘oidar’, but the reverse of ‘civic’ is ‘civic’. We just learned how to reverse a sequence by using a step value of -1. Thus we can easily write a function that accomplishes this as follows: And that’s it! The expression word == word[::-1] is evaluated to either True or False. If the string we pass in is equal to its reverse, then the expression evaluates to True, and True is returned. If the string we pass in does not equal its reverse, then the expression evaluates to False, and False is returned. If we recall, lists are mutable objects in python. In other words, they are able to be mutated, or changed. Thus, we can use slice assignment operation to mutate or edit a list in place. Notice how we can replace a slice of our list with more or less elements. We can also delete a part or slice of a list using the del keyword: Note: Strings and tuples are not mutable. Thus we can not edit or mutate them like we can with lists. Slicing a list will return a copy of that list and not a reference to the original list. We can see this here: if we assign our list slice to another list, since the list slice returns a copy and not a reference to the original list, we can modify the new list (since lists are mutable) without affecting the original list: In contrast, when we slice a string, a reference to the original string object is returned, and not a copy. And remember, strings are not mutable in Python. We can use Python’s identity operator (is) and the equality operator (==) to confirm that slicing a list returns a copy or a different object than the original list, but slicing a string returns a reference to the original string object: The equality operator (==) checks if the values are equal. The identity operator (is) checks if the two variables point to the same object in memory. levelup.gitconnected.com When we use the indexing syntax in python, which includes the use of a colon within square brackets, the built-in slice function is actually being used to create a slice object. slice(stop) slice(start, stop[,step]) The slice function can be used in two different ways to create a slice object (similar to the range function which creates a range object). If we pass one argument to the slice function, then that will be the stop value. If we pass in three arguments to the slice function, then they will take on the start, stop, and step values. In other words, the start and step parameters will default to None. Here are some examples showing the slice objects used when using indexing syntax to slice a list: Using the slice function to create a slice object can be useful if we want to save a specific slice object and use it multiple times. We can do so by first instantiating a slice object and assigning it to a variable, and then using that variable within square brackets. If you enjoy reading stories like these and want to support me as a writer, consider signing up to become a Medium member. It’s $5 a month, giving you unlimited access to stories on Medium. If you sign up using my link, I’ll earn a small commission. lmatalka90.medium.com In this tutorial, we first reviewed that indexing a sequence means to extract a single element using by using its positive or negative index within square brackets. We then compared indexing a sequence to slicing a sequence, which can retrieve a portion of a sequence. We learned how to slice a sequence with square brackets and a colon, including the different ways to specify which portion we want to retrieve. And since lists are mutable in Python, we saw how we can use slice assignment to change portions of our lists. We then saw the differences between slicing a list and slicing a string, in that slicing a list returns a copy of that list, but slicing a string returns a reference to the original string object. Lastly, we saw how the slice object is actually being used when using slicing notation in Python.",100,0,12,Towards Data Science,2021-01-01,2021
https://towardsdatascience.com/how-to-present-machine-learning-results-to-non-technical-people-e096cc1b9f76,How to Present Machine Learning Results to Non-Technical People,Showing model results stakeholders can…,1,14,['How to Present Machine Learning Results to Non-Technical People'],"As a data scientist it was difficult to explain machine learning results to non-technical stakeholders. After some trial and error I came up with a way to transform model results into a format my stakeholders were able to understand. Today I’d like to share my methodology that I’ve used with great success. This methodology can apply to any model that generates probability score values between 0 and 1. Below is an example of the output using this methodology on a sample set of purchase propensity model scores. The percentages in the “% of Total Purchases” column were calculated by taking purchases in the decile divided by total purchases of 31,197. These are the key takeaways from this table. I’ve seen data science presentations with model results that was beyond the grasp of non-technical people. However, if you presented model results with a table that showed the top 20% of customers captured 65% of purchases that’s easy for your stakeholders to understand. This second approach is similar to the first and is a view that also makes it easier to evaluate the model results as a data scientist. Below is an example of the output using the same set of sample purchase propensity scores. These are the takeaways using this output binned by score values. This output shows stakeholders your model captures 67% of the purchases accurately. It also helps you identify issues if the model is not predicting true positives and show areas where you can research further as in the case of the customers with a score between 0.1 and 0.19. As data scientists, the impulse is to show the raw model results but often we need to transform the output into a form stakeholders can understand. Now that you seen my approaches I hope you find it easier to articulate your model results for everyone to understand. medium.com towardsdatascience.com towardsdatascience.com",212,3,4,Towards Data Science,2021-01-01,2021
https://towardsdatascience.com/how-to-utilize-feature-importance-correctly-1f196b061192,How to utilize Feature Importance correctly,Feature Importance is a part of ,5,20,"['How to utilize Feature Importance correctly', 'Why Feature Importance❓', 'What is included in Feature Importance? ✋', 'Why not only Feature Importance score alone 🌓', 'Conclusion 🗽']","In training a machine learning model, the ideal thing is to condense the training features into a set of variables that contain as much information as possible. There are 3 reasons for this. 1, reducing the number of features means narrowing down the dimension, reducing sparsity, and increasing the statistical significance of the training result. If we have too many features, we will also need a larger number of samples representative enough for training. 2, reducing sparsity helps to prevent the over-fitting problem (the more the number of features, the more complex the model becomes, hence more tendency to be over-fitting). 3, which is obvious, is the efficient saving of computation power while achieving acceptable performance. Thus, feature selection is an important step in preprocessing data. Among popular approaches, feature importance is one of the most popular ones. However, choosing features through Feature Importance is not always as straight as an arrow. It is indeed required some techniques to make the right decision for selection. However, from my experience, I usually use feature importance after conducting other data cleaning, preprocessing steps as below. There are several ways to express Feature Importance. It can be score representing the features’ relevancy (using algorithm-based Feature Importance, such as tree-based Random Forest, XGBoost,..), dimension reduction from original features to synthesize all the information to a lower dimension (like Autoencoder or PCA), or domain knowledge selection (aka selection based on your own knowledge). With the variety of choices like this, which one should we choose? Even when we choose to use PCA, selecting the right number of remaining features seems to demand great exertion. Don’t you think we should go with the Feature Importance score or testing everything and see which performs the best? Yes, it’s correct, testing everything is the accurate answer. From my experience, I usually set the baseline as the original model without any selection, then try PCA with different drop-out ratios and check the metric. Then drop original features (mainly because I want to limit the amount of collected data needed, as well as optimize the training time while maintain or improve the performance). So Dimension Reduction is my first choice and the Feature Importance is the next essential step. But using only the Feature Importance score is hardly a good choice. Let me show you why. Feature Importance score here means any score generated from the trained model representing the weight or relevancy of the features to the prediction of the target feature. Below are the feature importance scores of Random Forest calculated based on RandomForestRegressor model (detail of formula is here), selecting feature with SelectFromModel (detail), and permutation score (detail of formula is here). Data used for demonstration is California housing in sklearn. Looking at the above, how confident are you in deciding to keep only MedInc and drop others (based on the first graph) or to retain MedInc, Latitude, Longitude, and AveOccup based on the second one, or even just drop AveBedrms and Population? If I were you, I merely can make any decision with this. Why? Because dropping this or that does not ensure better performance, it even leads to a worse one if you choose unconsciously. The selection from the Permutation score slightly improves from the original dataset. Let’s see another experiment with the Boston dataset (source). This looks more consistent than the California dataset, doesn’t it? I guess the decision to keeping ‘RM’ and ‘LSTAT’ features is much more confident now. Let see how the performance changes if the model is trained on these 2 features. None of these selections performs equally on par with the original model, which means reducing features does not work here. These 2 examples show different strategies on how to reduce the number of features, and either the metric or the score can help us decide here. “No, we need a different approach!” There is one way to decide better which features should be removed. This technique is traditional but effective and holistic. We decide by seeing the performance of the model after dropping features. First let’s drop each feature one at a time, then drop one after another, measure their performance, and decide the elbow or the level that is sufficient. Dropping any among HouseAge, AveRooms, Population and AveBedrms obviously retain the performance at the same level with the original. However, removing MedInc surprisingly lowers the test MSE. When dropping them cumulatively from the least relevant feature to the most relevant (based on Feature Importance score), we can clearly see that the elbow in train MSE appeared when removing Latitude and other features prior to it, but the overfitting issue was worsened at the time dropping Longitude. Hence, we have more confidence to condense the model into 4 features: “Longitude”, “Latitude”, “AveOccup”, “MedInc”. Let’s take a look at the Boston dataset. Removing either RM, LSTAT or DIS worsens the performance, and if we only keep RM, LSTAT or DIS in the model, not only the train MSE but also test MSE significantly picked up. From looking at this, we can decide to keep NOX, CRIM, DIS, LSTAT, and RM. The score is not the only way to go. Using only the score to decide the approach in feature selection seems to be subjective and empirical. Using the appropriate method is like deciding the right metric for your model. Knowing exactly why you do that and what effect it brings to the model are keys to the next success for your machine learning.",244,2,5,Towards Data Science,2021-01-01,2021
https://towardsdatascience.com/15-lesser-known-useful-sklearn-models-you-should-use-now-a1a566e680a6,15 Lesser-Known Useful SkLearn Models You Should Use Now,Some of the more interesting models that can…,5,77,"['15 Lesser-Known Useful SkLearn Models You Should Use Now', 'Data', 'For Continuous Problems', 'For Classification Problems', 'Conclusion']","Sk Learn is likely one of the most popular machine-learning modules for Python. This is for good reason, as SkLearn has a fantastic catalog of usable models, scalers, tools, and even encoders! While there are some rather popular models that are very well-known, SkLearn is such a large library that it can be easy to forget all of the functions and classes that come with it. While documentation is a great place to start, another great way to expand your modeling capabilities is to get more familiar with is to just use different models. With that in mind, I have come to enjoy a lot of different models from the SkLearn. There are a lot of great models that go severely under-used. Today I wanted to bring some of my favorite models from SkLearn to your attention, so maybe next time you’re facing a unique problem, you’ll know the model and its corresponding application! Today, we are going to be fitting models in order to demonstrate the usage of said models. Of course, to do this we are going to need some one-dimensional arrays to pass in order to predict features. In simpler terms, we need data to train off of. Given that we are working in a notebook, I figured it should be open-source, so if you would like to see these models fit in a notebook you can see the source here: notebook Since we are going to be reviewing classification models as well as continuous models, we are going to require targets that facilitate those feature types respectively. I am going to be using an old .CSV file that I have lying around called weatherHistory.csv: Now let’s have a look!: Since we are going to be using both classification and continuous models, I am going to need both a categorical and continuous Y. For the continuous target, I decided to use temperature. For classification, I went ahead and used the Precipitation Type feature. For my predicting features, I am going to use the Humidity feature: Now we will train test split that data accordingly into two dataframes: Of course, I am not going to be doing much processing to this data just to fit some models for an example. That in mind, those models can’t be fit to data with missing values, so let’s get a summation of the missing value counts in our dataframe. Wanna hear something funny? I could have sworn it was is_null() instead of isnull(). Looking at the summary, It is clear that I might have gotten a bit ahead of myself, let’s drop some bad observations: Now let’s put that into 1-dimensional arrays: SkLearn often requires these arrays to be reshaped to vertical, as it prefers features in columns of matrices as opposed to rows of matrices. Let’s reshape these puppies, and then we will be ready to fit some models. In order to do this, we’re going to need to turn these one dimensional arrays into NumPy arrays. This is because SkLearn is far more integrated with NumPy, though it does like Pandas Series in a lot of cases NumPy arrays are much more dynamic and commonly used: Among the two different types of models typically used for supervised models are continuous models. These models predict values that are quantitative, rather than qualitative. That being said, many of these models are going to utilize regression in order to estimate continuous values. Isotonic, or monotonic regression is an awesome form of regression that many machine-learning engineers have never even heard of. isotonic regression can be a very accurate model for predicting continuous targets, but also has its own limitations in that regard. A great example of this is that this model is often prone to over-fit, and often getting the model to work well is going to be balancing the bias and trying to increase accuracy. Another significant problem with this model is that the data must be non-decreasing. This means that typical applications of this model are often going to be involved in economical and business scenarios. So with that in mind, while this model might be a very useful one for someone working with economic data, for a lot of scientific work it isn’t necessarily the greatest model. However, isotonic regression in the proper application and balanced bias can be an incredibly powerful predictive model! If you would like to learn more about isotonic regression, you can check out these two articles I wrote, one where I compose an isotonic regressor from scratch in C++, and the other I elaborate on how exactly the model works: towardsdatascience.com towardsdatascience.com In order to fit this model, we are going to first need to use the make_regressor function which will give us a basic regression model at which we can build isotonic regression on top of. Let’s do that: Another very useful tool that only applies to certain data characteristics is Orthagonal Matching Pursuit. This model is used to take sparse-coded signals and remove noise and abnormalities in said data. This means that these machine-learning algorithms are utilized to fix certain incoming signals based on data, which I think is a pretty great application for machine-learning. While the primary use of Orthagonal Matching Pursuit might be relatively straightforward, the uses of this model could be potentially much further. Given that this is quite a unique model on the spectrum, how does it work? Orthagonal Matching Pursuit forms the exact operation described in its name. To dissect this definition, let’s look at the words individually: So basically, we are searching for perfect matches to our data for where it relies on a multi-dimensional span of data, D, which would likely be a dictionary type inside of the programming world. In Wikipedia’s words, .The idea is to create an approximate signal (f) from Hilbert space (H) as a weighted sum of statistical functions — meaning PDFs/CDFs/Gamma. Although the data that we have certainly would not be a great application for this particular model, so instead for this example I am going to create some sparse signals to pass as data: Now we will distort our target data: And fit our model: If you’ve been using machine-learning for any extended period of time, it is likely that you have heard of Lasso regression. Lasso regression is a fantastic and quite standardized tool that has been used frequently in machine-learning for an extended period of time now. Most of the time when predicting continuous targets, this is most certainly my first choice of model. However, the LARS lasso model is not your normal Lasso regressor. The LARS in “ LARS lasso model” is short for Least Angle regression. Least Angle regression is a machine-learning algorithm for predicting continuous features. It is most useful for working with data with incredibly high dimensions. This model works by a linear subset of covariates. One great thing about this model is that while the ceilings in terms of dimensional are dramatically raised from a traditional model, it really isn’t all that slow compared to a model that might typically be used in this way. That being said, it is important to remember that while this model is derived from Least Angle regression, that does not make it linear — and that is where the lasso part of the model comes in. Of course, if a linear model is what you’re looking for, you can utilize the same concepts on a traditional linear regression model using Least Angle regression, which is also in SkLearn. As discussed in the similar application of Least Angle regression to a lasso model, Least Angle regression is a model used for predicting continuous features that typically work by using a linear subset of covariates. This is the most significant difference between Least Angle regression and LARS Lasso regression. Least Angle regression will be referring to the linear version of that model. Computationally, Least Angle regression has the advantage of being just as fast as forward selection. While that is a monsterous benefit, its biggest strength is in scenarios where p >> n. If two variables are equally correlated, then their coefficients should increase at the same rate. Another great implementation of a concept in SkLearn is stochastic gradient descent. Stochastic gradient descent is a method that is used iteratively to optimize mathematical functions and build cost. It is likely that you have heard of gradient descent, which is similar — however, the stochastic in this model’s name means that we only use a single training example for epoch. This is high-end of the two extremes, starting with batch gradient descent, which will use the whole batch per epoch and mini-batch in the middle being a combination of the two. The gradient of the loss in stochastic gradient descent is estimated each sample at a time and the model is updated along the way with a decreasing strength schedule, also known as learning-rate in typical machine-learning terms. With regressive learning methods, there can often be short-comings to models that are difficult to overcome. This can certainly be said for linear models, which might have a hard time perfectly fitting to sparse data, or data with multiple dimensions to it. Fortunately, SkLearn has a decent implementation of Huber regression. This form of regression can be used to work with outliers and avoid modeling errors that might be easy to make using the typical models that are available in the SkLearn package. This can be useful because while this model is useful and fits well, it is also relatively simple, meaning that over-fitting and over-sampling aren’t typically problems that are encountered when working with this model. Huber regression optimizes the squared loss (mean squared error) and is considerably robust to outliers compared to models like simple linear regression. There is actually a very interesting paper published by Art B. Owen at Stanford University that might be worth checking out for those uninitiated or unfamiliar with this method of modeling. You can check it out if you’re interested: Here is the remark I found the most valid for analyzing what exactly this model is going to do with this data mathematically: “ The least squares criterion is well suited to yi with a Gaussian distribution but can give poor performance when yi has a heavier tailed distribution or what is almost the same, when there are outliers. Huber (1981) describes a robust estimator employing a loss function that is less affected by very large residual values” Needless to say, this model is incredibly cool! I think it definitely has its use in knocking out outliers as a contributing problem to the difficulty of predicting continuous problems — which is often understating, but seems obvious in the realm of elementary statistics. Now that we’re familiar with this model somewhat mathematically, we can actually consider fitting it in Sklearn (that was a joke.) While this “ model” might be more of concept to utilize with other models, it is certainly going to be incredibly useful! A very common pattern with machine-learning is to use non-linear functions to create linear predictions. This will maintain a lot of the speed of the model while not wasting any of the prediction power. A great example of this is polynomial regression over simple linear regression. In examples where polynomial regression is used, it is fit with a higher dimension of data built with functions. Thanks to the use of polynomial features, this model can be fit on and used to solve a wide-range of continuous problems easily. In order to actually use polynomial regression in sklearn, we are actually going to use Polynomial Features: Ordinary least squares is another really cool mathematical machine-learning model for predicting continuous features. Ordinary least squares is also a linear model, and fits to coefficients that are created to minimize the sum of squares between points in the data. The weights for ordinary least squares rely heavily on the independence of features for predicting the target. That being said, this model is incredibly useful for implementations with a single feature. Furthermore, it can be used with multiple features but will certainly require the features to be weighted well towards the target. Looking into this description, it is easy to see the exact niche that a model like ordinary least squares is going to fit into in our machine-learning arsenal. The SkLearn implementation of OLS, unfortunately is not as straightforward as most. The OLS coefficients are actually contained beneath LinearRegression classes as .coef_: Going back to the magic of support vector machines, allow me to introduce you to NuSVR. NuSVR is of course the same model and machine implementation as NuSVC. Both of these models utilize libsvm and uses a parameter, nu, to control the number of support vectors in the machine. However, unlike NuSVC, where nu replaces C, here nu replaces the parameter epsilon of epsilon-SVR. The advantage of using this model is that parameters are provided for adjustment of the vector machine that is used to assist in estimation of this regression problem. That being said, this model is great for predicting continuous features with a lot of features that may or may not be incredibly important to our target. As with the LARS lasso, it is likely that you have heard of lasso regression. As a refresher, it is a fantastic non-linear model for predicting continuous features that is very commonly used. The difference in the multi-task lasso is that that the multi-task lasso is going to use the L1/L2 norm as regularize. To demonstrate this, let’s look at the optimization objective for Lasso regression: In this example, ||W||_21 would be modified to fit this formula for a multi-task lasso: Essentially what this means is that we are getting the summation of each row. In machine-learning, there might be a lot of continuous problems, but LinearSVC is a support vector machine type of model. The SkLearn implementation of the model was created using libsvm. While the kernel can be non-linear, its SMO does typically not scale very well to a large number of samples. This is where the Linear Support Vector Classifier comes in handy. That being said, while there are some other great SVC implementations in Sklearn that might even be more well-known, LinearSVC is certainly a model that is well-worth being aware of! SVC typically works in a multi-class mode implemented with one class weighted against one other class. In the case of LinearSVC, all of those classes are weighted against each other class — making the model more comprehensive than many of the SVC competitors. As for the SkLearn class, it is an awesome and nearly flawless implementation. It supports both dense and sparse data incredibly well, and can be an incredible model — easily one of the best for linear classification in Python in my opinion! As discussed when we briefly discussed the SGDRegressor, stochastic gradient descent is where each batch is used to weigh features at each iteration in cost. The SGDClassifier of course is the exact same concept now being applied to a classification problem. Like the SGDRegressor, this model is a great implementation that can be valuable when working with a large set of features in particular. While it might take a dramatic drop to performance, it might need to be weighed whether or not the resulting predictions are worth those particular efforts. Fitting the SGDRegressor is quite straightforward, and as with many on this list will follow the typical SkLearn convention of The Bernoulli naive Bayes classification model can be used just as any Bayesian classification model is used, however does have a trick up its sleeve: The Bernoulli distribution. You might be familiar with this distribution, as its CDF often appears near logistic classification models. I would say that this model’s usage is very similar to that of MultinomialNB. However, while MultinomialNB works significantly better with counts and occurrence, BernoulliNB uses the Bernoulli distribution and is designed for bool-type features. This of course brings back recollection of my initial statement, where Bernoulli is used for LogisticRegression, that being said, this model has essentially the same use — so it makes sense that it uses the same distribution. Although this model is typically used for predicting binary features, today we are just going to be passing some typical categorical data that might be more applicable to MultinomialNB. That being said, it is important to remember the use-cases for both of these models because they are very powerful, but should be used for their appropriate feature types. There is not a point in using a model if you don’t understand the target that you are trying to predict, and I think the difference between these models highlights the need for data scientists to understand that different models require different types of features to work well. A great lesson to learn from just an import, but regardless it follows the typical convention we have come to expect from SkLearn: If you’ve been working with machine-learning models, especially continuous models, it’s likely you’ve heard of ridge regression. Ridge regression is a popular model used for predicting continuous features. RidgeClassification is of course the classification equivalent of this exact model for classification problems. The classification version of the model converts the target into {-1, 1} and then models it into a regression problem with the typical ridge regression. I think this is a really cool concept because applying regression and other continuous methods of solving problems to an entirely different problem like classification is really cool in my opinion. Fortunately, even though this model is really awesome and seems like an advanced concept, the wonderful SkLearn has made it incredibly easy to use with typical convention for the library: Probably one of the coolest models on this list is CalibratedClassifierCV. This model uses cross-validation both to estimate parameters of a classifier. The model can be used with a logistic regressor as a base, which will make it a great model for classifying boolean types. However, since this model can actually take different base estimators, a commonly used model is actually isotonic regression. Isotonic regression is a really cool model, but in my opinion becomes a lot cooler when combined with a classification problem. This means that the thresholds are now attached to classes, rather than arbitrary quantile amounts inside of continuous data. Unlike many of the other solutions for predicting targets on this list, this is another one that is going to be an additive for other models. In other words, we can calibrate essentially any classifier by simply building one for it. In this example I am going to be building one using Gaussian Naive Bayes, another classification model in SkLearn similar to Multinomial Naive Bayes. “ By the way, in my opinion this is a really cool methodology, I really think that SkLearn hit the mark on the way that objects are used and the convention that they use as classes.” SkLearn is such an awesome library that machine-learning engineers these days might take for granted. There are lot of models in the library that are absolutely incredible and might go mostly ignored because of the champions that are already available. How many of these models have you used? I hope that these descriptions and introductions to these awesome models was entertaining and perhaps even helpful in the model selection for your next project. Thank you very much for reading, and happy new year! I think these models will be very valuable assets in the future. Maybe if you run into a good binary classification problem, or a linear modeling problem, you will think back to this article I wrote in my first year. Hopefully this article inspires deeper digging and research to learn more about modeling, because it really is a lot of fun to learn about.",265,2,16,Towards Data Science,2021-01-01,2021
https://towardsdatascience.com/statistical-simulation-in-python-part-2-91f71f474f77,FAANG Ask These 4 Python Simulations in 2021,,9,47,"['FAANG Ask These 4 Python Simulations in 2021', 'Introduction', 'Question 1: Uniform Distribution', 'Question 2: Binomial Distribution', 'Question 3: Poisson Distribution', 'Question 4: Normal Distribution', 'Takeaways', 'My Data Science Interview Sequence', 'Enjoy reading this one?']","Updated on Jan-10–2021 Statistical Simulation is the most heavily tested topic in Data Science/Engineering Interviews! If we go over DS Interview questions posted at Glassdoor, statistical simulation is the key skill that all big tech companies expect their applicants to excel in. Interview scenarios may either be asked to perform a power analysis of A/B experiments or construct a binomial distribution to simulate user behaviors in a programming language, R or Python. These are not difficult questions but require a deep understanding of fundamental statistics and fluent programming skills. Without deliberate practice, these questions may likely trip you over. This post presents 4 types of the most often tested statistical distributions in Data Science interviews and live-code solutions in Python. In a previous post, I have covered the fundamentals of statistical thinking and R codes. In case you missed it, here is the portal: towardsdatascience.com Disclaimer: I assume my fellow readers understand the statistical basics (e.g., what a binomial distribution is) and some familiarity with the Python programming environment (e.g., how to write a simple for loop). Here is a light refresher on common statistical distributions by Zijing Zhu. In R or Python, please answer the following question. For a sequence of numbers, (a1,a2,a3,a4,…,an), please write a function that randomly returns each element, ai, with probability ai/∑ai. (Condition 1) For example, for a sequence (1,2,3,4), the function returns element 1 with a probability of 1/10 and 4 with a probability of 4/10. (Condition 2) You can use any library, but no random.choice(). (Condition 3) This is a real interview question that I asked by a travel company. Let’s break it down. The question asks for a function that returns an element proportional to its weights, ai/∑ai. It can be completed in two steps: For step 1, we do something like the follow: Here, there is a catch with the question: we can’t use the built-in method, random.choice() (Condition 3). Hypothetically, it would be a much easier question if we are allowed to import the Numpy package. Alternatively, we have to develop something to perform the same functionality as the random choice. Back then, I was clueless right on the spot. My interviewer kindly offered his first hint: you can use a uniform distribution with a range from 0 to 1 and compare the generated value (named a) to the cumulative probability sum (named cum_prob[i]) at each position i. If cum_prob[i] > a, then return the corresponding value from the sequence. The idea sounds great, and let’s check how the Python codes look like. People make a common mistake by trying to use a control flow (if-else statements) to filter out scenarios. It is doable for a small sample size but not practical if there are thousands of numbers in the sequence. We are not using 1,000+ “if, elif, else” statements to tell Python what to do with the numbers, right? After looking back at this question months later, the most challenging part is to come up with the idea of using cumulative probability sum to simulate the process. It is more doable now after the detailed step-by-step explanations. An online shopping website (e.g., Amazon, Alibaba, etc.) wants to test out two versions of banners that will appear on the website’s top. The engineering team assigns the probability of visiting version A at 0.6 and version B at 0.4. After 10,000 visits, there are 6050 visitors being exposed to version A, and 3950 people exposed to version B. What is the probability that there are 6050 cases when the randomization process is correct? In other words, the probability for version A is indeed 0.6. This is a part of a hypothesis-testing question. Let’s break it down. There are two versions, A and B, and the experiment assigns the treatment to 10,000 people. So, it is a perfect setting for the adoption of a binomial distribution. However, the probability of receiving version A is slightly higher than version B. The final answer should return the probability of more than 6050 people receiving version A out of 10,000 trials. These pieces of information remind us to combine a binomial distribution with a conditional for loop, as shown below. The result is close to 15%, which carries practical value. This is a hypothesis testing question. Since the probability of observing 6,000 or above visitors is 15%, we fail to reject the null hypothesis and conclude there is no statistical difference between 6,000 and 6,050 visitors out of 10,000. In other words, the probability for version A is 0.6. We are have learned hypothesis testing and how to reject or fail to reject the null hypothesis, but a question like this makes me think twice about statistical simulation. My medium blog has 500 visits per day, and the number of visits follows a Poisson distribution. Out of 1000 times, what is the ratio of more than 510 visits per day? Write a function to simulate the process. This is a rather straightforward question. Since the question asks for how many times an event occurs in a specified time, we can follow a Poisson procedure. 31.8% of the simulation results have more than 510 visits. After publishing this post, the number of Medium blog skyrockets to another level. Write a function to generate X samples from a normal distribution and plot the histogram. This is a question asked by Google. It is a relatively easy coding question with two steps: Here it goes. We generate a normal distribution with 100 numbers and set X equals to 10. The complete Python code is available on my Github. Medium recently evolved its Writer Partner Program, which supports ordinary writers like myself. If you are not a subscriber yet and sign up via the following link, I’ll receive a portion of the membership fees. leihua-ye.medium.com towardsdatascience.com towardsdatascience.com towardsdatascience.com Please find me on LinkedIn and Youtube. Also, check my other posts on Artificial Intelligence and Machine Learning.",29,2,6,Towards Data Science,2021-01-01,2021
https://towardsdatascience.com/data-science-trends-based-on-4-years-of-kaggle-surveys-60878d68551f,Data Science Trends Based on 4 Years of Kaggle Surveys,"How have diversity, salaries, and the tools and…",1,26,['Data Science Trends Based on 4 Years of Kaggle Surveys'],"Since 2017, the website Kaggle.com has performed an annual survey of data scientists and others interested in the field of data science and machine learning. The survey questions range from demographic questions, such as gender and level of higher education, to questions about programming languages, tools, and machine learning algorithms used. A full description, as well as the results, of each year’s survey can be found on Kaggle’s website for 2017, 2018, 2019, and 2020. Since it is the end of the year, and there are now four years of survey results, I thought it would be interesting to look at trends in the field over the past few years. New algorithms and techniques continue to be developed, so it will be interesting to see if newer techniques are replacing older ones, or simply being used alongside existing ones. The number of respondents has been consistently high, with between ~17,000 to 24,000 participants each year. Of those, approximately 2,400 to 4,100 survey respondents each year identify their job title as ‘data scientist’. On the left is a plot of the number of respondents, broken down by their reported job title. There are many respondents who have job titles other than data scientist. For the purposes of this article, I combined “business analysts” and “data analysts” into one category. For some reason, “machine learning (ML) engineer” only appeared as an option in the 2017 and 2020 surveys, so this category is missing in the 2018 and 2019 results. Before diving into the results, there were many questions that were only asked on one or two of the surveys, so I do not include that information here. The questions that were consistent across at least three of the four years of surveys were the demographic questions (such as age, job title, etc.) and questions about programming languages and machine learning algorithms actively used at work. Furthermore, the focus here is to look at trends in the practice of data science in industry, and, in fact, a lot of the questions in the Kaggle surveys are geared towards working professionals. Therefore, I focus on respondents who are employed in the field. In terms of people who are employed as data scientists, or in a job that involves work related to data science, the overwhelming majority are male. Over the past few years, there has been a slight improvement in the percent who are non-male, but except for data and business analysts, the percent of males in the field has remained solidly above 80%. The median total yearly compensation (i.e., salary plus bonus) has increased slightly for most job titles,¹ except for those with the job title “software engineer”, for whom salaries remained flat.² The final demographic trend I look at here is the formal education that data scientists have received. Interestingly, the fraction of data scientists with neither a PhD nor a Master’s degree has increased slightly, from 27% to 32%. There has been a proliferation of online courses in data science and machine learning, as well as bootcamps. Perhaps, the trend shown in these surveys is reflecting an increase in people coming into the field with this non-formal education. Moreover, this steady decrease in the fraction of workers with graduate-level degrees was true across the board for respondents with job titles other than ‘data scientist’. A common question for those entering the field of data science is which programming language they should learn. Looking at the programming languages used by data scientists, software engineers, and machine learning engineers, over 78% reported using Python at work across the four years of the survey. For data and business analysts, the fraction using Python has increased dramatically, from 61% to 87%. There is still a solid camp of data scientists who use R, however, the fraction of data scientists who are using R in the field is dropping rapidly. For both data scientists and data analysts, the percent of workers using R has dropped over 33 percentage points (from 64% to 23% for data scientists). In parallel to this question of what programming languages data science practitioners are actively using, Kaggle also asked respondents which one language they would most recommend to an aspiring data scientist to learn. Across the board, the percent recommending Python has increased, with R being the language seeing the largest corresponding decrease. Another interesting trend is the drop in the fraction of data scientists and data analysts using SQL, approximately 30 percentage points. Despite this, however, the fraction of data scientists recommending that aspiring data scientists learn SQL first has actually increased slightly (from 3% to 7%). For Machine Learning Engineers, the usage of different programming languages has remained flat, or even slightly decreased. This can either be due to Machine Learning Engineers becoming more focused on a single language and using less of a variety of languages, or there are other languages they are using more than some of the ones included here from the survey. Although, in terms of language Machine Learning Engineers recommend learning, the percent recommending Python increased slightly. For people with job titles other than the ones shown in the plots here, there are similar trends, in that Python usage has increased, while R usage has decreased. Finally, the heart of data science is the techniques used for analyzing data and making predictions. In some years of the surveys, there were questions about general techniques and how much time is spent on different parts of the data science workflow. However, one question that appeared in three out of the four surveys was specifically about the machine learning algorithms used (for some reason, this question did not appear in the 2018 survey). Perhaps reassuring to aspiring data scientists, the more “basic” methods of linear and logistic regression are still very popular among practitioners, with over 80% of data scientists saying that they use these methods at work. In general, the second most popular category of machine learning algorithms is decision trees and random forests. The popularity of these methods has remained steady, despite the increasing popularity of gradient boosting machines (such as, XGBoost). Another algorithm that has seen a significant increase in usage across the board is convolutional neural networks (CNNs), which are commonly used with image data. For each category of job title, usage of CNNs increased by 20 percentage points. For machine learning engineers, this method continues to be almost as popular as linear and logistic regression. While not quite as dramatically as the usage of CNNs, the usage of recurrent neural networks (RNNs), which are used for time series and sequence data (such as, sequences of words), has also increased. Interestingly, the usage of standard dense neural networks has actually decreased over the last four years. Perhaps, this is due to the increasing usage of more specialized neural networks, like CNNs and RNNs. One category of machine learning algorithms that is missing here from this survey is unsupervised learning methods, such as clustering, and dimensionality reduction techniques. It would be interesting to see trends in the usage of these techniques as well. After going through four years of Kaggle survey data, here are some significant trends in data science that may be worth further study: Hopefully, Kaggle will continue to perform this survey for years to come, so that we can continue to examine these trends, and pick up on some new trends, as the field of data science continues to evolve. [1] One note here is that I did not adjust for inflation, so any increase in salary may be less significant than it appears because of the effects of inflation. [2] I am only including salaries from respondents based in the U.S. because the 2017 survey asked for respondents to give their salaries in their native currency, whereas later surveys asked for respondents to give a reply in a USD range. Therefore, there are likely trends in the currency conversion over the four years covered here that could effect any trends seen in the data.",102,1,8,Towards Data Science,2021-01-01,2021
https://towardsdatascience.com/decision-trees-14a48b55f297,Decision Trees,An Overview of Classification and Regression Trees in Machine Learning,1,37,['Decision Trees'],"This post will serve as a high-level overview of decision trees. It will cover how decision trees train with recursive binary splitting and feature selection with “information gain” and “Gini Index”. I will also be tuning hyperparameters and pruning a decision tree for optimization. The two decision tree algorithms covered in this post are CART (Classification and Regression Trees) and ID3 (Iterative Dichotomiser 3). Decision trees are very popular for predictive modeling and perform both, classification and regression. Decision trees are highly interpretable and provide a foundation for more complex algorithms, e.g., random forest. The structure of a decision tree can be thought of as a Directed Acyclic Graph, a sequence of nodes where each edge is directed from earlier to later. This graph flows in one direction and no object can be a child of itself. Take a look at the DAG above, we can see it starts with a root node, the best attributes become interior nodes, i.e., decision nodes. Then, the internal nodes check for a condition and perform a decision, partitioning the sample space in two. The leaf nodes represent a classification, when the record reaches the leaf node, the algorithm will assign the label of the corresponding leaf. This process is referred to as recursive partitioning of the sample space. Terminology when working with decision trees: Decision trees use some cost function in order to choose the best split. We’re trying to find the best attribute/feature that performs the best at classifying the training data. This process is repeated until a leaf node is reached and therefore, is referred to as recursive binary splitting. When performing this procedure all values are lined up and the tree will test different splits and select the one returning the lowest cost, making this a greedy approach. Something to note, since the algorithm repeatedly partitions the data into smaller subsets, the final subsets (leaf nodes) consist of few or only one data points. This causes the algorithm to have low bias and high variance. A widely used metric with decision trees is entropy. Shannon’s Entropy, named after Claude Shannon provides us with measures of uncertainty. When it comes to data, entropy tells us how messy our data is. A high entropy value indicates less predictive power, think of the entropy of a feature as the amount of information in that feature. Decision trees work to maximize the purity of the classes when making splits, providing more clarity of the classes in the leaf nodes. The entropy is calculated before and after each split. If the entropy increases, another split will be tried or the branch of the tree will stop, i.e., the current tree has the lowest entropy. If the entropy decreases, the split will be kept. The formula for calculating entropy of an entire dataset: where 𝑛 is the number of groups and (𝑝𝑖) is the probability of belonging to the ith group. Let’s say we have a dataset containing 462 positive (1) labels and 438 negative (0) labels. We can calculate the entropy of the dataset by: Information gain uses entropy as a measure of impurity. It is the difference in entropy from before to after the split, and will give us a number to how much the uncertainty has decreased. It is also the key criterion used in the ID3 classification tree algorithm. To calculate the information gain: When performing classification tasks, the Gini index function is used. From Corrado Gini, this function informs us of how “pure” the leaf nodes in the tree are. The gini impurity will always be a value from 0 to 0.5, the higher the value, the more disordered the group is. To calculate the gini impurity: where (𝑝𝑖) is the probability of belonging to the ith group. The equation above states the gini impurity is 1 minus the sum of the different probabilities in each split. When decision trees train by performing recursive binary splitting, we can also set parameters for stopping the tree. The more complex decision trees are, the more prone they are to overfitting. We can prune the tree by trimming it using the hyperparameters: There are more parameters that can be changed, for a list and a more detailed explanation, take a look over the documentation. Let’s build a decision tree classifier with sklearn. I will be using The Titanic dataset, with the target being the Survived feature. The dataset I’m loading in has previously been cleaned. For a description on the features in the dataset, see the data dictionary below. Importing the necessary libraries Loading in and previewing the dataset Defining predictor and target features, performing train test split, and preprocessing data Training a decision tree classifier The decision tree classifier is performing better on the train set than the test set, indicating the model is overfit. Decision trees are prone to overfitting since the recursive binary splitting procedure will continue until a leaf node is reached, resulting in an overly complex model. This is where we would perform hyperparameter tuning and pruning to optimize the classifier. Plotting the tree It may be helpful to plot the tree in order to visually see the splits. We can plot the tree with a few extra libraries. Feature Importance If we want to check the feature importances of the model, we can use the .feature_importances_ attribute from the decision tree classifier. Feature importance is calculated using the gini importance. Optimizing a decision tree classifier with grid search cv By running a cross-validated grid search we can input a parameter dictionary containing different values for the decision tree hyperparameters. I have used the pruning hyperparameters mentioned above with the default 5 fold cross validation. By running the cross-validated grid search, the best parameters improved our bias-variance tradeoff. The first model with default parameters performed 20% better on the train set than the test set, indicating low-bias and high variance in the tree. The decision tree with the hyperparameters set from the grid search shows the variance was decreased with a 5% drop-off in accuracy from the train and test sets. Decision trees performing regression tasks also partition the sample place into smaller sets like with classification. The goal for regression trees is to recursively partition the sample space until a simple regression model can be fit to the cells. The leaf nodes in a regression tree are the cells of the partition. The simple regression models being fit to each partition take the mean of the dependent variable for the partition, i.e., the sample mean is used to make predictions. We used entropy above as a measure of impurity for performing classification. With regression, the CART algorithm utilizes mean squared error as a measure of impurity. When evaluated the performance of the model, we will be looking at the root mean squared error (RMSE). This is just the square root of the mean of squared errors. By taking the square root we can measure the size of error that weights large errors more than the mean. The metric we will use for evaluating the goodness of fit for our model is the R-squared value. The r-squared tells us the percentage of the variance in the dependent variables explain collectively (Frost et al., 2020). CART can be used for more than just regression. Here’s an interesting article from Neptune.ai where decision trees are used to detect outliers/anomalies in time series data. Let’s go ahead and build a decision tree regressor with sklearn. I will be using the Ames Housing dataset retrieved from kaggle. For the purposes of this tutorial I will only be using 3 continuous features and the target feature. Loading in the dataset, defining predictor and target features, and performing train test split Training a decision tree regressor Once again, the decision tree is overfitting to the train set. Similarly to classification, we can run a cross-validated grid search to optimize the decision tree. Optimizing Decision tree regressor with grid search cv By running the cross-validated grid search with the decision tree regressor, we improved the performance on the test set. The r-squared was overfitting to the data with the baseline decision tree regressor using the default parameters with an r-squared score of .9998. Using the parameters from the grid search, we increased the r-squared on the test set from .42 to .56. The train r-squared being .58 tells us the model is not overfitting to the training data and will perform similarly on the unseen test set. Decision trees are great predictive models that can be used for both classification and regression. They are highly interpretable and powerful for a plethora of machine learning problems. While there are many similarities between classification and regression tasks, it is important to understand different metrics used for each. The hyperparameters for decision trees are helpful in combating their tendency to overfit to the training data. Something to note, while performing a grid search can help in finding optimal hyperparameters for a decision tree, they can also be very computationally expensive. You may have a grid search running for hours or even days depending on the possible parameters chosen. I hope this post was helpful in gaining a better understanding to regression and classification trees. If there was anything left out or that I could’ve explained more clearly please feel free to leave feedback! Thanks so much for taking the time to check out the post.",19,0,10,Towards Data Science,2021-01-01,2021
https://towardsdatascience.com/art-of-the-graduate-school-essay-f59b14c79649,Art of the Graduate School Essay,5 simple tricks that got me into 12 data science masters programs,7,34,"['Art of the Graduate School Essay', 'Tip 1 | ”You’re Not Like The Other Girls”', 'Tip 2 | Keep It Chronological', 'Tip 3 | Kill Your Darlings', 'Tip 4 | Make An Entrance', 'Tip 5 | Be Honest.', 'Parting Thoughts']","Grad school SOPs: They can feel like such a beast. I wasn’t a perfect student when I was applying to M.S. in Data Science programs; in fact, I was far from it. I had never touched Python, never ran a machine learning algorithm, and hadn’t had a sexy internship at Google or Facebook to speak of. Having always pegged myself as the internationally-minded, writer, liberal arts-type, I had spent most of my undergrad taking French and Chinese, interning and studying abroad and even pursuing a taekwondo career (in full transparency, I was a math minor, but that was because I was a sucker for punishment more than anything else). Little to say: I did not come from a computer science or heavy statistics background before my master’s, so getting into all 12 programs I had applied to came as a complete shock to the system. This included UChicago, USC, UMich, UVA, Georgetown, Duke, and Tufts — and many of them had also offered me big scholarships to boot! Now that the dust has settled, I truly believe it was my writing skills that saved the day, giving me a platform to build out my life story and offer up reasons why I deserved a chance. With careful diction, I was able to navigate deficits, expand upon strengths, and provide a compelling narrative that showed upwards momentum and a willingness to learn. And taking a gamble on me would have paid off; since attending graduate school, I’m the student body president of my class and currently hold a 3.95 GPA with one semester left until graduation. This article is both for those who want a few solid writing tips and for those who want to get a data science degree but are nervous they don’t have the background. I’m here to tell you that 1) I believe in you, 2) I’m living proof you can do it, and 3) with a good enough narrative, the panel at your top data science program just might think you can do it, too! Be like a WW2-era sailor writing to his 11 girlfriends at home — don’t confuse names, recall what makes each of them special, and convey the feeling to them that they’re the only one you have eyes for. It doesn’t matter if Harvard knocked on your door tomorrow — Rutgers or bust, darn it. Who her is will vary based on the audience you’re writing to, of course. Within your essay it’s important to list reasons why the school you’re writing about makes them uniquely special for you. Do your research on this question — for me, I spent at least an hour studying the core curriculum, professor areas of expertise, and then calmly laid out how my own interests aligned with those specializations in my essays. I’ll link one of my own essays below, with the “your special to me” paragraph coming in the latter half of the second page: drive.google.com Additionally, if they ask what other schools you’re applying to in another section of the application, only list schools with similar master’s programs who they currently beat out in ranking. So if Stanford ranks higher than Brown and Columbia at data science, you can put them both on your Stanford application as other schools you’re thinking about, but don’t put Stanford on your Brown or Columbia applications. Accepting a student who doesn’t end up attending makes a school look worse, so they won’t accept a highly-qualified student if they think they’ll go someplace else. In gist: Don’t send out cookie-cutter essays; convince every school you apply for that they are best matched to your goals and interests (and implying that you’d definitely attend if they accepted you!). A chronological story is compelling because it is a simple way to convey structure and momentum. Or, more specifically, “this is what happened, this is what I’m doing now, and this is why your school is the clear next step in my plans”. You don’t need to be strictly chronological (you’ll see in my SOP, included above, that I am only chronological at some points in the story), but when you are trying to convey any sort of trajectory it is best to keep things orderly. For example, I used this tool when describing how my initial interest in data science was originally peaked, how I had since committed to that passion, and why graduate school was the clear next step for me. What I lacked in background I was able to make up for in positive momentum through a simple sequence of events. In gist: People have been telling stories for thousands of years; tap into the ancient art in order to maintain reader interest and keep your point from getting too topsy-turvy. Writing is as much destruction as it is creation, and every sentence in a graduate school essay must be indispensable to the overall story of you. This is hard because we tend to write rough drafts with a lot of fluff and disjointedness, and then get somewhat attached to parts of that fluff that aren’t pulling their weight. Stephen King calls this fluff “darlings”, and I’m convinced half of learning how to be a “good” writer is being able to let go of one’s own little darlings. “Kill your darlings, kill your darlings, even when it breaks your egocentric little scribbler’s heart, kill your darlings.” — Stephen King In gist: Don’t be cautious about cutting out words — or entire paragraphs — when you realize they have begun to distract from the main point. This is why it’s so great to have proof-readers; they aren’t as attached to the words like you are and can help clear the weeds to keep your message on track. If you don’t have anyone who can proofread, take a few days off between drafts and you’ll find yourself coming back to your story with a fresher, more objective lens each time. When I say you don’t always have to be chronological, this is what I mean. The first paragraph, specifically, should be exciting. No time for a slow build up here — bring the reader right into the thick of the action and make them take notice on what you’re about to say. This was my most commonly used intro paragraph to my graduate school essays. It wasn’t perfect, but I quickly laid out the situation, task, and action, leaving the next few paragraphs to explain the result. My pivot point for choosing to pursue data science came last April when my team competed at the University of Chicago Econometrics Games. Like a Hackathon for young econometricians, the games pitted students of economics from Cambridge to Santa Clara. The objective was to stage, and answer, a question of economic importance within fourteen hours. Having learned a handful of research techniques under Dr. Jane Doe of the Research Seminar in Quantitative Economics, I was nervous and excited for the opportunity to test my applied econometrics knowledge for the first time. Being an econ major, I was able to draw from an econometrics competition I had completed to begin explaining my initial interest in the field of data analytics. In gist: I’d encourage you to think of some event — as closely related to data science as you can — and use that to grab the reader’s attention as fast as possible. Similar to behavioral interviews, you can use the STAR method to structure your story if you’re finding yourself stuck. “If you’re embarrassed by anything, that’s a sign your not doing it right.” — Anonymous In the world today, it’s easy to run away with all kinds of inflation — grade inflation (schools), monetary inflation (government), resumé inflation (pretty much everyone). Everybody wants to look better than everybody else, and as a result everyone ends up over-exaggerating into oblivion. Honesty matters. It really does. Review panels know that the perfect candidate doesn’t exist, so if you go in trying to convince them that you are then they will become suspicious. It may not necessarily be as impressive to mention you’ve taken two online courses in Python instead of completely mastering it by age 5, but if you do say you are a Python expert without any substantial coursework or internships it will likely stir up suspicion about your honesty (and suddenly everything in your application becomes more up for debate). Of course, it’s important to emphasize your strengths and be confident in what you do know. To tell when I might have gone too far, I like to think back on a quote I heard a few years ago: “If you’re embarrassed by anything, that’s a sign your not doing it right.” (P.S. I can’t find this quote on the internet anywhere — if anyone figures out the original source, I’d love to give the author proper credit!) In gist: Be confident about what you know and honest about what you don’t. As an ex-liberal arts, current data science grad student on winter break, I love to stretch my writings legs whenever I get the chance. I hope some of these tips I used were helpful as you complete your own applications. towardsdatascience.com towardsdatascience.com Personally, going to graduate school has been the best decision I ever made, I absolutely love what I learn everyday and I am excited to work in the data science field going forward. That being said, this path isn’t for everyone and there’s many ways one can start their own data science journey — if a bootcamp or self-study works better for you, then by all means go for it! 😊 If you have any questions for me, feel free to reach me at my personal website, LinkedIn or Twitter, and of course you can give me a follow on Medium. Thanks again for reading!",66,7,8,Towards Data Science,2021-01-01,2021
https://towardsdatascience.com/sql-interview-prep-the-next-level-e329a67086d,SQL Interview Prep: The Next Level,Knowing the difference between an inner join and a left join is not…,8,85,"['SQL Interview Prep: The Next Level', 'Challenge #1: Percentage of total', 'Challenge #2: Pupcake Customer Acquisition', 'Challenge #3: Grouping credit cards by customer', 'Challenge #4: Select one address per customer', 'Challenge #5: Black Friday Sales — Part I', 'Challenge #6: Black Friday Sales — Part II', 'Go forth and interview awesomely!']","As someone who has sat on both Data Science and Data Engineering teams, I’ve also sat in on my fair share of interviews. Every interviewer has their own deal-breakers; mine is SQL. If you have conceptual issues with SQL, then that means you might wind up unintentionally pulling data incorrectly. Data Scientists need to pull data correctly in order to build train/test sets for their Machine Learning (ML) models. Data Engineers need to pull data correctly in order to provide reliable data sources for stakeholders. If a Data Engineer makes a mistake, that mistake can affect more than one ML model- it can propagate out to all of the Data Science, analytics, marketing, and finance teams that subscribe to that dataset. SQL skills are important. Most entry-level interviewees know the fundamentals of a SELECT * statement, GROUP BY, and all of the different types of JOINs, but they struggle to really conceptualize the data. Let’s go over a few real-world examples that will hopefully boost your SQL interview prep to the next level. Let’s say that it’s your first day working as a Data Scientist for an online retailer, Pets R Us. You have a table, customers, that keeps track of all customer records. Each customer record includes a customer ID, cust_id, and the creation date for that record, cre8_dt. Can you write a SQL query that calculates the percentage of customer records that were created prior to ‘2007–12–22'? Here’s the hack-ish way I would have done this on my first day as a Data Scientist. First I would have counted the total number of records in the table: Gives us: Then I would have counted the number of records that were created before ‘2007–12–22’: Gives us: Then I would have used the calculator on my phone to divide 3,400 by 1,210,000 and multiply by 100 to get 0.28%. Ha! Got it! But… don’t do it that way in an interview! Here’s a more concise way to do it: Gives us: The trick here is to first use a sub-query to create a flag for whether or not an account was created before or after our date threshold. Then, we can GROUP BY that flag in the main query. When we build the perc_of_tot column, the COUNT(*) counts the total number of accounts in each group (one group for cre8_flag = ‘before’, another group for cre8_flag = ‘after’). Since we don’t include a PARTITION BY after the OVER(), SUM(COUNT(*)) sums the counts over the entire result set, which in this case is made up of two values: COUNT(*) for the ‘before’ group and COUNT(*) for the ‘after’ group. It’s only been a week since you joined the Data Science team at Pets R Us, and your manager wants you to work on the new Pupcake acquisition campaign. She asks you to build a ML model that predicts the likelihood that an existing customer who has never bought Pupcakes will buy a Pupcake within the next four weeks. Let’s think about the data you need to pull in order to build your train/test sets for this model. First, you need to aggregate customer sales data over two time periods: (1) an outcome period (let’s say four weeks prior to today) and (2) a baseline period (let’s say twelve weeks prior to the outcome period). Assume that you managed to pull two datasets so far. You wrote each dataset to a table: What you really need is a list of customers who purchased at least one item from Pets R Us during the baseline period but did not purchase any Pupcakes during that time. Can you write a SQL query that generates such a list of customers, using the two tables above? Here’s the solution: This is a useful trick, and I’ve been asked to do this in an interview before! It’s actually called a LEFT ANTI JOIN in Scala and PySpark. The LEFT ANTI JOIN above will only return customers from the left-hand table (cust_basel) that do not exist in the right-hand table (cust_pupcakes_basel). Let’s go back to the Pets R Us customers table from our first example. We’ve already gone over the cust_id and cre8_dt columns. Each record also has a full_nm column and an email_adr column. Assume that this table is unique on cust_id (i.e. each cust_id appears only once). Here’s a sample of five rows from the table: In addition to the customers table, you also have access to the cards table, which stores credit card information. Each record has a card_id (internally generated by Pets R Us to ensure that each card has a unique ID), the cardholder’s full name (full_nm), card expiration date (exp_dt), the cust_id associated with the card, and the most recent date the card was used to make a Pets R Us purchase (last_purch_dt). First question (without inspecting the data): do you think the cards table is unique on cust_id? Answer: No, because each customer can have multiple credit cards. Here is a small sample of what the cards table looks like: Second question: can you write a SQL query that displays the total number of active credit cards associated with each cust_id, along with that customer’s full name and email address? Here’s what most entry-level candidates do: Most people begin by first using the cards table to count the number of cards for each cust_id. They then throw that into a sub-query and join it back to the customers table to get the full_nm and email_adr. Here’s a more concise way of doing it: Notice that we actually don’t need a sub-query here! Since we know that each cust_id only has one unique full_nm and email_adr associated with it, we can just do a JOIN to the cards table and GROUP BY cust_id, full_nm, and email_adr. However, if the customers table is not unique on cust_id, then the above query might produce duplicate cust_id’s. Follow-up question: how do you check if there are duplicate cust_id’s in the customers table? Answer: The main point here is that you can add extra columns to the GROUP BY clause even if you’re not necessarily grouping by them. In the above case, we’re technically only grouping by cust_id, while the full_nm and email_adr columns are just customer attributes. However, we can throw other columns into the GROUP BY as long as they are unique on cust_id. I have seen this discussed in a SQL interview before! Here’s another challenge: using the customers and cards tables, write a SQL query that displays each cust_id, their full_nm, and card_adr. First, before diving into the SQL, let’s inspect the data we have and think about it conceptually. Scroll back up to the previous section and take another look at a few rows from the cards table. It looks like Dwayne Johnson (cust_id = 2) has at least three credit cards, and each card has a slightly different address associated with it. So if some customers have multiple credit cards, and each card could theoretically have a different address associated with it (or even the same address formatted differently), how do we choose exactly one address for each customer? Let’s take a closer look at Dwayne’s three cards. One has expired while the other two are still active. The active cards have the same address but they are formatted differently. The expired card has a totally different address. It looks like perhaps Dwayne might have moved recently and the expired card reflects his old address. Solution: What we need to do first is PARTITION all of the records in the cards table by cust_id. This means that each customer has their own partition that includes only their own credit cards. We tell SQL how to order the cards within each partition: by last_purch_dt. The card with the most recent last_purch_dt will be ranked first. Once we rank all the cards, we select only the card that is ranked first for each customer. This way, we will be left with only one card (and one address) per customer. There are other ways to select one card per customer. For example, you could take exp_dt into account or use some kind of transactions table to determine the frequency with which each card has been used in the past month. The point here is to know that you should use some kind of PARTITION BY statement. Also, be sure not to over-think tasks in an interview setting; try to solve the problem without adding unnecessary complexity. You can always layer on top of your solution if the interviewer wants to follow-up. If a candidate is a SQL whiz but makes something simple unnecessarily complicated, that can still reflect poorly upon them. Your business stakeholders at Pets R Us would like to analyze the distribution of Black Friday sales per customer over the past 10 years. First, before we dive headfirst into SQL, let’s think about everything we need in order to fulfill this request. We need a table with all the Pets R Us transactions in it. Each transaction will be associated with a customer and a date. First we should filter the transactions to only those from Black Friday. Then we can aggregate sales by customer. You could probably just look up the dates for Black Friday from the past 10 years and hard-code them into your SQL query. However, in most cases it’s generally best to avoid hard-coding. Plus, this is an interview- so what would be the fun of that?? The Data Engineering team has provided the Data Science team with a dimensional date table, dim_date, to help with date filtering tasks such as this one. Here are a few sample rows from dim_date: Question: using dim_date, can you write a SQL query that generates a list of dates for all the Black Fridays between 2010 and 2020? Hint: use the rule that Thanksgiving occurs on the fourth Thursday of every November. Solution: The output should look like this: Follow-up question: what happens if we change order of the column names in the PARTITION BY clause? Answer: This will not change the output since the same records in dim_date get grouped together regardless of whether you sort them by year first, by month first, or by day-of-week first. However, the output could change if you have multiple columns in the ORDER BY clause and you change their order. Let’s say that we wrote the output of the Black Friday query to a table: blk_fri_dates. Now that we have a table with all of the dates we want to use for filtering transactions, let’s take a look at a few records from the transactions table so we can get a feel for the transactional data: Question: using the transactions table, the blk_fri_dates table, and any other Pets R Us tables that we have discussed above, sum up the total sales for each customer for each Black Friday between 2010 and 2020. Solution: First, we start with the transactions table. We INNER JOIN the transactions table to the blk_fri_dates table, since we only want to grab transactions that took place on Black Friday. We also have to INNER JOIN the transactions table to the cards table, since we need to look up which cust_id corresponds to which card_id for each transaction. Here are a few rows of sample output: Follow-up question: if there are duplicate rows in the blk_fri_dates table, what happens to the above output? Answer: imagine that the blk_fri_dates table erroneously has two rows for the year 2020: In this case, if we INNER JOIN this table to transactions, each Black Friday transaction from 2020 will be double counted. Follow-up question: if blk_fri_dates has duplicate rows, how can you adjust the above SQL query so that you still get the correct Black Friday sales per customer? Answer: Bonus: we can further optimize this query by using a LEFT SEMI JOIN on the blk_fri_dates table instead of an INNER JOIN: The LEFT SEMI JOIN above will select only the transactions from transactions (left-hand table) where there are one or more matches on the id_date column in blk_fri_dates (right-hand table). The great thing about LEFT SEMI JOINs is that if there are duplicate rows in blk_fri_dates, the LEFT SEMI JOIN won’t duplicate transactions like an INNER JOIN will. A LEFT SEMI JOIN usually performs faster than an INNER JOIN because it can only return columns from the left-hand table. Since we don’t need to select any columns from blk_fri_dates in this query, the LEFT SEMI JOIN is a great choice. Consider a LEFT SEMI JOIN the next time you need to do some filtering! Let’s revisit the above query that aggregates total sales for each customer for each Black Friday between 2010 and 2020. Think about what your business stakeholders originally asked for: “the distribution of Black Friday sales per customer over the past 10 years.” Is the solution above thoroughly sufficient for this purpose? Hint: what happens if a customer has associated transactions in the transactions table for Black Fridays 2010-2014 and Black Fridays 2016–2020, but not Black Friday 2015? Answer: if a customer (let’s say cust_id=20) did not make any transactions on Black Friday 2015, then the output rows for total Black Friday sales for that customer will look something like this: Notice how there is simply no record for Black Friday 2015? What we really want is something that looks like this: When you get a question like this in an interview, it is almost certainly a LEFT JOIN question. The aggregated Black Friday sales output will go on the right-hand side. But what should go on the left-hand side? On the left-hand side, we need every possible combination of customer and Black Friday. In other words, we need to write a SQL query that generates every possible cust_id, black_friday pair. First, generate output with every unique customer: Second, generate output with every unique Black Friday between 2010–2020: Third, multiply them together with a CROSS JOIN: Note: remember that CROSS JOINs are very computationally expensive, so use with caution. Question: using the query we just wrote to generate every possible unique customer, Black Friday pair, and the query we wrote in the previous section to aggregate total Black Friday sales for each customer, write a SQL query that outputs total Black Friday sales for each year for each customer. If a customer made no Black Friday transactions for any year, then they should show up as $0.00 for that year. I used a WITH clause to build a temporary table since I didn’t want my SQL query to get messy with too many sub-queries. Notice that I selected the cust_id and black_friday columns from the left-hand table. This is important: if you accidentally select cust_id or black_friday from the right-hand table, you could get nulls, which negates the purpose of the LEFT JOIN. I used a COALESCE to replace any nulls from the LEFT JOIN with a 0.0. Also note that I like to add qualitative comments at the top of each sub-query to keep track of what my code is doing. I have seen people use two, three, or more levels of sub-queries without including any comments. I find this hard to read. I recommend including comments in order to facilitate collaboration and make it easier for team members to review your code. Let’s take a step back after all of that work. There’s actually one more thing that we need to account for before we throw a bunch of histograms together and send them to our stakeholders. Hint: take a closer look at a few customer records: Only Karen Gillan’s Pets R Us account was in existence for every Black Friday between 2010–2020. Dwayne Johnson’s account wasn’t created until after Black Friday 2020. Should Dwayne Johnson’s zero Black Friday sales be included in the histogram for 2010? For 2015? For 2020? No, no, and no. Should Awkwafina’s zero Black Friday sales be included in the histogram for 2007? No. For 2008? Yes. In this article, we won’t discuss specifically how to address the above consideration, but think about how you would approach the problem, either programmatically or with SQL. There are multiple correct approaches to this! If you think of multiple approaches, which approach has the best performance? If you think of just one approach, how can you optimize it? This could be a great qualitative discussion towards the end of an interview. As you can see, pulling data can get tricky very quickly. It’s extremely important to think qualitatively about the task at hand and to anticipate how to maneuver around any eccentricities or known flaws in your data. One last piece of advice: KNOW YOUR DATA. — every Data Engineer I have ever met If you’re in an interview, don’t be afraid to ask questions about the dataset at hand. It shows an interviewer that you’re detail-oriented, thoughtful, and less likely to make mistakes on the job. The above examples are variations of actual interview questions that I have seen plenty of interviewees struggle with. Sometimes my team really likes a candidate, but their SQL is so undeveloped that we worry it would take them too long to get up to speed. Also, be sure to ruthlessly demonstrate a positive attitude. That means that even when things get tough, and you think you couldn’t be performing any worse, or you think the interview questions are totally unfair, stay positive! As an interviewer, I’m not just looking for the right answer. I’m looking for someone who is professional, collaborative, and quite frankly, nice. Who would you want to hire to join your team: Bravestone or Van Pelt?? I hope that this article was helpful in boosting you to the NEXT LEVEL in your SQL interview prep. Go forth and show your next interviewer that you’re ready to join their team. Good luck!",44,3,18,Towards Data Science,2021-01-01,2021
https://towardsdatascience.com/the-ultimate-guide-to-counterfactual-explanations-for-classification-models-e9ee8ed90cfd,The Ultimate Guide to Counterfactual Explanations for Classification Models,Most Human Interpretable…,4,24,"['The Ultimate Guide to Counterfactual Explanations for Classification Models', 'Intuition', 'Illustration', 'Conclusion']","Let’s say Paul applies for a loan at ABC Bank. He receives a mail from the bank expressing deep regret to inform him that his loan application has been rejected. Rejected! After being shattered for a while, Paul may be curious to know what happened. So he goes to the bank and asks for the person who takes decisions on loan applications. He is directed to Jane, who takes the decisions on loan applications. Paul asks Jane why his application got rejected. She tells him his asking amount is too high, and his salary is less than minimum, and so on. But then, he asks her what could he have done to get his loan application approved. She tells him his asking amount needs to be reduced by so much, increase his salary by so much, come back after certain years, so on. But come on! Paul can’t suddenly change his salary right! Neither can he wait for years for the loan. So he asks Jane what is the smallest change in his application he can do to get his loan approved. If Jane were replaced by an AI model, what the model would give Paul is called the Counterfactual Explanation. Counterfactual explanations provide the smallest change in the input feature values required to change the output of an instance to a predetermined/desired output However, as in Paul’s case, not all features can be changed. So a requirement or constraint on Counterfactual explanations is that they perturb as few features as possible to obtain the desired output. There are multiple ways of determining counterfactual explanations proposed by multiple researchers. If you are interested in understanding the difference between each of them, you can take a look at this book by Christoph Molnar. However, a simple method to achieve this is as follows: Caveat: The thing with counterfactual explanations is that it is applicable only to supervised classification tasks. It is not amenable to regression or unsupervised tasks. It is important to know this To illustrate the use of Counterfactual Explanations, I will be using an illustrated example from the alibi library. The example explains a shallow Neural Network model on the Boston House Pricing dataset. The code for the illustration can be found on the alibi documentation page. Since the Boston House Dataset is present in the sklearn library, the dataset is loaded from sklearn directly. Housing dataset is a regression dataset, on which counterfactuals are not effective. Hence we need to convert it into a classification type of dataset. We choose the median of the target value, which is the house price. Any data point (house in the set of houses) with a price less than median is labelled 0 and above the median is labelled 1. The variant of counterfactual explanation used here is the one that is guided by prototypes. Here, prototypes are counterfactual examples (data points). They are built by building k-d trees or encoders, so that counterfactual explanations can be built fast. It is built based on this paper, which you can refer to for more details on how the algorithm works. A custom neural network model is built for this illustration: Now that we have our model, we want to understand a particular outcome. A particular test instance is chosen: The original label of this data point is 0, which indicates that the price of this house is below median price. Now let us run the counterfactual on it (CounterFactualProto calls counterfactual explanations by prototypes). Let us print out the original and the explained outcomes (both should be different or contrastive to each other). We can determine how much of which value has changed between the original feature set and the explanation: So this says that for the price to be above median, this house needed to have an age of 6.5 years lesser than it currently has, and an LSTAT value of 4.8 units less than it currently has. Counterfactuals are the most natural way of explaining model behaviour to humans. However, it has certain limitations, the most important one of which is that it only applies to classification problems. Another problem is that sometimes it provides explanations which, practically, cannot be fulfilled to reverse the decision. For example, age cannot be reversed. So we try to run counterfactuals by imposing that age cannot be changed. Note that there can be multiple counterfactual explanations for a single datapoint, since there can be many ways to reach the decision boundary. We can choose the one that respects our business constraints of any other practical constraints. Having said this, it is a powerful arrow in your XAI quiver!",16,0,5,Towards Data Science,2021-01-01,2021
https://towardsdatascience.com/rewriting-sql-queries-with-pandas-ac08d9f054ec,Rewriting SQL Queries with Pandas,Practical guide for both SQL and Pandas,1,31,['Rewriting SQL Queries with Pandas'],"Pandas is a Python library for data analysis and manipulation. SQL is a programming language that is used to handle relational databases. What they have in common is that both Pandas and SQL operate on tabular data (i.e. tables consist of rows and columns). Since both Pandas and SQL deal with tabular data, similar operations or queries can be completed using either one. In this article, we will rewrite SQL queries with Pandas syntax. Thus, it will be a practical guide for both of them. I have an SQL table and a Pandas dataframe that contains 15 rows and 4 columns. Let’s start with displaying the first 5 rows. We have some data about items sold at different retail stores. In the following examples, I will write down a query task and complete it with both SQL and Pandas. Task: Find the average price of items for each store. We need to group the prices based on store id column and calculate the average value for each store. In SQL, we apply the aggregate function (avg) while selecting the column. The group by clause groups the prices based on the categories in the store id column. In Pandas, we first use the groupby function and then apply the aggregation. Task: Modify the result in the previous example by renaming the price column as “average_price” and sorting the stores based on average price. In SQL, we will use the AS keyword for renaming and add the order by clause at the end to sort the results. In Pandas, there are many options for renaming the price column. I will be using the named agg method. The sort_values function is used to sort the results. Task: Find all the items whose store id is 3. We just need to filter the rows based on store id column. In SQL, it is done by using the where clause. It is also a pretty simple operation in Pandas. Task: Find the most expensive item sold at each store. This task involves both group by and aggregation. In SQL, the max function is applied to the price column and the values are grouped by the store id. In Pandas, we first group the selected columns by the store id and then apply the max function. Task: Find all the items that contains the word ‘egg’. This task involves filtering but different than the ones we have done in the previous example. In SQL, we will use the like keyword with the where clause. The ‘%egg%’ notation indicates that we want every description that involves the ‘egg’ character sequence in it. In Pandas, we will use the contains function of the str accessor. Task: Find all the items that contains the word “liter” in the description and more expensive than 2 dollars. It is similar to the task in the previous example with one additional condition. In SQL, we can place multiple conditions in the where clause. In Pandas, we can apply multiple filtering conditions as below: Task: Find all the items whose description starts with ‘ice’. This is another text based filtering. In SQL, we can either use the like operator or the left function to compare the first three characters of the description with ‘ice’. In Pandas, we can use the startswith function of the str accessor. Both Pandas and SQL are popular tools used in the field of data science. They are proven to be efficient and practical. If you are working or plan to work in data science domain, I strongly suggest to learn both. We have done some basic queries to retrieve data using both Pandas and SQL. It is a good practice to compare them by doing the same operations. It will improve your practical skills as well as the understanding of the syntax. Thank you for reading. Please let me know if you have any feedback.",111,0,6,Towards Data Science,2021-01-01,2021
https://towardsdatascience.com/probability-theory-for-data-scientists-fcfa7be05291,Probability Theory for Data Scientists,"Probability, conditional probability, and joint probability",1,44,['Probability Theory for Data Scientists'],"Life is full of surprises so we are never sure how things will turn out. However, we can make guesses based on previous experience or logic. Depending on the characteristics of an event, we might be very successful at our guesses. In math, we do not make guesses. Instead, we calculate probabilities. Life is also full of surprises for math so having a calculated probability value does not guarantee the correct answer. However, we will know that our guesses are based on math, objective, and not biased. The probability theory is of great importance for data science. One needs to possess a comprehensive understanding of the probability theory to be a well-performing data scientist. For instance, probability distributions play a key role in predictive analytics. Probability simply means the likelihood of an event to occur and always takes a value between 0 and 1 (0 and 1 inclusive). The probability of event A is denoted as p(A) and calculated as the number of the desired outcome divided by the number of all outcomes. For example, when you roll a die, the probability of getting a number less than three is 2 / 6. The number of desired outcomes is 2 (1 and 2); the number of total outcomes is 6. I have box with 1 blue and 4 yellow balls in it. If I randomly pick a ball from this box, it will likely to be yellow. The probability of picking a yellow ball, p(yellow), is 4 / 5 which is equal to 0.8 (or 80%). The number of desired outcome is 4 and the number of total outcomes is 5. When someone asks you if you think it will rain, your first reaction is usually to look at the sky. If there are dark clouds, your are more likely to answer “yes”. You check the conditions before giving an answer. We observe a similar logic behind the idea of conditional probability. The probability of event A given that event B has occurred is denoted as p(A|B). Conditional probability is the likelihood of an event A to occur given that another event that has a relation with event A has already occurred. The formula of the conditional probability is given below: P(A ∩ B) is the probability that both events A and B occur. P(B) is the probability that event B occurs. Let’s go over an example to comprehend the idea of conditional probability. In the image below, we see a probability space (Ω) which indicates all the probabilities add up to 1. The unconditional probability of event A, P(A) = 0.1 + 0.3 + 0.12 = 0.52 The conditional probability of event A given that B2 occurred, P(A | B2), P(A | B2) = P(A ∩ B2) / P(B2) = 0.12 / 0.16 = 0.75 Given that B2 occurred, the probability of event A increases. Conditional probability is a fundamental concept in probability theory and statistics. For instance, Bayesian statistics arises from an interpretation of the conditional probability. In machine learning, Naive Bayes Algorithm is based on the Bayes’ theorem and thus the conditional probability. There are some important points to emphasize about the conditional probability. In other words, if two events are independent of each other, the conditional probability of A given B is equal to the probability of A. It comes from a property of the joint probability. Joint probability is the probability of two events occurring together. If two events are independent, the joint probability is calculated by multiplying the probabilities of each event. P(A ∩ B) = P(A) * P(B) If we put that in the equation of the conditional probability: For instance, we have calculated P(A | B2) as 0.75 earlier. Let’s also calculate P(B2 | A) and compare the results. P(A | B2) = P(A ∩ B2) / P(B2) = 0.12 / 0.16 = 0.75 P(B2 | A) = P(B2 ∩ A) / P(A) = 0.12 / 0.52 = 0.23 Let’s do one more example to finish up. The following table shows the number of female and male students enrolled in the sociology and music classes. There are 103 students. We will first calculate the unconditional probabilities. P(Female) = 52 / 103 = 0.505 P(Male) = 51 / 103 = 0.495 P(Music) = 47 / 103 = 0.456 P(Sociology) = 56 / 103 = 0.544 P(Female) means the probability that a student is female. The joint probabilities can be calculated by dividing the number in a cell by the total number of students. For instance, the probability that a student is female and enrolled in the sociology class: P(Female ∩ Sociology) = 24 / 103 = 0.233 We can calculate the other joint probabilities similarly. The following table contains all the probabilities for these events. We will calculate the conditional probabilities now. P(Female | Music) = P(Female ∩ Music) / P(Music) = 0.272 / 0.456 = 0.596 P(Male | Sociology) = P(Male ∩ Sociology) / P(Sociology) = 0.311 / 0.544 = 0.572 Uncertainty is ubiquitous in our lives. Thus, any field in science needs to handle uncertainty both practically and theoretically. The probability theory is crucially important in the field of data science as well. Since we cannot eliminate uncertainty, we need proactive ways to efficiently handle it. For instance, probability distributions of variables play a key role in predictive analytics. One needs to possess a comprehensive understanding of the probability theory to be a good data scientist. Thank you for reading. Please let me know if you have any feedback.",84,1,6,Towards Data Science,2021-01-01,2021
https://towardsdatascience.com/recommendation-systems-via-matrix-factorization-28e7d0aa8ca7,Recommendation Systems via Matrix Factorization,Exploring the MovieLens 100k dataset with SGD…,5,18,"['Recommendation Systems via Matrix Factorization', 'Problem 1: Simple Baseline Model with SGD and Autograd', 'Problem 2: One-Scalar-Per-Item Baseline with SGD and Autograd', 'Problem 3: One-Vector-Per-Item Collaborative Filtering with SGD and Autograd', 'Problem 4: Open-Ended Recommendation Challenge']","By Gavin Smith and XuanKhanh Nguyen This project was the third project for my machine learning class this semester. The project aims to train a machine learning algorithm using MovieLens 100k dataset for movie recommendation by optimizing the model's predictive power. We were given a clean preprocessed version of the MovieLens 100k dataset with 943 users' ratings of 1682 movies. The input to our prediction system is a (user id, movie id) pair. Our predictor's output will be a scalar rating y in range (1,5) — a rating of 1 is the worst possible, a rating of 5 is the best. Our main task is to predict the ratings of all user-movie pairs. The recommendation system is performed using four different models. For all problems, our tasks are to obtain the best possible prediction results on the validation set and the test set regarding Mean Absolute Error (MAE). Methodologies are explained in all sections, along with respective figures. Figure 1a shows how the mean absolute error for the simple baseline model, which only predicts a value mu for each example, changes over time. The left plot, which has a batch size of 10000, converges to an MAE value steadily because its batch size is large, whereas the right plot, which has a smaller batch size of 100, starts to converge to a value more erratically. We choose n_epoch=10 for this problem. For batch_size=10000, we start seeing some learning after 4 epochs; MAE starts to approach a constant value while the number of epochs keeps increasing. For batch_size=100, after 4 epochs, the data is less noisy. The line that could compute the optimal mu value would be “print(ag_np.mean(ag_np.asarray(train_tuple[2])))” which would compute the mean of all of the examples. This computation produces a value of 3.58, which is the same value as our model's mu value. This is true because if we are predicting the same value for every example, the mean of the scores will be “most” correct “most” of the time. Figure 2a shows show the mean absolute error of the scalar pattern model changes over time with stochastic gradient descent. The figure on the left shows how the error changes when the batch size is 10000, and since the batch size is larger the error gets lower at a steadier rate before settling around .75. The right graph shows the same process, but with a batch size of 100, so the error gets around .75 much faster, but its value is more unpredictable because the batch size is smaller. We choose n_epoch=250 for this problem. For batch_size=10000, we start seeing some learning after 200 epochs; MAE starts to approach a constant value while the number of epochs keeps increasing. For batch_size=100, after 200 epochs, the data is less noisy. To choose the step size, we first pick a large number for step size (step_size=1.5), and we see that the training plot diverging. So, we decrease the step size by 0.25. Eventually, we experience that step_size less than or equal to .75 doesn’t make training loss diverge. For this problem, we choose a step size is equal to 0.75. Table 2b shows each of the movies from the selected list and its learned per-movie rating adjustment parameter cj in order. We pick the model with the best MAE on the validation set, where the batch size is 10000, for 250 epochs, and the step size is 0.75. The list result is showed from this model. From this list, we can see that movies with a larger positive cj­ value tend to be more universally “lauded” movies, or “classics”, such as Star Wars or Indiana Jones, and movies with a lower value tend to be less so, like Jurassic Park 2 or Scream 2, these are horror movies and do not always appeal to a wide audience. In our problem, the bias of a movie would describe how well this movie is rated compared to the average across all movies. This value depends only on the movie and does not take into account the interaction between a user and the movie. Therefore, for a movie to be large and positive, it means the movie is likely to be rated highly by people, and a large negative value means that the movie is likely to be rated low. This figure shows how the one-vector-per-item collaborative filter model mean absolute error varies over time, with each graph showing the model with a different number of dimensions, K, that are used to represent users and movies. For all the graphs we can see that when only a short amount of time has passed, the models are underfitting the data as both the training and validation set errors are very high. All the models then start to overfit after 250 epochs, as can be seen by the fact that the training set error decreases, but the validation set error starts to increase again. As K increases, we see that the validation set error has a more defined dip around 350 epochs, and the model overfits much faster with larger K values. This figure shows how the mean absolute error changes with the number of epochs for our model with an alpha value of 0.1. We chose this alpha value by training models with many different alpha values using a grid search, and we found that an alpha value of 0.1 reduced the mean absolute error the most. With this alpha value, we were able to reduce the mean absolute error to a lower value than the model, which was trained with an alpha value of 0. For this problem, the batch size is fixed to 1000. We do early stopping to find the parameters that perform best on the validation set. 0.75 is the largest step size we can get that doesn’t make training loss diverge. To find the parameters that perform best on the validation set, we used early stopping. This table shows the mean absolute error for each of the models we trained in questions 1 through 3 on the validation sets and the test set. To determine the best version of each model, we tried to minimize the error on the validation set. For M1, this is easy because we are just choosing a fixed variable to make each guess, so there is only one real optimal value. For M2, we chose the model which used a batch size of 10000 because it minimized the mean squared error on the validation set. We used early stopping for each of the M3 models; we searched over different alpha values, batch sizes, and step sizes to determine which version of the model produced the smallest mean absolute error. We recommend using a K value of 50 because it had the smallest error. It could be beneficial to try models with a larger K value because it seems to be that with larger K values, the error decreases. The best model we found is M3 with a K value of 50 and an alpha value of 0.1; this model reduces both the validation set and test set errors. Additionally, an L2 penalty was added to M3 models. Setting L2 regularization on vector u and v force the values to be as small as possible. It can help us avoid overfitting. This figure shows a two-dimensional embedding of the learned per-movie vector vj for the movies in the select_movies data set. This was created using the best M3 model with K=2 factors. In this graph, movies are placed based on their factor vector. We can see that some movies that are similar tend to be grouped together; however, the grouping is not super obvious to us. In the lower right, we can see horror movies like Scream and Nightmare on Elm Street. We notice that movies with similar ratings will come out closer in the embedding space. One reason to explain this is our M3 model has learned that those movies are associated with a similar rating. To check our observation, we calculate the average rating from ratings_all_development_set dataset. The average rating for Sleepless in Seattle is 3.55, while the average rating for While you were sleeping is 3.56. And these two movies are placed close in the embedding space. For our open-ended recommendation model, we chose to use the KNNWithMeans classifier from the surprise package. KNNWithMeans works the same as the regular K nearest neighbors’ algorithm, where it calculates the similarity between K points and returns the prediction that is most in line with those points. KNNWithMeans differs in that you must specify a minimum and maximum K value because the algorithm only looks at points where the similarity is positive. After all, it would not make sense to use points that are negatively correlated. Also, this model considers the mean ratings of each user, which helps normalize the data. We chose to use this model because a good way of recommending movies to someone is to give them choices which are the most like movies they already like, and since KNN works by predicting based on the similarity between data points, we thought it would be the best choice. Another reason we also chose to use this model is that other options like SVD took significantly longer to train, so we would have less time to do in-depth hyperparameter searching on those models. To train our model, we used a grid search to find optimal hyperparameter values along with 5-fold cross-validation to validate our model. The hyperparameters we searched over were K, the maximum number of points to compare to, min_k, the minimum number of points to compare to, and sim_option, which controls how to compute the similarity between points. We found that a K value of 50, a min_k of 2, and the Pearson similarity option were the best. We determined that these were the best by choosing the values which minimized the mean absolute error on the validation sets of the cross-fold validation. This figure shows the three hyperparameters we tuned for our KNNWithMeans model. The left-most graph shows how the mean absolute error changes with an increase in K, the number of points which are compared to. As the number increases, the model starts to overfit less, as both training and validation error decrease. For a K value of 200, the model starts to underfit on the data because the prediction becomes more reliant on data points that are not similar to the point being predicted. From the second graph, we can see that any increase in the minimum number of neighbors to take into account increases the error, so the optimal value should be 1. Last, the right graph shows that the similarity option has almost no influence on our model’s performance, but the Pearson option has a slight edge over the other two. This figure shows the mean absolute error of our model on the held-out data set used for the leaderboard, as well as its performance on the validation set and test set when training our model. Table 4c shows that the mean absolute error of our model on the held-out data set is lower than the error on our cross-fold validation set. The reason for this is because the held-out data set has a smaller size (10000 ratings) compared to our training data (89992 ratings). There may be a case that the testing data is not a good representative of our training data. Therefore, it behaves well and gives a low error. When comparing this data to our models' results in table 3c, we can see that our leaderboard mean absolute error is the lowest and that our test error is higher than any of the M3 errors. The reason for this is because SVD decomposes the original matrix, the sense matrix is used for predicting the rating (use, item) pair. While in KNN, the prediction is made by finding a cluster of similar users to the input user whose rating is to be predicted and then take the average of those ratings for prediction. SVD does a better job in learning the training data; we see a smaller error on the validation set. One limitation that we faced was that more complex models like SVD took far too long for us to be able to tune the model effectively; if we had access to better computing power it would be more feasible to use models like this. Another thing that could be improved upon is looking at different methods of recording loss; when training our model, we only used mean absolute error, but it could be that the model performs better when very incorrect guesses are weighted more heavily like with mean squared error. Additionally, we would want to consider other user features such as age, gender, nationality, spoken language etc., and the item features like the main actors, the duration of the movie, spoken language, etc. Still considering user and item, we would try to model the fact that people who speak a certain language are more likely to view movies in that language or that may be older users are more likely to watch classic movies. To make a prediction, we look at the user profile and predict the rating.",86,0,11,Towards Data Science,2021-01-01,2021
https://towardsdatascience.com/deep-neural-networks-are-biased-at-initialisation-towards-simple-functions-a63487edcb99,"Deep Neural Networks are biased, at initialisation, towards simple functions",And why is this a very…,7,45,"['Deep Neural Networks are biased, at initialisation, towards simple functions', 'Classification problem on MNIST', 'Is there any reason to expect this?', 'But is this a good thing for learning?', 'So do neural networks work like this?', 'Conclusion', 'References']","This is a follow-up post to Neural networks are fundamentally Bayesian, which presented a candidate theory of generalisation from [1], based on ideas in [2,3]. The reading order is not too important — this post predominately summarises material in [2.3], which study why neural networks are biased at initialisation towards simple functions (with low Kolmogorov complexity). This work motivated results in [1], which explained how this bias at initialisation translates into good generalisation in the real world, and thus why neural networks enjoy the success they do! Thus, this post is best thought of as a prequel. We begin by asking the following question: What functions do neural networks express before being trained? More specifically, what is the probability that a neural network will express a function f after being randomly initialised? We will call this quantity P(f),¹ and take the random initialisation to be i.i.d. Gaussian (although [2] suggests that P(f) is not too sensitive to the type of initialisation). In general, changing one parameter in a neural network by a small amount will affect the raw output of the network— so in general, there is a 1–1 correspondence between the parameters in a neural network and the function it expresses². However, for many problems, we are not interested in the raw output of the network. This is best explained with an example problem. ¹Note P(f) is denoted Pᵦ ( f ) in Neural networks are fundamentally Bayesian.²If we take the domain to be the entire vector space over which the network is defined. Consider the problem of correctly classifying images of the handwritten digits 0–9 (the MNIST dataset). We are clearly not interested in the raw output of the network — we only care about its final (discrete) decision. Let us imagine, for simplicity, that we want to only classify the images as even or odd numbers. This can be done with a neural network with a single output neuron, and thresholding at zero — a positive output implies an even number, a negative output implies an odd number. So, if we consider a subset of m images in MNIST, which we call S, then the network N models a function f : S → {0, 1}ᵐ, where 1 corresponds to even and 0 to odd. This is because ultimately we (mostly) care about the post-thresholded output of the network — not the raw outputs — and in which case, small changes to parameters in N may not change the function expressed (i.e. the classification will not change). The notation {0, 1}ᵐ denotes that m images are being mapped either 0 or 1. We can then ask, what is P(f) for these functions? See Figures 1 and 2 below for two visualisations of P(f), for the system discussed above. P(f) was calculated by sampling from 10⁷ different random initialisations of a 2-hidden layer fully connected network, using 100 images from MNIST. It is clear from Figures 1 and 2 that there is a huge range in P(f). Results in [2] suggest that the range for the above problem is over 30 orders of magnitude. But why does this matter? It matters because, for a set of 100 images, there are 2¹⁰⁰ ≈ 10³⁰ different possible functions (i.e. possible ways of classifying each image as even or odd). Without information about N, we might assume that each function is equally likely, meaning P(f) ≈ 10⁻³⁰ (this would be the case where images are classified by unbiased coin flip). Given that P(f) can be as large as 0.05, neural networks are clearly not unbiased at initialisation. Instead, there is a strong bias towards certain types of functions at initialisation — before any training has taken place. There is a theorem originally due to Levin, and repurposed for input-output maps [4] which when used in the context of neural networks [3] states the following: For the map from the parameters of a neural network N to the function expressed by N, the following result holds: P(f) ≤ 2⁻ᴷ⁽ ᶠ ⁾⁺ᴼ⁽¹⁾, where K(f) is the Kolmogorov complexity of the function f and the O(1) terms are independent of f but dependent on N. There are some further conditions that need to be satisfied for the bound to hold for neural networks, but empirical evidence [3] plus a few theoretical results [2] indicate that it does, and is non-vacuous. In essence, this says that complex functions will have low P(f), and simple functions can have large P(f), if the bound is tight. However, Kolmogorov complexity is uncomputable — so proving this for general architectures and datasets would be, at best, non-trivial. Instead, a very clever experiment in [3] allows us to empirically test this upper bound. The results of this experiment are shown in Figure 3, where a fully connected network models functions of the form: f : {0,1}ⁿ→{0,1}, chosen because a suitable complexity measure exists — see [3] for details. Evidently, P(f) is exponentially larger for simple f. There are functions that lie below the bound, but it is argued in [7] that (very informally) there is a limit on the number of functions that can lie beyond a certain distance from the bound. We call this a simplicity bias — because P(f) is higher for simple functions. There is substantial further evidence from [1,2,3] that this simplicity bias is a general property of neural networks of different architectures and datasets³. For example, similar experiments were performed on MNIST and Cifar10 [1,3] where the CSR complexity measure was used to approximate K(f). There is also an analytic result that perceptrons acting on the boolean hypercube are biased towards low-entropy (and thus simple) functions, in [2]. In summary: Also note that functions with large P(f) have greater ‘volumes’ in parameter-space (see [1,2] for details). This is intuitively obvious — if you are more likely to randomly sample some function, it must have more associated parameters, and thus a greater volume in parameter-space. ³Bear in mind that the function is defined relative to a dataset, as it specifies its domain and co-domain. It is thought [5] that real-world data has an underlying simple description. For example, when we read handwritten digits we do not worry too much about precise details — if it’s a single oval-like shape, it’s probably a zero. We don’t need to take the exact pixel value of every pixel into account. If real-world data that we are interested in learning really does have a simple underlying description, then simple functions will generalise better than complex functions. Consider a supervised learning problem — a training set S from MNIST, containing m examples. Then, an ideal learning agent would be able to calculate the Kolmogorov complexity of all functions⁴ from the images of the digits (i.e. from the pixel values) to the classifications of the digits. It would then throw out all functions that did not correctly predict all m examples in S. Finally, of these functions, it would choose the one with the lowest Kolmogorov complexity. In other words, an ideal learning agent would choose the simplest function that fits the training data. ⁴Defined relative to some UTM (see [3] for details). At this point, if you have read Neural networks are fundamentally Bayesian, you can stop reading, as you already know the answer! If you haven’t, then please check it out, as it: Thus, neural networks generalise well because P(f) is much larger for simple functions, which generalise better. This has been a very brief summary of the main results in [2,3]. There are also further experiments which demonstrate that P(f) is not sensitive to the choice of initialisation, and how the observed simplicity bias is found in real-world datasets (e.g. cifar10 and MNIST), using a complexity measure called CSR. One final point concerns the strength of the inductive bias — obviously we only have an upper bound — it does not guarantee that the probability really does vary exponentially with complexity. If the bias were too weak, then we would not get good generalisation with high probability. The PAC-Bayes bound provides a probabilistic bound on generalisation. Applications of this bound in [1,6] show that, for cutting-edge architectures on real-world datasets, the simplicity bias in P(f) is sufficient to guarantee good generalisation. This will be the subject of a future post! Finally, if you think I have missed anything or said anything inaccurate, please let me know. Also note that this my interpretation of work done with a number of co-authors, and while I believe it to accurately approximate their views, it may not always be a perfect representation! [1] C. Mingard, G. Valle-Pérez, J. Skalse, A. Louis. Is SGD a Bayesian Sampler? Well, almost. (2020) https://arxiv.org/abs/2006.15191 [2] C. Mingard, J. Skalse, G. Valle-Perez, D. Martinez-Rubio, V. Mikulik, A. Louis. Neural Networks are a-priori biased towards low entropy functions. (2019) https://arxiv.org/abs/1909.11522 [3] G. Valle-Pérez, C. Camargo, A. Louis. Deep learning generalizes because the parameter-function map is biased towards simple functions. (2018) https://arxiv.org/abs/1805.08522 [4] Kamaludin Dingle, Chico Q Camargo, and Ard A Louis. Input–output maps are strongly biased towards simple outputs (2018). Nature communications, 9(1):761. [5] Jürgen Schmidhuber. Discovering problem solutions with low kolmogorov complexity and high generalization capability (1994). MACHINE LEARNING: PROCEEDINGS OF THE TWELFTH INTERNATIONAL CONFERENCE. [6] Guillermo Valle-Pérez, Ard A. Louis. Generalization bounds for deep learning (2020). https://arxiv.org/abs/2012.04115 [7] Dingle, K., Pérez, G.V. & Louis, A.A. Generic predictions of output probability based on complexities of inputs and outputs. Sci Rep 10, 4415 (2020). https://doi.org/10.1038/s41598-020-61135-7",147,0,8,Towards Data Science,2021-01-01,2021
