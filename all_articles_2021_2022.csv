url,title,subtitle,n_sections,n_paragraphs,section_titles,story_text,claps,responses,reading_time,publication,date,year
https://towardsdatascience.com/optimal-threshold-for-imbalanced-classification-5884e870c293,Optimal Threshold for Imbalanced Classification,How to choose¬†the‚Ä¶,1,28,['Optimal Threshold for Imbalanced Classification'],"Classification is one of the supervised learning techniques to conduct predictive analytics with the categorical outcome, it might be a binary class or multiclass. Nowadays, there is a lot of research and cases about classification using several algorithms, from basic to advanced like logistic regression, discriminant analysis, Na√Øve Bayes, decision tree, random forest, support vector machine, neural network, etc. They have been well developed and successfully applied to many application domains. However, the imbalanced class distribution of a data set has a problem because the majority of supervised learning techniques developed are for balanced class distribution. The imbalanced class distribution usually happens when we are studying a rare phenomenon such as medical diagnosis, risk management, hoax detection, and many more. Before talking intensively about imbalanced classification and how to handle this case, it will be good if we have a good foundation with a confusion matrix. A confusion matrix (also well-known as an error matrix) contains information about actual and predicted classifications done by a classification algorithm. The performance of such algorithms is commonly evaluated using the data in the matrix. The following table shows the confusion matrix for a two-class classifier. The classification with the two-class classifier will have four possible outcomes as follows. Read more about Type I Error and Type II Error HERE Furthermore, in order to evaluate our machine learning model or algorithm in classification case, there are a few evaluation metrics to explore but it‚Äôs tricky if we meet the imbalanced class. For imbalanced classification, we must choose the correct evaluation metrics to use with the condition they are valid and unbiased. It means that the value of these evaluation metrics will have to represent the actual condition of the data. For instance, accuracy will be actually biased in imbalanced classification because of the different distribution of classes. Take a look at the following study case to understand the statement above. Balanced classificationSuppose we are a Data Scientist in a tech company and asked for developing a machine learning model to predict whether our customer will be a churn or not. We have 165 customers where the 105 customers are categorized as not churn and the rest as churn customer. The model produces a given outcome as follows. As a balanced classification, accuracy may be the unbiased metric for evaluation. It represents the model performance correctly over the balanced class distribution. The accuracy, in this case, has a high correlation to the recall, specificity, precision, etc. According to the confusion matrix, that‚Äôs easier to conclude that our research has been produced as an optimal algorithm or model. Imbalanced classificationSimilar to the previous case but we modified the number of customers for constructing the imbalanced classification. Now, there are 450 customers in total where 15 customers are categorized as churn and the rest, 435 customers as not churn. The model produces a given outcome as follows. Looking at the accuracy in the confusion matrix above, the conclusion may be misleading because of the imbalanced class distribution. What does happen to the algorithm when it produces an accuracy of 0.98? The accuracy will be biased in this case. It doesn't represent the model performance as well. The accuracy is high enough but the recall is very bad. Furthermore, the specificity and precision equal to 1.0 because the model or algorithm doesn‚Äôt produce the False Positive. That is one of the consequences of imbalanced classification. However, F1-score will be the real representation of model performance cause it considers the recall and precision in its calculation. Note: to classify the data into positive and negative, there is still no a rigid policy In addition to some of the evaluation metrics that have been mentioned above, there are two important metrics to understand as follows. To compare the uses of evaluation metrics and determine the probability threshold for imbalanced classification, the real data simulation is proposed. The simulation generates the 10,000 samples with two variables, dependent and independent, with the ratio between major and minor classes is about 99:1. It belongs to the imbalanced classification, no doubt. To deal with the imbalanced class, threshold moving is proposed as the alternative to handling the imbalanced. Generating the synthetic observation or resample a certain data, theoretically, has its own risk, like create a new observation actually doesn‚Äôt appear in the data, decrease the valuable information of the data itself or create a flood of information. The X-axis or independent variable is the false positive rate for the predictive test. The Y-axis or dependent variable is the true positive rate for the predictive test. A perfect result would be the point (0, 1) indicating 0% false positives and 100% true positives. The geometric mean or known as G-mean is the geometric mean of sensitivity (known as recall) and specificity. So, it will be one of the unbiased evaluation metrics for imbalanced classification. Using the G-mean as the unbiased evaluation metrics and the main focus of threshold moving, it produces the optimal threshold for the binary classification in the 0.0131. Theoretically, the observation will be categorized as a minor class when its probability is lower than 0.0131, vice versa. One of the metrics to be discussed is Youden‚Äôs J statistics. Optimizing Youden‚Äôs J statistics will determine the best threshold for the classification. Youden‚Äôs J index gives a equals result of the threshold as using G-mean. It produces the optimal threshold for the binary classification in 0.0131. A precision-recall curve is a graph that represents the relationship between precision and recall. There are several evaluation metrics that are ready to use as the main focus for calculation. They are G-mean, F1-score, etc. As long as they are unbiased metrics for imbalanced classification, they can be applied in the calculation. Using the Precision-Recall curve and F1-score, it produces a threshold of 0.3503 for determining whether a given observation belongs to the major or minor class. It differs too much from the previous technique using the ROC curve because of the approaches. Threshold tuning is a common technique to determine an optimal threshold for imbalanced classification. The sequence of the threshold is generated by the researcher need while the previous techniques using the ROC and Precision & Recall to create a sequence of those thresholds. The advantages are the customization of the threshold sequence as the need but it will have a higher cost of computation. The syntax np.arrange(0.0, 1.0, 0.0001) means that there are 10,000 candidates of a threshold. Using a looping mechanism, it tries to find out the optimal threshold with the subject to maximize the F1-score as an unbiased metric. Finally, the looping mechanism was stopped and printed out the optimal threshold of 0.3227. Big thanks to Jason Brownlee who has been giving me the motivation to learn and work harder related to Statistics and machine learning implementation especially in threshold moving technique with a clear and proper article. Thanks! The machine learning algorithm mainly works well on the balanced classification because of their algorithm assumption using the balanced distribution of the target variable. Further, accuracy is no longer relevant to the imbalanced case, it‚Äôs biased. So, the main focus must be switched to those unbiased like G-mean, F1-score, etc. Threshold moving using ROC curve, Precision-Recall curve, threshold tuning curve can be the alternative solution to handling the imbalanced distribution since the resampling technique seems like it doesn‚Äôt make sense to the business logic. However, the options are open and the implementation must keep consideration of the business needs. [1] J. Brownlee. A Gentle Introduction to Threshold-Moving for Imbalanced Classification (2020). https://machinelearningmastery.com/.",,0,7,Towards Data Science,2021-01-01,2021
https://towardsdatascience.com/implementing-visualttransformer-in-pytorch-184f9f16f632,Implementing VisualTtransformer in¬†PyTorch,"Hi guys, happy new year! Today we are going to implement‚Ä¶",6,59,"['Implementing Vision Transformer (ViT) in PyTorch', 'CLS Token', 'Position Embedding', 'Attention', 'Residuals', 'Transformer']","I am on LinkedIn, come and say hi üëã Hi guys, happy new year! Today we are going to implement the famous Vi(sion) T(ransformer) proposed in AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE. Code is here, an interactive version of this article can be downloaded from here. ViT is available on my new computer vision library called glasses This is a technical tutorial, not your normal medium post where you find out about the top 5 secret pandas functions to make you rich. So, before beginning, I highly recommend you to: - have a look at the amazing The Illustrated Transformer website- watch Yannic Kilcher video about ViT- read Einops doc So, ViT uses a normal transformer (the one proposed in Attention is All You Need) that works on images. But, how? The following picture shows ViT‚Äôs architecture The input image is decomposed into 16x16 flatten patches (the image is not in scale). Then they are embedded using a normal fully connected layer, a special cls token is added in front of them and the positional encoding is summed. The resulting tensor is passed first into a standard Transformer and then to a classification head. That's it. The article is structure into the following sections: We are going to implement the model block by block with a bottom-up approach. We can start by importing all the required packages Nothing fancy here, just PyTorch + stuff First of all, we need a picture, a cute cat works just fine :) Then, we need to preprocess it The first step is to break-down the image in multiple patches and flatten them. Quoting from the paper: This can be easily done using einops. Now, we need to project them using a normal linear layer We can create a PatchEmbedding class to keep our code nice and clean Note After checking out the original implementation, I found out that the authors are using a Conv2d layer instead of a Linear one for performance gain. This is obtained by using a kernel_size and stride equal to the `patch_size`. Intuitively, the convolution operation is applied to each patch individually. So, we have to first apply the conv layer and then flat the resulting images. Next step is to add the cls token and the position embedding. The cls token is just a number placed in from of each sequence (of projected patches) cls_token is a torch Parameter randomly initialized, in the forward the method it is copied b (batch) times and prepended before the projected patches using torch.cat So far, the model has no idea about the original position of the patches. We need to pass this spatial information. This can be done in different ways, in ViT we let the model learn it. The position embedding is just a tensor of shape N_PATCHES + 1 (token), EMBED_SIZE that is added to the projected patches. We added the position embedding in the .positions field and sum it to the patches in the .forward function Now we need the implement Transformer. In ViT only the Encoder is used, the architecture is visualized in the following picture. Let‚Äôs start with the Attention part So, the attention takes three inputs, the famous queries, keys, and values, and computes the attention matrix using queries and values and use it to ‚Äúattend‚Äù to the values. In this case, we are using multi-head attention meaning that the computation is split across n heads with smaller input size. We can use nn.MultiHadAttention from PyTorch or implement our own. For completeness I will show how it looks like: So, step by step. We have 4 fully connected layers, one for queries, keys, values, and a final one dropout. Okay, the idea (really go and read The Illustrated Transformer ) is to use the product between the queries and the keys to knowing ‚Äúhow much‚Äù each element is the sequence in important with the rest. Then, we use this information to scale the values. The forward method takes as input the queries, keys, and values from the previous layer and projects them using the three linear layers. Since we implementing multi heads attention, we have to rearrange the result in multiple heads. This is done by using rearrange from einops. Queries, Keys and Values are always the same, so for simplicity, I have only one input ( x). The resulting keys, queries, and values have a shape of BATCH, HEADS, SEQUENCE_LEN, EMBEDDING_SIZE. To compute the attention matrix we first have to perform matrix multiplication between queries and keys, a.k.a sum up over the last axis. This can be easily done using torch.einsum The resulting vector has the shape BATCH, HEADS, QUERY_LEN, KEY_LEN. Then the attention is finally the softmax of the resulting vector divided by a scaling factor based on the size of the embedding. Lastly, we use the attention to scale the values and we obtain a vector of size BATCH HEADS VALUES_LEN, EMBEDDING_SIZE. We concat the heads together and we finally return the results. Note we can use a single matrix to compute in one shot queries, keys and values. The transformer block has residuals connection We can create a nice wrapper to perform the residual addition, it will be handy later on The attention‚Äôs output is passed to a fully connected layer composed of two layers that upsample by a factor of expansion the input Just a quick side note. I don‚Äôt know why but I‚Äôve never seen people subclassing nn.Sequential to avoid writing the forward method. Start doing it, this is how object programming works! Finally, we can create the Transformer Encoder Block ResidualAdd allows us to define this block in an elegant way Let‚Äôs test it you can also PyTorch build-in multi-head attention but it will expect 3 inputs: queries, keys, and values. You can subclass it and pass the same input. In ViT only the Encoder part of the original transformer is used. Easily, the encoder is L blocks of TransformerBlock. Easy peasy! The last layer is a normal fully connect that gives the class probability. It first performs a basic mean over the whole sequence. We can compose PatchEmbedding, TransformerEncoder and ClassificationHead to create the final ViT architecture. We can use torchsummary to check the number of parameters et voil√† I checked the parameters with other implementations and they are the same! In this article, we have seen how to implement ViT in a nice, scalable, and customizable way. I hope it was useful. By the way, I am working on a new computer vision library called glasses, check it out if you like Take care :) Francesco",477,7,6,Towards Data Science,2021-01-01,2021
https://towardsdatascience.com/the-ultimate-guide-to-acing-coding-interviews-for-data-scientists-d45c99d6bddc,The Ultimate Guide to Acing Coding Interviews for Data Scientists,,12,43,"['The Ultimate Guide to Acing Coding Interviews for Data Scientists', 'Introduction', 'Table of Contents', 'Why are Coding Questions Asked in DS Interviews?', 'Roles That are Likely to Have Coding Interviews', 'When to Expect a Coding Interview?', 'Different Categories of Coding Interviews', 'How to Prepare?', 'How You Are Evaluated?', 'Tips to Ace Coding Interviews', 'Final Thoughts', 'Thanks for Reading!']","Written by Emma Ding and Rob Wang Data science (DS) is a relatively new profession compared to other types of roles in the tech industry, such as software engineering and product management. Initially, DS interviews had a limited coding component, including only SQL or applied data manipulation sessions using Python or R. In recent years, however, DS interviews have shown an increased emphasis on computer science (CS) fundamentals (data structures, algorithms, and programming best practices). For someone looking to enter the data science profession, this trend towards more CS in interviews can be daunting. In this post, we hope to increase your understanding of the coding interview and teach you how to prepare for it. We will categorize different coding questions and provide tips to crack them so that you can have a stellar interview. You can reach out to us here if you think we might be able to make your journey easier in any way! Before you start reading, if you are a video person, feel free to check out this YouTube video for an abbreviated version of this post. What exactly is a coding interview? We use the phrase ‚Äúcoding interview‚Äù to refer to any technical session that involves coding in any programming language other than a query language like SQL. In today‚Äôs market, you can expect a coding interview with just about any data science job. Why? Coding is an essential part of your DS careers. Here are three reasons: To sum up, strong coding skills are necessary to perform well in many data science positions. If you cannot show that you possess those skills in the coding interview, you will not get the job. Of course, the level of coding required does differ depending on the position. Check this YouTube video if you‚Äôre interested in learning the differences between various DS roles. If you are looking for a data scientist role that falls into any of the categories below, the chances of encountering a coding interview are very high: In contrast, if you are interviewing for a DS role with a Product Analytics emphasis, there is a lower likelihood of encountering coding questions. Interviews for these roles do not often go beyond evaluating SQL proficiency, but general programming may still be tested from time to time. Candidates who do not possess a basic level of coding knowledge can be easily caught off guard during the interview and may fail to move forward in the process. Do not let that be you! Make sure you are prepared. You can start your preparation by learning what to expect with a coding interview. A coding interview can appear during the technical phone screen (TPS), onsite, or both. There could even be multiple rounds of coding interviews during the onsite portion, depending on the coding proficiency expected. In general, you should expect coding interviews in at least one stage of an overall DS interview loop. During the TPS, the delivery of the coding interview will typically be through online integrated development environments (IDEs) such as CoderPad, HackerRank, and CodeSignal. During onsite sessions, either an online IDE or a whiteboard can be used. In the current remote interview environment, the former is used by default. The length of a coding session ranges from 45 minutes to 1 hour and it usually involves one or more questions. The choice of language is typically flexible, but most candidates will choose Python for its simplicity. Based on our experiences interviewing with dozens of large and medium-sized companies, such as Airbnb, Amazon, Facebook, Intuit, Lyft, Robinhood, Slack, Snapchat, Square, Stitch Fix, Twitter, Upstart, and more, we have categorized coding questions into the following four types. This type of question aims at evaluating candidates‚Äô proficiency in introductory CS fundamentals. These fundamental topics can include, but are not limited to: Some additional topics such as Linked Lists and Graphs (Depth First Search or Breadth-First Search) are less likely to occur during this type of interview. Typically, multiple questions will be asked about a single scenario, ranging from simple to hard. Each question may cover a unique data structure or algorithm. Here is an example of a classic problem that revolves around finding the median of a list of numbers: This type of question may also appear as an applied business problem. For such questions, the candidate is expected to code up a solution to a hypothetical applied problem, which is usually related to the company‚Äôs business model. These questions are easy to medium in the level of difficulty (based on the categorization of Leetcode). The key here is to understand the business scenario and exact requirements before coding. These questions will require undergraduate-level mathematics and statistics knowledge in addition to coding capability. A few most commonly asked concepts include: Some common questions include: This type of question involves coding up a basic ML algorithm from scratch. Besides general coding ability, the interviewer will also be able to evaluate candidates‚Äô applied machine learning knowledge. You will need to be familiar with common families of machine learning models to answer these questions. Here is a list of the most common model families that appear frequently during coding interviews: Other model families, such as Support Vector Machines, Gradient Boosting Trees, and Naive Bayes are less likely to occur. You also are not likely to be required to code up a deep learning algorithm from scratch. This type of question is not as common as the other types. They ask candidates to carry out data processing and transformations without using SQL or any data analysis library such as pandas. Instead, candidates are only allowed to use a programming language of choice to solve the problems. Some common examples include: Knowing that you can expect these four types of questions will help you to prepare systematically. In the next section, we will share some tips on how exactly to do that. This list of types of questions may appear daunting at the first glance, but don‚Äôt be discouraged or overwhelmed! If you have a good grasp of basic CS knowledge and machine learning algorithms, and you take the time to prepare (which we will show you how to do in this section), then you will be able to ace the coding interview. To prepare for different categories of coding questions, we recommend the following strategies: For each of the four major question themes outlined above, begin by reviewing the fundamentals. These descriptions can be found in various online sources as well as books. Specifically: Once you feel relatively at home with the basics, expand the scope of your review to include a larger set of commonly encountered problems. You can find these on Leetcode, GeeksForGeeks, and GlassDoor. You can save the problem statements in an organized manner, ideally grouped by theme using tools such as Notion or Jupyter notebooks. For each of the topics, practice a lot of easy questions and a few medium ones. Taking the time to create a categorized collection of coding problems will not only benefit your current job search, but it will also prove helpful for future job searches. Relying on rote memorization will not be sufficient for acing the interview. To achieve a more comprehensive understanding, we recommend coming up with multiple solutions to the same problem and comparing the strengths and weaknesses (e.g. run-time/storage complexities) of the different approaches. To reinforce understanding, explain your solutions/approaches to a non-technical person using plain English. A higher-level understanding of the common problem approaches often has greater value than detailed implementation and can be especially helpful for adapting existing knowledge to new and unfamiliar settings. Work with a peer to do a mock interview, or conduct it by yourself. You can use an online coding platform, such as Leetcode, to solve real interview questions in a limited time window. Employ these preparation techniques, and you will go into your interview not only with more knowledge but also with more confidence! There are 4 major qualities you want to convey during your interview. The interviewer wishes to see candidates make logical connections between the information provided and the ultimate answer. You should therefore describe clearly what is needed for the computation and how you would write the code to solve the problem, before diving into the actual coding. The effectiveness of your communication matters significantly. Before coding, clearly communicate your thought process. If the interviewer asks questions at any point during the interview, you need to be able to explain the reasoning of your assumptions and choices. The interviewer will also evaluate your overall code quality. While the standard expectations in a DS interview would not be as high as those in a software engineering interview, candidates should still focus on several aspects: Just as with software engineering coding interviews, for DS coding interviews, it is reasonable to expect multi-part questions and sometimes multiple questions. In other words, speed is also important. Being able to solve more questions within a limited amount of time is a signal of overall proficiency. Before the interview, it is worth clarifying with recruiters what kinds of coding questions will be asked, as well as the approximate difficulty level. Lots of data science interviews do not require heavy programming, but that does not mean interviewers will not expect basic coding proficiency at your fingertips. Always ask your recruiter what to expect. If you make incorrect assumptions on the types of questions that can appear during interviews, you may end up preparing inadequately. During the interview, use these tips to answer coding questions effectively. Coding interviews, like other technical interviews, require systematic and effective preparation. Hopefully, our article has given you some insights into both what to expect in a coding interview for DS related positions and how to prepare for them. Remember: Enhancing your coding skills will be extremely rewarding not only for landing your dream job, but also for excelling in the job! If you like this post and want to support me‚Ä¶ pub.towardsai.net towardsdatascience.com",,0,11,Towards Data Science,2021-01-01,2021
https://towardsdatascience.com/customer-segmentation-in-online-retail-1fc707a6f9e6,Customer Segmentation in Online¬†Retail,A detailed step-by-step explanation on performing Customer‚Ä¶,15,91,"['Customer Segmentation in Online Retail', 'Understanding Customer Segmentation', 'Why segment your customers?', 'How to segment your customers?', 'Getting started', 'Data Snapshot', 'Data Attributes', 'Exploring the data', 'Understanding Cohort Analysis', 'Diving into Cohort Analysis', 'RFM Segmentation', 'Preprocessing data for K-means clustering', 'Clustering with K-means', 'Final Thoughts', 'Further analysis']","In this article, I am going to write about how to carry out customer segmentation and other related analysis on online retail data using python. This is going to get a bit long, so feel free to go through some sections at a time and come back again. Before going into the definition of customer segmentation, let us take a look at how online retail works and how the associated data would look like. When a person goes into a retail store and purchases a few items, the following basic data points should be generated: Now that we have developed a basic idea about how retail data looks like, let us think about how a company should think in order to make effective marketing policies. For a small company, the customer base is usually quite small and individually targetable. But, as a business grows in size, it will not be possible for the business to have an intuition about each and every customer. At such a stage, human judgments about which customers to pursue will not work and the business will have to use a data-driven approach to build a proper strategy. For a medium to large size retail store, it is also imperative that they invest not only in acquiring new customers but also in customer retention. Many businesses get most of their revenue from their ‚Äòbest‚Äô or high-valued customers. Since the resources that a company has, are limited, it is crucial to find these customers and target them. It is equally important to find the customers who are dormant/are at high risk of churning to address their concerns. For this purpose, companies use the technique of customer segmentation. One axiom frequently used in business and economics is the Pareto principle. This can be applied to understanding the revenue stream of a company as well. As per the Pareto Principle, 80% of outcomes result from 20% of all the causes of any given event. In business terms, we can say that 20% of customers contribute 80% share of the total revenue of a company. That‚Äôs why finding this set of people is important. I will explain the importance of customer segmentation in a detailed manner later in this article itself. Let us now try to understand what customer segmentation is and why is it such an effective tool for developing an effective strategy. Then, we will work on how to perform segmentation. Customer segmentation is the process of separating customers into groups on the basis of their shared behavior or other attributes. The groups should be homogeneous within themselves and should also be heterogeneous to each other. The overall aim of this process is to identify high-value customer base i.e. customers that have the highest growth potential or are the most profitable. Insights from customer segmentation are used to develop tailor-made marketing campaigns and for designing overall marketing strategy and planning. A key consideration for a company would be whether or not to segment its customers and how to do the process of segmentation. This would depend upon the company philosophy and the type of product or services it offers. The type of segmentation criterion followed would create a big difference in the way the business operates and formulates its strategy. This is elucidated below. Once the company has identified its customer base and the number of segments it aims to focus upon, it needs to decide the factors on whose basis it will decide to segment its customers. Factors for segmentation for a business to business marketing company: Factors for segmentation for a business to consumer marketing company: Customer segmentation has a lot of potential benefits. It helps a company to develop an effective strategy for targeting its customers. This has a direct impact on the entire product development cycle, the budget management practices, and the plan for delivering targeted promotional content to customers. For example, a company can make a high-end product, a budget product, or a cheap alternative product, depending upon whether the product is intended for its most high yield customers, frequent purchasers or for the low-value customer segment. It may also fine-tune the features of the product for fulfilling the specific needs of its customers. Customer segmentation can also help a company to understand how its customers are alike, what is important to them, and what is not. Often such information can be used to develop personalized relevant content for different customer bases. Many studies have found that customers appreciate such individual attention and are more likely to respond and buy the product. They also come to respect the brand and feel connected with it. This is likely to give the company a big advantage over its competitors. In a world where everyone has hundreds of emails, push notifications, messages, and ads dropping into their content stream, no one has time for irrelevant content. Finally, this technique can also be used by companies to test the pricing of their different products, improve customer service, and upsell and cross-sell other products or services. To start with customer segmentation, a company needs to have a clear vision and a goal in mind. The following steps can be undertaken to find segments in the customer base on a broad level. In the following analysis, I am going to use the Online Retail Data Set, which was obtained from the UCI Machine Learning repository. The data contains information about transnational transactions for a UK-based and registered non-store online retail. The link to the data can be found here. Before diving into insights from the data, duplicate entries were removed from the data. The data contained 5268 duplicate entries (about ~1%). Let us now look at the total number of products, transactions, and customers in the data, which correspond to the total unique stock codes, invoice number, and customer IDs present in the data. Thus, for 4070 products, there are 25900 transactions in the data. This means that each product is likely to have multiple transactions in the data. There are almost as many products as customers in the data as well. Since the data, taken from the UCI Machine Learning repository describes the data to based on transactions for a UK-based and registered non-store online retail, let us check the percentage of orders from each country in the data. The above graph shows the percentage of orders from the top 10 countries, sorted by the number of orders. This shows that more than 90% of orders are coming from United Kingdom and no other country even makes up 3% of the orders in the data. Therefore, for the purpose of this analysis, I will be taking data corresponding to orders from the United Kingdom. This subset will be made in one of the next steps and will be mentioned as required. Let us now look at the number of canceled orders in the data. As per the data, if the invoice number code starts with the letter ‚Äòc‚Äô, it indicates a canceled order. A flag column was created to indicate whether the order corresponds to a canceled order. All the canceled orders contain negative quantities (since it is a cancellation) and hence were removed from the data. Finally, I ran a check to confirm whether there were any orders with negative quantities in the orders that were not canceled. There were 1336 such cases. As we can see from the above figure, these cases are the ones where CustomerID values are NaNs. These cases were also removed from the data. Now, the data was filtered to contain orders only from the United Kingdom and finally, the structure of the data was checked by calling the .info() method: There were no nulls in any of the columns in the data, and there were a total of 349227 rows in the data. Let us now check the number of products, transactions, and customers in our cleaned data: Let us now try to understand cohort analysis so that we can perform it on our data. But, what is a Cohort? A cohort is a set of users who share similar characteristics over time. Cohort analysis groups the users into mutually exclusive groups and their behavior is measured over time. It can provide information about the product and customer lifecycle. There are three types of cohort analysis: Understanding the needs of various cohorts can help a company design custom-made services or products for particular segments. In the following analysis, we will create Time cohorts and look at customers who remain active during particular cohorts over a period of time that they transact over. Checking the date range of our data, we find that it ranges from the start date: 2010‚Äì12‚Äì01 to the end date: 2011‚Äì12‚Äì09. Next, a column called InvoiceMonth was created to indicate the month of the transaction by taking the first date of the month of InvoiceDate for each transaction. Then, information about the first month of the transaction was extracted, grouped by the CustomerID. Next, we need to find the difference between the InvoiceMonth and the CohortMonth column in terms of the number of months. The following code was used: After obtaining the above information, we obtain the cohort analysis matrix by grouping the data by CohortMonth and CohortIndex and aggregating on the CustomerID column by applying the pd.Series.nunique function. Here are the cohort counts obtained: What does the above table tell us? Consider CohortMonth 2010‚Äì12‚Äì01: For CohortIndex 0, this tells us that 815 unique customers made transactions during CohortMonth 2010‚Äì12‚Äì01. For CohortIndex 1, this tells that there are 289 customers out of 815 who made their first transaction during CohortMonth 2010‚Äì12‚Äì01 and they also made transactions during the next month. That is, they remained active. For CohortIndex 2, this tells that there are 263 customers out of 815 who made their first transaction during CohortMonth 2010‚Äì12‚Äì01 and they also made transactions during the second-next month. And so on for higher CohortIndices. Let us now calculate the Retention Rate. It is defined as the percentage of active customers out of total customers. Since the number of active customers in each cohort corresponds to the CohortIndex 0 values, we take the first column of the data as the cohort sizes. From the above retention rate heatmap, we can see that there is an average retention of ~35% for the CohortMonth 2010‚Äì12‚Äì01, with the highest retention rate occurring after 11 months (50%). For all the other CohortMonths, the average retention rates are around 18‚Äì25%. Only this percentage of users are making transactions again in the given CohortIndex ranges. From this analysis, a company can understand and create strategies to increase customer retention by providing more attractive discounts or by doing more effective marketing, etc. RFM stands for Recency, Frequency, and Monetary. RFM analysis is a commonly used technique to generate and assign a score to each customer based on how recent their last transaction was (Recency), how many transactions they have made in the last year (Frequency), and what the monetary value of their transaction was (Monetary). RFM analysis helps to answer the following questions: Who was our most recent customer? How many times has he purchased items from our shop? And what is the total value of his trade? All this information can be critical to understanding how good or bad a customer is to the company. After getting the RFM values, a common practice is to create ‚Äòquartiles‚Äô on each of the metrics and assigning the required order. For example, suppose that we divide each metric into 4 cuts. For the recency metric, the highest value, 4, will be assigned to the customers with the least recency value (since they are the most recent customers). For the frequency and monetary metric, the highest value, 4, will be assigned to the customers with the Top 25% frequency and monetary values, respectively. After dividing the metrics into quartiles, we can collate the metrics into a single column (like a string of characters {like ‚Äò213‚Äô}) to create classes of RFM values for our customers. We can divide the RFM metrics into lesser or more cuts depending on our requirements. Let‚Äôs get down to RFM analysis on our data now. Firstly, we need to create a column to get the monetary value of each transaction. This can be done by multiplying the UnitValue column with the Quantity column. Let‚Äôs call this the TotalSum. Calling the .describe() method on this column, we get: This gives us an idea of how consumer spending is distributed in our data. We can see that the mean value is 20.86 and the standard deviation is 328.40. But the maximum value is 168,469. This is a very large value. Therefore, the TotalSum values in the Top 25% of our data increase very rapidly from 17.85 to 168,469. Now, for RFM analysis, we need to define a ‚Äòsnapshot date‚Äô, which is the day on which we are conducting this analysis. Here, I have taken the snapshot date as the highest date in the data + 1 (The next day after the date till which the data was updated). This is equal to the date 2011‚Äì12‚Äì10. (YYYY-MM-DD) Next, we confine the data to a period of one year to limit the recency value to a maximum of 365 and aggregate the data on a customer level and calculate the RFM metrics for each customer. As the next step, we create quartiles on this data as described above and collate these scores into an RFM_Segment column. The RFM_Score is calculated by summing up the RFM quartile metrics. We are now in a position to analyze our results. The RFM_Score values will range from 3 (1+1+1) to 12 (4+4+4). So, we can group by the RFM scores and check the mean values of recency, frequency, and monetary corresponding to each score. As expected, customers with the lowest RFM scores have the highest recency value and the lowest frequency and monetary value, and the vice-versa is true as well. Finally, we can create segments within this score range of RFM_Score 3‚Äì12, by manually creating categories in our data: Customers with an RFM_Score greater than or equal to 9 can be put in the ‚ÄòTop‚Äô category. Similarly, customers with an RFM_Score between 5 to 9 can be put in the ‚ÄòMiddle‚Äô category, and the rest can be put in the ‚ÄòLow‚Äô category. Let us call our categories the ‚ÄòGeneral_Segment‚Äô. Analyzing the mean values of recency, frequency, and monetary, we get: Note that we had to create the logic for distributing customers into the ‚ÄòTop‚Äô, ‚ÄòMiddle‚Äô, and ‚ÄòLow‚Äô category manually. In many scenarios, this would be okay. But, if we want to properly find out segments on our RFM values, we can use a clustering algorithm like K-means. In the next section, we are going to preprocess the data for K-means clustering. K-means is a well-known clustering algorithm that is frequently used for unsupervised learning tasks. I am not going into details regarding how the algorithm works here, as there are plenty of resources online. For our purpose, we need to understand that the algorithm makes certain assumptions about the data. Therefore, we need to preprocess the data so that it can meet the key assumptions of the algorithm, which are: Let us check the first assumption by building histograms of Recency, Frequency, and MonetaryValue variables using the seaborn library: From the above figure, all the variables do not have a symmetrical distribution. All of them are skewed to the right. To remove the skewness, we can try the following transformations: 1. Log transformations2. Box-Cox transformations3. Cube root transformations I will use the Log transformation here. Since the log transformation cannot be used for negative values, we need to remove them, if they exist. One common practice one can use here is to add a constant value to get a positive value and this is generally taken as the absolute of the least negative value of the variable to each observation. However, in our data, we do not have any negative values since we are dealing with customer transactions dataset. Checking the distribution of the recency, frequency, and monetary variables, we get this by called the .describe() method: From the above description, we can see that the minimum MonetaryValue for a particular customerID is 0. This transaction therefore does not make any sense and needs to be removed. Checking the occurrence: This customer was removed from the data. We also see that we do not get a constant mean and standard deviation values. To check that, we will standardize the data. Applying the log transformation on the data first and passing it through the StandardScaler() method from the sklearn library, we obtained the preprocessed data. Checking the distribution of RFM variables for symmetrical distribution now: As we can see from the above plots, skewness has been removed from the data. In this section, we will build multiple clusters upon our normalized RFM data and will try to find out the optimal number of clusters in our data using the elbow method. Attached below is the code for this purpose. For each cluster, I have also extracted information about the average of the intracluster sum of squares through which we can build the elbow plot to find the desired number of clusters in our data. One can also use silhouette analysis to find the optimal number of clusters. You can read more about it in my previous article here. For the purpose of this analysis, I have only used the elbow plot method. From the above plot, we can see that the optimal number of clusters is 3 or 4. Let us now compare the clustering performance. For this purpose, I calculated the mean values of recency, frequency, and monetary metrics to get the following result: From the above table, we can compare the distribution of mean values of recency, frequency, and monetary metrics across 3 and 4 cluster data. It seems that we get a more detailed distribution of our customer base using k=4. However, this may not be a very visually appealing method to extract insights. Another commonly used method to compare the cluster segments is Snakeplots. They are commonly used in marketing research to understand customer perceptions. Let us build a snake plot for our data with 4 clusters below. Before building snake plots, we need to melt the data into along format so RFM values and metric names are stored in 1 column each. Link to understanding the pd.melt method: link. From the above snake plot, we can see the distribution of recency, frequency, and monetary metric values across the four clusters. The four clusters seem to be separate from each other, which indicates a good heterogeneous mix of clusters. As the final step in this analysis, we can extract this information now for each customer that can be used to map the customer with thei relative importance by the company: From the above analysis, we can see that there should be 4 clusters in our data. To understand what these 4 clusters mean in a business scenario, we should look back the table comparing the clustering performance of 3 and 4 clusters for the mean values of recency, frequency, and monetary metric. On this basis, let us label the clusters as ‚ÄòNew customers‚Äô, ‚ÄòLost customers‚Äô, ‚ÄòBest customers‚Äô, and ‚ÄòAt risk customers‚Äô. Below is the table giving the RFM interpretation of each segment and the points that a company is recommended to keep in mind while designing the marketing strategy for that segment of customers. If you liked the article and found it informative, please share it to spread knowledge :). Thank you! Link to the Github repository for this project: link [1] https://www.shopify.in/encyclopedia/customer-segmentation [2] https://learn.datacamp.com/courses/customer-segmentation-in-python [3] https://looker.com/blog/creating-actionable-customer-segmentation-models [4] https://www.business2community.com/customer-experience/4-types-of-customer-segmentation-all-marketers-should-know-02120397 [5] https://www.intercom.com/blog/customer-segmentation/",236,1,19,Towards Data Science,2021-01-01,2021
https://towardsdatascience.com/7-most-recommended-data-science-skills-to-learn-in-2021-ac26933f0e8a,7 Most Recommended Skills to Learn in 2021 to be a Data Scientist,Recommended by some of the¬†largest‚Ä¶,11,34,"['7 Most Recommended Skills to Learn in 2021 to be a Data Scientist', 'Introduction', '1) SQL', '2) Data Visualizations & Storytelling', '3) Python', '4) Pandas', '5) Git/Version Control', '6) Docker', '7) Airflow', 'Thanks for Reading!', 'Terence Shin']","Happy New Year! To kick off 2021, I wanted to share the seven most recommended data science skills from dozens of interactions and discussions with some of the largest data leaders in the world, including the Head of Data & Analytics @ Google, the Senior Director of Engineering @ NVIDIA, and the VP of Data Science and Engineering @ Wealthsimple. While this article may be more anecdotal, I feel like this article shares a valuable perspective. I‚Äôm specifically not referring to data from scraped job postings because from my experiences, there seems to be quite a disconnect between job descriptions and what‚Äôs actually done on the job. You might notice that none of the seven skills have anything to do with machine learning or deep learning, and this is not a mistake. Currently, there is a much higher demand for skills that are used in the pre-modeling phases and post-modeling phases. And so, the seven most recommended skills to learn actually overlap with the skills of a data analyst, a software engineer, and a data engineer. I wrote an article specifically on why you shouldn‚Äôt learn machine learning first ‚Äî you can check it out below: towardsdatascience.com With that said, let‚Äôs dive into the seven most recommended data science skills to learn in 2021: SQL is the universal language in the world of data. Whether you‚Äôre a data scientist, a data engineer, or a data analyst, you‚Äôll need to know SQL. SQL is used to extract data from a database, manipulate data, and create data pipelines ‚Äî essentially, it‚Äôs important for almost every pre-analysis/pre-modeling stage in the data lifecycle. Developing strong SQL skills will allow you to take your analyses, visualizations, and modeling to the next level because you will be able to extract and manipulate the data in advanced ways. Also, writing efficient and scalable queries is becoming more and more important for companies that work with petabytes of data. If you think creating data visualizations and storytelling are specific to the role of a data analyst, think again. Data visualizations simply refer to data that is presented visually ‚Äî it can be in the form of graphs, but it can also be presented in unconventional ways. Data storytelling takes data visualizations to the next level ‚Äî data storytelling refers to ‚Äúhow‚Äù you communicate your insights. Think of it like a picture book. A good picture book has good visuals, but it also has an engaging and powerful narrative that connects the visuals. Developing your data visualization and storytelling skills are essential because you‚Äôre always selling your ideas and your models as a data scientist. And it‚Äôs especially important when communicating with others who are not as technologically savvy. From my interactions, Python seems to be the go-to programming language to learn over R. That doesn‚Äôt mean that you can‚Äôt be a data scientist if you use R, but it just means that you‚Äôll be working in a language that is different from what the majority of people use. Learning Python syntax is easy, but you should be able to write efficient scripts and leverage the wide-range of libraries and packages that Python has to offer. Python programming is a building block for applications like manipulating data, building machine learning models, writing DAG files, and more‚Ä¶ Arguably the most important library to know in Python is Pandas, which a package for data manipulation and analysis. As a data scientist, you‚Äôll be using this package all the time, whether you‚Äôre cleaning data, exploring data, or manipulating the data. Pandas has become such a prevalent package, not only because of it‚Äôs functionality, but also because DataFrames have become a standard data structure for machine learning models. Git is the main version control system that is used in the tech community. If that doesn‚Äôt make sense, consider this example. In high school or university, if you ever had to write an essay, you might have saved different versions of your essay as you progressed through it. For example: üìÇFinal Essay ‚îîüìÅEssay_v1  ‚îîüìÅEssay_v2 ‚îîüìÅEssay_final ‚îîüìÅEssay_finalfinal ‚îîüìÅEssay_OFFICIALFINAL All jokes aside, Git is a tool that serves the same purpose, except that it‚Äôs a distributed system. This means that files (or repositories) are stored both locally and in a central server. Git is extremely important for several reasons, with a few being that: Docker is a containerization platform that allows you to deploy and run applications, like machine learning models. It‚Äôs becoming increasingly important that data scientists not only know how to build models but how to deploy them as well. In fact, a lot of job postings are now requiring some experience in model deployment. The reason that it‚Äôs so important to learn how to deploy models is that a model delivers no business value until it is actually integrated with the process/product that it is associated with. Airflow is a workflow management tool that allows you to automate‚Ä¶ well workflows. More specifically Airflow allows you to create automated workflows for data pipelines and machine learning pipelines. Airflow is powerful because it allows you productionalize tables that you may want to use for further analysis or modeling, and it‚Äôs also a tool that you can use to deploy machine learning models. I hope that this helps guide your learnings and gives you some direction for the new year. This is a lot to learn so I would definitely choose a couple of skills that sound most interesting to you and go from there. Do keep in mind that this more of an opinionated article that is backed by anecdotal experience, so take what you want from this article. But as always, I wish you the best in your learning endeavors! Not sure what to read next? I‚Äôve picked another article for you: towardsdatascience.com and another! towardsdatascience.com",1000,10,6,Towards Data Science,2021-01-01,2021
https://towardsdatascience.com/understanding-the-confusion-matrix-from-scikit-learn-c51d88929c79,Understanding the Confusion Matrix from Scikit¬†learn,Clear representation of output of confusion‚Ä¶,5,42,"['Understanding the Confusion Matrix from Scikit learn', 'INTRODUCTION', 'UNDERSTANDING THE STRUCTURE OF CONFUSION MATRIX', 'CONCLUSION:', 'Thanks for reading!']","In one of my recent projects ‚Äî a transaction monitoring system generates a lot of False Positive alerts (these alerts are then manually investigated by the investigation team). We were required to use machine learning to auto close those false alerts. Evaluation criteria for the machine learning model was a metric Negative Predicted Value that means out of total negative predictions by the model how many cases it has identified correctly. NPV = True Negative / (True Negative + False Negative) The cost of false-negative is extremely high because these are the cases where our model is saying they are not-fraudulent but in reality, they are fraudulent transactions. To get into action I would quickly display the confusion_matrix and below is the output from the jupyter notebook. My binary classification model is built with target = 1 (for fraud transactions) so target= 0 (for non fraud). Depending upon how you interpret the confusion matrix, you can either get an NPV of 90% or 76%. Because ‚Äî TN = cm[0][0] or cm[1][1] ie. 230 or 74 FN = cm[1][0] ie. 24 I referred to confusion matrix representation from Wikipedia. This image from Wikipedia shows that predicted labels are on the horizontal levels and actual labels are on the verticals levels. This implies, TN = cm[1][1] ie. 76 FN = cm[1][0] ie. 24 NPV = 76% Scikit learn documentation says ‚Äî Wikipedia and other references may use a different convention for axes. Oh Wait! documentation doesn‚Äôt mention anything clear, isn‚Äôt it? They say Wikipedia and other references may use a different convention for axes. What do you mean by ‚Äúmay use a different convention for axes‚Äù? We have seen that if you use the wrong convention for axes your model evaluation metric may completely go off the track. If you read through the documentation and towards the bottom you will find this example Here, they have flattened the matrix output. On our example this implies that, TN = cm[0][0] ie. 230 FN = cm[1][0] ie. 24 NPV = 90% Clearly understanding the structure of the confusion matrix is of utmost importance. Even though you can directly use the formula for most of the standard metrics like accuracy, precision, recall, etc. Many times you are required to compute the metrics like negative predictive value, false-positive rate, false-negative rate which are not available in the package out of the box. Now, if I ask you to pick the correct option for the confusion matrix that is the output of confusion_matrix. Which one would you pick? Would your answer be ‚ÄúA‚Äù because that‚Äôs what Wikipedia says or would it be ‚ÄúC‚Äù because sklearn documentation says so? Consider these are your y_true and y_pred values. By looking at the given lists, we can calculate the following: TP (True Positive) = 1 FP (False Positive) = 4 TN (True Negative) = 0 FN (False Negative) = 2 For your classic Machine Learning Model for binary classification, mostly you would run the following code to get the confusion matrix. If we fill it back to the confusion matrix, we get the confusion matrix as below TN (True Negative) = cm[0][0] = 0 FN (False Negative) = cm[1][0] = 2 TP (True Positive) = cm[1][1] = 1 FP (False Positive) = cm[0][1] = 4 However, if you were to add a simple parameter ‚Äúlabels‚Äù. TP (True Positive) = cm[0][0] = 1 FP (False Positive) = cm[1][0] = 4 TN (True Negative) = cm[1][1] = 0 FN (False Negative) = cm[0][1] = 2 The correct representation of the default output of the confusion matrix from sklearn is below. Actual labels on the horizontal axes and Predicted labels on the vertical axes. 2. By adding the labels parameter, you can get the following output",359,6,5,Towards Data Science,2021-01-01,2021
https://towardsdatascience.com/what-is-p-value-370056b8244d,What is¬†p-value?,Detailed explanation of¬†p-value,9,87,"['What is the p-value?', 'Santa Claus‚Äôs Cookie Shop', 'Can We Believe in Santa‚Äôs Words?', 'Core Concept of Inferential Statistics', 'Sampling Distribution Review', 'Testing Hypothesis Statements', 'Recap', 'Icon Attribution', 'Reference']","If you google ‚Äúwhat is p-value‚Äù, the first result shown on the page is the definition from Wikipedia: ‚Äú The p-value is the probability of obtaining test results at least as extreme as the results actually observed, under the assumption that the null hypothesis is correct.‚Äù ‚Äî Wikipedia Hmm‚Ä¶ the good thing is we know this definition is correct; the bad thing is this definition is too correct to understand. So‚Ä¶today‚Äôs game is trying to break down this sentence! Since now is the holiday season! (Hooray~), let‚Äôs invite Gingerbread Man to join us for fun! Santa Claus‚Äôs cookie shop is selling their famous product ‚Äî gingerbread cookie! Santa is very proud of his cookies. He believes his product is the most delicious one in the world. Also, Santa said that the average weight (Œº) of each product (a bag of gingerbread cookies) is 500g. Now is the 21st century. Santa already has his own factories and automated machines to help him make cookies. As you know, there‚Äôs no perfect machine and production process, so there is a variance between different bags of cookies. Assume that we know the bags of cookie is normally distributed with a standard deviation (œÉ) equals 30g. So, if Satna‚Äôs claim is true (the average weight of one bag of cookies = 500g), we could expect the distribution of one bag of cookies looks like below: But, as a curious customer who really loves gingerbread cookies, I am wondering‚Ä¶does the average weight of a bag of cookies really equal 500g? What if Santa deceives customers and gives us less than 500g cookies? How do we validate Santa‚Äôs words? Here, is where ‚Äúhypothesis testing‚Äù comes in. To implement hypothesis testing, firstly, let‚Äôs set up our null hypothesis (H0) and the alternative hypothesis (H1). As a reasonable person, we should not suspect others without having any evidence. So, we assume Santa is honest about his business (H0). If we want to check whether his cookies is less than 500g, we need to collect data and have enough evidence to support our guess (H1). So‚Ä¶we have the hypothesis statement set up as follows: H0: Average weight of one bag of cookies (Œº) = 500gH1: Average weight of one bag of cookies (Œº) < 500g Since we are unsure about how our population distribution looks like, I use the dashed line to represent possible distributions. If Santa‚Äôs claim is true, we could expect one bag of cookies has a distribution with a mean weight equals to 500g (left picture). However, if Santa's claim is not true and the mean weight of cookies is less than 500g, the population distribution should look differently (any of right picture). Cool! The problem statement is set. So now, the next question is: how to test our hypothesis statement? Maybe just weigh all bags of cookies so that we could know the exact population distribution? Well‚Ä¶obviously, it is IMPOSSIBLE for us to collect ALL the cookies (population) produced from Santa Claus‚Äôs cookie shop!!! So‚Ä¶what should we do? Here, ‚Äúinferential statistic‚Äù comes in handy! In inferential statistics, what we are interested in is the population parameters (attributes of the population). However, it is almost impossible to collect all the data of the whole population to calculate the parameters. As a result, we sampling from the whole population to get the sample data. Then, we calculate the statistic (attributes of the sample) from the sample data as our estimator to help us infer the unknown population parameters. (As the picture below) Examples of parameters and statistics: - parameters: population mean (Œº), population standard deviation (œÉ) ‚Ä¶- statistics: sample mean (xÃÑ), sample standard deviation (s) ‚Ä¶ Testing our hypothesis statement is also an inferential statistic work. The process is the same as above. But now, we are not interested in a single unknown parameter; instead, we are interested in ‚Äúwhether we can reject the null hypothesis?‚Äù. How to answer this question? The same method is used ‚Äî we calculate the statistics from our sample data for inferring the answer to this question. The statistics used here called Test Statistics. Great! So now, we know we should collect sample data and calculate the test statistic in order to test the hypothesis statement. But‚Ä¶let‚Äôs pause for a second. Before jumping into the testing part, let me quickly review the concept of sampling distribution to make sure we are on the same page. Sampling Distribution is the distribution of the sample statistic. Let‚Äôs take one of the statistics ‚Äî sample mean (xÃÑ) ‚Äî as an example. If we sampling from the population many times, we could get many sample datasets (sample 1 to sample m). Then, if we calculate the sample mean (xÃÑ) from each sample dataset, we could get m data points of the sample mean (xÃÑ). Use these data points, we could draw a distribution of sample mean (xÃÑ). Since this distribution is from the sample statistic, we called the distribution Sampling Distribution of sample mean (xÃÑ). The same idea applies to other statistics. For example, if we calculate the test statistic from each sample dataset, we could get the sampling distribution of the test statistic. A sampling distribution is similar to all the other distributions, it shows how likely (probability) the statistic value might appear if we sampling from the population many times. I‚Äôll use brown color to represent the sampling distribution curve in the following sections. Nice! Now, it‚Äôs time to jump into the testing part! The first thing we need to do is to have a sample dataset. So, I go to Santa Clause‚Äôs cookie shop and randomly pick up 25 bags of cookies (n) as our sample data. Also, I calculate the mean weight (xÃÑ) of this sample is 485g. The first part of testing is to compare our sample statistic to the null hypothesis so that we can know how far away our sample statistic is from the expected value. To do so, we first assume the null hypothesis is true. What does this mean? This means, in our case, we assume the population distribution of one bag of cookies is really equals to 500g. And if this statement is true, according to Central Limit Theorem, we could have a sampling distribution of sample mean (xÃÑ) looks like the below picture (mean value of the sample mean = 500g) if we sampling from this population many times. p-value definition:‚Äú The p-value is the probability of obtaining test results at least as extreme as the results actually observed, under the assumption that the null hypothesis is correct.‚Äù ‚Äî Wikipedia So now, if the null hypothesis is true, we could easily see that our sample mean is 15g below (485‚Äì500 = -15) the expected mean value (500g). Hmm‚Ä¶ but ‚Äú15g‚Äù is only a number, which is not very helpful for us to explain the meaning. Also, if we want to calculate the probability under the curve, it is inefficient to calculate it case by case (imagine there are numerous distributions, each of them has its own mean and standard deviation‚Ä¶you really don‚Äôt want to calculate the probability for many many times‚Ä¶) So, what should we do? We standardize our value so that the mean value of distribution always equals zero. The benefit of standardization is that statisticians already generate a table that includes the area under each standardized value. So that we don‚Äôt need to calculate the area case by case. All we need to do is to standardize our data. How to standardize? In our case, we use the z-score to transform our data. And z-score is the Test Statistic in our case. The below picture shows the sampling distribution of the test statistic (z-score). We can see that if our sample data exactly equal to the null hypothesis (the population mean =500g, the sample mean = 500g), we should have the test statistic equals to 0. In our case, our sample mean equals 485g, which gives us the test statistic equals to -2.5 (-2.5 is the test result we observed from our sample data). This indicates that our sample data has 2.5 standard errors below the expected value. p-value definition:‚Äú The p-value is the probability of obtaining test results at least as extreme as the results actually observed, under the assumption that the null hypothesis is correct.‚Äù ‚Äî Wikipedia Here, I want to mention that the test statistic is chosen based on different cases. You might hear different kinds of statistical tests, such as z-test, t-test, chi-square test‚Ä¶Why we need different kinds of tests? Because we might need to test different types of data (categorical? quantitative?), we might have different purpose of testing (testing for mean? proportion?), the data we have might have a different distribution, we might only have limited attributes of our data‚Ä¶‚Ä¶Hence, how to choose a suitable testing method is another crucial work. In this case, since we are interested in testing the mean value, also, I assume our population data is normally distributed with known population standard deviation (œÉ). Based on our condition, we choose the z-test for this case. ** Please refer to the assumptions of different kinds of statistical test if you are interested in when to use each statistical test.** Okay, so now we have our test statistic. we know how far away our test statistic is from the expected value when the null hypothesis is true. Then, what we really want to know is: how likely (probability) we get this sample data if the null hypothesis is true? To answer this question, we need to calculate the probability. As you know, the probability between one point to the other point is the area under our sampling distribution curve between these two points. So here, we do not calculate the probability of a specific point; instead, we calculate the probability from our test statistic point to infinite ‚Äî indicates the cumulative probability of all the points which farther away from our test statistic (also farther away from the expected test statistic). This cumulative probability is our p-value. You might wonder why we don‚Äôt calculate the probability of the specific test statistic (one point). Here are two possible explanations I found in this post: (1) Mathematically, the probability of a specific point on the probability curve is zero. To calculate the probability, we need to calculate the area under the curve. (2) To decide whether we should reject the null hypothesis, we use p-value compare to the significant level. since the significant level is the cumulative probability, we need to use the same format to compare two of them. Hence, the p-value should also be a cumulative probability. (I‚Äôll introduce siginificant level later) p-value definition:‚Äú The p-value is the probability of obtaining test results at least as extreme as the results actually observed, under the assumption that the null hypothesis is correct.‚Äù ‚Äî Wikipedia Wonderful! We just explained all the parts of the p-value definition! Let‚Äôs calculate the p-value in our case. As I mentioned before, the best thing to use the test statistic is that statisticians already understand the attributes of the sampling distribution. So we could just look up the z-table, or use any statistical software to help us get the p-value. In our case, we have p-value equals 0.0062 (0.62%). Please note that we are doing the one-tail test in our case. That is, we only consider one direction for calculating ‚Äúextreme‚Äù probability in the distribution. Since our alternative hypothesis (H1) is set up as ‚Äúmean value less than 500g‚Äù, we only care about the value that less than our test statistics (left-hand side). We can identify which tail we should focus on based on our alternative hypothesis. If the alternative hypothesis includes:(1) interested attribute less than (<) expected value: focus on left-tailed(2) interested attribute greater than (>) expected value: focus on right-tailed(3) interested attribute not equal to (‚â†) expected value: focus on two-tailed Notes:interested attribute- mean, proportion‚Ä¶(In our case is mean value)expected value- specific number‚Ä¶(In our case is 500g) Now, we have p-value = 0.0062. Hmm‚Ä¶ it is a small number‚Ä¶but what does this mean? This means, under the condition that our null hypothesis is true (population mean really equals 500g), if we sampling from this population distribution 1000 times, we will have 6.2 times chance to get this sample data (sample mean = 485g) or other samples with sample mean less than 485g. In other words, if we get sample data with a sample mean equals to 485g, there are two possible explanations: Or‚Ä¶ 2. The assumption of the ‚Äúnull hypothesis is true‚Äù is incorrect. This sample data (sample mean equals 485g) actually comes from other population distribution where the sample mean = 485g more likely to happen. Cool! So now we know that if our p-value is very small, that means either we get a very rare sample data or our assumption (null hypothesis is true) is incorrect. Then, the next question is: we only have the p-value now, but how to use it to judge when to reject the null hypothesis? In other words, how small the p-value is, we are willing to say that this sample comes from another population? Here, let‚Äôs introduce the judgment standard ‚Äî significant level (Œ±). The significant level is a pre-defined value that needs to be set before implementing the hypothesis testing. You can seem significant level as a threshold, which gives us a criterion of when to reject the null hypothesis. This criterion is set as below: if p-value ‚â§ significant level (Œ±), we reject the null hypothesis (H0).if p-value > significant level (Œ±), we fail to reject the null hypothesis (H0). Say, I set my significant level as 0.05. We can see the below picture, the red area is the significant level (In our case, it equals 0.05). We use the significant level as our criterion, if the p-value within (less than or equal to) the red area, we reject H0; if the p-value exceeds (greater than) the red area, we fail to reject H0. Here, I want to mention that the significant level (Œ±) also indicates the maximum risk we are acceptable for type I error (type I error means we reject H0 when H0 is actually true). It is easy to see why through the below picture ‚Äî The distribution curve below indicates the null hypothesis (H0) is true. And the red area is the probability we decide to reject the null hypothesis when it is true. If the p-value equals the significant level (our case is 0.05), then it would be the maximum probability we incorrectly reject H0 when H0 is true. In our case, we have p-value = 0.0062, which smaller than 0.05, as a result, we can reject our null hypothesis. In other words, based on our testing, we are sad to say that we have enough evidence to support our alternative hypothesis (the mean value of one bag of cookies is less than 500g). And that means‚Ä¶ we have enough evidence to say that Santa cheats to us‚Ä¶. Well‚Ä¶what happens if we change the significant level to 0.005? The result will be different. Since 0.0062 > 0.005, we then fail to reject H0. So here is the tricky part, since the significant level is subjective, we need to determine it before the testing. Otherwise, we might very likely to cheat ourselves after knowing the p-value. Thank you for reading to this point. Let‚Äôs have a quick recap to close today‚Äôs game! Part 1: To test whether our sample data support the alternative hypothesis or not, we first assume the null hypothesis is true. So that we can know how far away our sample data from the expected value given by the null hypothesis. p-value definition:‚Äú The p-value is the probability of obtaining test results at least as extreme as the results actually observed, under the assumption that the null hypothesis is correct.‚Äù ‚Äî Wikipedia Part 2: Based on the distribution, data types, purpose, known attributes of our data, choose an appropriate test statistic. And calculate the test statistic of our sample data. (Test statistic shows how far away our sample data from the expected value) p-value definition:‚Äú The p-value is the probability of obtaining test results at least as extreme as the results actually observed, under the assumption that the null hypothesis is correct.‚Äù ‚Äî Wikipedia Part 3: Calculate the probability (area under the sampling distribution curve) from the test statistic point to infinite (indicates more extreme) at the direction represent your alternative hypothesis(left-tailed, right-tailed, two-tailed). p-value definition:‚Äú The p-value is the probability of obtaining test results at least as extreme as the results actually observed, under the assumption that the null hypothesis is correct.‚Äù ‚Äî Wikipedia This cumulative probability is the p-value. If we have a very small p-value, it might indicate two possible meaning:(1) we are so ‚Äúlucky‚Äù to get this very rare sample data!(2) This sample data is not from our null hypothesis distribution; instead, it is from other population distribution. (So that we consider to reject the null hypothesis) To determine whether we could reject the null hypothesis, we compare the p-value to the pre-defined significant level (threshold). if p-value ‚â§ significant level (Œ±), we reject the null hypothesis (H0).if p-value > significant level (Œ±), we fail to reject the null hypothesis (H0). ** Thank you for reading! Welcome to any feedback, suggestions, and comments. I would be grateful if you could let me know what you think and the possible errors within my article. ** Gingerbread Man icon made by iconixar from Flaticon Paper Bag icon made by catkuro from Flaticon [1] Everything you Should Know about p-value from Scratch for Data Science[2] (video) What is a Hypothesis Test and a P-Value? | Puppet Master of Statistics[3] (video) Hypothesis Testing: Test Statistic (one-sample t-test) I Statistics 101 #3 | MarinStatsLectures[4] (video) What is a p-value? (Updated and extended version)[5] How t-Tests Work: t-Values, t-Distributions, and Probabilities[6] Interpreting P values[7] P-values Explained By Data Scientist[8] Test statistics explained[9] What is a test statistic?[10] Test Statistic: What is it? Types of Test Statistic[11] Why do p-values include the probability of obtaining more extreme values than the test statistic?[12] Wikipedia - p-value [13] Bluman, A. G. (2018). Elementary statistics: A step by step approach. New York, NY: McGraw-Hill Higher Education.[14] Ê≤àÊòé‰æÜ. (2016). ÁîüÁâ©Áµ±Ë®àÂ≠∏ÂÖ•ÈñÄÁ¨¨ÂÖ≠Áâà, ‰πùÂ∑ûÂúñÊõ∏ÊñáÁâ©ÊúâÈôêÂÖ¨Âè∏",116,3,14,Towards Data Science,2021-01-01,2021
https://towardsdatascience.com/natural-language-generation-part-2-gpt-2-and-huggingface-f3acb35bc86a,Natural Language Generation Part 2: GPT2 and Huggingface,Learn to use Huggingface and GPT-2 to train¬†a‚Ä¶,1,24,['Natural Language Generation Part 2: GPT2 and Huggingface'],"So it‚Äôs been a while since my last article, apologies for that. Work and then the pandemic threw a wrench in a lot of things so I thought I would come back with a little tutorial on text generation with GPT-2 using the Huggingface framework. This will be a Tensorflow focused tutorial since most I have found on google tend to be Pytorch focused, or light on details around using it with Tensorflow. If you don‚Äôt want to read my whole post and just see how it works, I have the following Colab notebook as an outline for people to reference here. This post will be basically going over whats in the notebook so should be easy to reference back and forth. In my last tutorial, I used Markov chains to learn n-gram probabilities from presidential speeches and used those probabilities to generate similar text output given new starting input. Now we will go a step further and utilize a more state of the art architecture to create text output that should be more accurate and realistic. If you haven't already heard about GPT-2, its a language model from OpenAI trained on a mass amount of data from the web using an architecture called the Transformer. Here is a good visual overview of the transformer architecture used by GPT-2 that should help give you intuition on how it works. GPT-2 is not the most advanced version of the language model from Open AI, but its one that has many reference implementations and frameworks to use compared to the newer GPT-3 model. As well its a version of the model that can run on Colab and is fairly straight forward to setup and hopefully even easier after this tutorial :) Let's talk about the data For our task we will create a model to generate financial article titles. If we started the task of training the language model from scratch we would need lots and lots of examples (GPT-2 was trained on 8 million web pages). Fine tuning from the pre-trained model means we don‚Äôt need to use nearly the same amount to get decent results on our specific task. The plan is to get a decent amount of examples, couple hundred thousand, and then split them into train and eval sets. I decided to grab data from reddit titles in the /r/investing subreddit and titles extracted from US Financial News Articles dataset from Kaggle. Some of the examples from the joined dataset are not just finance related, since many financial news sites also report on non-financial events and the subreddit data has a mix of investing advice and questions. The titles pulled from reddit submissions are about 100k and the titles extracted from the Kaggle dataset are about another 179k. That should be enough examples so as to not over fit on our task and give us a rich set of possible text to generate from within the ‚Äúfinancial‚Äù domain. Data format The format of the data seems to make or break the training and output of these models I have found. For GPT-2 if you want to just generate a whole bunch of text, say a book or articles, you can throw all the examples into a single document with no special tokens between examples. However if you want to generate output that follows a certain pattern or prompt, you should add special tokens into the dataset to make it more clear what pattern GPT-2 should attempt to learn to output. Below is the basic format for an example in the dataset for our title generation task. Each example is then concatenated together as one long string. We don‚Äôt have to add a start token for training since GPT-2 only needs the ‚Äò<|endoftext|>‚Äô token to split examples, but with this leading token we can then have the model generate new random output on each run when we prompt it with ‚Äú<|title|>‚Äù first. You can set the start token to be whatever you want really, or have none at all, but I have found that setting these tokens to something that wont be likely to show up in the vocab of the data makes it easier to generate coherent text and you won‚Äôt be as likely to fall into a repetitive cycle. The gist above shows the cell step that is used to create our train and eval sets. As you can see when we read in the dataset line by line, then append the <|title|> token to the input then rejoin with <|endoftext|> and write back out to their respective file. Now that we have these two files written back out to the Colab environment, we can use the Huggingface training script to fine tune the model for our task. How to fine tune GPT-2 For fine tuning GPT-2 we will be using Huggingface and will use the provided script run_clm.py found here. I tried to find a way to fine tune the model via TF model calls directly, but had trouble getting it to work easily so defaulted to using the scripts provided. Some things like classifiers can be trained directly via standard TF api calls, but the language models seem to not be fully supported when I started this work. Its possible newer versions of Huggingface will support this. The script above will run the fine tuning process using the medium sized GPT-2 model, though if you are using standard Colab you might only be able to run the small GPT-2 model due to resource limits on the vm. For myself I am using Colab Pro which gives me access to more powerful base machines and GPU‚Äôs. Depending on your use case regular Colab may be sufficient or you can use GCP if you really need access to more powerful GPU instances for longer times. Transformer models are very computationally expensive due to their architecture, so when training on a GPU it can easily take hours or days with a large enough dataset. For the investing title dataset, 5 epochs on a p100 took over 3‚Äì4 hours while on a v100 it only took 1.5 to 2 hours depending on the settings I used. Its up to some luck it seems on which GPU you get when starting up your Colab instance. I found I was usually able to get a v100 every other day after a multi hour training session. One thing to call out in the above script call is that I am using mixed precision in the model training with the ‚Äî fp16 argument. Using mixed precision shaved off about 30 mins of training time with no noticeable drop in model performance when compared to a single precision trained model on our data. At the end of the model training there is an eval step that happens which tells us our models perplexity. As you can see our title generation GPT-2 model gets us a perplexity score of around 10.6 which isn't bad considering it only ran for 5 epochs. So now that we have trained our new language model to generate financial news titles, lets give it a try! We will want to use the path to the directory that the script outputs the model file to, and load it up to see if it will output some great new finance article / reddit titles for us! To load into TF we will want to import the TFGPT2LMHeadModel and then call from_pretrained, making sure to set the from_pt flag to True. This way it will load the Pytorch model into TF compatible tensors. We will also use the pre-trained GPT-2 tokenizer for creating our input sequence to the model. The pre-trained tokenizer will take the input string and encode it for our model. When using the tokenizer also be sure to set return_tensors=‚Äùtf‚Äù. If we were using the default Pytorch we would not need to set this. With these two things loaded up we can set up our input to the model and start getting text output. After creating the input we call the models generate function. Huggingface has a great blog that goes over the different parameters for generating text and how they work together here. I suggest reading through that for a more in depth understanding. The below parameters are ones that I found to work well given the dataset, and from trial and error on many rounds of generating output. The one thing with language models is that you have to try a number of different parameter options to start to see some good output, and even then sometimes it takes many runs to get output that fits your task so do not be surprised if initial results are less than stellar. Below is some of the output that was generated by our investing title model given the ‚Äú<|title|>‚Äù token as the prompt. From the generated examples above they look like believable article and reddit titles. Still sometimes when running you can get some funny output like the one below. Well, that was maybe a bit long of a post, but hopefully, you found it useful for learning how to use Huggingface to fine tune a language model and generate some text using a Tensorflow back end. Now with these techniques you can start to come up with different tasks and models for your own work/interests. For instance, after building this title model I decided to see if I can generate a title and use that title to generate some sort of article, to varying degrees of success. Try experimenting for your self and see what you can come up with! Thanks for reading! Link to colab gist: https://gist.github.com/GeorgeDittmar/5c57a35332b2b5818e51618af7953351",144,0,8,Towards Data Science,2021-01-01,2021
https://towardsdatascience.com/shakespeare-versus-eminem-1de54e479279,Shakespeare versus Eminem‚Äî who‚Äôs the better¬†lyricist,"He is known for his poetry, his writings on¬†life‚Ä¶",13,62,"['Shakespeare versus Eminem‚Äîwho‚Äôs the better lyricist?', 'The Weigh-In', 'Rhythm And Poetry (r.a.p.)', 'Round 1: Averages, Shaky at (his) best‚Ä¶', 'Round 2: Normalized Averages‚Ä¶', 'Round 3: Your own worse Eminemeny‚Ä¶', 'Round 4: Shakespeareheading the conversation‚Ä¶', 'Round 5: Double entendres', 'Round 6: Elements of Eloquence', 'Round 7: Multi-syllable rhyme schemes', 'Round 8: Fastest Verses', 'Round 9: T.K.O.', 'The Aftermath']","Data science has crept into every conceivable nook and cranny of modern life. Apple uses a dedicated chip in your phone to inform Siri what to suggest to you. Even grocery store tags can update product prices in real-time. But in the meantime the real questions of life remain unanswered; we are no closer to answering what the meaning of life is, how to obtain true happiness, or whether Shakespeare is a better lyricist than Eminem. (Disclaimer: this analysis is heavily biased towards showing that b-rabbit is the better lyricist. I will take no measures to account for this bias. In fact, I‚Äôll try everything in my power to show it is so. Because it is the right thing to do. ‚ÄúIf I could only use this power for good I wouldn‚Äôt, not even if I could.‚Äù) Sorry if I‚Äôm being graphicBut I‚Äôm stiff as a statue, you sat on a shelfI feel like I‚Äôm a bust, maybe I‚Äôm just ahead of myself ‚Äî Eminem Both men are famous for being brilliant with word-play. Shakespeare invented more than 1700 words that we use nowadays. He produced 37 plays and a bunch of poetry. His longest work is Hamlet, which has 4,042 lines, (‚Äúbars‚Äù in rap parlance) and his complete works contain 884,647 words. Our data in this analysis of Shakespeare contains a selection of 36 works and 857,648 words. Contrary to common belief Shakespeare wasn‚Äôt impressive in his own lifetime, he was popular, sure, but other playwrights were more acclaimed. His rise to fame had to wait until long after he died. 100 years post mortem, in 1730, the Shakespeare Ladies‚Äô Club started promoting his works through local theatres which thrust him into the limelight and the fame that he enjoys nowadays. On the other side, Marshall Mathers (M&M: Eminem for the uninitiated) has been a legend ever since he hit the scene. To date, he has produced 12 studio albums. Eminem has a multitude of records (pun intended) to his name; his often lauded ‚ÄòRap God‚Äô holds a Guinness record for most words in a hit song ‚Äî 1,560 ‚Äî and, when it came out was the fasted delivery of syllables in a certain amount of one ‚Äî 157 syllables in 16.3, a tongue-twisting 9.6s syllables per second ‚Äîa record he broke with approximately .7 syllables more per second on ‚ÄòMajesty.‚Äô Then, in 2018 he broke his own record again with ‚ÄòGodzilla,‚Äô but now ‚Äúmaybe I‚Äôm just ahead of myself.‚Äù Oh, and Eminem holds an oscar. He was also the only person ever to have a number one song, album and movie in the same week! Our selection of Eminem works contains 17 different albums, 204 songs and a combined total of 167,080 words (and doesn‚Äôt include his many singles or mixtapes, freestyles, collaborative tapes, or ‚Äòfeatures where he stole the show‚Äô). Slim Shady doesn‚Äôt shy away from a challenge, on Jay-Z‚Äôs Renegade he straight up calls out Shakespeare. Has Shakespeare ever done that to Eminem? I think not! Shall I compare thee to a summer‚Äôs day? When in eternal lines to time thou grow‚Äôst: So long as men can breathe or eyes can see, So long lives this, and this gives life to thee. ‚Äî Shakespeare In his book Dataclysm, Christian Rudder shows that the use of words on Twitter is of a higher calibre than typically found in books, even those whom we consider ‚Äòthe great.‚Äô He argues, that the reason is that Twitter‚Äôs 140 character limit forces users to think hard and long about our words used. Twitterers will choose weightier words to convey more meaning in the limited space presented. There are many different indicators of text ‚Äòmaturity,‚Äô such as the Dale Chall readability test. Such a test does not capture the full brilliance of our Royal Rhymliness‚Äô but it‚Äôll do. Let‚Äôs see what happens if our protagonist and antagonist go toe-to-toe in an exchange of fisticuffs. Time for Round 1. Ding-ding. ‚ÄúAverages are where you hide the truth.‚Äù That wisdom is equally true here. Averages say little about the absolute lyrical literacy and the lavish lexicons of our heroes. In some respects, the works of Em and Shaky are very different; their lengths for instance. Eminem clocks in at an average of 827 words per song with on average 110 bars, while Shakespeare clocks an impressive 23,823 words per play over an average of 3,949 lines. Neither Em nor Shaky is at an advantage at this point. They occupy slightly different places in society. This round is therefore a little underwhelming. Neither of our legends truly packs the punch, but then again ‚Äúyou gotta be diamond to even climb in the ring.‚Äù This is where the fight gets a bit more interesting. This is where we see who punches above their weight in their verbal assaults. Eminem is not phased: ‚Äújust go hard, do the rope-a-dope Ali shuffle, and dance around his opponents.‚Äù For this, we want to see who uses more complicated words. We have used a list of 3,000 simple words. These words are used in everyday parlance and should be avoided as they are mainly filler. Eminem and Shakespeare are on par. Both average mid-20% ‚Äòhard words‚Äô in their works. This difference isn‚Äôt much, to be honest, but the advantage is for Shakespeare. However, when it comes to unique words per work Eminem outperforms Shakespeare. On average, Em‚Äôs works have 42% of unique words whereas Shakespeare only has about 15% unique words. This isn‚Äôt strange ‚ÄîShakespeare needs a lot of filler words to fill to long-winding works of his. Eminem swings hard, Shakespeare dodges. That guy‚Äôs ‚Äúnever even been charged in connection with battery. Bitch, he ain‚Äôt plugged into nothin.‚Äù Eminem often refers to being his own worse enemy. And he‚Äôs proven that he‚Äôs quite the ‚Äúspectacular archer.‚Äù His Rap God is an outlier, even by his own standards (see featured diagram below, top right-hand corner). As said, the song packs a whopping 1547 words, of which 617 words are unique. One song that is dramatically under-rated is the song shadyxv from the album with the same name. The song has ‚Äòonly‚Äô 1158 words but packs a whopping 611 unique words (almost as many as Rap God). For a full breakdown of songs in our analysis, see the bubble-plot below. Hover over to see which work is under consideration. Shakespeare also would like to get a word in edgewise. Although his works are long as they are boring there is much less variation in his works. Have a look for yourself, below: Both authors have a tendency to play with the language. They bend words in place and language becomes almost liquid in their hands. In Twelfth Night Shakespeare whips out his naughtier side: By my life, this is my lady‚Äôs hand: these be her very C‚Äôs, her U‚Äôs, and (read: ‚ÄòN‚Äô) her T‚Äôs; and thus makes she her great P‚Äôs ‚Äî Shakespeare However, on Marsh Eminem counters with his triple (?) entendre: S on my chest (Superman) like it‚Äôs plural ‚Äî Eminem S on my chest; placing an ‚Äòs‚Äô on ‚Äòchest‚Äô will make it plural. However pleural is also also the region in your chest that encapsulates your lungs. All fun and games, but also, plural ‚Äòs‚Äô (read: S.S.) makes for the abbreviation of Slim Shady. Or, consider the song transcending: One-two-three, chk-chk, one-two-threeChk-chk, one-two-three ‚Äî Eminem On My Darling, where the 1+2+3, 1+2+3, 1+2+3 = 666, the sign of the devil ‚Äî who he‚Äôs arguably battling within the song. But he brings these bars at exactly 1:23 into the song. There are too many to list here, it‚Äôs worth checking out Genius website to see how many you‚Äôve actually missed. Still light on his feet. Eminem moves quickly. Shakespeare looks old and tired. Mark Forsyth has written his book Elements of Eloquence in praise of Shakespeare's way with words, no one does it better than The Goat (‚Äúfor those who you who don‚Äôt know what a Goat is, it means the Greatest Of All Time, and I consider him one of those.‚Äù). Mark gives us a lot to work with (39 chapters in which Eminem could have bested Shakespeare). In his first and 25th chapter, he talks about alliteration (or assonance), words with similar letters or sounds in quick progression which satisfies the listeners ‚Äòneed‚Äô for rhyme. Mark gives us Shakespeare‚Äôs Midsummer Night‚Äôs Dream: Whereat, with blade, with bloody blameful blade,He bravely broached his boiling bloody breast; But I‚Äôm happy to raise him Eminem on Cleaning Out My Closet: Take a second to listen ‚Äòfore you think this record is dissin‚ÄôBut put yourself in my position, just try to envisionWitnessin‚Äô your mama poppin‚Äô prescription pills in the kitchenBitchin‚Äô that someone‚Äôs always goin‚Äô through hear purse and shit‚Äôs missin‚Äô Or consider our hero‚Äôs rhyme schemes; Shakespeare‚Äôs The Tempest goes: Full fathom five thy father lies;Of his bones are coral made;Those are pearls that were his eyes;Nothing of him that does fade,But doth suffer a sea-changeInto something rich and strange ‚Äî Shakespeare Shakespeare‚Äôs rhyme scheme is a simple A; B; A; B; C; C (where the ‚Äò;‚Äô indicates the end of a sentence. Compare this to Eminem‚Äôs Wide Awake verse: But they can‚Äôt see what I can see, there‚Äôs a vacancy in my tummyIt‚Äôs making me play hide-and-seek, like Jason I‚Äôm so hungryShe‚Äôs naked, see, no privacy but I can see she wants meSo patient, see, I try to be but gee, why does she taunt me?(etc.) ‚Äî Eminem The rhyme scheme here is ABCD; ABCD; ABCD; ABCD; etc for the next 14 bars! It‚Äôs clear that Shakespeare might have started it, Em is definitely here to finish it. There are five verses worth considering here. They are all impossibly fast. Now, go and try to rap along to any of these verses and you‚Äôll find ‚Äòwhat you see is a genius at work.‚Äô Godzilla is incidentally the third time in a row that Eminem has beaten his own record for fastest verse. (Okay, TKO was a JT song, my bad.) Shakespeare is hardly standing. One final Nail in the Coffin, Eminem‚Äôs way with music. It‚Äôs clear that ‚Äúpapers are hand grenades soon as I pull the pen out‚Äù On the amazing (bonus) track on Recovery Untitled, he raps a 4/4 time signature over a 3/4 beat. The result is virtuous ‚Äòhead bobbing‚Äô bars over a mesmerizing smooth waltzy flow. He‚Äôs against the ropes. He swings. He hits. Shakespeare goes down‚Ä¶ He‚Äôs the greatest of all time: Eminem. Although Shakespeare put up a formidable fight, all that matters is that he‚Äôs clearly no match for Marshall Mathers. So what does someone like that have on Shakespeare? Everything. So, where does it leave our heroes? In the Aftermath (get it?) there‚Äôs only one left standing: Shady. And although ‚Äòthat 5‚Äì0‚Äôs startin‚Äô to creep up on him‚Äô and I‚Äôm no kid anymore either. I guess when Em says: ‚ÄúSomewhere some kid is bumpin‚Äô this while he lip-syncs in the mirror, that‚Äôs who I‚Äôm doin‚Äô it for, the rest I don‚Äôt really even care‚Äù ‚Äî I‚Äôm that kid. The complete analysis and datasets can be found on my GitHub page, here. For the Slim Shady lyrics, I‚Äôve used the database on AZLyrics. For Shakespeare‚Äôs work, I‚Äôve used the Complete Works, 2nd Edition (1905, Oxford University Press). Thanks to Elizabeth Ter Sahakyan for her Plotly Visualization embedding in Medium, here.",298,5,9,Towards Data Science,2021-01-01,2021
https://towardsdatascience.com/cifar-100-pre-processing-for-image-recognition-task-68015b43d658,CIFAR-100: Pre-processing for image recognition task,Pre-processing or data preparation of a¬†popular‚Ä¶,5,53,"['CIFAR-100: Pre-processing for image recognition task', 'What is CIFAR-100? ü§î', 'How can one obtain the CIFAR-100 dataset? üôã', 'How to load this dataset? üöõ', 'Visualization:']","Recognition of images is a simple task for humans as it is easy for us to distinguish between different features. Somehow our brains are trained unconsciously with different or similar types of images that have helped us distinguish between features (images) without putting much effort into the task. For instance, after seeing a few cats, we can recognize almost every different type of cat we encounter in our life. üê± However, machines need a lot of training for feature extraction which becomes a challenge due to high computation cost, memory requirement, and processing power. In this article, we will discuss the pre-processing of one such use case. So, let‚Äôs dive deeper and understand how we can pre-process an image dataset to build a convolutional neural network model. üèäüèº Note: Convolutional Neural Network (CNN) is a class of deep neural networks commonly used to analyze images. A convolutional neural network model can be built to correctly recognize and classify colored images of objects into one of the 100 available classes of the CIFAR-100 dataset. So, let‚Äôs get started. üèÉüèª CIFAR-100 is a labeled subset of 80 million tiny images dataset where CIFAR stands for Canadian Institute For Advanced Research. The images were collected by Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. The dataset consists of 60000 colored images (50000 training and 10000 test) of 32 √ó 32 pixels in 100 classes grouped into 20 superclasses. Each image has a fine label (class) and a coarse label (superclass). The Python version of this dataset can be downloaded from the website of the University of Toronto Computer Science. The downloaded files are Python pickled objects produced using cPickle. Do not worry about this for now. We will go through each step together to use this dataset. After downloading the dataset from the website, we need to load it into our jupyter notebook. The files obtained from the website are Python pickled objects. The folder structure after unzipping looks like this: We can see that we have separate train and test files, and a meta file. Python Pickle or cPickle module can be used to serialize or deserialize objects in Python. Here, I am using Pickle. The load() method of Pickle can be used to read these files and analyze their structure. Read this to know more about pickling. Pickle needs binary data so we will open files as ‚Äòrb‚Äô and load it using the pickle load() method with ‚Äòlatin1‚Äô encoding. Let us first import the libraries which we will use in pre-processing. Here is the code to read these files. Read this to know why we mostly use ‚Äòlatin1‚Äô as the encoding. Let us now load our training set. The output looks like this: The training file has the above items in it. The coarse_labels and fine_labels are labels of the images (20, 100 respectively), the data file has the image data in the form of a NumPy array, filenames is a list stating the names of the files, and batch_label is the label of the batch. Let us check the length of the dataset. The output looks like this: So, there are 50,000 images in the training dataset and each image is a 3 channel 32 √ó 32 pixel image (32 √ó 32 √ó 3 = 3072). Let us have a look at the unique fine labels. The output looks like this: So, there are 100 different fine labels for the images ranging from 0 to 99. Let us now have a look at the unique coarse labels. The output looks like this: So, there are 20 different coarse labels for the images ranging from 0 to 19. Let us check what is there in the batch_label file. The output looks like this: Here we have only one batch, so batch_label is a string stating that. As we are done with exploring the different files in the training dataset except for the data file itself, let us first unpickle our test dataset and meta file. Meta file has a dictionary of fine labels and coarse labels. For clarity, I have printed them separately. Here, is the output. Our task will be to recognize images and provide them fine labels. Let us now create dataframes using the labels, which will help us in visualization. A glimpse of the two dataframes: Let us now look at our data. The output is a NumPy array. In order to perform the task of image recognition and classification, a convolutional neural network has to be built which requires a 4D array as the input. So, the data has to be transformed to acquire that shape. For instance, the training dataset had 50000 images with the shape (50000, 3072), so we need to transform these images to acquire the following shape using the reshape and transpose operation of NumPy array: (Number of instances √ó Width √ó Height √ó Depth) The width, height, and depth are the dimensions of the image where depth is nothing but the number of color channels in the image which is 3 in our case as the images are RGB. The following diagram illustrates the form of 4D input for the convolutional neural network model. Let us write code for this transformation of images. We are now done with our transformation. Let us create visualizations to see these images. The output looks like this: Let us display some more images. We can see from the visualization that the quality of images is low and the position of the object in the image varies a lot. It would be difficult to train a model to recognize and classify such images. üôÜüèª Let us now work on the test dataset. In order to make predictions, the labels of the images have been converted to categorical matrix structure from the existing 1D NumPy array structure. We are now done with our pre-processing and we will look at how to build a convolutional neural network model for this dataset in another article. Here is the link to the GitHub repository which has all this code. Please feel free to use this in your work to train a classic CNN model that can classify images. üòä Related article: towardsdatascience.com Thank you, everyone, for reading this. Do share your valuable feedback or suggestion regarding this post! Happy reading! üìó üñå LinkedIn",95,1,8,Towards Data Science,2021-01-01,2021
https://towardsdatascience.com/how-the-lstm-improves-the-rnn-1ef156b75121,How the LSTM improves the¬†RNN,,1,51,['How the LSTM improves the RNN'],"The advantage of the Long Short-Term Memory (LSTM) network over other recurrent networks back in 1997 came from an improved method of back propagating the error. Hochreiter and Schmidhuber called it ‚Äúconstant error back propagation‚Äù [1]. But what does it mean to be ‚Äúconstant‚Äù? We‚Äôll go through the architecture of the LSTM and understand how it forward and back propagates to answer the question. We will make some comparisons to the Recurrent Neural Network (RNN) along the way. If you are not familiar with the RNN, you may want to read about it here. However, we should first understand what is the issue of the RNN that demanded for the solution the LSTM presents. That issue is the exploding and vanishing of the gradients that comes from the backward propagation step. Back propagation is the propagation of the error from its prediction up until the weights and biases. In recurrent networks like the RNN and the LSTM this term was also coined Back Propagation Through Time (BPTT) since it propagates through all time steps even though the weight and bias matrices are always the same. The figure above depicts a portion of a typical RNN with two inputs. The green rectangle represents the feed forward calculation of net inputs and their hidden state activations using an hyperbolic tangent (tanh) function. The feed forward calculations use the same set of parameters (weight and bias) in all time steps. In red we see the BPTT path. For large sequences one can see that the calculations stack. This is important because it creates an exponential factor that depends greatly on the values of our weights. Everytime we go back a time step, we need to make an inner product between our current gradient and the weight matrix. We can imagine our weight matrix to be a scalar and let‚Äôs say that the absolute scalar is either around 0.9 or 1.1. Also, we have a sequence as big as 100 time steps. The exponential factor created by multiplying these values one hundred times would raise a vanishing gradient issue for 0.9: 0.9¬π‚Å∞‚Å∞ = 0.000017(‚Ä¶) and an exploding gradient issue for 1.1: 1.1¬π‚Å∞‚Å∞ = 13780.61(‚Ä¶) Essencially, the BPTT calculation at the last time step would be similar to the following: Note that although the representation is not completely accurate, it gives a good idea of the exponential stacking of the weight matrices in the BPTT of an RNN with n inputs. W_h is the weight matrix of the last linear layer of the RNN. Next, we would be adding a portion of these values to the weight and bias matrices. You can see that we either barely improve the parameters, or try to improve so much that it backfires. Now that we understand these concepts of vanishing and exploding gradients, we can move on to learn the LSTM. Let‚Äôs start by its forward pass. Despite the differences that make the LSTM a more powerful network than RNN, there are still some similarities. It mantains the input and output configurations of one-to-one, many-to-one, one-to-many and many-to many. Also, one may choose to use a stacked configuration. Above we can see the forward propagation inside an LSTM cell. It is considerably more complicated than the simple RNN. It contains four networks activated by either the sigmoid function (œÉ) or the tanh function, all with their own different set of parameters. Each of these networks, also refered to as gates, have a different purpose. They will transform the cell state for time step t (c^t) with the relevant information that should be passed to the next time step. The orange circles/elipse are element-wise transformations of the matrices that preceed them. Here‚Äôs what the gates do: Forget gate layer (f): Decides which information to forget from the cell state using a œÉ function that modulates the information between 0 and 1. It forgets everything that is 0, remembers all that is 1 and everything in the middle are possible candidates. Input gate layer (i): This could also be a remember gate. It decides which of the new candidates are relevant for this time step also with the help of a œÉ function. New candidate gate layer (n): Creates a new set of candidates to be stored in the cell state. The relevancy of these new candidates will be modulated by the element-wise multiplication with the input gate layer. Output gate layer (o): Determines which parts of the cell state are output. The cell state is normalized through a tanh function and is multiplied element-wise by the output gate that decides which relevant new candidate should be output by the hidden state. On the left you can see the calculations performed inside an LSTM cell. The last two calculations are an external feed forward layer to obtain a prediction and some loss function that takes the prediction and the true value. The entire LSTM network‚Äôs architecture is built to deal with a three time step input sequence and to forecast a time step into the future like shown in the following figure: Putting the inputs and parameters into vector and matrix form may help understand the dimansionality of the calculations. Note that we are using four weight and bias matrices with their own values. This is the forward propagation of the LSTM. Now it is time to understand how the network back propagates and how it shines compared to the RNN. The improved learning of the LSTM allows the user to train models using sequences with several hundreds of time steps, something the RNN struggles to do. Something that wasn‚Äôt mentioned when explaining the gates is that it is their job to decide the relevancy of information that is stored in the cell and hidden states so that, when back propagating from cell to cell, the passed error is as close to 1 as possible. This ensures that there is no vanishing or exploding of gradients. Another simpler way of understanding the process is that the cell state connects the layers inside the cell with information that stabilizes the propagation of error somewhat like a ResNet does. Let‚Äôs see how the error is kept constant by going through the back propagation calculations. We‚Äôll start with the linear output layer. Now we‚Äôll go about the LSTM cell‚Äôs back propagation. But first let‚Äôs get a visual of the path we must take within a cell. As you can see, the path is quite complicated, which makes for computationally heavier operations than the RNN. Bellow you can see the back propagation of both outputs of an LSTM cell, the cell state and the hidden state. You can refer to the equations I showed above for the forward pass to get a better understanding of which equations we are going through. You can see that the information that travelled forward through the cell state is now going backwards modulated by the tanh‚Äô. Note that the prime (‚Äò) in œÉ‚Äô and tanh‚Äô represent the first derivative of both these functions. In the next steps, we are going back to the parameters in each gate. Output gate: New candidate gate: Input gate: Forget gate: We have calculated the gradient for all the parameters inside the cell. However, we need to keep back propagating until the last cell. Let‚Äôs see the last steps: You may see that the information that travels from cell state c¬≥ to c¬≤ largelly depends on the outputs of the output gate and the forget gate. At the same time, the output and forget gradients depend on the information that was previously stored in the cell states. These interactions should provide the constant error back propagation. Going further back into the global input (X¬≥), we add what is coming from all four gates together. Finally we deconcatenate the hidden state from the global input vector, go through the remaining cells and add all the gradients with respect to the parameters from all cells together. This story‚Äôs goal was to understand why the LSTM is capable of dealing with more complex problems than the RNN by keeping a constant flow of error throughout the backpropagation from cell to cell. We explored the resulting issues from the poor handling of complex sequences from the RNN giving raise to the exploding and vanishing gradients. Then we saw how these issues come to happen by exploring the flow of gradients in the RNN. Finally we introduced the LSTM, its forward pass and, by deconstructing its backward pass, we understood that the cell state is influenced by two gate units that are responsible for ensuring a constant back flow of the error. It is important to mention that as more experiments were performed with the LSTM there is a certain degree of complexity where this network stops being able to learn. Generally it goes to the thousand time steps before it happens which is already pretty good. This is leading to a gradual phase out of the LSTM as problems become more ambitious in favour of a newer network called the Transformer or BERT. You may have also heard of the GTP-3 for Natural Language Processing. These are very powerful networks with a great potential. However, the LSTM sure had its impact and was created with ingenuity and still is usefull today. Thanks for reading! [1] Sepp Hochreiter and J√ºrgen Schmidhuber, ‚ÄúLong Short-Term Memory‚Äù in Neural Computation, 1997, DOI: 10.1162/neco.1997.9.8.1735.",,0,8,Towards Data Science,2021-01-01,2021
https://towardsdatascience.com/creating-abstract-art-with-stylegan2-ada-ea3676396ffb,Creating Abstract Art with StyleGAN2 ADA,How I used Adaptive Discriminator Augmentation and Learning‚Ä¶,9,50,"['Creating Abstract Art with StyleGAN2 ADA', 'MachineRay 2', 'Transfer Learning', 'Create your Own Painting', 'Next Steps', 'Acknowledgments', 'Source Code', 'References', 'Appendix A ‚Äî Gallery of MachineRay 2 Results']","Back in August 2020, I created a project called MachineRay that uses Nvidia‚Äôs StyleGAN2 to create new abstract artwork based on early 20th century paintings that are in the public domain. Since then, Nvidia has released a new version of their AI model, StyleGAN2 ADA, that is designed to yield better results when generating images from a limited dataset [1]. (I‚Äôm not sure why they didn't call it StyleGAN3, but I‚Äôll refer to the new model as SG2A to save a few characters). In this article, I‚Äôll show you how I used SG2A to create better looking abstract paintings. Similar to the original Machine Ray, I am using abstract paintings that are in the public domain as my source images to train the SG2A system. I then change the aspect ratio as part of the post-processing. This diagram shows the flow through the system. Everything starts with the 850 images I scraped from WikiArt.org using a custom script. The images are pre-processed and fed into the Discriminator Network as the ‚Äúreal‚Äù images. A set of 512 random numbers are chosen and fed into the Style Mapper and Generator Networks to create the ‚Äúfake‚Äù images. Both the real and fake images are modified with Adaptive Discriminator Augmentation, the key innovation in SG2A. I‚Äôll discuss this further down in the article. The job of the Discriminator Network is to determine if the input is real or fake. The result is fed back into the three networks to train them. When the training is done, I post-process the output of the Generator Network to get the final images. I‚Äôll go through each step in more detail in the sections below. I use a Python script to gather images from WikiArt.org that are labeled as being ‚Äúabstract‚Äù and are in the public domain, i.e. created before 1925. The images are from the likes of Mondrian, Kandinsky, Klee, etc. The source code to gather the images is here. Here is a sample of the source paintings. Here are the steps I use for pre-processing the images: Here is a sample of pre-processed images. One of the major improvements in SG2A is dynamically changing the amount of image augmentation during training. Image augmentation has been around for a while. The concept is fairly simple. If you don‚Äôt have enough images to train a GAN, it can lead to poor performance, like overfitting, underfitting, or the dreaded ‚Äúmodel collapse‚Äù, where the generator repeats the same output image. A remedy for these problems is image augmentation, where you can apply transformations like rotation, scaling, translation, color adjustments, etc., to create additional images for the training set. A downside to image augmentation is that the transformations can ‚Äúleak‚Äù into the generated images which may not be desirable. For example, if you are generating human faces, you could use 90¬∞ rotation to augment the training data, but you may not want the generated faces to be rotated. Nvidia found that augmentations can be designed to be non-leaking on the condition that they are skipped with a non-zero probability. So if most of the images being fed into the discriminator are not rotated for augmentation, the generator will learn to not create images that are rotated [1]. ‚Ä¶the training implicitly undoes the corruptions and finds the correct distribution, as long as the corruption process is represented by an invertible transformation of probability distributions over the data space. We call such augmentation operators non-leaking. ‚Äî Tero Karras, et al. The new version of StyleGAN has a feature called Adaptive Discriminator Augmentation (ADA) that performs non-leaking image augmentations during training. A new hyperparameter, p, in the range of 0 to 1, determines how much and how often augmentations are to be applied to both the real images and the fake images during training. Here is a sample of augmentations with different values for p. You can see how the sample images show more variety with spatial and color changes as p increases from 0.0 to 1.0. (Note that SG2A does not show the augmentation done to the real images during training. So I added the functionality to do this in my fork of the code, here. You can run a test in the Google Colab, here.) The value p starts at 0 when training begins, and then increases or decreases if the system senses that there is overfitting or underfitting during training. The heuristic used in the system is based on the sign (positive or negative) of the output of the discriminator during the processing of a batch of generated images. If there are more positive output values than negative, then the trend is towards real images, which is an indication of overfitting. If there are more negative values than positive then the trend is towards fake images, which is an indication of underfitting. The value p is adjusted up or down accordingly after the batch ‚Äî up for overfitting and down for underfitting. A target value for p can be set, i.e. 0.7, so there is always a non-zero probability that the augmentations can be skipped, which avoids leaking. You can read more about Adaptive Discriminator Augmentation in Mayank Agarwal‚Äôs post here. I trained the system using Google Colab. It took about four days to run. The system has an Nvidia Tesla V100 GPU which can run up to 14 teraFLOPS (14 trillion floating-point operations per second). So it took about 4.6 exaFLOPS to train (about 4.6 quintillion floating-point operations). That‚Äôs a lot of math. Here‚Äôs the shell command I used to kick things off. Setting the aug parameter to ada enables the Adaptive Discriminator Augmentation to kick in when needed. Setting the target parameter to 0.7 prevents p from going over 0.7, which maximizes the dynamic augmentation without leaking any further augmentation into the final images. Here‚Äôs a graph of the training results over time. I am using the Fr√©chet Inception Distance (FID) as a metric for image quality and diversity, where a lower score is better [2]. You can read about the FID score in Cecelia Shao‚Äôs post here. You can see how StyleGAN2 ADA outperforms the original StyleGAN2 for the same number of iterations. The FID score for SG2A bottomed out at just over 100 after about 300 iterations. Here are some samples of the results. The GAN seems to produce a nice variety of abstract paintings with interesting compositions with varied color schemes. But there is room for improvement. It may not be intuitive, but it‚Äôs possible to improve the quality of the paintings by first training the GAN on a different, larger set of images, and then further train the model using the abstract paintings. This technique is called Transfer Learning. It was first described as a technique for training NNs in 1976 [3]. Transfer Learning is technique that uses a pre-trained neural network trained for Task 1 for achieving shorter training time in learning Task 2. [4] ‚Äî Stevo Bozinovski I remember learning in my Art History 101 class that many abstract painters started by painting figurative subjects, like people and landscapes. For example here are sequences of paintings by Mondrian, Klee, and Kandinsky that show their progressions from landscape to abstract art. I wanted to see if I could transfer the learning from creating landscape photos to perhaps improve its ability to create abstract paintings. As an experiment, I trained SG2A on 4,800 photographs of landscape photos from Flickr that are in the public domain. The dataset, by Arnaud Rougetet, is available on Kaggle, here. Here is a sample of the photos from Flickr. Note that I resized the photos to be 1,024 x 1,024 pixels each. I trained the SG2A system using another Google Colab to create new landscape images. It also took about four days to run the training. Here‚Äôs the shell command I used. The command is almost the same as the one I used to train using the abstract paintings. The only differences are the paths to the landscape folders on Google Drive. Here‚Äôs a graph that shows the FID scores of the training of landscapes (the green line). The score for the generated landscapes is much better than the abstract paintings. You can see that it bottomed out around 25 after about 150 iterations. The reason the landscapes have a better score than the abstract paintings is probably due to the larger training set ‚Äî 4,800 landscapes and only 850 abstract paintings. Here are some generated landscape photos that were created after four days of training SG2A. These look pretty good. The horizons are a bit distorted and the clouds seem somewhat fantastical, but they could otherwise pass as real landscape photos. Now let‚Äôs see if this learning can help with creating abstract art. The next step of the experiment was retraining SG2A to create abstract art starting with the model previously trained on landscapes. Here‚Äôs a flow diagram that shows the process. After the GAN was trained to create landscapes, it was further trained to create abstract paintings using prior knowledge of the landscapes. I trained the system using a third Google Colab. Here‚Äôs the shell command I used. I am training using abstract paintings, but the resume option starts the training with the GAN trained with landscapes. It took about four days to run the final training. Here is a graph of the FID scores. The new results are in blue. You can see that Transfer Learning helped improve the scores. The FID numbers for the latest training run settle in around 85, which is better than the previous run with a score of about 105. Here are some sample images that benefitted from the Transfer Learning. It may be subtle, but the images created with the model with Transfer Learning appear to be more refined, pleasing compositions. Please check out the appendix below for a gallery of samples. Once the GAN is trained, images can be generated using this command line. This will generate four images using random seeds 1, 2, 3, and 4. The truncation parameter, trunc, will determine how much variation there will be in the images. I found that the default of 0.5 is too low, and 1.5 gives much more variety. A Google Colab for generating images with a variety of aspect ratios is available here. It generates 21 images and lets you choose one to see at high resolution. Here is a sample of images. Additional work might include training on paintings of landscapes instead of photos for the Transfer Learning. This may help the GAN pick up on a painterly look to apply to abstract paintings. I would like to thank Jennifer Lim, Oliver Strimpel, Mahsa Mesgaran, and Vahid Khorasani for their help and feedback on this project. The 850 abstract paintings I collected can be found on Kaggle here. All source code for this project is available on GitHub. The sources are released under the CC BY-NC-SA license. [1] Karras, T., Aittala, M., Hellsten, J., Laine, S., Lehtinen, J., and Aila., T., ‚ÄúTraining Generative Adversarial Networks with Limited Data.‚Äù, October 7, 2020, https://arxiv.org/pdf/2006.06676.pdf [2] Eiter, T. and Mannila, H., ‚ÄúComputing the Discrete Fr√©chet Distance‚Äù, Christian Doppler Labor fuÃàr Expertensyteme, April 25, 1994, http://www.kr.tuwien.ac.at/staff/eiter/et-archive/cdtr9464.pdf [3] Bozinovskim S., and Fulgosi, A., ‚ÄúThe influence of pattern similarity and transfer learning upon training of a base perceptron B2.‚Äù Proceedings of Symposium Informatica, 3‚Äì121‚Äì5, 1976 [4] Bozinovski, S., ‚ÄúReminder of the first paper on transfer learning in neural networks, 1976‚Äù. Informatica 44: 291‚Äì302, 2020, http://www.informatica.si/index.php/informatica/article/view/2828",167,4,11,Towards Data Science,2021-01-01,2021
https://towardsdatascience.com/a-guide-to-expected-goals-63925ee71064,A Guide to Expected¬†Goals,Part 1: Data Exploration and Visualization,3,32,"['An Exploration of Expected Goals', 'What is xG?', 'Data Exploration']","Here, I will introduce the concept of expected goals (xG) and conduct an exploration of event data. This will represent the first part of a three part series on expected goals. Part II will be centered around constructing a machine-learning model from this event data, while Part III will explore the applications, strengths and deficiencies of this model. Results in football, more so than any other sport, can be greatly influenced by random moments and ‚Äúluck.‚Äù Near misses, deflected shots, goalkeeping errors, and controversial refereeing decisions alone can dictate the final result. Football is a game of inches. These effects are amplified by the fact that goals are rare events; a match produces 2.5 goals on average. Furthermore, a large majority of matches end in a draw or are decided by just a couple of goals, meaning a single goal can be largely significant to the result of a match. Luck and randomness can therefore have a notable effect when so many matches are defined by fine margins. This also makes performances difficult to evaluate; is a dogged 1‚Äì0 win a product of a deserving performance or a series of fortunate events? Sometimes, this is difficult to evaluate with the naked eye. It is our hope to quantify and qualify performances by eliminating as much randomness as possible when examining a match. In order to score a goal, you must first attempt a shot at goal. Assessing a performance ten or so years ago would simply entail taking a look at the total shots and shots on target. While these are useful tools for assessing chance creation, they do not tell the whole story as not all shots are created equal. There are many factors that influence the the likelihood of a shot resulting in a goal. This is where xG comes into play. xG measures the probability that a shot will result in a goal based on a number of factors. Such factors include the distance from where the shot was taken, angle with respect to the goal line, the game state (what is the score), if it was a header, if the shot came during a counter attack and other factors. For the purpose of simplicity, our exploration will focus on just three of these factors. We can use this metric to sum over all the chances in a match to determine how many goals a team should have scored based on the factors we aggregated in our model. We can go even further to apply this to a stretch of games, a season or even a manager‚Äôs tenure. xG therefore can serve as a gauge of how potent a team is in attack and how solid they are on the back. It can also be used to analyze a players ability to create shooting opportunities in dangerous areas and how well he takes his chances. In summary, the xG model helps us eliminate a portion of the random factors associated with scoring opportunities when we attempt to quantify a team‚Äôs ability to score goals, which in the end is the ultimate goal of football. We will see later that we can use xG to predict future results, guide decisions on player recruitment and evaluate coaching instructions, but first let‚Äôs try to explore some data. Before we get into building our xG model, we need to consider what sort of data we are interested in. Obviously, we need a large collection of shot data but more importantly we need the data to describe the type of shots that result in goals. We can deduce that the most important factors we need would be the distance from goal when the shot was taken, the angle with respect to the goal and what part of the body the shot was taken with. Football data is normally split into two forms: event data and tracking data. Event data records all on-ball events and where on the pitch they happened (such as shots, passes, tackles, dribbles), whereas tracking data records the positions of players and the ball through-out the game at regular intervals. The event data that I will use today comes from Wyscout. It covers all events from all matches across the top 5 domestics leagues in Europe (English Premier League, Ligue 1, Bundesliga, La Liga, Seria a) from the 2017/2018 season. While some of the findings in this section may seem elementary to those who have an extensive understanding of football, I always believe it is important to test our assumptions, as they can be misleading at times. I think the best place to start is to ask, where do most shots happen on the pitch? Right away, there are some conclusions we can draw. The distributions suggests that: The angle distribution agrees with the distance distribution in that shots taken from closer (larger angles) are much more difficult to produce. Just with a simple distribution chart, we can conclude that it is quite difficult to produce shots that are close and central to goal. While we now know how shots are distributed by distance and angle, we have yet to address how shots that result in goals differ from those that do not. The violin plot above plays a similar role to a box and whiskers plot but also provides use with the kernel distribution estimate of the data (essentially a smoothing of the distribution). In splitting up the data by the result of the shot, we can see that on average, shots that result in goals are taken from much closer to goal than shots that do not result in goals. The mean of shots resulting in goals is about 12 meters compared to about 18 meters for those that don‚Äôt bulge the net. Similarly, goals are typically scored from angles of 20 degrees to about 50 degrees. So while it is difficult to produce shooting chances close to goal, the violin plot suggests that those that are close and central tend to result in goals. Let‚Äôs see how headers impact the mean and the distributions. Headers, as we might expect, are normally taken within the 18 yard box (16.5 m). Interestingly the means and distributions of the results do not differ by much, so that is something we should consider down the line. We have gained some fabulous insight through some basic distribution plots but we can take this a step further. We can better visualize how these variables impact the result by plotting the density of shots on a pitch. That is, we would like to split the pitch up into bins, calculate the number of shots taken within each bin and then use a color gradient to visualize how the density differs from bin to bin. These density plots serve a similar function to the violin plots above but give us a much better visual understanding of which areas of the pitch normally produce shots and goals. This is because we can see how both distance and angle impact the distribution of shots on the same plot. As we learned with the violin plots: Before we delve deeper, we need to address why there is a sharp decrease in the number of shots on the edge of the box. This could be due to a number of reasons. There are other possibilities and it is difficult to make any sort of conclusion regarding the trough but for now we will have to live with it. Now to return to some more data visualization. As you might guess, we can plot a probability density to assess which areas of the pitch have a high probability of a shot resulting in the goal and which do not. As expected, the closer you shoot from the goal, the more likely the shot is to result in a goal. Notice that there are certain outliers in which the probability density in those bin are very high. This is because of the few shots taken from those areas, they resulted in goals. If we had say 10 seasons of data, we would see a much more homogenous probability density. One of the things that is most surprising to those who have never used xG is that the probability of scoring from beyond 11 meters out is in fact lower than maybe we appreciate when we see a game live. It is for this reason that this sort of analysis is important. We tend to over-estimate the quality of chance, such as shots from outside the box, when in fact there shots are quite difficult and inefficient. So, the next time your favorite player misses from 11 meters out, remember that he only really had a 3 in 10 chance of scoring anyway. Now what about headers? While headers exhibit a similar trend to regular shots, they have lower probability values overall. This seems to suggest that while headers happen closer to goal on average, they also represent a much more difficult chance to put away. As we will see down the line, this is an important discovery and one that impacts our interpretation of chance evaluation. Before we progress into some machine learning, I want to close with us trying to gain some insight into these trends. We have hypothesized that both the distance and the angle with the goal line affect the probability of a shot resulting in a goal and we have seen that with the graphs above. But what is the nature of this relationship? As we move away from the goal, how does the probability of scoring change? We can address these question as we have done before, with some well designed graphs. The first thing that pops out and is quite intriguing is that as we move further away from goal, the probability of scoring becomes exponentially more difficult. Now that is profound because it vastly diminishes the value of shots from distance. So why would this be? Up until now, we have ignored the fact that the angle with the goal decreases as we move away from the goal. So we have this sort of ‚Äòdoubling factor‚Äô for the distance. We can hypothesize that this is because as we increase the distance a shot is taken from, it not only has a longer distance to travel but the target also becomes smaller. Fantastic! Now we have seen the power of data visualization and how even the simplest graphs can help us discern the information locked behind large datasets. It is for this reason that spending time and effort on data exploration is so important. This will serve as a good foundation when we move onto some machine learning in the next part. See you soon! For all the code associated with the analysis and visualizations here, please visit my github.",116,0,10,Towards Data Science,2021-01-01,2021
https://towardsdatascience.com/visualize-bert-sequence-embeddings-an-unseen-way-1d6a351e4568,Visualize BERT sequence embeddings: An unseen¬†way,,10,30,"['Visualize BERT sequence embeddings: An unseen way', 'About', 'Why so many layers?', 'Hands onüí™', 'The companion Jupyter notebook', '‚ö†Ô∏è What we won‚Äôt be covering', 'Let‚Äôs get started', 'The Visualizationsüî¨üë®\u200d‚öïÔ∏è', 'Going furtherüôá\u200d‚ôÇÔ∏è', 'References']","Transformer-encoder based language models like BERT [1] have taken the NLP community by storm, with both research and development strata utilising these architectures heavily to solve their tasks. They have become ubiquitous by displaying state-of-the-art results on a wide range of language tasks like text classification, next-sentence prediction, etc. The BERT-base architecture stacks 12 encoder-layers, which brings it up to a whopping 100 million tuneable parameters! The BERT-large architecture takes it yet a notch higher with 24 encoder-layers and ~350 million parameters! ü§Ø In the forward pass for an input text sequence, the output from each of these encoder blocks can be seen as a sequence of contextualised embeddings. Each contextualised embedding sequence is then fed as an input to the next layer. This repetitive application of encoder layers enables: It is a known fact that when using these deep architectures, it is very easy to fall into the pits of overfitting. Looking at how each encoder layer offers its own embeddings for the input, an interesting question may arise in one‚Äôs head:‚ÄúAs I train my model, how effectively do each layer‚Äôs embeddings generalize on unseen data?‚ÄùIn simpler words, it is of interest to see to what extent each layer of BERT is able to find patterns in data that hold on unseen data. In this tutorial we will be talking about a really cool way to visualize how effective each layer is in finding patterns for a classification task. With the motivation set, let‚Äôs look at what we will be doing. Train a BERT model for multiple epochs, and visualize how well each layer separates out the data over these epochs. We will be training the BERT for a sequence classification task (using the BertForSequenceClassification class). The same exercise can be extended to other tasks with some tweaks in implementation details. For example, language modeling (using the BertForMaskedLM class). Re-train a language model on your own dataset, and inspect the characteristics of each cluster or the distribution of embeddings! Training: Visualization: Dataset: HatEval [2], a dataset with tweets labeled as Hateful/Neutral. However, feel free to load your own dataset in the companion Jupyter notebook. I am placing the complete code for data loading, model training and embedding visualization in this notebook. The code present in this tutorial is intended only for explanation purposes. Please refer the notebook for complete working code. Since this article focuses only on the visualization of layer embeddings, we will be walking through only relevant parts of the code. Rest of the code lies outside the scope of this tutorial.I assume a prior knowledge of the ü§óTransformers BERT basic workflow (data preparation, training/eval loops, etc). Extract Hidden States of each BERT encoder layer: Next, we define a function that can plot the layers‚Äô embeddings for a split of our dataset (eg- train/val/test) after an epoch: These computed values are finally plotted on a new plot using the Seaborn library. Finally, putting together what we‚Äôve seen till now inside a training loop: Here we call the visualize_layerwise_embeddings function once per epoch for every split of the dataset we want to visualize separately. I choose to visualize embeddings from the first 4 and last 4 layers. We have our visualizations ready!I took a step further to make things more convenient by stitching the different images into a gif! Once again, code is present in the notebook.Pretty aesthetic, right?ü§© Spend a moment on each layer‚Äôs output. Try to draw some interesting inferences out of them! I‚Äôll give some examples:- can you comment on how each layer is performing with each successive epoch?- the train accuracy of the classifier dropped from epoch 4 to epoch 5! Can you verify this fact from the above gif? ** Finally, we are more interested in knowing if our embeddings are helping us generalize. We can judge that by the validation split visualizations above. Some interesting questions from the top of my head are:- Which layer are generalizing better that the others?- How well is the last layer able to separate out the classes?- Do you see any difference in the separability between the embeddings for train and validation splits?- Does taking an average of the embeddings across all non-masked tokens of the sequence produce better results that taking embedding only for ‚Äò[CLS]‚Äô token? (You might have to tweak the notebook a little to answer this oneüòâ) Don‚Äôt stop just here, yet!Go and play around in the notebook provided, and try to mix and match different layers‚Äô output embeddings to see which combination helps you produce the best downstream performance! ** Answer: the embeddings for the 12th layer are more neatly clustered for epoch 4 as compared to epoch 5! It is a clear indicator of the classifier having hit and then over-shot a minima in the loss-function space. [1]: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, Delvin et al., 2019[2]: SemEval-2019 Task 5: Multilingual Detection of Hate Speech Against Immigrants and Women in Twitter, Basile et al., 2019",142,1,6,Towards Data Science,2021-01-01,2021
https://towardsdatascience.com/project-modeling-predicting-of-churning-customers-in-r-cb0a846ba94a,"<strong class=""markup--strong markup--h3-strong"">Project: Modeling &amp; Predicting of Churning Customers (in¬†R)</strong>",You have just been hired as a Data Scientist,8,40,"['Project: Modeling & Predicting of Churning Customers (in R)', '1. Introduction', 'Scenario:', 'Goal:', 'Features & Predictor:', '2. Data Wrangling', '3. Exploratory Data Analysis (EDA)', '4. Modeling + Confidence Intervals']","Table of Contents 1. Introduction 2. Data Wrangling 3. Exploratory Data Analysis 4. Modeling 5. References You have just been hired as a Data Scientist. A manager at the bank is disturbed with an alarming number of customers leaving their credit card services. You have been hired as a data scientist to predict who is gonna get leave their company so they can proactively go to the customer to provide them better services and turn customers‚Äô decisions in the opposite direction. A data collector hands you this data to perform Data Analysis and wants you to examine trends & correlations within our data. We would like to do model & predict who will leave our companyNote: Exit Flag is the discrete target variable here for which we have to create the predictive model, but we will also use Credit Limit for another modeling example walkthrough for a continuous predictor. In Part 1 of 3 of Data Wrangling, we read in our data file & install all required libraries/packages for our project. We also examine if there are any problems with our dataset, & hence see that there are no issues. In Part 2 of 3 of Data Wrangling, we manipulate the data to get only the columns we want & to remove NA & Unknown values in our data. We also examine the dimensions & unique values for our discrete variables. 6 distinct discrete types for Income_Category :$60K ‚Äî $80K, Less than $40K ,$80K ‚Äî $120K ,$40K ‚Äî $60K ,$120K + , Unknown  4 distinct discrete types for Marital_Status: Married, Single, Divorced, Unknown 4 distinct discrete types for Card_Category: Blue, Gold, Siler, Platinum Note: We will also remove any rows/entries with a ‚ÄúUnknown‚Äù/NA value. We see here we initally have 10,127 rows & 23 columns, but we truncate that too 8348 rows by 9 columns. In Part 3 of 3 Data Wrangling, we rename our predictor Column Attrition_Flag to Exited_Flag. We also rename the binary output values for this predictor from Existing Customer/Attrited Customer to Current/Exited, respectivley. We lastly, also see the cout of each discrete feature with our discrete predictor. Above, we can evidently see that Current Customers had higher mean credit limits than did churning customers. In part 1 of 2 in EDA, we reorder the Factor levels of certain discrete variables, because we want them to display in order in our graphs. We have 2 discrete variables plotted against each other, with 1 of the variables being a fill. We examine the count of each visually. In part 2 of 2 in EDA, we explore 2 violin plots with quantiles, a visualization for 2 discrete variables, & area distribution plot. Quantiles can tell us a wide array of information. The 1st horizontal line tells us the 1st quantile, or the 25th percentile- the number that separates the lowest 25% of the group from the highest 75% of the credit limit. Next, the 2nd line is the Median, or 50th percentile ‚Äî the number in the middle of the credit limit. Lastly, the 3rd line is the 3rd quartile, Q3, or 75th percentile ‚Äî the number that separates the lowest 75% of the group from the highest 25% of the group. Quantiles are shown both in the violin plots above & below. We have only 16.07% of customers who have churned. Hence, it‚Äôs a bit difficult to train our model to predict churning customers. We will do our best with the data given, as with any model. Our goal in modeling is to provide a simple low-dimensional summary of a dataset.We use models to partition data into patterns and residuals. Fitted model is just the closest model from a family of models. Classification models are models that predict a categorical label. It will be interesting to study which characteristic(s) discriminates each category and to what extent. Predicting whether a customer will a customer will exit or stay with the company. Below we will use a logistic regression algorithm. First we must convert the non numeric variables (discrete variables) into factors. Factors are the data objects which are used to categorize the data and store it as levels. We fit the logistic regression model. The first step is to instantiate the algorithm. We declare binomial because 2 possible outcomes: exit/current. The significance code ‚Äò***‚Äô in the above below output shows the relative importance of the feature variablesAIC estimates the relative amount of information lost by a given model:the less information a model loses, the higher the quality of that model. Lower AIC values indicate a better-fit model, You have learned techniques of building a classification model in R using the powerful logistic regression algorithm. The baseline accuracy for the training & test data was 84 percent. Overall, the logistic regression model is beating the baseline accuracy by a big margin on both the train and test datasets, and the results are very good. Below I will show you another way to model this, using SVM (Support Vector Machine). Confidence intervals are of interest in modeling because they are often used in model validation. Next, we consider the 95% confidence interval of Credit Limit.As the Credit Limit is greater than 0, we narrow the confidence interval.There are 91.75% data locates within the confidence interval. We will keep the corresponding records and store the rest in another variable rest.data for latter analysis. #The following aims to build a model to predict the price. ## Evaluation We will use MSE as the criteria to measure the model performance. We Split 20% as test dataset and 80% as training dataset. To recal: AIC estimates the relative amount of information lost by a given model: The less information a model loses, the higher the quality of that model. Thus, the lower AIC values indicate a better-fit model, The AIC table shows the best model is Credit_Limit ~ Customer_Age + Gender + Dependent_count + Education_Level + Marital_Status + Income_Category + Card_CategoryWith a AIC of 72870.02, we want the lowest AIC. Gender Male, Income Categories, and Silver Card Card Categories were highly significant with respect to our predictor of Credit Limit. Here is access to the data set & code from my GitHub page: https://github.com/jararzaidi/ModelingChurningCustomersR https://www.kaggle.com/sakshigoyal7/credit-card-customers Recommendations & comments are welcomed!",94,0,14,Towards Data Science,2021-01-01,2021
https://towardsdatascience.com/stock-price-analysis-with-pandas-and-altair-ef1e178cc744,Stock Price Analysis with Pandas and¬†Altair,Practical guide for Pandas and¬†Altair,1,27,['Stock Price Analysis with Pandas and Altair'],"Stock price analysis is an example of time series analysis which is one of the primary areas in predictive analytics. Time series data contains measurements or observations attached to sequential time stamps. In case of stock prices, the time stamps can be seconds, minutes, or days depending on the task. This article is a practical guide that covers some basic and essential operations in stock price analysis. We will be using Pandas for data manipulation and Altair for data visualization. Both are Python libraries that are commonly used in data science. There are many resources to get stock price data. We will use the data reader API of Pandas. We first import the dependencies. We can now use the data reader to create Pandas dataframes that contain stock price data. We use the DataReader function to get the stock prices. The stock name, start and end dates, and the source are the required parameters. This function returns a dataframe as below: We have a separate dataframe for Apple, IBM, and Microsoft. It is better to combine them but we need a column to specify the name of the stock. The symbol column in each dataframe indicates the name. We can now combine them using the concat function of Pandas. I have only included the date, close, volume, and symbol columns. Let‚Äôs first create a basic line plot of Apple stock prices. The first line in the code is a top-level Chart object. We pass the data to be plotted to this object. A filter is applied using the symbol column to only include data that belongs to Apple stocks. In the next line, the type of the plot is specified. The encode function indicates the columns to be plotted. Whatever we write as an argument in the encode function must be linked to the data passed to the Chart object. The stock price of Apple seems to be continuously increasing except for a few occasional drops. The one in the April 2020 might be related to the global pandemic due to corona virus. We can show the stock prices of all three companies in one visualization. What distinguishes the companies is the symbol column. It is passed as an argument to the color parameter of the encode function. Apple and Microsoft stock prices follow a similar trend. IBM seems to suffer more from the pandemic. In the last line, we use the properties function to change the size of the figure. We can smooth the line by resampling the data. Resampling basically means representing the data with a different frequency. One option is to use the resample function Pandas. It aggregates data based on specified frequency and aggregation function. The code above resamples the Microsoft stock prices based on the average of 7-day periods. The only difference is that we pass the resampled data to the Chart object. The line seems much more smooth now. Altair makes it quite simple to have multiple plots in one visualization. For instance, we can plot the closing price and volume for Microsoft stocks as below. We assign each plot to a variable. The variables can easily be combined by logical operators. For instance, ‚Äú|‚Äù puts them side-by-side whereas ‚Äú&‚Äù puts the second plot under the first one. Altair provides many options to enrich the visualizations by conveying more information. For instance, we can add a line that indicates the average value. The first plot is a line plot just like the ones created earlier. The second plot is a rule that calculates the average value for each line. It is pretty straightforward to transform data with Altair. For instance, in the encode function of the second plot, the aggregation is indicated with a string (‚Äòaverage(Close)‚Äô). In our case, we can visually compare the mean values because there is a significant gap between values. However, it might be useful to indicate the mean when the values are close. There are many definitions of time series data, all of which indicate the same meaning in a different way. A straightforward definition is that time series data includes data points attached to sequential time stamps. We have covered some basic operations with Pandas and Altair. There is much more to cover in time series analysis. In fact, it is a highly complex topic and there is a great amount of research being done in this field. With that being said, it is almost beneficial to comprehend the basics and then start on the more advanced topics. Thank you for reading. Please let me know if you have any feedback.",94,1,5,Towards Data Science,2021-01-01,2021
https://towardsdatascience.com/cohort-analysis-with-python-2bdf05e36f57,Cohort Analysis with¬†Python,A data clustering skill that every e-commerce data analyst must¬†master,2,16,"['Cohort Analysis with Python', 'Introduction']","If you are a data analyst working for an e-commerce company, there is a high chance that one of your tasks at work is to discover insights from customer data to improve customer retention. However, the customer data is massive and each customer behaves differently. Customer A who was acquired in March 2020 displays a different behaviour from customer B who was acquired in May 2020. Therefore, it is essential to group customers into different clusters and then investigate the behaviour of each cluster over time. This is called cohort analysis. Cohort Analysis is the data analytic technique in understanding the behaviour of a special group of customers over a period of time. In this article, I will not go into details the theory of cohort analysis. If you do not know what cohort analysis is all about, I would strongly recommend you read this blog first. This article is more to show you how to segment customers into different cohorts and observe the retention rate for each cohort over a period of time. Let‚Äôs get into it! You can download the data here. Here is it not wise to just simply select df.loc[df['customer_type']]. Let me explain why. In this data, under the customer_type column, First_time refers to new customer, whereas Returning refers to returning customer. So if I first bought on 31 Dec 2019, the data will show me as new customer on 31 Dec 2019 but as returning customer on my 2nd, 3rd, ‚Ä¶ time. The cohort analysis looks at new customers and their subsequent purchases. Therefore, if we simply use df.loc[df['customer_type']=='First-time',] we would be ignoring the subsequent purchases of the new customers, which is not a correct way to analyse cohort behaviour. Thus, what I did here was that, first create a list of all the first-time customers and store it as first_time. Then from the original customer dataframe df select only those customers whose IDs fall within the first_time customers group. By doing this, we can make sure that we obtain the data that has only first-time customers and their subsequent purchases. Now, let‚Äôs drop the customer_type column because it is not necessary anymore. Also, convert the day column into correct date-time format purchase_rate function will determine whether that is a 2nd, 3rd, 4th purchase of each customer. join_date function allows us to identify the date the customer joins. age_by_month function gives us how many months from the current purchase of a customer to the first time purchase. Now the input is ready. Let‚Äôs create cohorts. How to interpret this tableTake cohort 2018‚Äì01 as an example. In Jan 2018, there were 462 new customers. Out of these 462, 121 customers came back and purchased in Feb 2018, 125 in Mar 2018 and so on and so forth. That is it. Hope you guys enjoyed and picked up something from this article. If you have any questions, feel free to put them down in the comment section below. Thank you for your read. Have a great day and happy new year üéâüéâüéâ",196,3,5,Towards Data Science,2021-01-01,2021
https://towardsdatascience.com/a-skill-to-master-in-python-d6054394e073,How to Slice Sequences in¬†Python,Learn how to slice lists and strings in¬†Python,6,74,"['How to Slice Sequences in Python', 'Indexing a Sequence', 'Slicing a Sequence', 'Slicing Strings', 'Palindrome Example', 'Slice Assignment']","Being able to efficiently slice sequences in Python (such as lists, strings, and tuples) is one of the most crucial skills to have when programming. Luckily, python provides indexing syntax that greatly facilitates the slicing process and makes it much more intuitive. In this tutorial, we will first review how to index a sequence and then move on to slicing sequences, specifically lists and strings. Furthermore, we will cover some important differences between slicing lists and slicing strings. We will then review slice assignment in lists. And lastly, we will look at what exactly is happening when we use the indexing syntax in Python. Before we start with slicing, let‚Äôs briefly review how to index elements in a sequence (specifically a list). Remember that we can access individual elements in a list by using their index within square brackets. Let‚Äôs look at a list of numbers below: The index of an element in a sequence is its location in that sequence. In the example above, we have a list of numbers, num_list, and the numbers below that list represent the indices of the corresponding elements. As we may recall, we can either index our sequence starting from the beginning (from the left) with positive indexing starting with index 0, or starting from the end of the sequence (from the right) with negative indexing, starting with index -1. In other words, if we want to retrieve the number 10 (or third element) from num_list, we can either use its positive index of 2, or negative index of -7: If we want to obtain the last number in our list, which is 40, we can do so using the index 8 or -1: Or we can just use the len() function as follows: If we use an index value that‚Äôs not present in our list or out of range, we will receive an IndexError: Now that we‚Äôve reviewed how to index a sequence using positive and negative indexing, let‚Äôs look at slicing. towardsdatascience.com We just saw how indexing can be used to retrieve individual elements from a list. Slicing, on the other hand, allows us to obtain a portion from our sequence, such as of a list or string. Sometimes to understand slicing it can be useful to imagine that the indices are pointing between the elements, rather than to the elements themselves. Although this is only useful when the step value is positive, meaning when we are slicing from left to right. More on step values later. The syntax for slicing a sequence is as follows: variable[start:stop:step] In order to slice a sequence, we need to use a colon within square brackets. In other words, the colon (:) in subscript notation [square brackets] make slice notation. Even though there are three possible values that we can provide within the brackets (the start value, stop value, and step/stride value), we don‚Äôt actually have to provide all three unless we need to, as we will see below. Let‚Äôs look at some examples. One way we can slice a sequence, such as a list, is by specifying the start and stop values. In other words, if we want all the elements between two specific points in our list, we can use the following format: variable[start:stop] variable[start:stop] returns the portion of the variable that starts with position start, and up to but not including position stop. For example, if we want to obtain all the elements from index 2 up to and including index 6, we can do so as follows: Notice how the start value is inclusive, but the stop value is exclusive. Thus, we start with index 2, which is the number 10, and go all the way to but not including the index 7, which would be the number 30 at index 6. If we imagine the indices as between the elements (as seen above), then that is further illustrated, since the index 7 is before the number 35. Since we did not provide a step value, the default step value is 1. Thus, we start with index 2, then take 1 step to index 3, then 1 step to index 4, and so on. In other words, because the step value is positive, we are increasing the index by 1 (moving to the right) as we are slicing our list. If we want to start at a specific number and go through the entire list, then we would only need to provide the start value. variable[start:] variable[start:] returns the portion of the variable that starts with position start, through the end of the sequence. For example, if we want to retrieve all the elements from the second index through the entire list, we can use the following code: As we can see, if we only provide an index before the colon, then that will be our start index, and we will obtain the rest of the elements in the list (since the step value is still 1). If we want to start from the beginning of the list and go up to a specific index, then we would only need to provide the stop value. variable[:stop] variable[:stop] returns the portion of the variable that starts at the beginning of the sequence, up to but not including position stop. For example, if we want to retrieve all the elements from the start of the list up to and including index 7, we can do so as follows: Thus, if no number is provided for the start value, then it assumes that we want to start from index 0. And since we want to retrieve all elements up until index 7, we would use the stop value of 8 since it is exclusive. We can also use -1 as the stop value. We can also mix and match the positive and negative indices. For example, if we want to retrieve all the elements between index 2 up to and including index 7, we can do so as follows: Note that in all instances, the stop value is to the right of the start value, since we are using a positive step value. In other words, the stop value must be in the direction of the step value, in relation to the start value. If the step value is positive, then the stop value must be right of the start value. If the step value is negative, then the stop value must be left of the start value. More on that later. We can also retrieve the entire list by just using a colon with no start or stop values. variable[:] variable[:] returns the entire sequence. towardsdatascience.com So far we‚Äôve only specified start and/or stop values, where we start at the start value, and end right before the stop value (since it is exclusive). But what if we don‚Äôt want all the elements between those two points? What if we want every other element? That‚Äôs where the step value comes in. Let‚Äôs say that we want every other value in our list, starting from index 0. Or perhaps we only want the elements at even indices. We can do so using the step value: variable[::step] Since we did not specify a start or stop value, it assumes that we want to start at the beginning of the sequence and go through the entire list. So it starts at index 0, then goes to index 2 (since the step is 2), then to index 4, and so on. Previously, we mentioned that the stop value must be in the same direction as the step value relative to the start value. In other words, if the step value is positive, which means we are moving to the right, the stop value must be to the right of the start value. If the step value is negative, then the stop value must be to the left of the start value. Otherwise, an empty list is returned: As we can see, in both examples, the start value is 8, and the stop value is 5, so the stop value is to the left of the start value. In the first example, the step value is +1. Since the stop value is to the left of the start value, and our step value is positive, an empty list is returned, since we cannot move in the direction of the stop value. However, in the second example, we changed the step value to -1. Thus, we start at index 8, which is 40, move 1 index in the negative or left direction to index 7, which is 35, then to index 6, which is 30. We do not go to index 5 because the stop value is exclusive. Perhaps the most important practical application of the step value is to reverse a sequence. For example, if we want to retrieve the entire list in reverse order, we can do so by using a -1 for the step value: Since we did not specify a start or stop value, then the entire sequence will be retrieved. However, since our step value is -1, it obtains the elements in reverse order. What if our stop value is greater than the highest index available in our sequence? Or if our start and/or stop values are out of range? In other words, what happens if we ask for more items than are present? For example, if we try the following: As we can see, even if we ask for more items than present in our sequence, it just returns whatever elements are present and does not give us an error. In contrast, if we try to index a single element that is out of range (instead of slicing), then we would get an IndexError as we saw earlier. Indexing and slicing work the same way for strings as well. Again, we can imagine the indices between the characters if we are using a positive step value as follows: Thus, to obtain the substring ‚Äòyt‚Äô via slicing, we can do so as follows: To reverse a string, we can do so by using a step value of -1: Let‚Äôs use what we learned to solve a very commonly asked python coding question. We want to write a function that takes in a string, and returns whether or not that string is a palindrome. A string is a palindrome if the reverse of the string is identical to the original string. For example, ‚Äòcivic‚Äô is a palindrome, but ‚Äòradio‚Äô is not, since the reverse of ‚Äòradio‚Äô is ‚Äòoidar‚Äô, but the reverse of ‚Äòcivic‚Äô is ‚Äòcivic‚Äô. We just learned how to reverse a sequence by using a step value of -1. Thus we can easily write a function that accomplishes this as follows: And that‚Äôs it! The expression word == word[::-1] is evaluated to either True or False. If the string we pass in is equal to its reverse, then the expression evaluates to True, and True is returned. If the string we pass in does not equal its reverse, then the expression evaluates to False, and False is returned. If we recall, lists are mutable objects in python. In other words, they are able to be mutated, or changed. Thus, we can use slice assignment operation to mutate or edit a list in place. Notice how we can replace a slice of our list with more or less elements. We can also delete a part or slice of a list using the del keyword: Note: Strings and tuples are not mutable. Thus we can not edit or mutate them like we can with lists. Slicing a list will return a copy of that list and not a reference to the original list. We can see this here: if we assign our list slice to another list, since the list slice returns a copy and not a reference to the original list, we can modify the new list (since lists are mutable) without affecting the original list: In contrast, when we slice a string, a reference to the original string object is returned, and not a copy. And remember, strings are not mutable in Python. We can use Python‚Äôs identity operator (is) and the equality operator (==) to confirm that slicing a list returns a copy or a different object than the original list, but slicing a string returns a reference to the original string object: The equality operator (==) checks if the values are equal. The identity operator (is) checks if the two variables point to the same object in memory. levelup.gitconnected.com When we use the indexing syntax in python, which includes the use of a colon within square brackets, the built-in slice function is actually being used to create a slice object. slice(stop) slice(start, stop[,step]) The slice function can be used in two different ways to create a slice object (similar to the range function which creates a range object). If we pass one argument to the slice function, then that will be the stop value. If we pass in three arguments to the slice function, then they will take on the start, stop, and step values. In other words, the start and step parameters will default to None. Here are some examples showing the slice objects used when using indexing syntax to slice a list: Using the slice function to create a slice object can be useful if we want to save a specific slice object and use it multiple times. We can do so by first instantiating a slice object and assigning it to a variable, and then using that variable within square brackets. If you enjoy reading stories like these and want to support me as a writer, consider signing up to become a Medium member. It‚Äôs $5 a month, giving you unlimited access to stories on Medium. If you sign up using my link, I‚Äôll earn a small commission. lmatalka90.medium.com In this tutorial, we first reviewed that indexing a sequence means to extract a single element using by using its positive or negative index within square brackets. We then compared indexing a sequence to slicing a sequence, which can retrieve a portion of a sequence. We learned how to slice a sequence with square brackets and a colon, including the different ways to specify which portion we want to retrieve. And since lists are mutable in Python, we saw how we can use slice assignment to change portions of our lists. We then saw the differences between slicing a list and slicing a string, in that slicing a list returns a copy of that list, but slicing a string returns a reference to the original string object. Lastly, we saw how the slice object is actually being used when using slicing notation in Python.",100,0,12,Towards Data Science,2021-01-01,2021
https://towardsdatascience.com/how-to-present-machine-learning-results-to-non-technical-people-e096cc1b9f76,How to Present Machine Learning Results to Non-Technical People,Showing model results stakeholders can‚Ä¶,1,14,['How to Present Machine Learning Results to Non-Technical People'],"As a data scientist it was difficult to explain machine learning results to non-technical stakeholders. After some trial and error I came up with a way to transform model results into a format my stakeholders were able to understand. Today I‚Äôd like to share my methodology that I‚Äôve used with great success. This methodology can apply to any model that generates probability score values between 0 and 1. Below is an example of the output using this methodology on a sample set of purchase propensity model scores. The percentages in the ‚Äú% of Total Purchases‚Äù column were calculated by taking purchases in the decile divided by total purchases of 31,197. These are the key takeaways from this table. I‚Äôve seen data science presentations with model results that was beyond the grasp of non-technical people. However, if you presented model results with a table that showed the top 20% of customers captured 65% of purchases that‚Äôs easy for your stakeholders to understand. This second approach is similar to the first and is a view that also makes it easier to evaluate the model results as a data scientist. Below is an example of the output using the same set of sample purchase propensity scores. These are the takeaways using this output binned by score values. This output shows stakeholders your model captures 67% of the purchases accurately. It also helps you identify issues if the model is not predicting true positives and show areas where you can research further as in the case of the customers with a score between 0.1 and 0.19. As data scientists, the impulse is to show the raw model results but often we need to transform the output into a form stakeholders can understand. Now that you seen my approaches I hope you find it easier to articulate your model results for everyone to understand. medium.com towardsdatascience.com towardsdatascience.com",212,3,4,Towards Data Science,2021-01-01,2021
https://towardsdatascience.com/how-to-utilize-feature-importance-correctly-1f196b061192,How to utilize Feature Importance correctly,Feature Importance is a part of ,5,20,"['How to utilize Feature Importance correctly', 'Why Feature Importance‚ùì', 'What is included in Feature Importance? ‚úã', 'Why not only Feature Importance score alone üåì', 'Conclusion üóΩ']","In training a machine learning model, the ideal thing is to condense the training features into a set of variables that contain as much information as possible. There are 3 reasons for this. 1, reducing the number of features means narrowing down the dimension, reducing sparsity, and increasing the statistical significance of the training result. If we have too many features, we will also need a larger number of samples representative enough for training. 2, reducing sparsity helps to prevent the over-fitting problem (the more the number of features, the more complex the model becomes, hence more tendency to be over-fitting). 3, which is obvious, is the efficient saving of computation power while achieving acceptable performance. Thus, feature selection is an important step in preprocessing data. Among popular approaches, feature importance is one of the most popular ones. However, choosing features through Feature Importance is not always as straight as an arrow. It is indeed required some techniques to make the right decision for selection. However, from my experience, I usually use feature importance after conducting other data cleaning, preprocessing steps as below. There are several ways to express Feature Importance. It can be score representing the features‚Äô relevancy (using algorithm-based Feature Importance, such as tree-based Random Forest, XGBoost,..), dimension reduction from original features to synthesize all the information to a lower dimension (like Autoencoder or PCA), or domain knowledge selection (aka selection based on your own knowledge). With the variety of choices like this, which one should we choose? Even when we choose to use PCA, selecting the right number of remaining features seems to demand great exertion. Don‚Äôt you think we should go with the Feature Importance score or testing everything and see which performs the best? Yes, it‚Äôs correct, testing everything is the accurate answer. From my experience, I usually set the baseline as the original model without any selection, then try PCA with different drop-out ratios and check the metric. Then drop original features (mainly because I want to limit the amount of collected data needed, as well as optimize the training time while maintain or improve the performance). So Dimension Reduction is my first choice and the Feature Importance is the next essential step. But using only the Feature Importance score is hardly a good choice. Let me show you why. Feature Importance score here means any score generated from the trained model representing the weight or relevancy of the features to the prediction of the target feature. Below are the feature importance scores of Random Forest calculated based on RandomForestRegressor model (detail of formula is here), selecting feature with SelectFromModel (detail), and permutation score (detail of formula is here). Data used for demonstration is California housing in sklearn. Looking at the above, how confident are you in deciding to keep only MedInc and drop others (based on the first graph) or to retain MedInc, Latitude, Longitude, and AveOccup based on the second one, or even just drop AveBedrms and Population? If I were you, I merely can make any decision with this. Why? Because dropping this or that does not ensure better performance, it even leads to a worse one if you choose unconsciously. The selection from the Permutation score slightly improves from the original dataset. Let‚Äôs see another experiment with the Boston dataset (source). This looks more consistent than the California dataset, doesn‚Äôt it? I guess the decision to keeping ‚ÄòRM‚Äô and ‚ÄòLSTAT‚Äô features is much more confident now. Let see how the performance changes if the model is trained on these 2 features. None of these selections performs equally on par with the original model, which means reducing features does not work here. These 2 examples show different strategies on how to reduce the number of features, and either the metric or the score can help us decide here. ‚ÄúNo, we need a different approach!‚Äù There is one way to decide better which features should be removed. This technique is traditional but effective and holistic. We decide by seeing the performance of the model after dropping features. First let‚Äôs drop each feature one at a time, then drop one after another, measure their performance, and decide the elbow or the level that is sufficient. Dropping any among HouseAge, AveRooms, Population and AveBedrms obviously retain the performance at the same level with the original. However, removing MedInc surprisingly lowers the test MSE. When dropping them cumulatively from the least relevant feature to the most relevant (based on Feature Importance score), we can clearly see that the elbow in train MSE appeared when removing Latitude and other features prior to it, but the overfitting issue was worsened at the time dropping Longitude. Hence, we have more confidence to condense the model into 4 features: ‚ÄúLongitude‚Äù, ‚ÄúLatitude‚Äù, ‚ÄúAveOccup‚Äù, ‚ÄúMedInc‚Äù. Let‚Äôs take a look at the Boston dataset. Removing either RM, LSTAT or DIS worsens the performance, and if we only keep RM, LSTAT or DIS in the model, not only the train MSE but also test MSE significantly picked up. From looking at this, we can decide to keep NOX, CRIM, DIS, LSTAT, and RM. The score is not the only way to go. Using only the score to decide the approach in feature selection seems to be subjective and empirical. Using the appropriate method is like deciding the right metric for your model. Knowing exactly why you do that and what effect it brings to the model are keys to the next success for your machine learning.",244,2,5,Towards Data Science,2021-01-01,2021
https://towardsdatascience.com/15-lesser-known-useful-sklearn-models-you-should-use-now-a1a566e680a6,15 Lesser-Known Useful SkLearn Models You Should Use¬†Now,Some of the more interesting models that¬†can‚Ä¶,5,77,"['15 Lesser-Known Useful SkLearn Models You Should Use Now', 'Data', 'For Continuous Problems', 'For Classification Problems', 'Conclusion']","Sk Learn is likely one of the most popular machine-learning modules for Python. This is for good reason, as SkLearn has a fantastic catalog of usable models, scalers, tools, and even encoders! While there are some rather popular models that are very well-known, SkLearn is such a large library that it can be easy to forget all of the functions and classes that come with it. While documentation is a great place to start, another great way to expand your modeling capabilities is to get more familiar with is to just use different models. With that in mind, I have come to enjoy a lot of different models from the SkLearn. There are a lot of great models that go severely under-used. Today I wanted to bring some of my favorite models from SkLearn to your attention, so maybe next time you‚Äôre facing a unique problem, you‚Äôll know the model and its corresponding application! Today, we are going to be fitting models in order to demonstrate the usage of said models. Of course, to do this we are going to need some one-dimensional arrays to pass in order to predict features. In simpler terms, we need data to train off of. Given that we are working in a notebook, I figured it should be open-source, so if you would like to see these models fit in a notebook you can see the source here: notebook Since we are going to be reviewing classification models as well as continuous models, we are going to require targets that facilitate those feature types respectively. I am going to be using an old .CSV file that I have lying around called weatherHistory.csv: Now let‚Äôs have a look!: Since we are going to be using both classification and continuous models, I am going to need both a categorical and continuous Y. For the continuous target, I decided to use temperature. For classification, I went ahead and used the Precipitation Type feature. For my predicting features, I am going to use the Humidity feature: Now we will train test split that data accordingly into two dataframes: Of course, I am not going to be doing much processing to this data just to fit some models for an example. That in mind, those models can‚Äôt be fit to data with missing values, so let‚Äôs get a summation of the missing value counts in our dataframe. Wanna hear something funny? I could have sworn it was is_null() instead of isnull(). Looking at the summary, It is clear that I might have gotten a bit ahead of myself, let‚Äôs drop some bad observations: Now let‚Äôs put that into 1-dimensional arrays: SkLearn often requires these arrays to be reshaped to vertical, as it prefers features in columns of matrices as opposed to rows of matrices. Let‚Äôs reshape these puppies, and then we will be ready to fit some models. In order to do this, we‚Äôre going to need to turn these one dimensional arrays into NumPy arrays. This is because SkLearn is far more integrated with NumPy, though it does like Pandas Series in a lot of cases NumPy arrays are much more dynamic and commonly used: Among the two different types of models typically used for supervised models are continuous models. These models predict values that are quantitative, rather than qualitative. That being said, many of these models are going to utilize regression in order to estimate continuous values. Isotonic, or monotonic regression is an awesome form of regression that many machine-learning engineers have never even heard of. isotonic regression can be a very accurate model for predicting continuous targets, but also has its own limitations in that regard. A great example of this is that this model is often prone to over-fit, and often getting the model to work well is going to be balancing the bias and trying to increase accuracy. Another significant problem with this model is that the data must be non-decreasing. This means that typical applications of this model are often going to be involved in economical and business scenarios. So with that in mind, while this model might be a very useful one for someone working with economic data, for a lot of scientific work it isn‚Äôt necessarily the greatest model. However, isotonic regression in the proper application and balanced bias can be an incredibly powerful predictive model! If you would like to learn more about isotonic regression, you can check out these two articles I wrote, one where I compose an isotonic regressor from scratch in C++, and the other I elaborate on how exactly the model works: towardsdatascience.com towardsdatascience.com In order to fit this model, we are going to first need to use the make_regressor function which will give us a basic regression model at which we can build isotonic regression on top of. Let‚Äôs do that: Another very useful tool that only applies to certain data characteristics is Orthagonal Matching Pursuit. This model is used to take sparse-coded signals and remove noise and abnormalities in said data. This means that these machine-learning algorithms are utilized to fix certain incoming signals based on data, which I think is a pretty great application for machine-learning. While the primary use of Orthagonal Matching Pursuit might be relatively straightforward, the uses of this model could be potentially much further. Given that this is quite a unique model on the spectrum, how does it work? Orthagonal Matching Pursuit forms the exact operation described in its name. To dissect this definition, let‚Äôs look at the words individually: So basically, we are searching for perfect matches to our data for where it relies on a multi-dimensional span of data, D, which would likely be a dictionary type inside of the programming world. In Wikipedia‚Äôs words, .The idea is to create an approximate signal (f) from Hilbert space (H) as a weighted sum of statistical functions ‚Äî meaning PDFs/CDFs/Gamma. Although the data that we have certainly would not be a great application for this particular model, so instead for this example I am going to create some sparse signals to pass as data: Now we will distort our target data: And fit our model: If you‚Äôve been using machine-learning for any extended period of time, it is likely that you have heard of Lasso regression. Lasso regression is a fantastic and quite standardized tool that has been used frequently in machine-learning for an extended period of time now. Most of the time when predicting continuous targets, this is most certainly my first choice of model. However, the LARS lasso model is not your normal Lasso regressor. The LARS in ‚Äú LARS lasso model‚Äù is short for Least Angle regression. Least Angle regression is a machine-learning algorithm for predicting continuous features. It is most useful for working with data with incredibly high dimensions. This model works by a linear subset of covariates. One great thing about this model is that while the ceilings in terms of dimensional are dramatically raised from a traditional model, it really isn‚Äôt all that slow compared to a model that might typically be used in this way. That being said, it is important to remember that while this model is derived from Least Angle regression, that does not make it linear ‚Äî and that is where the lasso part of the model comes in. Of course, if a linear model is what you‚Äôre looking for, you can utilize the same concepts on a traditional linear regression model using Least Angle regression, which is also in SkLearn. As discussed in the similar application of Least Angle regression to a lasso model, Least Angle regression is a model used for predicting continuous features that typically work by using a linear subset of covariates. This is the most significant difference between Least Angle regression and LARS Lasso regression. Least Angle regression will be referring to the linear version of that model. Computationally, Least Angle regression has the advantage of being just as fast as forward selection. While that is a monsterous benefit, its biggest strength is in scenarios where p >> n. If two variables are equally correlated, then their coefficients should increase at the same rate. Another great implementation of a concept in SkLearn is stochastic gradient descent. Stochastic gradient descent is a method that is used iteratively to optimize mathematical functions and build cost. It is likely that you have heard of gradient descent, which is similar ‚Äî however, the stochastic in this model‚Äôs name means that we only use a single training example for epoch. This is high-end of the two extremes, starting with batch gradient descent, which will use the whole batch per epoch and mini-batch in the middle being a combination of the two. The gradient of the loss in stochastic gradient descent is estimated each sample at a time and the model is updated along the way with a decreasing strength schedule, also known as learning-rate in typical machine-learning terms. With regressive learning methods, there can often be short-comings to models that are difficult to overcome. This can certainly be said for linear models, which might have a hard time perfectly fitting to sparse data, or data with multiple dimensions to it. Fortunately, SkLearn has a decent implementation of Huber regression. This form of regression can be used to work with outliers and avoid modeling errors that might be easy to make using the typical models that are available in the SkLearn package. This can be useful because while this model is useful and fits well, it is also relatively simple, meaning that over-fitting and over-sampling aren‚Äôt typically problems that are encountered when working with this model. Huber regression optimizes the squared loss (mean squared error) and is considerably robust to outliers compared to models like simple linear regression. There is actually a very interesting paper published by Art B. Owen at Stanford University that might be worth checking out for those uninitiated or unfamiliar with this method of modeling. You can check it out if you‚Äôre interested: Here is the remark I found the most valid for analyzing what exactly this model is going to do with this data mathematically: ‚Äú The least squares criterion is well suited to yi with a Gaussian distribution but can give poor performance when yi has a heavier tailed distribution or what is almost the same, when there are outliers. Huber (1981) describes a robust estimator employing a loss function that is less affected by very large residual values‚Äù Needless to say, this model is incredibly cool! I think it definitely has its use in knocking out outliers as a contributing problem to the difficulty of predicting continuous problems ‚Äî which is often understating, but seems obvious in the realm of elementary statistics. Now that we‚Äôre familiar with this model somewhat mathematically, we can actually consider fitting it in Sklearn (that was a joke.) While this ‚Äú model‚Äù might be more of concept to utilize with other models, it is certainly going to be incredibly useful! A very common pattern with machine-learning is to use non-linear functions to create linear predictions. This will maintain a lot of the speed of the model while not wasting any of the prediction power. A great example of this is polynomial regression over simple linear regression. In examples where polynomial regression is used, it is fit with a higher dimension of data built with functions. Thanks to the use of polynomial features, this model can be fit on and used to solve a wide-range of continuous problems easily. In order to actually use polynomial regression in sklearn, we are actually going to use Polynomial Features: Ordinary least squares is another really cool mathematical machine-learning model for predicting continuous features. Ordinary least squares is also a linear model, and fits to coefficients that are created to minimize the sum of squares between points in the data. The weights for ordinary least squares rely heavily on the independence of features for predicting the target. That being said, this model is incredibly useful for implementations with a single feature. Furthermore, it can be used with multiple features but will certainly require the features to be weighted well towards the target. Looking into this description, it is easy to see the exact niche that a model like ordinary least squares is going to fit into in our machine-learning arsenal. The SkLearn implementation of OLS, unfortunately is not as straightforward as most. The OLS coefficients are actually contained beneath LinearRegression classes as .coef_: Going back to the magic of support vector machines, allow me to introduce you to NuSVR. NuSVR is of course the same model and machine implementation as NuSVC. Both of these models utilize libsvm and uses a parameter, nu, to control the number of support vectors in the machine. However, unlike NuSVC, where nu replaces C, here nu replaces the parameter epsilon of epsilon-SVR. The advantage of using this model is that parameters are provided for adjustment of the vector machine that is used to assist in estimation of this regression problem. That being said, this model is great for predicting continuous features with a lot of features that may or may not be incredibly important to our target. As with the LARS lasso, it is likely that you have heard of lasso regression. As a refresher, it is a fantastic non-linear model for predicting continuous features that is very commonly used. The difference in the multi-task lasso is that that the multi-task lasso is going to use the L1/L2 norm as regularize. To demonstrate this, let‚Äôs look at the optimization objective for Lasso regression: In this example, ||W||_21 would be modified to fit this formula for a multi-task lasso: Essentially what this means is that we are getting the summation of each row. In machine-learning, there might be a lot of continuous problems, but LinearSVC is a support vector machine type of model. The SkLearn implementation of the model was created using libsvm. While the kernel can be non-linear, its SMO does typically not scale very well to a large number of samples. This is where the Linear Support Vector Classifier comes in handy. That being said, while there are some other great SVC implementations in Sklearn that might even be more well-known, LinearSVC is certainly a model that is well-worth being aware of! SVC typically works in a multi-class mode implemented with one class weighted against one other class. In the case of LinearSVC, all of those classes are weighted against each other class ‚Äî making the model more comprehensive than many of the SVC competitors. As for the SkLearn class, it is an awesome and nearly flawless implementation. It supports both dense and sparse data incredibly well, and can be an incredible model ‚Äî easily one of the best for linear classification in Python in my opinion! As discussed when we briefly discussed the SGDRegressor, stochastic gradient descent is where each batch is used to weigh features at each iteration in cost. The SGDClassifier of course is the exact same concept now being applied to a classification problem. Like the SGDRegressor, this model is a great implementation that can be valuable when working with a large set of features in particular. While it might take a dramatic drop to performance, it might need to be weighed whether or not the resulting predictions are worth those particular efforts. Fitting the SGDRegressor is quite straightforward, and as with many on this list will follow the typical SkLearn convention of The Bernoulli naive Bayes classification model can be used just as any Bayesian classification model is used, however does have a trick up its sleeve: The Bernoulli distribution. You might be familiar with this distribution, as its CDF often appears near logistic classification models. I would say that this model‚Äôs usage is very similar to that of MultinomialNB. However, while MultinomialNB works significantly better with counts and occurrence, BernoulliNB uses the Bernoulli distribution and is designed for bool-type features. This of course brings back recollection of my initial statement, where Bernoulli is used for LogisticRegression, that being said, this model has essentially the same use ‚Äî so it makes sense that it uses the same distribution. Although this model is typically used for predicting binary features, today we are just going to be passing some typical categorical data that might be more applicable to MultinomialNB. That being said, it is important to remember the use-cases for both of these models because they are very powerful, but should be used for their appropriate feature types. There is not a point in using a model if you don‚Äôt understand the target that you are trying to predict, and I think the difference between these models highlights the need for data scientists to understand that different models require different types of features to work well. A great lesson to learn from just an import, but regardless it follows the typical convention we have come to expect from SkLearn: If you‚Äôve been working with machine-learning models, especially continuous models, it‚Äôs likely you‚Äôve heard of ridge regression. Ridge regression is a popular model used for predicting continuous features. RidgeClassification is of course the classification equivalent of this exact model for classification problems. The classification version of the model converts the target into {-1, 1} and then models it into a regression problem with the typical ridge regression. I think this is a really cool concept because applying regression and other continuous methods of solving problems to an entirely different problem like classification is really cool in my opinion. Fortunately, even though this model is really awesome and seems like an advanced concept, the wonderful SkLearn has made it incredibly easy to use with typical convention for the library: Probably one of the coolest models on this list is CalibratedClassifierCV. This model uses cross-validation both to estimate parameters of a classifier. The model can be used with a logistic regressor as a base, which will make it a great model for classifying boolean types. However, since this model can actually take different base estimators, a commonly used model is actually isotonic regression. Isotonic regression is a really cool model, but in my opinion becomes a lot cooler when combined with a classification problem. This means that the thresholds are now attached to classes, rather than arbitrary quantile amounts inside of continuous data. Unlike many of the other solutions for predicting targets on this list, this is another one that is going to be an additive for other models. In other words, we can calibrate essentially any classifier by simply building one for it. In this example I am going to be building one using Gaussian Naive Bayes, another classification model in SkLearn similar to Multinomial Naive Bayes. ‚Äú By the way, in my opinion this is a really cool methodology, I really think that SkLearn hit the mark on the way that objects are used and the convention that they use as classes.‚Äù SkLearn is such an awesome library that machine-learning engineers these days might take for granted. There are lot of models in the library that are absolutely incredible and might go mostly ignored because of the champions that are already available. How many of these models have you used? I hope that these descriptions and introductions to these awesome models was entertaining and perhaps even helpful in the model selection for your next project. Thank you very much for reading, and happy new year! I think these models will be very valuable assets in the future. Maybe if you run into a good binary classification problem, or a linear modeling problem, you will think back to this article I wrote in my first year. Hopefully this article inspires deeper digging and research to learn more about modeling, because it really is a lot of fun to learn about.",265,2,16,Towards Data Science,2021-01-01,2021
https://towardsdatascience.com/statistical-simulation-in-python-part-2-91f71f474f77,FAANG Ask These 4 Python Simulations in¬†2021,,9,47,"['FAANG Ask These 4 Python Simulations in 2021', 'Introduction', 'Question 1: Uniform Distribution', 'Question 2: Binomial Distribution', 'Question 3: Poisson Distribution', 'Question 4: Normal Distribution', 'Takeaways', 'My Data Science Interview Sequence', 'Enjoy reading this one?']","Updated on Jan-10‚Äì2021 Statistical Simulation is the most heavily tested topic in Data Science/Engineering Interviews! If we go over DS Interview questions posted at Glassdoor, statistical simulation is the key skill that all big tech companies expect their applicants to excel in. Interview scenarios may either be asked to perform a power analysis of A/B experiments or construct a binomial distribution to simulate user behaviors in a programming language, R or Python. These are not difficult questions but require a deep understanding of fundamental statistics and fluent programming skills. Without deliberate practice, these questions may likely trip you over. This post presents 4 types of the most often tested statistical distributions in Data Science interviews and live-code solutions in Python. In a previous post, I have covered the fundamentals of statistical thinking and R codes. In case you missed it, here is the portal: towardsdatascience.com Disclaimer: I assume my fellow readers understand the statistical basics (e.g., what a binomial distribution is) and some familiarity with the Python programming environment (e.g., how to write a simple for loop). Here is a light refresher on common statistical distributions by Zijing Zhu. In R or Python, please answer the following question. For a sequence of numbers, (a1,a2,a3,a4,‚Ä¶,an), please write a function that randomly returns each element, ai, with probability ai/‚àëai. (Condition 1) For example, for a sequence (1,2,3,4), the function returns element 1 with a probability of 1/10 and 4 with a probability of 4/10. (Condition 2) You can use any library, but no random.choice(). (Condition 3) This is a real interview question that I asked by a travel company. Let‚Äôs break it down. The question asks for a function that returns an element proportional to its weights, ai/‚àëai. It can be completed in two steps: For step 1, we do something like the follow: Here, there is a catch with the question: we can‚Äôt use the built-in method, random.choice() (Condition 3). Hypothetically, it would be a much easier question if we are allowed to import the Numpy package. Alternatively, we have to develop something to perform the same functionality as the random choice. Back then, I was clueless right on the spot. My interviewer kindly offered his first hint: you can use a uniform distribution with a range from 0 to 1 and compare the generated value (named a) to the cumulative probability sum (named cum_prob[i]) at each position i. If cum_prob[i] > a, then return the corresponding value from the sequence. The idea sounds great, and let‚Äôs check how the Python codes look like. People make a common mistake by trying to use a control flow (if-else statements) to filter out scenarios. It is doable for a small sample size but not practical if there are thousands of numbers in the sequence. We are not using 1,000+ ‚Äúif, elif, else‚Äù statements to tell Python what to do with the numbers, right? After looking back at this question months later, the most challenging part is to come up with the idea of using cumulative probability sum to simulate the process. It is more doable now after the detailed step-by-step explanations. An online shopping website (e.g., Amazon, Alibaba, etc.) wants to test out two versions of banners that will appear on the website‚Äôs top. The engineering team assigns the probability of visiting version A at 0.6 and version B at 0.4. After 10,000 visits, there are 6050 visitors being exposed to version A, and 3950 people exposed to version B. What is the probability that there are 6050 cases when the randomization process is correct? In other words, the probability for version A is indeed 0.6. This is a part of a hypothesis-testing question. Let‚Äôs break it down. There are two versions, A and B, and the experiment assigns the treatment to 10,000 people. So, it is a perfect setting for the adoption of a binomial distribution. However, the probability of receiving version A is slightly higher than version B. The final answer should return the probability of more than 6050 people receiving version A out of 10,000 trials. These pieces of information remind us to combine a binomial distribution with a conditional for loop, as shown below. The result is close to 15%, which carries practical value. This is a hypothesis testing question. Since the probability of observing 6,000 or above visitors is 15%, we fail to reject the null hypothesis and conclude there is no statistical difference between 6,000 and 6,050 visitors out of 10,000. In other words, the probability for version A is 0.6. We are have learned hypothesis testing and how to reject or fail to reject the null hypothesis, but a question like this makes me think twice about statistical simulation. My medium blog has 500 visits per day, and the number of visits follows a Poisson distribution. Out of 1000 times, what is the ratio of more than 510 visits per day? Write a function to simulate the process. This is a rather straightforward question. Since the question asks for how many times an event occurs in a specified time, we can follow a Poisson procedure. 31.8% of the simulation results have more than 510 visits. After publishing this post, the number of Medium blog skyrockets to another level. Write a function to generate X samples from a normal distribution and plot the histogram. This is a question asked by Google. It is a relatively easy coding question with two steps: Here it goes. We generate a normal distribution with 100 numbers and set X equals to 10. The complete Python code is available on my Github. Medium recently evolved its Writer Partner Program, which supports ordinary writers like myself. If you are not a subscriber yet and sign up via the following link, I‚Äôll receive a portion of the membership fees. leihua-ye.medium.com towardsdatascience.com towardsdatascience.com towardsdatascience.com Please find me on LinkedIn and Youtube. Also, check my other posts on Artificial Intelligence and Machine Learning.",29,2,6,Towards Data Science,2021-01-01,2021
https://towardsdatascience.com/data-science-trends-based-on-4-years-of-kaggle-surveys-60878d68551f,Data Science Trends Based on 4 Years of Kaggle¬†Surveys,"How have diversity, salaries, and the tools¬†and‚Ä¶",1,26,['Data Science Trends Based on 4 Years of Kaggle Surveys'],"Since 2017, the website Kaggle.com has performed an annual survey of data scientists and others interested in the field of data science and machine learning. The survey questions range from demographic questions, such as gender and level of higher education, to questions about programming languages, tools, and machine learning algorithms used. A full description, as well as the results, of each year‚Äôs survey can be found on Kaggle‚Äôs website for 2017, 2018, 2019, and 2020. Since it is the end of the year, and there are now four years of survey results, I thought it would be interesting to look at trends in the field over the past few years. New algorithms and techniques continue to be developed, so it will be interesting to see if newer techniques are replacing older ones, or simply being used alongside existing ones. The number of respondents has been consistently high, with between ~17,000 to 24,000 participants each year. Of those, approximately 2,400 to 4,100 survey respondents each year identify their job title as ‚Äòdata scientist‚Äô. On the left is a plot of the number of respondents, broken down by their reported job title. There are many respondents who have job titles other than data scientist. For the purposes of this article, I combined ‚Äúbusiness analysts‚Äù and ‚Äúdata analysts‚Äù into one category. For some reason, ‚Äúmachine learning (ML) engineer‚Äù only appeared as an option in the 2017 and 2020 surveys, so this category is missing in the 2018 and 2019 results. Before diving into the results, there were many questions that were only asked on one or two of the surveys, so I do not include that information here. The questions that were consistent across at least three of the four years of surveys were the demographic questions (such as age, job title, etc.) and questions about programming languages and machine learning algorithms actively used at work. Furthermore, the focus here is to look at trends in the practice of data science in industry, and, in fact, a lot of the questions in the Kaggle surveys are geared towards working professionals. Therefore, I focus on respondents who are employed in the field. In terms of people who are employed as data scientists, or in a job that involves work related to data science, the overwhelming majority are male. Over the past few years, there has been a slight improvement in the percent who are non-male, but except for data and business analysts, the percent of males in the field has remained solidly above 80%. The median total yearly compensation (i.e., salary plus bonus) has increased slightly for most job titles,¬π except for those with the job title ‚Äúsoftware engineer‚Äù, for whom salaries remained flat.¬≤ The final demographic trend I look at here is the formal education that data scientists have received. Interestingly, the fraction of data scientists with neither a PhD nor a Master‚Äôs degree has increased slightly, from 27% to 32%. There has been a proliferation of online courses in data science and machine learning, as well as bootcamps. Perhaps, the trend shown in these surveys is reflecting an increase in people coming into the field with this non-formal education. Moreover, this steady decrease in the fraction of workers with graduate-level degrees was true across the board for respondents with job titles other than ‚Äòdata scientist‚Äô. A common question for those entering the field of data science is which programming language they should learn. Looking at the programming languages used by data scientists, software engineers, and machine learning engineers, over 78% reported using Python at work across the four years of the survey. For data and business analysts, the fraction using Python has increased dramatically, from 61% to 87%. There is still a solid camp of data scientists who use R, however, the fraction of data scientists who are using R in the field is dropping rapidly. For both data scientists and data analysts, the percent of workers using R has dropped over 33 percentage points (from 64% to 23% for data scientists). In parallel to this question of what programming languages data science practitioners are actively using, Kaggle also asked respondents which one language they would most recommend to an aspiring data scientist to learn. Across the board, the percent recommending Python has increased, with R being the language seeing the largest corresponding decrease. Another interesting trend is the drop in the fraction of data scientists and data analysts using SQL, approximately 30 percentage points. Despite this, however, the fraction of data scientists recommending that aspiring data scientists learn SQL first has actually increased slightly (from 3% to 7%). For Machine Learning Engineers, the usage of different programming languages has remained flat, or even slightly decreased. This can either be due to Machine Learning Engineers becoming more focused on a single language and using less of a variety of languages, or there are other languages they are using more than some of the ones included here from the survey. Although, in terms of language Machine Learning Engineers recommend learning, the percent recommending Python increased slightly. For people with job titles other than the ones shown in the plots here, there are similar trends, in that Python usage has increased, while R usage has decreased. Finally, the heart of data science is the techniques used for analyzing data and making predictions. In some years of the surveys, there were questions about general techniques and how much time is spent on different parts of the data science workflow. However, one question that appeared in three out of the four surveys was specifically about the machine learning algorithms used (for some reason, this question did not appear in the 2018 survey). Perhaps reassuring to aspiring data scientists, the more ‚Äúbasic‚Äù methods of linear and logistic regression are still very popular among practitioners, with over 80% of data scientists saying that they use these methods at work. In general, the second most popular category of machine learning algorithms is decision trees and random forests. The popularity of these methods has remained steady, despite the increasing popularity of gradient boosting machines (such as, XGBoost). Another algorithm that has seen a significant increase in usage across the board is convolutional neural networks (CNNs), which are commonly used with image data. For each category of job title, usage of CNNs increased by 20 percentage points. For machine learning engineers, this method continues to be almost as popular as linear and logistic regression. While not quite as dramatically as the usage of CNNs, the usage of recurrent neural networks (RNNs), which are used for time series and sequence data (such as, sequences of words), has also increased. Interestingly, the usage of standard dense neural networks has actually decreased over the last four years. Perhaps, this is due to the increasing usage of more specialized neural networks, like CNNs and RNNs. One category of machine learning algorithms that is missing here from this survey is unsupervised learning methods, such as clustering, and dimensionality reduction techniques. It would be interesting to see trends in the usage of these techniques as well. After going through four years of Kaggle survey data, here are some significant trends in data science that may be worth further study: Hopefully, Kaggle will continue to perform this survey for years to come, so that we can continue to examine these trends, and pick up on some new trends, as the field of data science continues to evolve. [1] One note here is that I did not adjust for inflation, so any increase in salary may be less significant than it appears because of the effects of inflation. [2] I am only including salaries from respondents based in the U.S. because the 2017 survey asked for respondents to give their salaries in their native currency, whereas later surveys asked for respondents to give a reply in a USD range. Therefore, there are likely trends in the currency conversion over the four years covered here that could effect any trends seen in the data.",102,1,8,Towards Data Science,2021-01-01,2021
https://towardsdatascience.com/decision-trees-14a48b55f297,Decision Trees,An Overview of Classification and Regression Trees in Machine¬†Learning,1,37,['Decision Trees'],"This post will serve as a high-level overview of decision trees. It will cover how decision trees train with recursive binary splitting and feature selection with ‚Äúinformation gain‚Äù and ‚ÄúGini Index‚Äù. I will also be tuning hyperparameters and pruning a decision tree for optimization. The two decision tree algorithms covered in this post are CART (Classification and Regression Trees) and ID3 (Iterative Dichotomiser 3). Decision trees are very popular for predictive modeling and perform both, classification and regression. Decision trees are highly interpretable and provide a foundation for more complex algorithms, e.g., random forest. The structure of a decision tree can be thought of as a Directed Acyclic Graph, a sequence of nodes where each edge is directed from earlier to later. This graph flows in one direction and no object can be a child of itself. Take a look at the DAG above, we can see it starts with a root node, the best attributes become interior nodes, i.e., decision nodes. Then, the internal nodes check for a condition and perform a decision, partitioning the sample space in two. The leaf nodes represent a classification, when the record reaches the leaf node, the algorithm will assign the label of the corresponding leaf. This process is referred to as recursive partitioning of the sample space. Terminology when working with decision trees: Decision trees use some cost function in order to choose the best split. We‚Äôre trying to find the best attribute/feature that performs the best at classifying the training data. This process is repeated until a leaf node is reached and therefore, is referred to as recursive binary splitting. When performing this procedure all values are lined up and the tree will test different splits and select the one returning the lowest cost, making this a greedy approach. Something to note, since the algorithm repeatedly partitions the data into smaller subsets, the final subsets (leaf nodes) consist of few or only one data points. This causes the algorithm to have low bias and high variance. A widely used metric with decision trees is entropy. Shannon‚Äôs Entropy, named after Claude Shannon provides us with measures of uncertainty. When it comes to data, entropy tells us how messy our data is. A high entropy value indicates less predictive power, think of the entropy of a feature as the amount of information in that feature. Decision trees work to maximize the purity of the classes when making splits, providing more clarity of the classes in the leaf nodes. The entropy is calculated before and after each split. If the entropy increases, another split will be tried or the branch of the tree will stop, i.e., the current tree has the lowest entropy. If the entropy decreases, the split will be kept. The formula for calculating entropy of an entire dataset: where ùëõ is the number of groups and (ùëùùëñ) is the probability of belonging to the ith group. Let‚Äôs say we have a dataset containing 462 positive (1) labels and 438 negative (0) labels. We can calculate the entropy of the dataset by: Information gain uses entropy as a measure of impurity. It is the difference in entropy from before to after the split, and will give us a number to how much the uncertainty has decreased. It is also the key criterion used in the ID3 classification tree algorithm. To calculate the information gain: When performing classification tasks, the Gini index function is used. From Corrado Gini, this function informs us of how ‚Äúpure‚Äù the leaf nodes in the tree are. The gini impurity will always be a value from 0 to 0.5, the higher the value, the more disordered the group is. To calculate the gini impurity: where (ùëùùëñ) is the probability of belonging to the ith group. The equation above states the gini impurity is 1 minus the sum of the different probabilities in each split. When decision trees train by performing recursive binary splitting, we can also set parameters for stopping the tree. The more complex decision trees are, the more prone they are to overfitting. We can prune the tree by trimming it using the hyperparameters: There are more parameters that can be changed, for a list and a more detailed explanation, take a look over the documentation. Let‚Äôs build a decision tree classifier with sklearn. I will be using The Titanic dataset, with the target being the Survived feature. The dataset I‚Äôm loading in has previously been cleaned. For a description on the features in the dataset, see the data dictionary below. Importing the necessary libraries Loading in and previewing the dataset Defining predictor and target features, performing train test split, and preprocessing data Training a decision tree classifier The decision tree classifier is performing better on the train set than the test set, indicating the model is overfit. Decision trees are prone to overfitting since the recursive binary splitting procedure will continue until a leaf node is reached, resulting in an overly complex model. This is where we would perform hyperparameter tuning and pruning to optimize the classifier. Plotting the tree It may be helpful to plot the tree in order to visually see the splits. We can plot the tree with a few extra libraries. Feature Importance If we want to check the feature importances of the model, we can use the .feature_importances_ attribute from the decision tree classifier. Feature importance is calculated using the gini importance. Optimizing a decision tree classifier with grid search cv By running a cross-validated grid search we can input a parameter dictionary containing different values for the decision tree hyperparameters. I have used the pruning hyperparameters mentioned above with the default 5 fold cross validation. By running the cross-validated grid search, the best parameters improved our bias-variance tradeoff. The first model with default parameters performed 20% better on the train set than the test set, indicating low-bias and high variance in the tree. The decision tree with the hyperparameters set from the grid search shows the variance was decreased with a 5% drop-off in accuracy from the train and test sets. Decision trees performing regression tasks also partition the sample place into smaller sets like with classification. The goal for regression trees is to recursively partition the sample space until a simple regression model can be fit to the cells. The leaf nodes in a regression tree are the cells of the partition. The simple regression models being fit to each partition take the mean of the dependent variable for the partition, i.e., the sample mean is used to make predictions. We used entropy above as a measure of impurity for performing classification. With regression, the CART algorithm utilizes mean squared error as a measure of impurity. When evaluated the performance of the model, we will be looking at the root mean squared error (RMSE). This is just the square root of the mean of squared errors. By taking the square root we can measure the size of error that weights large errors more than the mean. The metric we will use for evaluating the goodness of fit for our model is the R-squared value. The r-squared tells us the percentage of the variance in the dependent variables explain collectively (Frost et al., 2020). CART can be used for more than just regression. Here‚Äôs an interesting article from Neptune.ai where decision trees are used to detect outliers/anomalies in time series data. Let‚Äôs go ahead and build a decision tree regressor with sklearn. I will be using the Ames Housing dataset retrieved from kaggle. For the purposes of this tutorial I will only be using 3 continuous features and the target feature. Loading in the dataset, defining predictor and target features, and performing train test split Training a decision tree regressor Once again, the decision tree is overfitting to the train set. Similarly to classification, we can run a cross-validated grid search to optimize the decision tree. Optimizing Decision tree regressor with grid search cv By running the cross-validated grid search with the decision tree regressor, we improved the performance on the test set. The r-squared was overfitting to the data with the baseline decision tree regressor using the default parameters with an r-squared score of .9998. Using the parameters from the grid search, we increased the r-squared on the test set from .42 to .56. The train r-squared being .58 tells us the model is not overfitting to the training data and will perform similarly on the unseen test set. Decision trees are great predictive models that can be used for both classification and regression. They are highly interpretable and powerful for a plethora of machine learning problems. While there are many similarities between classification and regression tasks, it is important to understand different metrics used for each. The hyperparameters for decision trees are helpful in combating their tendency to overfit to the training data. Something to note, while performing a grid search can help in finding optimal hyperparameters for a decision tree, they can also be very computationally expensive. You may have a grid search running for hours or even days depending on the possible parameters chosen. I hope this post was helpful in gaining a better understanding to regression and classification trees. If there was anything left out or that I could‚Äôve explained more clearly please feel free to leave feedback! Thanks so much for taking the time to check out the post.",19,0,10,Towards Data Science,2021-01-01,2021
https://towardsdatascience.com/art-of-the-graduate-school-essay-f59b14c79649,Art of the Graduate School¬†Essay,5 simple tricks that got me into 12 data science masters¬†programs,7,34,"['Art of the Graduate School Essay', 'Tip 1 | ‚ÄùYou‚Äôre Not Like The Other Girls‚Äù', 'Tip 2 | Keep It Chronological', 'Tip 3 | Kill Your Darlings', 'Tip 4 | Make An Entrance', 'Tip 5 | Be Honest.', 'Parting Thoughts']","Grad school SOPs: They can feel like such a beast. I wasn‚Äôt a perfect student when I was applying to M.S. in Data Science programs; in fact, I was far from it. I had never touched Python, never ran a machine learning algorithm, and hadn‚Äôt had a sexy internship at Google or Facebook to speak of. Having always pegged myself as the internationally-minded, writer, liberal arts-type, I had spent most of my undergrad taking French and Chinese, interning and studying abroad and even pursuing a taekwondo career (in full transparency, I was a math minor, but that was because I was a sucker for punishment more than anything else). Little to say: I did not come from a computer science or heavy statistics background before my master‚Äôs, so getting into all 12 programs I had applied to came as a complete shock to the system. This included UChicago, USC, UMich, UVA, Georgetown, Duke, and Tufts ‚Äî and many of them had also offered me big scholarships to boot! Now that the dust has settled, I truly believe it was my writing skills that saved the day, giving me a platform to build out my life story and offer up reasons why I deserved a chance. With careful diction, I was able to navigate deficits, expand upon strengths, and provide a compelling narrative that showed upwards momentum and a willingness to learn. And taking a gamble on me would have paid off; since attending graduate school, I‚Äôm the student body president of my class and currently hold a 3.95 GPA with one semester left until graduation. This article is both for those who want a few solid writing tips and for those who want to get a data science degree but are nervous they don‚Äôt have the background. I‚Äôm here to tell you that 1) I believe in you, 2) I‚Äôm living proof you can do it, and 3) with a good enough narrative, the panel at your top data science program just might think you can do it, too! Be like a WW2-era sailor writing to his 11 girlfriends at home ‚Äî don‚Äôt confuse names, recall what makes each of them special, and convey the feeling to them that they‚Äôre the only one you have eyes for. It doesn‚Äôt matter if Harvard knocked on your door tomorrow ‚Äî Rutgers or bust, darn it. Who her is will vary based on the audience you‚Äôre writing to, of course. Within your essay it‚Äôs important to list reasons why the school you‚Äôre writing about makes them uniquely special for you. Do your research on this question ‚Äî for me, I spent at least an hour studying the core curriculum, professor areas of expertise, and then calmly laid out how my own interests aligned with those specializations in my essays. I‚Äôll link one of my own essays below, with the ‚Äúyour special to me‚Äù paragraph coming in the latter half of the second page: drive.google.com Additionally, if they ask what other schools you‚Äôre applying to in another section of the application, only list schools with similar master‚Äôs programs who they currently beat out in ranking. So if Stanford ranks higher than Brown and Columbia at data science, you can put them both on your Stanford application as other schools you‚Äôre thinking about, but don‚Äôt put Stanford on your Brown or Columbia applications. Accepting a student who doesn‚Äôt end up attending makes a school look worse, so they won‚Äôt accept a highly-qualified student if they think they‚Äôll go someplace else. In gist: Don‚Äôt send out cookie-cutter essays; convince every school you apply for that they are best matched to your goals and interests (and implying that you‚Äôd definitely attend if they accepted you!). A chronological story is compelling because it is a simple way to convey structure and momentum. Or, more specifically, ‚Äúthis is what happened, this is what I‚Äôm doing now, and this is why your school is the clear next step in my plans‚Äù. You don‚Äôt need to be strictly chronological (you‚Äôll see in my SOP, included above, that I am only chronological at some points in the story), but when you are trying to convey any sort of trajectory it is best to keep things orderly. For example, I used this tool when describing how my initial interest in data science was originally peaked, how I had since committed to that passion, and why graduate school was the clear next step for me. What I lacked in background I was able to make up for in positive momentum through a simple sequence of events. In gist: People have been telling stories for thousands of years; tap into the ancient art in order to maintain reader interest and keep your point from getting too topsy-turvy. Writing is as much destruction as it is creation, and every sentence in a graduate school essay must be indispensable to the overall story of you. This is hard because we tend to write rough drafts with a lot of fluff and disjointedness, and then get somewhat attached to parts of that fluff that aren‚Äôt pulling their weight. Stephen King calls this fluff ‚Äúdarlings‚Äù, and I‚Äôm convinced half of learning how to be a ‚Äúgood‚Äù writer is being able to let go of one‚Äôs own little darlings. ‚ÄúKill your darlings, kill your darlings, even when it breaks your egocentric little scribbler‚Äôs heart, kill your darlings.‚Äù ‚Äî Stephen King In gist: Don‚Äôt be cautious about cutting out words ‚Äî or entire paragraphs ‚Äî when you realize they have begun to distract from the main point. This is why it‚Äôs so great to have proof-readers; they aren‚Äôt as attached to the words like you are and can help clear the weeds to keep your message on track. If you don‚Äôt have anyone who can proofread, take a few days off between drafts and you‚Äôll find yourself coming back to your story with a fresher, more objective lens each time. When I say you don‚Äôt always have to be chronological, this is what I mean. The first paragraph, specifically, should be exciting. No time for a slow build up here ‚Äî bring the reader right into the thick of the action and make them take notice on what you‚Äôre about to say. This was my most commonly used intro paragraph to my graduate school essays. It wasn‚Äôt perfect, but I quickly laid out the situation, task, and action, leaving the next few paragraphs to explain the result. My pivot point for choosing to pursue data science came last April when my team competed at the University of Chicago Econometrics Games. Like a Hackathon for young econometricians, the games pitted students of economics from Cambridge to Santa Clara. The objective was to stage, and answer, a question of economic importance within fourteen hours. Having learned a handful of research techniques under Dr. Jane Doe of the Research Seminar in Quantitative Economics, I was nervous and excited for the opportunity to test my applied econometrics knowledge for the first time. Being an econ major, I was able to draw from an econometrics competition I had completed to begin explaining my initial interest in the field of data analytics. In gist: I‚Äôd encourage you to think of some event ‚Äî as closely related to data science as you can ‚Äî and use that to grab the reader‚Äôs attention as fast as possible. Similar to behavioral interviews, you can use the STAR method to structure your story if you‚Äôre finding yourself stuck. ‚ÄúIf you‚Äôre embarrassed by anything, that‚Äôs a sign your not doing it right.‚Äù ‚Äî Anonymous In the world today, it‚Äôs easy to run away with all kinds of inflation ‚Äî grade inflation (schools), monetary inflation (government), resum√© inflation (pretty much everyone). Everybody wants to look better than everybody else, and as a result everyone ends up over-exaggerating into oblivion. Honesty matters. It really does. Review panels know that the perfect candidate doesn‚Äôt exist, so if you go in trying to convince them that you are then they will become suspicious. It may not necessarily be as impressive to mention you‚Äôve taken two online courses in Python instead of completely mastering it by age 5, but if you do say you are a Python expert without any substantial coursework or internships it will likely stir up suspicion about your honesty (and suddenly everything in your application becomes more up for debate). Of course, it‚Äôs important to emphasize your strengths and be confident in what you do know. To tell when I might have gone too far, I like to think back on a quote I heard a few years ago: ‚ÄúIf you‚Äôre embarrassed by anything, that‚Äôs a sign your not doing it right.‚Äù (P.S. I can‚Äôt find this quote on the internet anywhere ‚Äî if anyone figures out the original source, I‚Äôd love to give the author proper credit!) In gist: Be confident about what you know and honest about what you don‚Äôt. As an ex-liberal arts, current data science grad student on winter break, I love to stretch my writings legs whenever I get the chance. I hope some of these tips I used were helpful as you complete your own applications. towardsdatascience.com towardsdatascience.com Personally, going to graduate school has been the best decision I ever made, I absolutely love what I learn everyday and I am excited to work in the data science field going forward. That being said, this path isn‚Äôt for everyone and there‚Äôs many ways one can start their own data science journey ‚Äî if a bootcamp or self-study works better for you, then by all means go for it! üòä If you have any questions for me, feel free to reach me at my personal website, LinkedIn or Twitter, and of course you can give me a follow on Medium. Thanks again for reading!",66,7,8,Towards Data Science,2021-01-01,2021
https://towardsdatascience.com/sql-interview-prep-the-next-level-e329a67086d,SQL Interview Prep: The Next¬†Level,Knowing the difference between an inner join and a left join is¬†not‚Ä¶,8,85,"['SQL Interview Prep: The Next Level', 'Challenge #1: Percentage of total', 'Challenge #2: Pupcake Customer Acquisition', 'Challenge #3: Grouping credit cards by customer', 'Challenge #4: Select one address per customer', 'Challenge #5: Black Friday Sales ‚Äî Part I', 'Challenge #6: Black Friday Sales ‚Äî Part II', 'Go forth and interview awesomely!']","As someone who has sat on both Data Science and Data Engineering teams, I‚Äôve also sat in on my fair share of interviews. Every interviewer has their own deal-breakers; mine is SQL. If you have conceptual issues with SQL, then that means you might wind up unintentionally pulling data incorrectly. Data Scientists need to pull data correctly in order to build train/test sets for their Machine Learning (ML) models. Data Engineers need to pull data correctly in order to provide reliable data sources for stakeholders. If a Data Engineer makes a mistake, that mistake can affect more than one ML model- it can propagate out to all of the Data Science, analytics, marketing, and finance teams that subscribe to that dataset. SQL skills are important. Most entry-level interviewees know the fundamentals of a SELECT * statement, GROUP BY, and all of the different types of JOINs, but they struggle to really conceptualize the data. Let‚Äôs go over a few real-world examples that will hopefully boost your SQL interview prep to the next level. Let‚Äôs say that it‚Äôs your first day working as a Data Scientist for an online retailer, Pets R Us. You have a table, customers, that keeps track of all customer records. Each customer record includes a customer ID, cust_id, and the creation date for that record, cre8_dt. Can you write a SQL query that calculates the percentage of customer records that were created prior to ‚Äò2007‚Äì12‚Äì22'? Here‚Äôs the hack-ish way I would have done this on my first day as a Data Scientist. First I would have counted the total number of records in the table: Gives us: Then I would have counted the number of records that were created before ‚Äò2007‚Äì12‚Äì22‚Äô: Gives us: Then I would have used the calculator on my phone to divide 3,400 by 1,210,000 and multiply by 100 to get 0.28%. Ha! Got it! But‚Ä¶ don‚Äôt do it that way in an interview! Here‚Äôs a more concise way to do it: Gives us: The trick here is to first use a sub-query to create a flag for whether or not an account was created before or after our date threshold. Then, we can GROUP BY that flag in the main query. When we build the perc_of_tot column, the COUNT(*) counts the total number of accounts in each group (one group for cre8_flag = ‚Äòbefore‚Äô, another group for cre8_flag = ‚Äòafter‚Äô). Since we don‚Äôt include a PARTITION BY after the OVER(), SUM(COUNT(*)) sums the counts over the entire result set, which in this case is made up of two values: COUNT(*) for the ‚Äòbefore‚Äô group and COUNT(*) for the ‚Äòafter‚Äô group. It‚Äôs only been a week since you joined the Data Science team at Pets R Us, and your manager wants you to work on the new Pupcake acquisition campaign. She asks you to build a ML model that predicts the likelihood that an existing customer who has never bought Pupcakes will buy a Pupcake within the next four weeks. Let‚Äôs think about the data you need to pull in order to build your train/test sets for this model. First, you need to aggregate customer sales data over two time periods: (1) an outcome period (let‚Äôs say four weeks prior to today) and (2) a baseline period (let‚Äôs say twelve weeks prior to the outcome period). Assume that you managed to pull two datasets so far. You wrote each dataset to a table: What you really need is a list of customers who purchased at least one item from Pets R Us during the baseline period but did not purchase any Pupcakes during that time. Can you write a SQL query that generates such a list of customers, using the two tables above? Here‚Äôs the solution: This is a useful trick, and I‚Äôve been asked to do this in an interview before! It‚Äôs actually called a LEFT ANTI JOIN in Scala and PySpark. The LEFT ANTI JOIN above will only return customers from the left-hand table (cust_basel) that do not exist in the right-hand table (cust_pupcakes_basel). Let‚Äôs go back to the Pets R Us customers table from our first example. We‚Äôve already gone over the cust_id and cre8_dt columns. Each record also has a full_nm column and an email_adr column. Assume that this table is unique on cust_id (i.e. each cust_id appears only once). Here‚Äôs a sample of five rows from the table: In addition to the customers table, you also have access to the cards table, which stores credit card information. Each record has a card_id (internally generated by Pets R Us to ensure that each card has a unique ID), the cardholder‚Äôs full name (full_nm), card expiration date (exp_dt), the cust_id associated with the card, and the most recent date the card was used to make a Pets R Us purchase (last_purch_dt). First question (without inspecting the data): do you think the cards table is unique on cust_id? Answer: No, because each customer can have multiple credit cards. Here is a small sample of what the cards table looks like: Second question: can you write a SQL query that displays the total number of active credit cards associated with each cust_id, along with that customer‚Äôs full name and email address? Here‚Äôs what most entry-level candidates do: Most people begin by first using the cards table to count the number of cards for each cust_id. They then throw that into a sub-query and join it back to the customers table to get the full_nm and email_adr. Here‚Äôs a more concise way of doing it: Notice that we actually don‚Äôt need a sub-query here! Since we know that each cust_id only has one unique full_nm and email_adr associated with it, we can just do a JOIN to the cards table and GROUP BY cust_id, full_nm, and email_adr. However, if the customers table is not unique on cust_id, then the above query might produce duplicate cust_id‚Äôs. Follow-up question: how do you check if there are duplicate cust_id‚Äôs in the customers table? Answer: The main point here is that you can add extra columns to the GROUP BY clause even if you‚Äôre not necessarily grouping by them. In the above case, we‚Äôre technically only grouping by cust_id, while the full_nm and email_adr columns are just customer attributes. However, we can throw other columns into the GROUP BY as long as they are unique on cust_id. I have seen this discussed in a SQL interview before! Here‚Äôs another challenge: using the customers and cards tables, write a SQL query that displays each cust_id, their full_nm, and card_adr. First, before diving into the SQL, let‚Äôs inspect the data we have and think about it conceptually. Scroll back up to the previous section and take another look at a few rows from the cards table. It looks like Dwayne Johnson (cust_id = 2) has at least three credit cards, and each card has a slightly different address associated with it. So if some customers have multiple credit cards, and each card could theoretically have a different address associated with it (or even the same address formatted differently), how do we choose exactly one address for each customer? Let‚Äôs take a closer look at Dwayne‚Äôs three cards. One has expired while the other two are still active. The active cards have the same address but they are formatted differently. The expired card has a totally different address. It looks like perhaps Dwayne might have moved recently and the expired card reflects his old address. Solution: What we need to do first is PARTITION all of the records in the cards table by cust_id. This means that each customer has their own partition that includes only their own credit cards. We tell SQL how to order the cards within each partition: by last_purch_dt. The card with the most recent last_purch_dt will be ranked first. Once we rank all the cards, we select only the card that is ranked first for each customer. This way, we will be left with only one card (and one address) per customer. There are other ways to select one card per customer. For example, you could take exp_dt into account or use some kind of transactions table to determine the frequency with which each card has been used in the past month. The point here is to know that you should use some kind of PARTITION BY statement. Also, be sure not to over-think tasks in an interview setting; try to solve the problem without adding unnecessary complexity. You can always layer on top of your solution if the interviewer wants to follow-up. If a candidate is a SQL whiz but makes something simple unnecessarily complicated, that can still reflect poorly upon them. Your business stakeholders at Pets R Us would like to analyze the distribution of Black Friday sales per customer over the past 10 years. First, before we dive headfirst into SQL, let‚Äôs think about everything we need in order to fulfill this request. We need a table with all the Pets R Us transactions in it. Each transaction will be associated with a customer and a date. First we should filter the transactions to only those from Black Friday. Then we can aggregate sales by customer. You could probably just look up the dates for Black Friday from the past 10 years and hard-code them into your SQL query. However, in most cases it‚Äôs generally best to avoid hard-coding. Plus, this is an interview- so what would be the fun of that?? The Data Engineering team has provided the Data Science team with a dimensional date table, dim_date, to help with date filtering tasks such as this one. Here are a few sample rows from dim_date: Question: using dim_date, can you write a SQL query that generates a list of dates for all the Black Fridays between 2010 and 2020? Hint: use the rule that Thanksgiving occurs on the fourth Thursday of every November. Solution: The output should look like this: Follow-up question: what happens if we change order of the column names in the PARTITION BY clause? Answer: This will not change the output since the same records in dim_date get grouped together regardless of whether you sort them by year first, by month first, or by day-of-week first. However, the output could change if you have multiple columns in the ORDER BY clause and you change their order. Let‚Äôs say that we wrote the output of the Black Friday query to a table: blk_fri_dates. Now that we have a table with all of the dates we want to use for filtering transactions, let‚Äôs take a look at a few records from the transactions table so we can get a feel for the transactional data: Question: using the transactions table, the blk_fri_dates table, and any other Pets R Us tables that we have discussed above, sum up the total sales for each customer for each Black Friday between 2010 and 2020. Solution: First, we start with the transactions table. We INNER JOIN the transactions table to the blk_fri_dates table, since we only want to grab transactions that took place on Black Friday. We also have to INNER JOIN the transactions table to the cards table, since we need to look up which cust_id corresponds to which card_id for each transaction. Here are a few rows of sample output: Follow-up question: if there are duplicate rows in the blk_fri_dates table, what happens to the above output? Answer: imagine that the blk_fri_dates table erroneously has two rows for the year 2020: In this case, if we INNER JOIN this table to transactions, each Black Friday transaction from 2020 will be double counted. Follow-up question: if blk_fri_dates has duplicate rows, how can you adjust the above SQL query so that you still get the correct Black Friday sales per customer? Answer: Bonus: we can further optimize this query by using a LEFT SEMI JOIN on the blk_fri_dates table instead of an INNER JOIN: The LEFT SEMI JOIN above will select only the transactions from transactions (left-hand table) where there are one or more matches on the id_date column in blk_fri_dates (right-hand table). The great thing about LEFT SEMI JOINs is that if there are duplicate rows in blk_fri_dates, the LEFT SEMI JOIN won‚Äôt duplicate transactions like an INNER JOIN will. A LEFT SEMI JOIN usually performs faster than an INNER JOIN because it can only return columns from the left-hand table. Since we don‚Äôt need to select any columns from blk_fri_dates in this query, the LEFT SEMI JOIN is a great choice. Consider a LEFT SEMI JOIN the next time you need to do some filtering! Let‚Äôs revisit the above query that aggregates total sales for each customer for each Black Friday between 2010 and 2020. Think about what your business stakeholders originally asked for: ‚Äúthe distribution of Black Friday sales per customer over the past 10 years.‚Äù Is the solution above thoroughly sufficient for this purpose? Hint: what happens if a customer has associated transactions in the transactions table for Black Fridays 2010-2014 and Black Fridays 2016‚Äì2020, but not Black Friday 2015? Answer: if a customer (let‚Äôs say cust_id=20) did not make any transactions on Black Friday 2015, then the output rows for total Black Friday sales for that customer will look something like this: Notice how there is simply no record for Black Friday 2015? What we really want is something that looks like this: When you get a question like this in an interview, it is almost certainly a LEFT JOIN question. The aggregated Black Friday sales output will go on the right-hand side. But what should go on the left-hand side? On the left-hand side, we need every possible combination of customer and Black Friday. In other words, we need to write a SQL query that generates every possible cust_id, black_friday pair. First, generate output with every unique customer: Second, generate output with every unique Black Friday between 2010‚Äì2020: Third, multiply them together with a CROSS JOIN: Note: remember that CROSS JOINs are very computationally expensive, so use with caution. Question: using the query we just wrote to generate every possible unique customer, Black Friday pair, and the query we wrote in the previous section to aggregate total Black Friday sales for each customer, write a SQL query that outputs total Black Friday sales for each year for each customer. If a customer made no Black Friday transactions for any year, then they should show up as $0.00 for that year. I used a WITH clause to build a temporary table since I didn‚Äôt want my SQL query to get messy with too many sub-queries. Notice that I selected the cust_id and black_friday columns from the left-hand table. This is important: if you accidentally select cust_id or black_friday from the right-hand table, you could get nulls, which negates the purpose of the LEFT JOIN. I used a COALESCE to replace any nulls from the LEFT JOIN with a 0.0. Also note that I like to add qualitative comments at the top of each sub-query to keep track of what my code is doing. I have seen people use two, three, or more levels of sub-queries without including any comments. I find this hard to read. I recommend including comments in order to facilitate collaboration and make it easier for team members to review your code. Let‚Äôs take a step back after all of that work. There‚Äôs actually one more thing that we need to account for before we throw a bunch of histograms together and send them to our stakeholders. Hint: take a closer look at a few customer records: Only Karen Gillan‚Äôs Pets R Us account was in existence for every Black Friday between 2010‚Äì2020. Dwayne Johnson‚Äôs account wasn‚Äôt created until after Black Friday 2020. Should Dwayne Johnson‚Äôs zero Black Friday sales be included in the histogram for 2010? For 2015? For 2020? No, no, and no. Should Awkwafina‚Äôs zero Black Friday sales be included in the histogram for 2007? No. For 2008? Yes. In this article, we won‚Äôt discuss specifically how to address the above consideration, but think about how you would approach the problem, either programmatically or with SQL. There are multiple correct approaches to this! If you think of multiple approaches, which approach has the best performance? If you think of just one approach, how can you optimize it? This could be a great qualitative discussion towards the end of an interview. As you can see, pulling data can get tricky very quickly. It‚Äôs extremely important to think qualitatively about the task at hand and to anticipate how to maneuver around any eccentricities or known flaws in your data. One last piece of advice: KNOW YOUR DATA. ‚Äî every Data Engineer I have ever met If you‚Äôre in an interview, don‚Äôt be afraid to ask questions about the dataset at hand. It shows an interviewer that you‚Äôre detail-oriented, thoughtful, and less likely to make mistakes on the job. The above examples are variations of actual interview questions that I have seen plenty of interviewees struggle with. Sometimes my team really likes a candidate, but their SQL is so undeveloped that we worry it would take them too long to get up to speed. Also, be sure to ruthlessly demonstrate a positive attitude. That means that even when things get tough, and you think you couldn‚Äôt be performing any worse, or you think the interview questions are totally unfair, stay positive! As an interviewer, I‚Äôm not just looking for the right answer. I‚Äôm looking for someone who is professional, collaborative, and quite frankly, nice. Who would you want to hire to join your team: Bravestone or Van Pelt?? I hope that this article was helpful in boosting you to the NEXT LEVEL in your SQL interview prep. Go forth and show your next interviewer that you‚Äôre ready to join their team. Good luck!",44,3,18,Towards Data Science,2021-01-01,2021
https://towardsdatascience.com/the-ultimate-guide-to-counterfactual-explanations-for-classification-models-e9ee8ed90cfd,The Ultimate Guide to Counterfactual Explanations for Classification Models,Most Human Interpretable‚Ä¶,4,24,"['The Ultimate Guide to Counterfactual Explanations for Classification Models', 'Intuition', 'Illustration', 'Conclusion']","Let‚Äôs say Paul applies for a loan at ABC Bank. He receives a mail from the bank expressing deep regret to inform him that his loan application has been rejected. Rejected! After being shattered for a while, Paul may be curious to know what happened. So he goes to the bank and asks for the person who takes decisions on loan applications. He is directed to Jane, who takes the decisions on loan applications. Paul asks Jane why his application got rejected. She tells him his asking amount is too high, and his salary is less than minimum, and so on. But then, he asks her what could he have done to get his loan application approved. She tells him his asking amount needs to be reduced by so much, increase his salary by so much, come back after certain years, so on. But come on! Paul can‚Äôt suddenly change his salary right! Neither can he wait for years for the loan. So he asks Jane what is the smallest change in his application he can do to get his loan approved. If Jane were replaced by an AI model, what the model would give Paul is called the Counterfactual Explanation. Counterfactual explanations provide the smallest change in the input feature values required to change the output of an instance to a predetermined/desired output However, as in Paul‚Äôs case, not all features can be changed. So a requirement or constraint on Counterfactual explanations is that they perturb as few features as possible to obtain the desired output. There are multiple ways of determining counterfactual explanations proposed by multiple researchers. If you are interested in understanding the difference between each of them, you can take a look at this book by Christoph Molnar. However, a simple method to achieve this is as follows: Caveat: The thing with counterfactual explanations is that it is applicable only to supervised classification tasks. It is not amenable to regression or unsupervised tasks. It is important to know this To illustrate the use of Counterfactual Explanations, I will be using an illustrated example from the alibi library. The example explains a shallow Neural Network model on the Boston House Pricing dataset. The code for the illustration can be found on the alibi documentation page. Since the Boston House Dataset is present in the sklearn library, the dataset is loaded from sklearn directly. Housing dataset is a regression dataset, on which counterfactuals are not effective. Hence we need to convert it into a classification type of dataset. We choose the median of the target value, which is the house price. Any data point (house in the set of houses) with a price less than median is labelled 0 and above the median is labelled 1. The variant of counterfactual explanation used here is the one that is guided by prototypes. Here, prototypes are counterfactual examples (data points). They are built by building k-d trees or encoders, so that counterfactual explanations can be built fast. It is built based on this paper, which you can refer to for more details on how the algorithm works. A custom neural network model is built for this illustration: Now that we have our model, we want to understand a particular outcome. A particular test instance is chosen: The original label of this data point is 0, which indicates that the price of this house is below median price. Now let us run the counterfactual on it (CounterFactualProto calls counterfactual explanations by prototypes). Let us print out the original and the explained outcomes (both should be different or contrastive to each other). We can determine how much of which value has changed between the original feature set and the explanation: So this says that for the price to be above median, this house needed to have an age of 6.5 years lesser than it currently has, and an LSTAT value of 4.8 units less than it currently has. Counterfactuals are the most natural way of explaining model behaviour to humans. However, it has certain limitations, the most important one of which is that it only applies to classification problems. Another problem is that sometimes it provides explanations which, practically, cannot be fulfilled to reverse the decision. For example, age cannot be reversed. So we try to run counterfactuals by imposing that age cannot be changed. Note that there can be multiple counterfactual explanations for a single datapoint, since there can be many ways to reach the decision boundary. We can choose the one that respects our business constraints of any other practical constraints. Having said this, it is a powerful arrow in your XAI quiver!",16,0,5,Towards Data Science,2021-01-01,2021
https://towardsdatascience.com/rewriting-sql-queries-with-pandas-ac08d9f054ec,Rewriting SQL Queries with¬†Pandas,Practical guide for both SQL and¬†Pandas,1,31,['Rewriting SQL Queries with Pandas'],"Pandas is a Python library for data analysis and manipulation. SQL is a programming language that is used to handle relational databases. What they have in common is that both Pandas and SQL operate on tabular data (i.e. tables consist of rows and columns). Since both Pandas and SQL deal with tabular data, similar operations or queries can be completed using either one. In this article, we will rewrite SQL queries with Pandas syntax. Thus, it will be a practical guide for both of them. I have an SQL table and a Pandas dataframe that contains 15 rows and 4 columns. Let‚Äôs start with displaying the first 5 rows. We have some data about items sold at different retail stores. In the following examples, I will write down a query task and complete it with both SQL and Pandas. Task: Find the average price of items for each store. We need to group the prices based on store id column and calculate the average value for each store. In SQL, we apply the aggregate function (avg) while selecting the column. The group by clause groups the prices based on the categories in the store id column. In Pandas, we first use the groupby function and then apply the aggregation. Task: Modify the result in the previous example by renaming the price column as ‚Äúaverage_price‚Äù and sorting the stores based on average price. In SQL, we will use the AS keyword for renaming and add the order by clause at the end to sort the results. In Pandas, there are many options for renaming the price column. I will be using the named agg method. The sort_values function is used to sort the results. Task: Find all the items whose store id is 3. We just need to filter the rows based on store id column. In SQL, it is done by using the where clause. It is also a pretty simple operation in Pandas. Task: Find the most expensive item sold at each store. This task involves both group by and aggregation. In SQL, the max function is applied to the price column and the values are grouped by the store id. In Pandas, we first group the selected columns by the store id and then apply the max function. Task: Find all the items that contains the word ‚Äòegg‚Äô. This task involves filtering but different than the ones we have done in the previous example. In SQL, we will use the like keyword with the where clause. The ‚Äò%egg%‚Äô notation indicates that we want every description that involves the ‚Äòegg‚Äô character sequence in it. In Pandas, we will use the contains function of the str accessor. Task: Find all the items that contains the word ‚Äúliter‚Äù in the description and more expensive than 2 dollars. It is similar to the task in the previous example with one additional condition. In SQL, we can place multiple conditions in the where clause. In Pandas, we can apply multiple filtering conditions as below: Task: Find all the items whose description starts with ‚Äòice‚Äô. This is another text based filtering. In SQL, we can either use the like operator or the left function to compare the first three characters of the description with ‚Äòice‚Äô. In Pandas, we can use the startswith function of the str accessor. Both Pandas and SQL are popular tools used in the field of data science. They are proven to be efficient and practical. If you are working or plan to work in data science domain, I strongly suggest to learn both. We have done some basic queries to retrieve data using both Pandas and SQL. It is a good practice to compare them by doing the same operations. It will improve your practical skills as well as the understanding of the syntax. Thank you for reading. Please let me know if you have any feedback.",111,0,6,Towards Data Science,2021-01-01,2021
https://towardsdatascience.com/probability-theory-for-data-scientists-fcfa7be05291,Probability Theory for Data Scientists,"Probability, conditional probability, and joint probability",1,44,['Probability Theory for Data Scientists'],"Life is full of surprises so we are never sure how things will turn out. However, we can make guesses based on previous experience or logic. Depending on the characteristics of an event, we might be very successful at our guesses. In math, we do not make guesses. Instead, we calculate probabilities. Life is also full of surprises for math so having a calculated probability value does not guarantee the correct answer. However, we will know that our guesses are based on math, objective, and not biased. The probability theory is of great importance for data science. One needs to possess a comprehensive understanding of the probability theory to be a well-performing data scientist. For instance, probability distributions play a key role in predictive analytics. Probability simply means the likelihood of an event to occur and always takes a value between 0 and 1 (0 and 1 inclusive). The probability of event A is denoted as p(A) and calculated as the number of the desired outcome divided by the number of all outcomes. For example, when you roll a die, the probability of getting a number less than three is 2 / 6. The number of desired outcomes is 2 (1 and 2); the number of total outcomes is 6. I have box with 1 blue and 4 yellow balls in it. If I randomly pick a ball from this box, it will likely to be yellow. The probability of picking a yellow ball, p(yellow), is 4 / 5 which is equal to 0.8 (or 80%). The number of desired outcome is 4 and the number of total outcomes is 5. When someone asks you if you think it will rain, your first reaction is usually to look at the sky. If there are dark clouds, your are more likely to answer ‚Äúyes‚Äù. You check the conditions before giving an answer. We observe a similar logic behind the idea of conditional probability. The probability of event A given that event B has occurred is denoted as p(A|B). Conditional probability is the likelihood of an event A to occur given that another event that has a relation with event A has already occurred. The formula of the conditional probability is given below: P(A ‚à© B) is the probability that both events A and B occur. P(B) is the probability that event B occurs. Let‚Äôs go over an example to comprehend the idea of conditional probability. In the image below, we see a probability space (Œ©) which indicates all the probabilities add up to 1. The unconditional probability of event A, P(A) = 0.1 + 0.3 + 0.12 = 0.52 The conditional probability of event A given that B2 occurred, P(A | B2), P(A | B2) = P(A ‚à© B2) / P(B2) = 0.12 / 0.16 = 0.75 Given that B2 occurred, the probability of event A increases. Conditional probability is a fundamental concept in probability theory and statistics. For instance, Bayesian statistics arises from an interpretation of the conditional probability. In machine learning, Naive Bayes Algorithm is based on the Bayes‚Äô theorem and thus the conditional probability. There are some important points to emphasize about the conditional probability. In other words, if two events are independent of each other, the conditional probability of A given B is equal to the probability of A. It comes from a property of the joint probability. Joint probability is the probability of two events occurring together. If two events are independent, the joint probability is calculated by multiplying the probabilities of each event. P(A ‚à© B) = P(A) * P(B) If we put that in the equation of the conditional probability: For instance, we have calculated P(A | B2) as 0.75 earlier. Let‚Äôs also calculate P(B2 | A) and compare the results. P(A | B2) = P(A ‚à© B2) / P(B2) = 0.12 / 0.16 = 0.75 P(B2 | A) = P(B2 ‚à© A) / P(A) = 0.12 / 0.52 = 0.23 Let‚Äôs do one more example to finish up. The following table shows the number of female and male students enrolled in the sociology and music classes. There are 103 students. We will first calculate the unconditional probabilities. P(Female) = 52 / 103 = 0.505 P(Male) = 51 / 103 = 0.495 P(Music) = 47 / 103 = 0.456 P(Sociology) = 56 / 103 = 0.544 P(Female) means the probability that a student is female. The joint probabilities can be calculated by dividing the number in a cell by the total number of students. For instance, the probability that a student is female and enrolled in the sociology class: P(Female ‚à© Sociology) = 24 / 103 = 0.233 We can calculate the other joint probabilities similarly. The following table contains all the probabilities for these events. We will calculate the conditional probabilities now. P(Female | Music) = P(Female ‚à© Music) / P(Music) = 0.272 / 0.456 = 0.596 P(Male | Sociology) = P(Male ‚à© Sociology) / P(Sociology) = 0.311 / 0.544 = 0.572 Uncertainty is ubiquitous in our lives. Thus, any field in science needs to handle uncertainty both practically and theoretically. The probability theory is crucially important in the field of data science as well. Since we cannot eliminate uncertainty, we need proactive ways to efficiently handle it. For instance, probability distributions of variables play a key role in predictive analytics. One needs to possess a comprehensive understanding of the probability theory to be a good data scientist. Thank you for reading. Please let me know if you have any feedback.",84,1,6,Towards Data Science,2021-01-01,2021
https://towardsdatascience.com/recommendation-systems-via-matrix-factorization-28e7d0aa8ca7,Recommendation Systems via Matrix Factorization,Exploring the MovieLens 100k dataset with¬†SGD‚Ä¶,5,18,"['Recommendation Systems via Matrix Factorization', 'Problem 1: Simple Baseline Model with SGD and Autograd', 'Problem 2: One-Scalar-Per-Item Baseline with SGD and Autograd', 'Problem 3: One-Vector-Per-Item Collaborative Filtering with SGD and Autograd', 'Problem 4: Open-Ended Recommendation Challenge']","By Gavin Smith and XuanKhanh Nguyen This project was the third project for my machine learning class this semester. The project aims to train a machine learning algorithm using MovieLens 100k dataset for movie recommendation by optimizing the model's predictive power. We were given a clean preprocessed version of the MovieLens 100k dataset with 943 users' ratings of 1682 movies. The input to our prediction system is a (user id, movie id) pair. Our predictor's output will be a scalar rating y in range (1,5) ‚Äî a rating of 1 is the worst possible, a rating of 5 is the best. Our main task is to predict the ratings of all user-movie pairs. The recommendation system is performed using four different models. For all problems, our tasks are to obtain the best possible prediction results on the validation set and the test set regarding Mean Absolute Error (MAE). Methodologies are explained in all sections, along with respective figures. Figure 1a shows how the mean absolute error for the simple baseline model, which only predicts a value mu for each example, changes over time. The left plot, which has a batch size of 10000, converges to an MAE value steadily because its batch size is large, whereas the right plot, which has a smaller batch size of 100, starts to converge to a value more erratically. We choose n_epoch=10 for this problem. For batch_size=10000, we start seeing some learning after 4 epochs; MAE starts to approach a constant value while the number of epochs keeps increasing. For batch_size=100, after 4 epochs, the data is less noisy. The line that could compute the optimal mu value would be ‚Äúprint(ag_np.mean(ag_np.asarray(train_tuple[2])))‚Äù which would compute the mean of all of the examples. This computation produces a value of 3.58, which is the same value as our model's mu value. This is true because if we are predicting the same value for every example, the mean of the scores will be ‚Äúmost‚Äù correct ‚Äúmost‚Äù of the time. Figure 2a shows show the mean absolute error of the scalar pattern model changes over time with stochastic gradient descent. The figure on the left shows how the error changes when the batch size is 10000, and since the batch size is larger the error gets lower at a steadier rate before settling around .75. The right graph shows the same process, but with a batch size of 100, so the error gets around .75 much faster, but its value is more unpredictable because the batch size is smaller. We choose n_epoch=250 for this problem. For batch_size=10000, we start seeing some learning after 200 epochs; MAE starts to approach a constant value while the number of epochs keeps increasing. For batch_size=100, after 200 epochs, the data is less noisy. To choose the step size, we first pick a large number for step size (step_size=1.5), and we see that the training plot diverging. So, we decrease the step size by 0.25. Eventually, we experience that step_size less than or equal to .75 doesn‚Äôt make training loss diverge. For this problem, we choose a step size is equal to 0.75. Table 2b shows each of the movies from the selected list and its learned per-movie rating adjustment parameter cj in order. We pick the model with the best MAE on the validation set, where the batch size is 10000, for 250 epochs, and the step size is 0.75. The list result is showed from this model. From this list, we can see that movies with a larger positive cj¬≠ value tend to be more universally ‚Äúlauded‚Äù movies, or ‚Äúclassics‚Äù, such as Star Wars or Indiana Jones, and movies with a lower value tend to be less so, like Jurassic Park 2 or Scream 2, these are horror movies and do not always appeal to a wide audience. In our problem, the bias of a movie would describe how well this movie is rated compared to the average across all movies. This value depends only on the movie and does not take into account the interaction between a user and the movie. Therefore, for a movie to be large and positive, it means the movie is likely to be rated highly by people, and a large negative value means that the movie is likely to be rated low. This figure shows how the one-vector-per-item collaborative filter model mean absolute error varies over time, with each graph showing the model with a different number of dimensions, K, that are used to represent users and movies. For all the graphs we can see that when only a short amount of time has passed, the models are underfitting the data as both the training and validation set errors are very high. All the models then start to overfit after 250 epochs, as can be seen by the fact that the training set error decreases, but the validation set error starts to increase again. As K increases, we see that the validation set error has a more defined dip around 350 epochs, and the model overfits much faster with larger K values. This figure shows how the mean absolute error changes with the number of epochs for our model with an alpha value of 0.1. We chose this alpha value by training models with many different alpha values using a grid search, and we found that an alpha value of 0.1 reduced the mean absolute error the most. With this alpha value, we were able to reduce the mean absolute error to a lower value than the model, which was trained with an alpha value of 0. For this problem, the batch size is fixed to 1000. We do early stopping to find the parameters that perform best on the validation set. 0.75 is the largest step size we can get that doesn‚Äôt make training loss diverge. To find the parameters that perform best on the validation set, we used early stopping. This table shows the mean absolute error for each of the models we trained in questions 1 through 3 on the validation sets and the test set. To determine the best version of each model, we tried to minimize the error on the validation set. For M1, this is easy because we are just choosing a fixed variable to make each guess, so there is only one real optimal value. For M2, we chose the model which used a batch size of 10000 because it minimized the mean squared error on the validation set. We used early stopping for each of the M3 models; we searched over different alpha values, batch sizes, and step sizes to determine which version of the model produced the smallest mean absolute error. We recommend using a K value of 50 because it had the smallest error. It could be beneficial to try models with a larger K value because it seems to be that with larger K values, the error decreases. The best model we found is M3 with a K value of 50 and an alpha value of 0.1; this model reduces both the validation set and test set errors. Additionally, an L2 penalty was added to M3 models. Setting L2 regularization on vector u and v force the values to be as small as possible. It can help us avoid overfitting. This figure shows a two-dimensional embedding of the learned per-movie vector vj for the movies in the select_movies data set. This was created using the best M3 model with K=2 factors. In this graph, movies are placed based on their factor vector. We can see that some movies that are similar tend to be grouped together; however, the grouping is not super obvious to us. In the lower right, we can see horror movies like Scream and Nightmare on Elm Street. We notice that movies with similar ratings will come out closer in the embedding space. One reason to explain this is our M3 model has learned that those movies are associated with a similar rating. To check our observation, we calculate the average rating from ratings_all_development_set dataset. The average rating for Sleepless in Seattle is 3.55, while the average rating for While you were sleeping is 3.56. And these two movies are placed close in the embedding space. For our open-ended recommendation model, we chose to use the KNNWithMeans classifier from the surprise package. KNNWithMeans works the same as the regular K nearest neighbors‚Äô algorithm, where it calculates the similarity between K points and returns the prediction that is most in line with those points. KNNWithMeans differs in that you must specify a minimum and maximum K value because the algorithm only looks at points where the similarity is positive. After all, it would not make sense to use points that are negatively correlated. Also, this model considers the mean ratings of each user, which helps normalize the data. We chose to use this model because a good way of recommending movies to someone is to give them choices which are the most like movies they already like, and since KNN works by predicting based on the similarity between data points, we thought it would be the best choice. Another reason we also chose to use this model is that other options like SVD took significantly longer to train, so we would have less time to do in-depth hyperparameter searching on those models. To train our model, we used a grid search to find optimal hyperparameter values along with 5-fold cross-validation to validate our model. The hyperparameters we searched over were K, the maximum number of points to compare to, min_k, the minimum number of points to compare to, and sim_option, which controls how to compute the similarity between points. We found that a K value of 50, a min_k of 2, and the Pearson similarity option were the best. We determined that these were the best by choosing the values which minimized the mean absolute error on the validation sets of the cross-fold validation. This figure shows the three hyperparameters we tuned for our KNNWithMeans model. The left-most graph shows how the mean absolute error changes with an increase in K, the number of points which are compared to. As the number increases, the model starts to overfit less, as both training and validation error decrease. For a K value of 200, the model starts to underfit on the data because the prediction becomes more reliant on data points that are not similar to the point being predicted. From the second graph, we can see that any increase in the minimum number of neighbors to take into account increases the error, so the optimal value should be 1. Last, the right graph shows that the similarity option has almost no influence on our model‚Äôs performance, but the Pearson option has a slight edge over the other two. This figure shows the mean absolute error of our model on the held-out data set used for the leaderboard, as well as its performance on the validation set and test set when training our model. Table 4c shows that the mean absolute error of our model on the held-out data set is lower than the error on our cross-fold validation set. The reason for this is because the held-out data set has a smaller size (10000 ratings) compared to our training data (89992 ratings). There may be a case that the testing data is not a good representative of our training data. Therefore, it behaves well and gives a low error. When comparing this data to our models' results in table 3c, we can see that our leaderboard mean absolute error is the lowest and that our test error is higher than any of the M3 errors. The reason for this is because SVD decomposes the original matrix, the sense matrix is used for predicting the rating (use, item) pair. While in KNN, the prediction is made by finding a cluster of similar users to the input user whose rating is to be predicted and then take the average of those ratings for prediction. SVD does a better job in learning the training data; we see a smaller error on the validation set. One limitation that we faced was that more complex models like SVD took far too long for us to be able to tune the model effectively; if we had access to better computing power it would be more feasible to use models like this. Another thing that could be improved upon is looking at different methods of recording loss; when training our model, we only used mean absolute error, but it could be that the model performs better when very incorrect guesses are weighted more heavily like with mean squared error. Additionally, we would want to consider other user features such as age, gender, nationality, spoken language etc., and the item features like the main actors, the duration of the movie, spoken language, etc. Still considering user and item, we would try to model the fact that people who speak a certain language are more likely to view movies in that language or that may be older users are more likely to watch classic movies. To make a prediction, we look at the user profile and predict the rating.",86,0,11,Towards Data Science,2021-01-01,2021
https://towardsdatascience.com/deep-neural-networks-are-biased-at-initialisation-towards-simple-functions-a63487edcb99,"Deep Neural Networks are biased, at initialisation, towards simple functions",And why is this a¬†very‚Ä¶,7,45,"['Deep Neural Networks are biased, at initialisation, towards simple functions', 'Classification problem on MNIST', 'Is there any reason to expect this?', 'But is this a good thing for learning?', 'So do neural networks work like this?', 'Conclusion', 'References']","This is a follow-up post to Neural networks are fundamentally Bayesian, which presented a candidate theory of generalisation from [1], based on ideas in [2,3]. The reading order is not too important ‚Äî this post predominately summarises material in [2.3], which study why neural networks are biased at initialisation towards simple functions (with low Kolmogorov complexity). This work motivated results in [1], which explained how this bias at initialisation translates into good generalisation in the real world, and thus why neural networks enjoy the success they do! Thus, this post is best thought of as a prequel. We begin by asking the following question: What functions do neural networks express before being trained? More specifically, what is the probability that a neural network will express a function f after being randomly initialised? We will call this quantity P(f),¬π and take the random initialisation to be i.i.d. Gaussian (although [2] suggests that P(f) is not too sensitive to the type of initialisation). In general, changing one parameter in a neural network by a small amount will affect the raw output of the network‚Äî so in general, there is a 1‚Äì1 correspondence between the parameters in a neural network and the function it expresses¬≤. However, for many problems, we are not interested in the raw output of the network. This is best explained with an example problem. ¬πNote P(f) is denoted P·µ¶ ( f ) in Neural networks are fundamentally Bayesian.¬≤If we take the domain to be the entire vector space over which the network is defined. Consider the problem of correctly classifying images of the handwritten digits 0‚Äì9 (the MNIST dataset). We are clearly not interested in the raw output of the network ‚Äî we only care about its final (discrete) decision. Let us imagine, for simplicity, that we want to only classify the images as even or odd numbers. This can be done with a neural network with a single output neuron, and thresholding at zero ‚Äî a positive output implies an even number, a negative output implies an odd number. So, if we consider a subset of m images in MNIST, which we call S, then the network N models a function f : S ‚Üí {0, 1}·µê, where 1 corresponds to even and 0 to odd. This is because ultimately we (mostly) care about the post-thresholded output of the network ‚Äî not the raw outputs ‚Äî and in which case, small changes to parameters in N may not change the function expressed (i.e. the classification will not change). The notation {0, 1}·µê denotes that m images are being mapped either 0 or 1. We can then ask, what is P(f) for these functions? See Figures 1 and 2 below for two visualisations of P(f), for the system discussed above. P(f) was calculated by sampling from 10‚Å∑ different random initialisations of a 2-hidden layer fully connected network, using 100 images from MNIST. It is clear from Figures 1 and 2 that there is a huge range in P(f). Results in [2] suggest that the range for the above problem is over 30 orders of magnitude. But why does this matter? It matters because, for a set of 100 images, there are 2¬π‚Å∞‚Å∞ ‚âà 10¬≥‚Å∞ different possible functions (i.e. possible ways of classifying each image as even or odd). Without information about N, we might assume that each function is equally likely, meaning P(f) ‚âà 10‚Åª¬≥‚Å∞ (this would be the case where images are classified by unbiased coin flip). Given that P(f) can be as large as 0.05, neural networks are clearly not unbiased at initialisation. Instead, there is a strong bias towards certain types of functions at initialisation ‚Äî before any training has taken place. There is a theorem originally due to Levin, and repurposed for input-output maps [4] which when used in the context of neural networks [3] states the following: For the map from the parameters of a neural network N to the function expressed by N, the following result holds: P(f) ‚â§ 2‚Åª·¥∑‚ÅΩ ·∂† ‚Åæ‚Å∫·¥º‚ÅΩ¬π‚Åæ, where K(f) is the Kolmogorov complexity of the function f and the O(1) terms are independent of f but dependent on N. There are some further conditions that need to be satisfied for the bound to hold for neural networks, but empirical evidence [3] plus a few theoretical results [2] indicate that it does, and is non-vacuous. In essence, this says that complex functions will have low P(f), and simple functions can have large P(f), if the bound is tight. However, Kolmogorov complexity is uncomputable ‚Äî so proving this for general architectures and datasets would be, at best, non-trivial. Instead, a very clever experiment in [3] allows us to empirically test this upper bound. The results of this experiment are shown in Figure 3, where a fully connected network models functions of the form: f : {0,1}‚Åø‚Üí{0,1}, chosen because a suitable complexity measure exists ‚Äî see [3] for details. Evidently, P(f) is exponentially larger for simple f. There are functions that lie below the bound, but it is argued in [7] that (very informally) there is a limit on the number of functions that can lie beyond a certain distance from the bound. We call this a simplicity bias ‚Äî because P(f) is higher for simple functions. There is substantial further evidence from [1,2,3] that this simplicity bias is a general property of neural networks of different architectures and datasets¬≥. For example, similar experiments were performed on MNIST and Cifar10 [1,3] where the CSR complexity measure was used to approximate K(f). There is also an analytic result that perceptrons acting on the boolean hypercube are biased towards low-entropy (and thus simple) functions, in [2]. In summary: Also note that functions with large P(f) have greater ‚Äòvolumes‚Äô in parameter-space (see [1,2] for details). This is intuitively obvious ‚Äî if you are more likely to randomly sample some function, it must have more associated parameters, and thus a greater volume in parameter-space. ¬≥Bear in mind that the function is defined relative to a dataset, as it specifies its domain and co-domain. It is thought [5] that real-world data has an underlying simple description. For example, when we read handwritten digits we do not worry too much about precise details ‚Äî if it‚Äôs a single oval-like shape, it‚Äôs probably a zero. We don‚Äôt need to take the exact pixel value of every pixel into account. If real-world data that we are interested in learning really does have a simple underlying description, then simple functions will generalise better than complex functions. Consider a supervised learning problem ‚Äî a training set S from MNIST, containing m examples. Then, an ideal learning agent would be able to calculate the Kolmogorov complexity of all functions‚Å¥ from the images of the digits (i.e. from the pixel values) to the classifications of the digits. It would then throw out all functions that did not correctly predict all m examples in S. Finally, of these functions, it would choose the one with the lowest Kolmogorov complexity. In other words, an ideal learning agent would choose the simplest function that fits the training data. ‚Å¥Defined relative to some UTM (see [3] for details). At this point, if you have read Neural networks are fundamentally Bayesian, you can stop reading, as you already know the answer! If you haven‚Äôt, then please check it out, as it: Thus, neural networks generalise well because P(f) is much larger for simple functions, which generalise better. This has been a very brief summary of the main results in [2,3]. There are also further experiments which demonstrate that P(f) is not sensitive to the choice of initialisation, and how the observed simplicity bias is found in real-world datasets (e.g. cifar10 and MNIST), using a complexity measure called CSR. One final point concerns the strength of the inductive bias ‚Äî obviously we only have an upper bound ‚Äî it does not guarantee that the probability really does vary exponentially with complexity. If the bias were too weak, then we would not get good generalisation with high probability. The PAC-Bayes bound provides a probabilistic bound on generalisation. Applications of this bound in [1,6] show that, for cutting-edge architectures on real-world datasets, the simplicity bias in P(f) is sufficient to guarantee good generalisation. This will be the subject of a future post! Finally, if you think I have missed anything or said anything inaccurate, please let me know. Also note that this my interpretation of work done with a number of co-authors, and while I believe it to accurately approximate their views, it may not always be a perfect representation! [1] C. Mingard, G. Valle-P√©rez, J. Skalse, A. Louis. Is SGD a Bayesian Sampler? Well, almost. (2020) https://arxiv.org/abs/2006.15191 [2] C. Mingard, J. Skalse, G. Valle-Perez, D. Martinez-Rubio, V. Mikulik, A. Louis. Neural Networks are a-priori biased towards low entropy functions. (2019) https://arxiv.org/abs/1909.11522 [3] G. Valle-P√©rez, C. Camargo, A. Louis. Deep learning generalizes because the parameter-function map is biased towards simple functions. (2018) https://arxiv.org/abs/1805.08522 [4] Kamaludin Dingle, Chico Q Camargo, and Ard A Louis. Input‚Äìoutput maps are strongly biased towards simple outputs (2018). Nature communications, 9(1):761. [5] J√ºrgen Schmidhuber. Discovering problem solutions with low kolmogorov complexity and high generalization capability (1994). MACHINE LEARNING: PROCEEDINGS OF THE TWELFTH INTERNATIONAL CONFERENCE. [6] Guillermo Valle-P√©rez, Ard A. Louis. Generalization bounds for deep learning (2020). https://arxiv.org/abs/2012.04115 [7] Dingle, K., P√©rez, G.V. & Louis, A.A. Generic predictions of output probability based on complexities of inputs and outputs. Sci Rep 10, 4415 (2020). https://doi.org/10.1038/s41598-020-61135-7",147,0,8,Towards Data Science,2021-01-01,2021
https://towardsdatascience.com/dos-and-donts-for-testing-a-new-data-scientist-candidate-c0dedf907db9,Dos and Don‚Äôts for Testing a New Data Scientist Candidate,,5,39,"['Dos and Don‚Äôts for Testing a New Data Scientist Candidate', 'Using a Pre-Built Data Science Skills Test', 'Create Your Own Test', 'Dos and Don‚Äôts When Testing New Data Scientist Candidates', 'Conclusion']","Testing candidates for a Data Scientist position gives a hiring organization a great sense of how well they can do job-related tasks and manage time effectively. Skills that Data Scientists need to succeed vary by company or even by teams within a company, so testing candidates should be tailored. In general though, Data Science is a process that includes many steps and independent skills that aggregate to something greater than the sum of its parts. While difficult to test for, in this article, I will share the guidelines I feel are important to follow when testing a new Data Scientist candidate. Technically, testing can be done at any part of the interview process, even before the interview process begins, but only if you want to give a candidate a reason NOT to work for you. For me, there are two times at which to give tests to Data Scientist Candidates: After an initial screening is when you can test for bare-bones skillsets with short to-the-point tests that focus on them. If the job requires Python and SQL as skills, a test that asks candidates to do basic data manipulation or to write down how they would query a dataset using a WHERE clause is appropriate. The point here is to filter out those that are obviously not capable of functioning independently on the team. When the candidate has already spoken with somebody on the team, and they look appropriate for the role, now is a good time to give an in-depth test of their data science abilities. This is when your expectations of this candidate of actually performing on your team are at their highest, so you want them to walk you through their data science approach. Live. I suggest creating a test for candidates at this point that both tests their ability to function in their roles and to be creative or innovative. Remember, reciting what hyperparameters can be used to train a model or answering probability theory questions doesn‚Äôt always directly translate into innovative problem solving or being a team player. Make sure you test for what your organization values in a Data Scientist. No matter what Data Science skillset you want to test for, there is probably a test already designed for it. If you work in Human Resources and/or don‚Äôt have the time or experience to design a test of your own, search online for ‚Äúdata science candidate test‚Äù and be greeted with a dozen or so hits for websites of companies that provide them. I have taken these types of tests as a candidate and I feel that these tests are great for screening large numbers of candidates. (I took a test once that basically asked how to select a column in a pandas dataframe). This is what you give candidates before first-round interviews, but be sure to follow the Dos and Don‚Äôts at the end of the article. Even though you can outsource the actual administering of the test and analyzing of results, here are some questions to ask yourself before engaging: Obviously, the results of the tests should tell you who is right for the job you are hiring for, not some other job. Also, respect the candidates‚Äô time by not expecting them to give up a full day for your test. Unless, of course, they are on the very short list of candidates; a handful or so. As a general rule: the length and involvement of the test given should directly correlate with how serious you are considering them as a candidate for the job. The reason this lane is reserved for top candidates is that you won‚Äôt have the time to administer or review tests for all candidates as a screening mechanism, instead rely on resumes or basic testing for that purpose. I personally find this way more appealing because you can find out how a candidate will behave working on your team. You can design questions specifically around how they would tackle an actual problem in the role, or something hypothetical. Here are questions to ask yourself while creating the test: Note: When I saw ‚Äúwe‚Äù, I mean the team the new Data Scientist will be working on. This applies even when just one person is making the test. A comprehensive skills list needs to include basic skills, advanced skills, and nice-to-have skills that can expand your team‚Äôs horizons. I suggest writing these in a column and highlighting the basic skills because these have the option of being tested earlier in the interview process. Here is an illustrative example to work from (add and remove whatever you want, don‚Äôt get salty in the comments): Skills (Basic): Skills (Advanced): Skills (Nice-to-have): Creating your own list should be easy. My suggestions is to enlist the help of the team with a 1/2 hour meeting on the subject. Just ask. The skills that get the most votes are the most important, and frame the meeting request as defining what skills are basic, advanced, and nice-to-have for the role. Part of being a Data Scientist is being comfortable handling data and making inferences from data. In my opinion, your test has to have data in it to be viable. Give your candidate something to get their hands on (metaphorically) and manipulate with code or whatever tool you want to see them work with. For the data you provide, here are some guidelines: The data is relevant to the industry they would work in. If the job is financial technology, give financial time-series data. If its something in higher education, give them anonymized demographic data. The data is not confidential or sensitive. This might be a no-brainer, but don‚Äôt release sensitive data to non-employee candidates over email. Use publicly available data to create a test from when in doubt, or make up your own. Make the data a little dirty. This is great to test candidates on what they do with dirty data. This step in testing is often overlooked. How long would you give yourself to take this test? That‚Äôs your answer. If you assign twenty tasks that call for justification down to the basic premise then you have to give more time. Based on tests I have taken and constructed, here are some guidelines: Ask yourself: If this was the end of the month, or the end of a sprint, would this submitted work be at, above, or below the team‚Äôs expectations? In my opinion, this is the most natural way to assess the results. Basically, you want to check off that the candidate can handle the easy tasks, do well on the advanced tasks, and give a little indication that they will expand the team‚Äôs horizons in at least one direction. Maybe they are better at time-series analysis than you, good communicator, actually add comments to the code or anything else that stands out. DO give feedback. No matter what. There‚Äôs nothing worse than no follow up for 3 months, then you get a robotic email saying the position is filled. Everyone reading this knows what I am referring to, and for that reason alone you need to avoid doing it. Job seekers, especially Data Scientists, understand the competitiveness of the market, but they also remember those that treated their time as valuable. DO test for the skillset relevant to the job. Along with the job description and interview process, candidates need a realistic image of the job. Giving a test that quizzes on docker containers and the latest in image recognition neural network architecture when you are really looking at data cleansing and automating reports will ensure turnover. Data Scientists leave companies all the time when the expectations are different from reality. Don‚Äôt make your organization look like posers. DO give the test to your current employees. Seriously, see how well your current employees do on the test. It‚Äôs a great barometer for what to expect from your candidates. It‚Äôs also a great reality check for the expectation that your new Data Scientist will be an expert at everything data related, where none of your team members are. DO NOT administer tests prior to any direct human contact. If your response to receiving an application is to generate an email prompting them to take a test, then you might as well just say: ‚ÄúThank you for your interest, and even though you‚Äôre just a number to us, take time out of your schedule to do this while we provide no genuine interest in you and no indication of any further meaningful contact.‚Äù You need to give something, even if in an email, showing your interest in them as a candidate that justifies them spending their time on any testing. In my opinion, if they haven‚Äôt spoken to somebody in the department that is conducting the hiring, then they shouldn‚Äôt be taking any tests. DO NOT administer long tests as a part of your screening process. When I say ‚Äúscreening process‚Äù, I am referring to the process of screening out candidates before going to first or second stage interviews. If you want to screen out people that can‚Äôt do basic tasks, then a short test for that is appropriate. We are talking a maximum of 30 minutes. Hours of testing just to have the information on file in case you need it is bad practice and a drain on the collective time of the industry. DO NOT administer those ‚Äúintelligence tests‚Äù that ask you to guess the next sequence of shapes in a series. (You know what I‚Äôm talking about). For real, what the &%$! is this? I‚Äôve been working as a Data Scientist for years and this never comes up unless under the context of what bothers candidates. You never hear back how the test went anyways and it‚Äôs not like companies need it as justification to not call you back for interviews, so why bother? ‚ÄúThank you for your interest, but your aptitude at guessing the next image in a series of shapes is, by itself, a reason for us to consider other candidates.‚Äù ‚Äî Nobody Testing is great tool to determine how well a Data Scientist candidate will fit in with your team, but it can also tarnish your organization‚Äôs reputation if you do not respect the time of candidates and pose unnecessary hoops to jump through. You can easily make sure the tests you administer align with the job you are hiring for by writing down what skills the job needs with the help of the other Data Scientists. Following the dos and don‚Äôts outlined in this article can be a great outline for how your organization conducts testing of new Data Scientist candidates.",43,0,8,Towards Data Science,2021-01-01,2021
https://towardsdatascience.com/thirty-two-python-tools-and-package-libraries-to-increase-your-machine-learning-productivity-fc7bf785c5c2,Thirty-two Python Tools and Package Libraries to Increase your Machine Learning Productivity,,8,94,"['Thirty-two Python Tools and Package Libraries to Increase your Machine Learning Productivity', 'Python', 'Python IDEs', 'Python Development Tools', 'Cloud Services', 'DevOps (CD/CI/CD) and Machine Learning operations (MLOps) Tools', 'Python Library Packages', 'Summary']","We used Python predominately (95%) over the last seven years because: We used C to speedup Python when Numba could not be used. We tried Go, but it did not work out. towardsdatascience.com 4. Python GIL (lack of concurrency on multicore machines) is bypassed more and more each day by the cloud, Spark, package implementation (i.e.,XGBoost), and strong typing with the introduction of type hinting starting in Python 3.5. medium.com Python‚Äôs runtime speed seems to gather the majority of criticism. A lot of criticism may disappear if some way is found to compile Python. Meanwhile, Python is the predominant choice for machine learning. We used EMACS for 15 years. We were those people who learned computer science and accidentally absorbed some software engineering along the way coding in LISP. We stopped porting EMACS or using someone else‚Äôs port to new hardware and OS platform. We started using other IDEs, as we worked with Java, Scala, R, Matlab, Go, and Python. We discuss only Python-related tools, such as IDEs, for the rest of this blog. We think Python will be eventually drop in popularity as the first choice for Machine Learning, just not in the next few years. I think there are three good choices for Python IDE. Jupyter Notebook enables you to embed text, embed code, and run code interactively. It is based on the lab notebook. Project Jupyter exists to develop open-source software, open-standards, and interactive computing services across dozens of programming languages. ‚Äî Jupyter Project towardsdatascience.com fast.ai coded a complete set of Jupyter Notebook tools. Please look them over. It is a Python programming environment called nbdev, which allows you to create complete python packages, including tests and a rich documentation system, all in Jupyter Notebooks. ‚Äî Jeremy Howard fast.ai coded a complete set of Jupyter Notebook tools. Please look them over. It is a Python programming environment called nbdev, which allows you to create complete python packages, including tests and a rich documentation system, all in Jupyter Notebooks. ‚Äî Jeremy Howard PyCharm and VSCode are the most popular IDEs (Interactive Development Environments) for Python. We use PyCharm (or VSCode) to develop, document, test and debug. Both integrate with inline documentation formatting, version control (git or GitHub), testing packages, coverage, linters, type hint checkers, and code formats. The Python IDE for Professional Developers ‚Äî JetBrains dr-bruce-cottman.medium.com Black formats your code into a superset of the PEP-8 standard. We use it to format all code files in a project triggered by PyCharm, VSCode, or GitHub actions. Codacy is currently our favorite ‚Äúpain-in-the-***‚Äù (PITA) development tool. It catches more errors and suspect code than pylint, some of the stylistic warning we ignore. We think of today as an automated code review tool. As codacy states in their tag-line: Automate code reviews on your commits and pull requests. Coverage.py is the tool we use for measuring the amount of code covered by our Pytest framework. We use git for local file version control. Once unit tests pass on one of the local machines, we push out our code to our repo on the GitHub cloud. Mypy type checks programs that have type annotations conforming to PEP 484. mypy is often used in Continuous Integration to prevent type errors. Mypy joins our other developer tools such as pytest, black, pylint, and Codacy. medium.com We use pylint to find errors and suspect code on our local coding node. We use today for ‚Äúlinting‚Äù full project codes for the pushes to the Github repo. Over our careers, we have used many different frameworks. We settled on Pytest for unit testing primarily as minimal boilerplate is required. We use Scalene instead of the built-in Python profiler. Scalene is a high-performance CPU and memory profiler for Python that does several things that other Python profilers cannot do. It runs orders of magnitude faster than other profilers while delivering far more detailed information. We use Kubernetes on the cloud to manage a group of Docker containers. Kubernetes is open-sourced by Google. Also, it is possible to use Kubernetes n a local cluster of networked machines. Kubernetes probably works locally, but we have not ever used it this way. Kubernetes, also known as K8s, is an open-source system for automating deployment, scaling, and managing containerized applications. ‚Äî Kubernetes documentation. We use kuberflow to build Machine Learning pipelines of Kubernetes pods. We use kuberflow only on the Goggle Cloud Platform (GCP). It is open-sourced by Google and should work on other cloud platforms. However, GCP packages kuberflow as a cloud PaaS (Platform as Service). As a GCP PaaS, kuberflow has a convenient GUI template and DAG (Directed Acyclic Graph) display of the Machine Learning pipeline. Continuous Development is the first part of the traditional Continuous Integration process. Continuous Deployment follows Continuous Integration (CD/CI/CD). An introduction and overview of DevOps can be found at: medium.com DevOps (Development Operations) was created for the computer language code lifecycle. MLOps (Machine Learning Operations) extends DevOps for the Machine Learning pipeline lifecycle. An introduction and overview of MLOps can be found at: medium.com We found MLFlow effective for specifying machine projects with five or fewer steps where some other framework performed any data preprocessing. We used MLFlow for the Machine Learning pipeline that followed a Spark-based distributed data pre-processing frontend. Perhaps we were using MLFlow wrong, but inputs and outputs seemed to be file-based. However, we are enamored with the MLFlow Tracking component. The MLflow Tracking component enables us to log model parameters, code versions, metrics, and output files for display in dashboards built with Streamlit (discussed later). Note: We rarely use MLFlow now. We use Photon.ai for quick experiments and use kuberflow for production Machine Learning pipelines on the cloud. Photon.ai incorporates Scikit-Learn, pycluster, and other Machine Learning (ML) or deep learning (DL) frameworks with one unifying paradigm. Photon.ai adopts Scikit-Learn‚Äôs Estimator and Transformer class method architecture. Photon.ai adds code that reduces manual coding and error by transforming pre- and post-learner algorithms into elements with their argument signature. Examples of elements are several data cleaners, scalers, imputers, class balancers, cross-validators, hyper-parameter tuners, and ensembles. Photon.ai chains elements into a Machine Learning pipeline. Two or more pipelines are composed of a conjunctive (and) or disjunctive (or) operator to create a DAG (directed acyclic graph). The original Photon.ai source code, the extended Photon.ai source code, examples, arXiv paper, and documentation are found by clicking on the desired link. You can look at how photon.ai extends scikit-learn into an MLOps tool in the following blog: towardsdatascience.com Our experience leads us to predict that GitHub Actions will be a significant choice for Continuous Development, Continuous Integration, and Continuous Deployment (CD/CI/CD) on and off GitHub. Continuous Development is when any push from a local repo goes to the project version control Dev repo. At the GitHub repo, CD/CI scripts run for PEP-8 formatting compliance, unit testing, documentation testing, and code quality reviews. It is the core base of the GitHub Action script that we show in: medium.com Docker creates an image of an application and its dependencies as a complete stand-alone component that can be moved onto most cloud vendor offerings, Linux, Windows OS (operating system), and MacOS. Docker-Compose is used to manage several containers at the same time for the same application. This tool offers the same features as Docker but allows you to have more complex applications. A Docker image is similar to a Photon.ai element; the significant difference is that Kubernetes load balances Docker by image replication and manages a DAG Docker images are nodes across a distributed system. Building Docker images are detailed in the following blogs: towardsdatascience.com dr-bruce-cottman.medium.com Diagrams enable you to create high-quality architecture DAG (Directed Acyclic Graphs). Diagrams have a concept referred to as nodes. Nodes are how the Diagrams package organizes the icons into different groups where each node is in the public domain or cloud service. The rendering of high-quality architecture diagrams of Azure, AWS, and GCP is shown using in the following blog: medium.com HiPlot is an interactive visualization tool that enables us to discoverer correlations and patterns in high-dimensional data. HiPlot uses a technique known as parallel plots, which we use to visualize and filter high-dimensional data. Why not use Tableau? We do if the customer has a license. HiPlot is open-sourced by Facebook and thus is license-free. We can use HiPlot anywhere we go. We think it is better at displaying high-dimensional data than Tableau. HiPlot uses Streamlit, our favorite, for replacing Flask, Django, and other GUI front-end used for Machine Learning display. You can dive more in-depth in the tutorial: HiPlot component for Streamlit. The source code for HiPlot and the documentation for Hiplot are found at the associated URL links. Python has the tried and true logger package. A good read on the logger package is in this article. However, I choose to use the recently released loguru package because it is easier to use than logger, and loguru is process and thread-safe, while the logger is not out-of-the-box process safe. Ref: loguru project. You can learn how I use loguru in: towardsdatascience.com We use Pyclustering for a broader and sometimes faster selection of unsupervised Machine Learning cluster algorithms than sk-learn. pyclustering is an open-source Python, C++ data-mining library under BSD-3-Clause License. The library provides tools for cluster analysis, data visualization, and contains oscillatory network models. ‚Äî Pyclustering Documentation. You can look at a detailed study of pycluster Kmeans and Kmedoids in the following blog: towardsdatascience.com We use pysim for python-based simulations modeled as coupled differential equations. Some of us are ex-physicists. What we appreciate is that you can connect one simulated system to other simulated systems. Simple systems become complex systems as they hook together to create a complex simulation. Smote is probably the most widely known package for augmenting underrepresented data class counts so that they are equal to the highest data class count. In other words, balancing unbalanced structured data classes to predict the lower count classes better. What is not done, so often, is that you can continue argumenting the data of all classes. You can learn how we use smote for argumentation of all structured data classes in: towardsdatascience.com If you use Spark and Keras or Tensorflow, use Sparkflow to speed up your pytorch training by N partitions, where N should be your batch size or number of GPUs, whichever is smaller. You use SparkTorch for pytorch. Lightning to pytorch is similar as Keras is to Tensorflow. We use Lightning and Keras to raise us a couple of levels above the complexities of pytorch or Tensorflow. Streamlit is an open-source Python framework that we use to quickly develop and deploy web-based GUIs for Machine Learning applications. In earlier blog posts, we compared Flask to Streamlit using two different examples. We found that Flask needed about a hundred code lines while Streamlit needed ten lines of code to accomplish the same task. towardsdatascience.com towardsdatascience.com spaCy is the fastest package we know for Natural Language Processing (NLP) operations. spaCy is an NLP library implemented both in Python and Cython. Because of Cython, parts of spaCy are faster than if implemented in Python. Spacy is available for operating systems MS Windows, macOS, and Ubuntu and runs natively on Nvidia GPUs. spaCy is a good choice if you want to go into production with your NLP application. If you use a selection from spaCy, Hugging Face, fast.ai, and GPT-3, you perform SOTA (state-of-the-art) research of different NLP models (our opinion at the time of writing this blog). medium.com We use mFST to build and deploy Finite-State Machines for Natural Language Parsing. We do not get into Finite-State Machine (FSM) here. mFST is the Python library for working with Finite-State Machines based on OpenFST. mFST is a thin wrapper for OpenFST and exposes all of OpenFST‚Äôs methods for manipulating FSTs. ‚Äî mFST paper. If you ever try something other than a Convoluted Neural Net (CNN), you might try FSMs, but only if you are well-grounded in FSM theory. We recommend you start with a simple CNN. I have listed and summarized the most useful blogs, sites, and newsletters for us as of n December 24, 2020. I feel confident that I have missed some of your favorites given the rapid growth of the Machine Learning field. You can be assured that we will have more favorite packages, maybe a new language or two new blogs by the end of 2021. I hope some of these tools and packages are new for you and hope you find them useful. Happy New Year!",195,0,11,Towards Data Science,2021-01-01,2021
https://towardsdatascience.com/transfer-learning-and-rnn-the-corenlp-series-pt-iii-2944f42a19a0,Transfer Learning and RNN‚Ää‚Äî‚Ääthe CoreNLP Series pt¬†III,This post aims at testing the ability of¬†CoreNLP‚Ä¶,9,38,"['Transfer Learning and RNN ‚Äî the CoreNLP Series', 'Training Limitations of RNN Model', 'Transfer Learning', 'The Task', 'Target domains: restaurants and electronics', 'Methodology', 'Results', 'Conclusion', 'Bibliography']","Hello and welcome to the third (and probably last? I haven‚Äôt decided yet‚Ä¶üôÑ) part of this coreNLP series! In previous posts of this series I had introduced the coreNLP library and gone into the theory behind their way of computing sentence embeddings for sentiment analysis. This is going to be a more practical article, in which I am going to address one of the biggest limitations of the RNN: it is very difficult (almost impossible) to re-train the model. In this post, I am going to explain the concept of transfer learning as a solution to this training issue. I will also test the RNN on two datasets not seen during training on the task of binary sentiment classification. From my experience using coreNLP, the most annoying aspect of Socher‚Äôs recursive model (apart from the time it takes to run) is that it is basically impossible to re-train. CoreNLP actually makes it very easy for you to re-train the model with a simple command. However the problem is that you need a very particular training dataset. As described in the previous post, in order to compositionally learn the sentiment of the sentence, a softmax classifier was built at every node of the RNN‚Äôs parse tree. This means that for training this RNN one would basically need true labels (sentiment scores ranging from 0‚Äì4) at every node of the tree! Let‚Äôs go a bit more in depth into what this implies‚Ä¶ In order to do this particular training, a corpus of labelled parsed trees was created: the Stanford Sentiment Treebank. This corpus consists of 11,855 individual sentences extracted from movie reviews, more specifically from the Rotten Tomatoes dataset introduced by Pang and Lee (2005). The raw textual data was parsed, obtaining 215,154 unique phrases. These phrases were manually labelled according to their sentiment by three human judges. Therefore, the Sentiment Treebank includes sentiment labels for every possible phrase in thousands of sentences, allowing RNTN to be trained on predicting sentiment at every node of a sentence. This corpus was highly effective for the task of training and testing the RNN, but its complexity makes it very hard to replicate. Imagine‚Ä¶ one would need to to manually label every word, subphrase, phrase and sentence of the corpus!! This is a big limitation, since it makes it very difficult to re-train and test the model on some corpus other than movie reviews. We cannot retrain the RNN due to the limitations stated above. However, in this post we are interested in using the RNN to make predictions on other datasets, not necessarily movie reviews from Rotten Tomatoes. How do we face the issue that the training and testing data are different? There is a whole area of study in Machine Learning that tries to reuse classifiers trained in one dataset (or domain) on a different one. This is called transfer learning or domain adaptation. This is a field in which the aim is to generalise a classifier that is trained on a source domain to a target domain. There are many reasons why one would like to reuse a classifier trained on one domain (e.g. movie reviews) to make predictions on another domain (e.g. restaurant reviews). Arguably the main one is the issue that we are facing in this very article: sometimes gathering training data is a difficult and expensive task. This issue is recurrent in the real world, and therefore very sophisticated transfer learning techniques have been developed in order to address it. I‚Äôm not going to go in depth into them. My main aim in this post to simply get an insight into the how well the RNN trained on the movie review domain is able to generalise to other domains. In this post we will test the pre-trained RNN on the task of binary sentiment classification at the sentence level. The RNN was trained on the source domain: movie reviews. The testing will be performed on two different target domains: restaurant reviews and electronic reviews. In terms of performance, Socher et al (2013) report that the RNN can achieve an accuracy of 85.4% on binary sentiment classification at the sentence level. The aim will be to compare the performance of the RNN on the source domain vs. target domains. That way we will be able to draw some conclusions regarding how well Socher‚Äôs model is able to transfer its learning to make predictions to other datasets not seen during training. For testing the domain adaptation abilities of the RNN, I have chosen two datasets as target domains. Both of them are based on reviews, in order to ensure that the style of writing would remain sort of constant. The difference would be on the subject of the reviews: electronics and restaurants. Dataset 1 contains sentences from reviews categorised on the Amazon website under the category of ‚Äòcell phones and accessories‚Äô, and are part of a larger dataset collected by McAuley and Leskovec. The instances of dataset 2 were obtained from the Yelp Challenge dataset, which is a larger dataset of restaurant reviews posted on the Yelp platform. Both datasets are equal in size: 1000 sentences each. The sentences are binary labelled as negative (class 0) or positive (class 1) according to their sentiment. Reviews with neutral rating were not considered in order to intensify the polarity of the datasets. Classes are balanced, 500 instances for each sentiment. Length of reviews and its distribution is very similar across both datasets. The process is divided into three steps. All the code can be found on my GitHub, but I will go through some of the main parts now! The first script, formatting_and_eda.py, get the statistics above and generates the histogram. It also generates the .txt files within the directory test_data. The files X_amazon.txt and X_yelp.txt are the reviews on which the RNN is going to make predictions. The files Y_amazon.txt and Y_yelp.txt are the true labels, that we will use for evaluation. 2. RNN predictions The second script, coreNLP_pipeline4.py, runs the coreNLP pipeline. This coreNLP pipeline was built to predict the sentiment score of a single sentence. The predicted score is outputted as a distribution over the five different class labels (1‚Äì5). Our results are going to be printed out onto predictions_amazon.txt and predictions_yelp.txt.They will be structured in the form of dataframes with the following columns: review_id, sent_id, sentence, score, very_neg, neg, neu, pos, very_pos Where very_neg, neg, neu, pos, very_pos are the probability that the sentence very negative, negative, neutral, positive and very positive as predicted by the RNN. The file must with two arguments: the input file and the name of the dataset. For example: java -cp ‚Äú*‚Äù coreNLP_pipeline4.java test_data/X_amazon.txt amazon I run it inside the stanford-corenlp-4.1.0 folder and twice: one time for the amazon data and another for the yelp data. 3. Evaluation of Results Once we have the results of the predictor, the only thing left is to evaluate how good they are against the true values of Y. This is what evaluation.py does. However, the values of Y_true are binary: either 0 or 1 and the values of Y_pred are distributed probabilities over five values: very_positive, positive, neutral, negative and very negative. So, in order to do evaluate the results we must first binarise them. In order to convert them into 1s and 0s we drop the neutral column and sum the positive and negative probabilities into a total positive and negative score. If the total positive score is bigger than the total negative score for one sentence, then its target value will be 1 and vice-versa. For evaluation we will compute accuracy, precision, recall and a confusion matrix. Socher et al (2013) report that the RNTN can achieve an accuracy of 85.4% on binary sentiment classification at the sentence level. As can be seen on the table below, the classification accuracy for the target domains is lower: 79% accuracy for electronics domain and 80% accuracy for restaurants domain. However, despite this underperformance, we can still confidently say that Socher‚Äôs recursive model is able to perform cross-domain classification on the target domains of electronics and restaurants to a good standard. It is also interesting the insight that the other metrics obtained when performing this evaluation can provide into the behaviour of the recursive classifier. It seems that the model performs better in classifying class 1 (positive sentences) than class 0 (negative sentences) for both target domains. There also seems to be no major difference in performance between the two target domains. And this is all for now! I hope you enjoyed it and that you be motivated for using this super cool model on different domains, being confident that the performance will be relatively good. See you next time! ‚úåüèª Socher, R., Perelygin, A., Wu, J., Chuang, J., Manning, C.D., Ng, A. and Potts, C., 2013, October. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 conference on empirical methods in natural language processing (pp. 1631‚Äì1642). Available at: https://www.aclweb.org/anthology/D13-1170 Stanford CoreNLP. (n.d.). Stanford CoreNLP ‚Äî Natural language software. Available at: https://stanfordnlp.github.io/CoreNLP/",19,0,7,Towards Data Science,2021-01-01,2021
https://towardsdatascience.com/data-wrangling-solutions-dynamically-creating-variables-when-slicing-dataframes-fc5613c46831,Data Wrangling Solutions‚Ää‚Äî‚ÄäDynamically Creating Variables When Slicing Dataframes,Making Python¬†Easy,4,20,"['Data Wrangling Solutions ‚Äî Dynamically Creating Variables When Slicing Dataframes', 'Assumption and Recommendation', 'Solution', 'Closing note']","When working on a data science project, we spend more than 70% of our time adjusting data to our needs. While munging data, we encounter many scenarios for which ready-made solutions are not available in standard libraries like pandas. One such scenario is when we have to create multiple dataframes from a single dataframe. We encounter this scenario when we have a categorical variable, and we want to split the dataframe based on the different values of this variable. A visual representation of this case is as below: Given the scenario presented above, one can suggest splitting the dataframe manually. Yes, that is a possible solution, but only when the number of categories present in the variable is small. The problem gets challenging when the number of categorical values runs into tens or hundreds. In Python, we do not have any ready-to-use function for this problem. Therefore, we will provide a workaround solution to use the Python dictionaries. The keys in this dictionary will be the different categories of the variable. The value component of the dictionary will be the dataframe slice itself. A step by step approach to implement this solution is detailed below: Being hands-on is the key to master programming. We recommend that you continue to implement the codes as you follow through with the tutorial. The sample data and the associated Jupiter notebook is available in the Scenario_1 folder of this GitHub link. If you are new to GitHub, learn its basics from this tutorial. Also, to set up the Python environment on your system and learn the basics of Anaconda distribution, refer to this tutorial. This tutorial assumes that the reader has at least an intermediate knowledge of working with Python and associated packages like Pandas and Numpy. Following is the list of Python concepts and pandas functions/ methods used in the tutorial: In this tutorial, we will be using the famous cars.csv data set. The data set has details like mileage, horsepower, weight on ~400 car models. Our objective is to split this dataframe into multiple dataframes based on the variable year. The dictionary for this data set and the sample data snapshot is as follows: Once you have the data available, the next step is to import it to your Python environment. We have used Pandas‚Äô ‚Äòread_csv‚Äô function to read the data in Python. Once we have read the data, apply the groupby method to the dataframe. Use the same column as the argument which wants to use to slice the dataframe. By default, a groupby object in Pandas has two major components: By converting the groupby object into a tuple, we intend to combine the categorical values and their associated dataframe. To achieve this, pass the groupby object as an argument to the Python function, tuple. Notice the two components of the tuple object. The first value, 70, is the year of manufacturing, and the second value is the sliced dataframe itself. Finally, we will convert the tuple object into a dictionary using the python function dict. The dictionary created in the last step is the workaround solution we were referring to in the tutorial. The only difference between this solution and the manual creation of actual variables is the variable names. To use the sliced data, rather than using the variable names, we can use the dictionary with the correct key value. Let us understand how: In the above code, when using the shape attribute, we used a dictionary object rather than using the specific variable names. Did you know that by having the data wrangling tips up in your sleeves, you can reduce your model building life cycle by more than 20%? I hope that the solution presented above was helpful. Stay tuned for more data wrangling solutions in future tutorials. HAPPY LEARNING ! ! ! !",12,0,5,Towards Data Science,2021-01-01,2021
https://towardsdatascience.com/do-you-need-to-pay-to-learn-data-science-e71f75493e5a,Do You Need To Pay To Learn Data¬†Science?,Is Money an essential Requirement‚Ä¶,6,63,"['Do You Need To Pay To Learn Data Science?', 'A Simple Journey', 'Best Free Resources', 'Clarification', 'Additional Motivation', 'Conclusion:']","‚ÄúBe a lifelong student. The more you learn, the more you earn and the more self-confidence you will have.‚Äù ‚Äî Brian Tracy. In the modern generation, data is a valuable resource, and the emerging trends show a steep increase in the popularity of the subject of Data Science. Hence, this occasion is probably the best for you to invest your precious time and efforts to enhance your skills in the field of Data Science. And develop into a blossoming successful Data Scientist of the future! But if you are a beginner enthusiast of the subject or even an intermediate learner, what is the best course of action to learn Data Science effectively and efficiently? A common question asked by beginner Data Science enthusiasts is how much money do you actually need to pay to learn Data Science and become an expert by mastering all essential aspects of the field. In this article, I will aim to answer these questions with a little backstory while also clarifying some misconceptions. We will then look at some of the best free resources and useful links for becoming proficient at Data Science. Finally, we will get motivated to learn more and consistently focus on self-improvement as Data Science is a constantly advancing field, and we must evolve simultaneously as well. Three years ago, I had almost no experience with Data Science. My background was from Electronics and Communications, where I worked mainly on Robotics projects. I enjoyed planning, building, and constructing unique robots. However, I realized at one point in time that it would be super awesome if I could integrate Artificial Intelligence into my robotic projects to make them more innovative. I wanted my robot to have features like face recognition, object detection, and speech translation, among various other unique entities and characteristics. At this moment, Artificial Intelligence sparked my interest immensely, and I began my journey towards Data Science. By Googling, I found out that the best resources to get started with Data Science was Programming with Python and dealing with mathematical concepts. For me, Math was never a major issue as I always enjoyed it a lot. I also noticed that because of my background, I also fulfilled most of the essential Data Science requirements for math, which includes concepts of statistics, probability, and calculus. Don‚Äôt Worry as I will mention some of the best free resources for learning math as well in the upcoming sections! Programming, however, was not something that I had much experience with and had to practice quite a bit. I had worked a lot with Java in high school, so I knew some of the essentials of object-oriented programming. The main objective for me was to learn Python and the basics of Data Science so that I could create innovative and unique real-world applications with the immense knowledge I have gained. Long story short, I learned most of the essential requirements for Python programming in under one month and became quite proficient using it in about three months. That is around the average time a complete beginner with no experience but a lot of interest would take to learn and grasp most of the necessary topics. Data Science is a continuously developing and advancing field. I studied the subject and worked on numerous projects for the next couple of years. I gained most of my knowledge from the free courses and resources I utilized during this time. I finally did an additional paid course to earn a certificate for my learnings. However, the majority of time was spent with free resources, which are absolutely fantastic due to the amount of valuable information they provide. In the next section, let us analyze and explore some of the best free valuable resources for learning Data Science! There are tons of sensational research papers, lots of free documentations, video tutorials, concise guides, and so much more to improve your knowledge and understanding of the subject of Data Science. In this section, let us discuss the best options available for each essential concept, namely Math, Python, and Basics of Data Science, individually. Note: The asterisk (*) symbol will denote resources that I have personally used. Otherwise, it is according to research or from other knowledgeable people that have recommended them. I will create another article covering useful resources more extensively. Do let me know if you guys would be interested in the same! This section of the article is more like a disclaimer to clarify any misconceptions surrounding the topic we discussed today. Is it possible to learn Data Science without for free?  ‚Äî Short answer is yes. But‚Ä¶. It is absolutely possible to learn data science for free, and you will notice that in the previous sections of the article where I have provided concise details regarding this topic. The main objective of the article is the learning process behind picking up Data Science from scratch with free resources rather than utilizing some paid online courses (some of which could be a scam as well). It is not to tell you to not get a Masters Degree or PhD in the subject. If you have the opportunity to accomplish these Degrees in esteemed universities, I promise you that it will be extremely helpful in gaining further knowledge and better understanding with respect to Data Science. These are some of the paid degrees that you should consider engaging in to learn more. (If you receive a scholarship for the same, then that is a win-win!) However, if you are not able to afford them or you have not yet received an opportunity to dwell in a university of your preference, all hope is not lost as you can learn Data Science from your home with multiple free resources. The current situation of the world also provides an ideal Situation for you to pick up Data Science and master it! The first link provided in the final conclusion section of this article is a concise twenty five minute guide of how to master data science concepts in 12 months in the year 2021! ‚ÄúA data scientist is someone who can obtain, scrub, explore, model, and interpret data, blending hacking, statistics, and machine learning. Data scientists not only are adept at working with data, but appreciate data itself as a first-class product.‚Äù ‚Äî Hillary Mason, founder, Fast Forward Labs. In my opinion, as long as you enjoy what you are doing and you are happy with it, then nothing really matters. Make sure you do what you love and enjoy. There are tons of awesome and fantastic projects you can create with Data Science. I will cover more on this in my next article! The field of Data Science is spectacular, with so many new ideas and innovations that are to be uncovered in the future. I am glad all of us Data Science enthusiasts can contribute enormously to this continuously advancing and rapidly developing field in numerous ways. But if you find out that Data Science is not something you like, then don‚Äôt simply jump on the bandwagon because other people want to do so. There are so many other wonderful options out there in the world. Understand your own interests and follow your dreams! Check out the last link in the next section to learn more about this topic. We are surrounded by Artificial Intelligence and Data Science all around us, and I find the fast pace of progress in this field extremely fascinating. I am excited about the newer technologies in the future and the ultimate rise of Data Science. The amount of hype generated by Data Science, along with the various job opportunities created, makes Data Science a step everyone must consider exploring and learning! This is the best time for all of us Data Science enthusiasts and Data Scientists to explore the various options and opportunities out there in the world. Efficiently utilizing your time will help in creating a better hemisphere with fabulous Data Science projects. Let‚Äôs all contribute to the evolution of Data Science by constructing amazing models and projects to solve complex tasks and find solutions to complicated questions! From the points stated in this article, we can figure out that it is not a compulsory requirement for you to pay money in order to master the essentials of Data Science. There is a wide array of free resources that you can utilize and benefit from. However, if you want to take programming, math, or any online Data Science courses, I would highly recommend that you do in-depth research on that particular course. Make sure you read all the reviews and consider the opinions of other people who have taken up that course. With the increasing popularity of Data Science, there are a lot of scams and scam sites running around the internet. So, it is better to be safe than sorry! I would highly recommend checking out the free courses first for a complete beginner. It is ultimately your choice if you want to follow the suggestion or not. If you are able to find a valuable course or resource you can benefit from in a reasonable amount of money, then it is even recommended to take up those courses to achieve higher skill ceilings and greater knowledge. I wish you all good luck on your Data Science Journey! If you guys liked this article, please feel free to check out some of my other stories that you might enjoy also reading! towardsdatascience.com towardsdatascience.com towardsdatascience.com towardsdatascience.com towardsdatascience.com Thank you all for sticking on till the end. I hope you guys enjoyed reading this article. I wish you all have a wonderful day ahead!",288,0,9,Towards Data Science,2021-01-01,2021
https://towardsdatascience.com/why-you-need-a-data-science-mentor-in-2021-f2ca7372c7a7,Why You Need a Data Science Mentor in¬†2021,5 Things to Ask Your Mentor to Help Your Career and¬†Learning,7,19,"['Why a Data Science Mentor Can Help You in 2021', '1. What Type of Contributor Are You?', '2. Develop a Working Style', '3. Understand What You Want', '4. Show Your Value-Add', '5. Tell Your Story', 'Summary']","Having a mentor in data science can be beneficial to progress your career and your learning. When I began my role as a data scientist, I adopted two mentors. One was a senior person who had been there since the team‚Äôs conception, and the other was a technical fellow who was looking to mentor younger women. The first 6 to 8 months of my job, I spent learning from my mentors. I asked them tons of questions, threw ideas at them, and tried to understand what I wanted for my career progression. This past year, I have learned a lot from these two individuals about my career and learning. I want to share with you some of these insights. In my first month as a data scientist, I sat down with a senior member of my team, and I asked him what his thoughts were about the team, projects I could work on, and what to look forward to. He asked me one question: What type of contributor are you? He wanted to know if I was interested in staying an individual contributor, leading a team, progressing into a management position, or becoming a technical fellow. Being new to the team and company, it was a hard call to make, but this was the best question I could have been asked at the time. This question got me thinking of what I wanted in the next 2‚Äì5 years and was when I began to plan out my goals for work. Why do you need a mentor? They can ask you hard questions that require you to reflect on your decisions and next steps. Another interesting tidbit I picked up from this mentor was his philosophy on working styles. He has this pattern that has worked well for him in which he works on high-value projects, gets them 75% complete, and then passes on the next 25% to someone else. This allows him to work on the project, learn from it, and then teach and mentor the next person on that project. No matter if you agree with this working philosophy or have your own, I learned from this that everyone has a way that they work best. Through many conversations and my own experiences, I have found the style that works well for me and have implemented it for my projects. Working with a mentor has shown me that different people work differently but can achieve comparable results. Why do you need a mentor? A mentor can show you different ways to tackle the same problem. Learning how they approach a specific situation, such as developing a working style, will allow you to compare those approaches to your own and let you decide how best you work. As I continued to traverse these two mentorships, one of my main topics of conversations with my other mentor was career growth. I had frequent conversations with her about what I was learning, what I wanted for the future, and how I could get there. Through this relationship development, I was given a person I could receive feedback on my plan with. I started by bringing her my first annual review summary and goals. I outlined everything with her and asked for her advice. She told me that I needed to learn to show the value-add of my work and outline my long-term goals. She helped me better understand what it meant to mentor individuals, lead a team, and share a vision with that team. As I lead my team now, I see many of the lessons she has taught me to come through in the way I develop our team goals, roadmap our work, and present the value-added to the broader group. Why do you need a mentor? Your mentor(s) can help you better understand what you want in your job and career. You may change your mind as you go, but you can work towards it if you have a plan in place. The most challenging lesson to learn came from both my mentors, learn to showcase your value-added to those who care. The ones who care are your manager, stakeholders, or executives who want to see what value you have added to the business. In the past year, I have learned to approach this differently than before. Typically, I would write a few sentences on what the project was, what I did in the project, and what steps I took to get that done. Like the STAR method, I outlined the situation (project), task, and actions taken. What I was missing in the assessment of my work was what the results were? I did not explain the outcome of my work. As I worked with my mentors, I understood how better to track a project‚Äôs performance to compare those values and draft my results. I could summarize these results in one slide and showcase my work‚Äôs impact and my team‚Äôs work to those who wanted to see it. Why do you need a mentor? Having a closely aligned mentor with your work can help you with valuable skills, such as learning to track your projects‚Äô performance. They can teach you how to showcase your work positively to stakeholders and share how it added value to the business. As the year progressed, I have presented at universities and begun to mentor interns. During this time, I spoke to my mentors about the experiences and what I could do to improve. The most significant piece of advice I received from one of my mentors was telling my story. I know who I am, what I have accomplished, and what I have learned through these experiences. As I worked with different universities, college students, and mentees, I refined my story and began to focus on the lessons I learned. This was one of the main reasons I began to write on Medium. Through these conversations, I realized that I do have experiences I commonly speak on that would translate well into written form. Why do you need a mentor? A mentoring relationship can help teach you more about yourself as you think through challenging questions they ask and reflect on your discussions. Mentors can also teach you how to present your story and develop your personal brand as you grow as a data scientist. A mentor in data science can help teach you many things. If someone on your team can aid you in your projects, provide feedback, and help push you along when you need a hand. Consider finding a mentor in 2021, especially if you are starting in data science. Finding someone who will advocate for you and support you can be a valuable asset to you as you learn and grow in your position. Here are five questions you can ask your mentor: What have you learned from your mentors this past year? Are you considering a mentor for 2021? If you would like to read more, check out some of my other articles below! towardsdatascience.com towardsdatascience.com towardsdatascience.com",27,1,6,Towards Data Science,2021-01-01,2021
https://towardsdatascience.com/how-to-connect-a-local-r-project-to-github-from-a-mac-4f183d57477e,How to connect a local R project to GitHub from a¬†Mac,Step by step guide with screenshots,4,17,"['How to connect a local R project to GitHub from a Mac', '1. Download the GitHub for Mac app', '2. Open GitHub Desktop and make the connection', '3. Verify that the connection was made']","It‚Äôs fairly straightforward to set up a GitHub connection when creating a new R project. Sometimes, however, you end up with a repo that is only stored locally. Maybe you received a project from someone else who doesn‚Äôt use GitHub, or maybe you‚Äôre having an off day and simply forgot to start out with a GitHub connection. Either way, you can easily copy this repo into GitHub and set up a connection. This step-by-step guide will show you how. First, head to the GitHub for Mac webpage: http://mac.github.com/ You should see the following screen: Click the purple ‚ÄúDownload for macOS‚Äù button in the middle of the screen. A zip file should immediately start downloading. Once that download is complete, double click the zip file. GitHub Desktop should appear in your downloads folder as shown below: Drag GitHub Desktop from the downloads folder into ‚ÄúApplications,‚Äù which is highlighted below on the left. You should see a purple GitHub Desktop icon appear (see bottom right): Double click on the GitHub Desktop icon. You may receive a warning stating that ‚ÄúGitHub Desktop‚Äù is an app downloaded from the Internet and asking if you want to open it. If you see this warning, select ‚ÄúOpen‚Äù: Once GitHub Desktop is open, select ‚ÄúAdd Existing Repository.‚Äù Your screen may look somewhat different from this if it is your first time ever opening GitHub Desktop, but there should still be an ‚ÄúAdd Existing Repository‚Äù option. The following window should appear. Click the ‚ÄúChoose‚Ä¶‚Äù button and navigate to the folder that contains your R project. Once you‚Äôve selected the file that contains your local project, you will get the following notice: ‚ÄúThis directory does not appear to be a Git repository. Would you like to create a repository here instead?‚Äù Click the blue ‚Äúcreate a repository‚Äù text. The following window should appear. Simply click the ‚ÄúCreate Repository‚Äù button in the bottom right. When the following screen appears, click ‚ÄúPublish repository.‚Äù The following window will pop up, and you can simply click ‚ÄúPublish Repository.‚Äù If you wish to make your repo public, you can uncheck the box for ‚ÄúKeep this code private.‚Äù Depending on the size of your project, it may take a few minutes to publish. You should be all set once the project has pushed to GitHub! You can make sure that the connection was successfully established by logging into your GitHub account and verifying that you see your new repo under ‚ÄúRepositories.‚Äù Additionally, when your project is open in RStudio you should now see the ‚ÄúGit‚Äù tab where you can push and pull any updates. And that‚Äôs it! Typically, you will want to stick with the standard workflow of creating a GitHub repo as soon as you create your R project. This way, you will be able to take full advantage of GitHub‚Äôs version control capabilities. Your code will always be backed up, and you can access earlier versions of your work later on. When it is not possible to follow this workflow, however, GitHub Desktop provides a convenient way to connect an existing local project to a GitHub repo.",51,0,4,Towards Data Science,2021-01-01,2021
https://towardsdatascience.com/deep-learning-is-becoming-overused-1e6b08bc709f,Deep Learning Is Becoming¬†Overused,Understanding the data is the first port of¬†call,5,43,"['Deep Learning Is Becoming Overused', 'Time Series Analysis', 'Regression Analysis: Predicting Customer ADR Values', 'Conclusion', 'References']","There is always a danger when any model is used in a black-box fashion to analyse data, and models of the deep learning family are no exception. Don‚Äôt get me wrong ‚Äî there are certainly occasions where a model such as a neural network can outperform more simplistic models ‚Äî but there are plenty of examples where this is not the case. To use an analogy ‚Äî suppose you need to buy a vehicle of some sort for transportation purposes. Buying a truck is a worthwhile investment if you regularly need to transport large items across long distances. However, it is a blatant waste of money if you simply need to go to the local supermarket to pick up some milk. A car (or even a bicycle if you are climate-conscious) is sufficient to carry out the task in question. Deep learning is starting to be used in the same way. We are starting to simply feed these models with the relevant data, assuming that performance will surpass that of simpler models. Moreover, this is often done without properly understanding the data in question; i.e. recognising that deep learning would not be necessary if one had an intuitive grasp of the data. I work most often with time series analysis, so let‚Äôs consider an example in this regard. Suppose that a hotel is looking to forecast the average daily rate (or the average rate per day) that it charges across its customer base. The average daily rates for each customer are averaged on a weekly basis. An LSTM model is configured as follows: Here is the predicted vs. actual weekly ADR: An RMSE of 31 is obtained relative to a mean of 160. The size of the RMSE (root mean squared error) is 20% of the size of the mean ADR. While the error is not excessively high ‚Äî it is admittedly a little disappointing given that the purpose of a neural network is to outperform other models in terms of maximising accuracy. Moreover, this particular LSTM model is a one-step forecast ‚Äî meaning that the model cannot make long-range forecasts without having all data before time t available. That said, have we gotten a bit ahead of ourselves in applying an LSTM model to the data right away? Let‚Äôs bring the horse back before the cart and get an overall view of the data first. Here is a 7-week moving average of the ADR fluctuations: We can see clear evidence of a seasonal pattern when the data is smoothed out over a 7-week moving average. Let‚Äôs take a closer look at the autocorrelation function for the data. We can see that the peak correlation (after the series of negative correlations) is at lag 52, indicating that yearly seasonality is present in the data. Using this information, an ARIMA model is configured using pmdarima to forecast the last 15 weeks of ADR fluctuations, with the p, d, q coordinates automatically selected to minimise the Akaike Information Criterion. According to the output above, ARIMA(0,1,1)(0,1,0)[52] is the configuration that is the model of best fit according to AIC. Using this model, an RMSE of 10 is obtained relative to the mean ADR of 160. This is a lot lower than the RMSE achieved by the LSTM (which is a good thing) and accounts for just over 6% of the size of the mean. Through proper analysis of the data, one would recognise that the presence of a yearly seasonal component in the data makes the time series more predictable ‚Äî and use of a deep learning model to try to forecast such a component would be largely redundant. Let‚Äôs take a different spin on the above problem. Instead of trying to forecast the average weekly ADR, let‚Äôs now try and predict an ADR value for each customer. Two regression-based models are used for this purpose: The following features are used in both models to predict an ADR value for each customer: Using the mean absolute error as the performance measure, let‚Äôs compare the obtained MAE relative to the mean across both models. A LinearSVR with an epsilon of 0.5 is defined and trained across the training data: Predictions are now made using the feature values in the test set: Here is the mean absolute error relative to the mean: The MAE is 28% of the size of the mean. Let‚Äôs see if a regression-based neural network can do any better. The neural network is defined as follows: The model is then trained across 30 epochs using a batch size of 150: With the features from the test set now fed into the model, here are the MAE and mean values: We see that the MAE is only slightly lower than that achieved using the SVM. In this regard, it is hard to justify the use of a neural network in predicting customer ADR when the linear SVM model showed virtually the same level of accuracy. In any event, factors such as the choice of features used to ‚Äúexplain‚Äù ADR are of more relevance than the model itself. As the saying goes, ‚Äúgarbage in, garbage out‚Äù. If feature selection is poor, then the output of the model will also be poor. In this case, while both regression models have shown a degree of predictive power, it is quite possible that either 1) selection of other features in the dataset could improve accuracy further, or 2) there is simply too much variation in ADR that can be accounted for by the features in the dataset. For instance, the dataset tells us nothing about factors such as income level for each customer, which would be expected to significantly influences their average spend per day. In the two examples above, we have seen that use of ‚Äúlighter‚Äù models have been able to match (or surpass) the accuracy achieved by deep learning models. While there are cases where data can be quite complex as to require an algorithm learning patterns in the data ‚Äúfrom scratch‚Äù, this tends to be the exception rather than the rule. As with any data science problem, the key is firstly in understanding the data one is working with. The choice of model is secondary. Many thanks for your time, and any questions or feedback are greatly appreciated! The datasets and Jupyter notebooks for the above examples can be found here. Disclaimer: This article is written on an ‚Äúas is‚Äù basis and without warranty. It was written with the intention of providing an overview of data science concepts, and should not be interpreted as professional advice in any way.",17,0,7,Towards Data Science,2021-01-01,2021
https://towardsdatascience.com/analysis-of-the-chelsea-squad-for-the-2020-21-premier-league-season-a7b38176bd41,Analysis of the Chelsea squad for the 2020‚Äì21 Premier League¬†season,,1,31,['Analysis of the Chelsea squad for the 2020‚Äì21 Premier League season'],"With the beginning of the New Year, we have reached the ‚Äòmid-point‚Äô of quite an unusual Premier League season. It will technically be considered the mid-point when all teams play 19 games (in previous seasons, most teams would have played 19 games by the beginning of the new year). However the late start to the season due to Covid-19 has disrupted the usual cycle. Therefore, currently, we have reached only 16 games. Due to this newly normalized mid-point, this would be a great time to analyze the season so far. Therefore, I will be using this article to analyze the League season so far of the club I support i.e. Chelsea using Data Science methodologies. The data that I use for this analysis has stats only for 14 games. In this article, I wish to answer three questions:- Q. Who are the most over-performing and under-performing goalscorers in the squad? Q. Which players are the most unlucky creators in the squad? Q. Which players have the best minutes to goal ratio? The data has been taken from the website called fbref.com. You can have a look at the data here. Some information about the data: There is information for 30 Chelsea players with 25 features or metrics. Some of the metrics are statistics of expected goals (xG), expected assists (xA) and so on. Q1. Who are the most over-performing and under-performing goalscorers in the squad? For this question, we will be looking at the Expected goals stats (or the xG). The expected goals is the number of goals a player should be scoring. This number is based upon various factors such as distance away from goal, angle towards the goal etc. For example, a chance within the penalty area will have a higher xG in comparison to a shot taken from 40 yards away from the goal. In simple terms, a chance that you are more likely or expected to score, is given a higher xG. For this example, we will be taking the difference of the goals scored from the xG of each player, which we will call XGDiff. If a player is over-performing, the XGDiff will be positive, which indicates the player is scoring more goals than is expected from them. Whereas, a negative XGDiff indicates that the player is scoring lesser goals than expected. Most top goalscorers are expected to have a positive XGDiff or a number just below 0. In this question, instead of Goals, we will be considering the Goals per 90. This number is calculated by dividing the number of goals with the quotient of the minutes played and 90. The Goals per 90 indicates the number of goals that a player scores every 90 minutes. This stat is more logical since it makes it easier to compare the stats of a player who has played 900 minutes versus a player who has played only 300 minutes. This metric brings about a more level playing field and it is especially useful for our dataset where we have players with over 1400 minutes and less than 300 minutes. Therefore, we will be using the Goals per 90 and xG per 90 metrics. We will subtract the two in order to get the XGDiff. We can see that Callum Hudson-Odoi has the highest XGDiff in the entire Chelsea Squad with 0.38. This means that every 90 minutes, he is scoring 0.38 goals more than he is supposed to. We can also notice that Timo Werner has the lowest XGDiff with -0.18. This means that every 90, Werner is scoring 0.18 goals lesser than expected. Q.2 Which players are the most unlucky creative players in the squad? For this question, we will be looking at the Expected assists metric. Similar to expected goals, expected assists is the number of assists a player should get in a game. The passes that leads to a shot are counted as expected assists. A through ball that leads to a shot in the Penalty area will have a higher xA than a pass which led to a shot from over 40 yards. Therefore, similar to XGDiff, XADiff is the difference between Assists and expected assists. However, in contrast to XGDiff, a negative XADiff is considered better than a positive one. A negative XADiff suggests that a player is creating high quality chances which is not getting finished by the Strikers/Forwards. On the other hand, a positive XADiff suggests that the player may be getting assists that they don‚Äôt probably deserve. Continuing with the early analogy, a player that provides a basic sideways pass that leads to a 40 yard screamer will have the assist, but will have a low expected assist score for that pass, leading to a positive XADiff. On the other hand, a player that provides a defence-splitting pass which does not get finished by the striker, will not get the assist, but will receive a high expected assist score for that action, leading to a negative XADiff. Hence a negative XADiff is considered superior. Ideally, most great creators will have a positive or close to zero XADiff because not all their assists will be swerving crosses or splitting through balls. However, what this metric allows us to measure is who are the players that are actually creating quality chances and not getting assists purely due to external factors such as poor finishing or great defending. The xA stat allows teams to find diamonds in the rough from unlikely teams who are creating quality chances but being let down purely due to factors outside their control. Similar to xG, we have used Assists per 90 and xA per 90. We can see that Christian Pulisic has the lowest XADiff with -0.15. This implies that in every 90 minutes, Pulisic should have had 0.15 assists more than he has now. On the other hand, we can see that Hakim Ziyech has the highest XADiff. This means that every 90 minutes, Hakim Ziyech has 0.24 assists more than he is expected to have. Hakim Ziyech is arguably one of the best creative players in the current Chelsea squad and this justifies my earlier point that the best creators will not always have a negative XADiff. The objective of this metric is to understand who is the most unluckiest creator and that is Christian Pulisic. When we compare the XADiff and XGDiff of the entire Chelsea squad, we can gather great insights about what type of season a player is having. I have used Power Bi to plot the XADiff vs XGDiff of each player. Q.3. Which player has the best minutes to goal ratio? The minutes to goal ratio is the number of minutes it takes for a player to score a goal. This is an extremely useful metric to calculate how effective a Striker/Forward is. Therefore, most top strikers have a low minutes to goal ratio. To calculate Minutes to Goal ratio or Minutes per Goal, we divide the Minutes played by Goals scored. Therefore, we can see that Tammy Abraham has the best Minutes per Goal followed by Olivier Giroud, with 138 and 145 respectively. This means that both the players are able to score a goal in less than 2 games or less than 180 minutes. It is a great sign that 2 of Chelsea‚Äôs strikers have the lowest Minutes per Goal. However, the worrying aspect is Timo Werner, who has a considerable high Minutes per Goal of 300 which suggests he requires more than 3 games to score a goal. However, these goals include penalty goals. In order to truly understand the lethal nature of the forwards, we need to consider goals excluding penalties. In order to find this, we subtract the Penalties from the Goals. And finally, we divide it by the Minutes to get the Minutes per Non-Penalty goals. As we can see from the above graph, the data has not changed much i.e. Tammy Abraham and Olivier Giroud still have the lowest Minutes per Goal. However, you will notice that Jorginho is no longer considered since all his goals were penalties. Thank you for reading. I have fetched my data from fbref.com and the following link leads to the data fbref.com If you are interested in understanding my methods for analysis, you can take a look at my Github repository and Jupyter Notebook for this project. github.com I would suggest you read articles regarding expected goals and assists from https://fbref.com/en/ , if you are interested in learning more. You can connect with me on LinkedIn here: https://www.linkedin.com/in/samuelvarkey/ Wishing everyone a Happy New Year.",22,1,7,Towards Data Science,2021-01-01,2021
https://towardsdatascience.com/is-that-red-wine-good-enough-49326a4c08e4,Is that Red Wine Good¬†Enough?,An introductory tutorial to introduce basic workflow of predictive‚Ä¶,12,30,"['Is that Red Wine Good Enough?', 'Exploring Data', 'Exploring Features', 'Exploring Predictors Visually', 'Checking Correlation', 'Feature Engineering', 'Fitting Model', 'Fitting Model on Training Data', 'Checking Model Performance', 'Summary Insight', 'Thanks for Reading!', 'Arafath Hossain']","Let‚Äôs assume that we have been hired by a winery to build a predictive model to check the quality of their red wine. The traditional way of wine testing is done by a human expert. Thus the process is prone to human error. The goal is to establish a process of producing an objective method of wine testing and combining that with the existing process to reduce human error. For the purpose of building the predictive model, we‚Äôll use a dataset provided by UCI machine learning repository. We‚Äôll try to predict wine quality based on features associated with wine. Goal: Loading data, libraries and primary glimpsing over data From the features we see ‚Äòquality‚Äô is our target feature. And we have total 11 features to be used as the predictors. Since we will cover talk about the classification model, we‚Äôll convert our target feature from continuous to binary class. So that we would be able to fit one of the very widely used yet very easy classification models. Distribution of original target feature labels After transformation we have 53.47% cases classified records as good wines vs 46.53% as bad wines. We have a nice distribution of our target classes here! Which is very nice. Otherwise, we would‚Äôve had to deal with Data Balancing. Though we won‚Äôt cover that area in this tutorial, it‚Äôs a great discussion area to delve into. So some extra points for those who‚Äôll learn about it! In short, we would like to have a balanced distribution of observations from different labels in our target feature. Otherwise, some ML algorithms tend to overfit. Exploring acidity We have multiple features that are continuous and can plot them similarly. Which means we‚Äôll have to re write the code that we have just wrote in code chunk: viz_acidity again and again. In coding, we don‚Äôt want to do that. So we‚Äôll create a function and wrap that around our code so that it can be reused in future! If it sounds too much, just stick with it. Once you see the code, it‚Äôll make a lot more sense. We can quickly check correlations among our predictors. Highly correlated features don‚Äôt add new information to the model and blurrs the effect of individual feature on the predictor and thus makes it difficult to explain effect of individual features on target feature. This problem is called Multicollinearity. As a general rule, we don‚Äôt want to keep features with very high correlation. All these are great questions and worth having a good understanding about. So again extra points for those who‚Äôll learn about ! Before making any decision based on correlation, check distribution of the feature. Unless any two features have a linear relation, correlation doesn‚Äôt mean much. Based on the insight gained from the data exploration, some features may need to be transformed or new features can be created. Some common feature engineering tasks are: This tutorial won‚Äôt cover feature engineering but it‚Äôs a great area to explore. A great data exploration followed by necessary feature engineering are the absolute necessary prerequisites before fitting any predictive model! In practical world we train our predictive models on historical data which is called Training Data. Then we apply that model on new unseen data, called Test Data, and measure the performance. thus we can be sure that our model is stable or not over fitted on training data. But since we won‚Äôt have access to new wine data, we‚Äôll split our dataset into training and testing data on a 80:20 ratio. Let‚Äôs check the data balance in training and test data. We‚Äôll fit Logistic Regression classification model on our dataset. Let‚Äôs plot the variables with the lowest p values/highest absolute z value. We‚Äôll check how our model performs by running it on our previously unseen test data. We‚Äôll compare the predicted outcome with the actual outcome and calculate some typically used binary classification model performance measuring metrics. Model Perfomance Summary: So let‚Äôs summarize about what we have learned about wine testing from our exercise: ‚ÄúPeople could tell the difference between wines under ¬£5 and those above ¬£10 only 53% of the time for whites and only 47% of the time for reds.‚Äù By reading this article, you should have a fundamental understanding of how functions in R works, as well as how it can be used in your personal life! Not sure what to read next? I‚Äôve picked another article for you: curious-joe.medium.com",10,0,9,Towards Data Science,2021-01-01,2021
https://towardsdatascience.com/reusing-paper-filters-for-espresso-ef1899ae2b35,Reusing Paper Filters for¬†Espresso,Advanced Paper Filter¬†Theory‚Ä¶,6,44,"['Reusing Paper Filters for Espresso', 'Shot Performance Metrics', 'Paper Age', 'Don‚Äôt Reuse a Bottom Filter', 'Cleaning Paper Filters', 'Further readings of mine:']","Previously, I have discussed how paper filters affect flow as well as how important having them cut to size affects their effectiveness. In this piece, I will answer the important question of the day: how lazy can I be? How many times can I use any given paper filter for espresso? I will look at this statistically over the span of a few months as well as a few precise examples related to the bottom filter. Using a paper filter on top can be reused quite a lot, but my machine performs worse with a top filter. For a bottom filter, single use is the best or after the right kind of cleaning. In this case, a second use is massively noticable. For a middle filter, it seems somewhere between 4 and 6 uses it optimal. This is used when doing staccato tamping or staccato espresso. I‚Äôm putting these definitions here because just after this section, I evaluate different amounts of paper filters using these metrics. I used two metrics for evaluating the differences between shots: Final Score and Coffee Extraction. Final score is the average of a scorecard of 7 metrics (Sharp, Rich, Syrup, Sweet, Sour, Bitter, and Aftertaste). These scores were subjective, of course, but they were calibrated to my tastes and helped me improve my shots. There is some variation in the scores. My aim was to be consistent for each metric, but some times the granularity was difficult and affected the final score. Total Dissolved Solids (TDS) is measured using a refractometer, and this number is used to determine the percentage of coffee extracted into the cup in combined with the output weight of the shot and the input weight of the coffee, called Extraction Yield (EY). I reused my filters because it took time to cut them, and I also don‚Äôt like waste if I don‚Äôt have to. I also kept note of how old each filter was (i.e. how many times it had been used). So an age of 1 is a new filter. I then looked at some standard metrics of performance. There seems to be a slight degradation in taste as represented by Final Score. For EY and TDS, there is a definite drop off after 6 uses or so. Age doesn‚Äôt affect paper weight, so the amount of solids remaining is very small, but they have an impact. There is also an impact on Time to Cover Filter (TCF) and Total Time. TCF is a good indicator of flow, but the variable isn‚Äôt quite separated from roast age in this graph. I compared TDS and EY in control charts with all the data and then data for new, sort of new, and old. There doesn‚Äôt seem to be a ton of separation in these graphs. So I went to summarizing the data using an average. This data can be summarized, and the trends seem clear based on the averages. The preference is to use new filters or appropriately cleaned filters. The bottom filter can only be used once. For whatever reason, it can‚Äôt be cleaned enough to not jam up. I started using one because of some difficult roasts, and once I discovered this fact, I changed to only use a fresh one. In this test, I used a filter once, rinsed it, let it dry, and reused it again with all other shot parameters the same. The differences are especially noticeable in the post-shot puck analysis. Darker spots indicate slow flow due to channeling elsewhere. The top half of the puck is a much lighter tamp, so if it evenly falls apart, that‚Äôs usually indicative of more even and better extraction. In this case, that was the first time using the filter. For the second time, the top layer has larger clumps particularly in the middle that indicate not enough extraction occurred. We can look at the flow rates of the shots, and it is clear that New vs Used Once has a big difference. The flow rate was much more slowed down. For reference, Pre-infusion was 45 seconds, and the jitter in the flow is due to pressure pulsing. When we look at the smoothed flow rate, the peak during Pre-infusion was lower and then it became more constant. The same was true for infusion. The blue line is more typical of what a really good shot looks like. The new filter had a higher TDS, EY, and taste. This indicates that something of the coffee remains in the filter even after rinsing that still restricts flow. This drove me to wonder what happens when a filter is washed with soap and reused. I wanted to see if I could clean paper filters even though many people suggested just to use a new one. My issue with not reusing paper filters comes down to cost, waste, and time. By cleaning them, I could save myself in all three areas because batch cleaning is faster than individual cutting. Previously, I had only rinsed the filters. I then tried soap and water, and it was better than reusing a filter. However, it wasn‚Äôt as good as using a new filter. Then I tried isopropyl alcohol. That seemed to do the trick! The result was so similar to the new filter that it was hard to tell the difference both in taste and extraction yield. This study has informed my behavior greatly as it has put numbers to a technique that doesn‚Äôt have much understanding. I have now switched to using only new paper filters or isopropyl washed filters, and it has helped me maintain consistency and shot excellence. Additionally, while writing up this series of articles, I decided to take another look at cloth filters, and I‚Äôve switched my middle filter to a cloth filter in a staccato tamped shot. This has dramatically changed the flow and the shots, so I‚Äôve also decided to look at other materials for this layer in the middle or on the bottom of the espresso puck. I‚Äôm interested in a better shot and technique, but more than that, I‚Äôm curious to understand better how everything is functioning inside the puck. If you like, follow me on Twitter and YouTube where I post videos of espresso shots on different machines and espresso related stuff. You can also find me on LinkedIn. You can also follow me on Medium. Comparing Coffee using Pattern Recognition A Review of Coffee Data: Grades and Flavors Coffees by Region, Process, Grade, and Price The Economics of Home Roasting Coffee Coffee Bean Degassing Deconstructed Coffee: Split Roasting, Grinding, and Layering for Better Espresso Pre-infusion for Espresso: Visual Cues for Better Espresso The Shape of Coffee To Stir or To Swirl: Better Espresso Experience Spicy Espresso: Grind Hot, Tamp Cold for Better Coffee Staccato Espresso: Leveling Up Espresso Improving Espresso with Paper Filters Coffee Solubility in Espresso: An Initial Study Staccato Tamping: Improving Espresso without a Sifter Espresso Simulation: First Steps in Computer Models Pressure Pulsing for Better Espresso Coffee Data Sheet",1,0,7,Towards Data Science,2021-01-01,2021
https://towardsdatascience.com/transformers-explained-visually-part-2-how-it-works-step-by-step-b49fa4a64f34,"Transformers Explained Visually (Part 2): How it works, step-by-step","A Gentle Guide to the Transformer under the hood, and its end-to-end operation.",12,64,"['Transformers Explained Visually (Part 2): How it works, step-by-step', 'Architecture Overview', 'Embedding and Position Encoding', 'Matrix Dimensions', 'Encoder', 'Decoder', 'Attention', 'Multi-head Attention', 'Attention Masks', 'Generate Output', 'Training and Loss Function', 'Conclusion']","This is the second article in my series on Transformers. In the first article, we learned about the functionality of Transformers, how they are used, their high-level architecture, and their advantages. In this article, we can now look under the hood and study exactly how they work in detail. We‚Äôll see how data flows through the system with their actual matrix representations and shapes and understand the computations performed at each stage. Here‚Äôs a quick summary of the previous and following articles in the series. My goal throughout will be to understand not just how something works but why it works that way. And if you‚Äôre interested in NLP applications in general, I have some other articles you might like. As we saw in Part 1, the main components of the architecture are: Data inputs for both the Encoder and Decoder, which contains: The Encoder stack contains a number of Encoders. Each Encoder contains: The Decoder stack contains a number of Decoders. Each Decoder contains: Output (top right) ‚Äî generates the final output, and contains: To understand what each component does, let‚Äôs walk through the working of the Transformer while we are training it to solve a translation problem. We‚Äôll use one sample of our training data which consists of an input sequence (‚ÄòYou are welcome‚Äô in English) and a target sequence (‚ÄòDe nada‚Äô in Spanish). Like any NLP model, the Transformer needs two things about each word ‚Äî the meaning of the word and its position in the sequence. The Transformer combines these two encodings by adding them. The Transformer has two Embedding layers. The input sequence is fed to the first Embedding layer, known as the Input Embedding. The target sequence is fed to the second Embedding layer after shifting the targets right by one position and inserting a Start token in the first position. Note that, during Inference, we have no target sequence and we feed the output sequence to this second layer in a loop, as we learned in Part 1. That is why it is called the Output Embedding. The text sequence is mapped to numeric word IDs using our vocabulary. The embedding layer then maps each input word into an embedding vector, which is a richer representation of the meaning of that word. Since an RNN implements a loop where each word is input sequentially, it implicitly knows the position of each word. However, Transformers don‚Äôt use RNNs and all words in a sequence are input in parallel. This is its major advantage over the RNN architecture, but it means that the position information is lost, and has to be added back in separately. Just like the two Embedding layers, there are two Position Encoding layers. The Position Encoding is computed independently of the input sequence. These are fixed values that depend only on the max length of the sequence. For instance, These constants are computed using the formula below, where In other words, it interleaves a sine curve and a cos curve, with sine values for all even indexes and cos values for all odd indexes. As an example, if we encode a sequence of 40 words, we can see below the encoding values for a few (word position, encoding_index) combinations. The blue curve shows the encoding of the 0th index for all 40 word-positions and the orange curve shows the encoding of the 1st index for all 40 word-positions. There will be similar curves for the remaining index values. As we know, deep learning models process a batch of training samples at a time. The Embedding and Position Encoding layers operate on matrices representing a batch of sequence samples. The Embedding takes a (samples, sequence length) shaped matrix of word IDs. It encodes each word ID into a word vector whose length is the embedding size, resulting in a (samples, sequence length, embedding size) shaped output matrix. The Position Encoding uses an encoding size that is equal to the embedding size. So it produces a similarly shaped matrix that can be added to the embedding matrix. The (samples, sequence length, embedding size) shape produced by the Embedding and Position Encoding layers is preserved all through the Transformer, as the data flows through the Encoder and Decoder Stacks until it is reshaped by the final Output layers. This gives a sense of the 3D matrix dimensions in the Transformer. However, to simplify the visualization, from here on we will drop the first dimension (for the samples) and use the 2D representation for a single sample. The Input Embedding sends its outputs into the Encoder. Similarly, the Output Embedding feeds into the Decoder. The Encoder and Decoder Stacks consists of several (usually six) Encoders and Decoders respectively, connected sequentially. The first Encoder in the stack receives its input from the Embedding and Position Encoding. The other Encoders in the stack receive their input from the previous Encoder. The Encoder passes its input into a Multi-head Self-attention layer. The Self-attention output is passed into a Feed-forward layer, which then sends its output upwards to the next Encoder. Both the Self-attention and Feed-forward sub-layers, have a residual skip-connection around them, followed by a Layer-Normalization. The output of the last Encoder is fed into each Decoder in the Decoder Stack as explained below. The Decoder‚Äôs structure is very similar to the Encoder‚Äôs but with a couple of differences. Like the Encoder, the first Decoder in the stack receives its input from the Output Embedding and Position Encoding. The other Decoders in the stack receive their input from the previous Decoder. The Decoder passes its input into a Multi-head Self-attention layer. This operates in a slightly different way than the one in the Encoder. It is only allowed to attend to earlier positions in the sequence. This is done by masking future positions, which we‚Äôll talk about shortly. Unlike the Encoder, the Decoder has a second Multi-head attention layer, known as the Encoder-Decoder attention layer. The Encoder-Decoder attention layer works like Self-attention, except that it combines two sources of inputs ‚Äî the Self-attention layer below it as well as the output of the Encoder stack. The Self-attention output is passed into a Feed-forward layer, which then sends its output upwards to the next Decoder. Each of these sub-layers, Self-attention, Encoder-Decoder attention, and Feed-forward, have a residual skip-connection around them, followed by a Layer-Normalization. In Part 1, we talked about why Attention is so important while processing sequences. In the Transformer, Attention is used in three places: The Attention layer takes its input in the form of three parameters, known as the Query, Key, and Value. The Transformer calls each Attention processor an Attention Head and repeats it several times in parallel. This is known as Multi-head attention. It gives its Attention greater power of discrimination, by combining several similar Attention calculations. The Query, Key, and Value are each passed through separate Linear layers, each with their own weights, producing three results called Q, K, and V respectively. These are then combined together using the Attention formula as shown below, to produce the Attention Score. The important thing to realize here is that the Q, K, and V values carry an encoded representation of each word in the sequence. The Attention calculations then combine each word with every other word in the sequence, so that the Attention Score encodes a score for each word in the sequence. When discussing the Decoder a little while back, we briefly mentioned masking. The Mask is also shown in the Attention diagrams above. Let‚Äôs see how it works. While computing the Attention Score, the Attention module implements a masking step. Masking serves two purposes: In the Encoder Self-attention and in the Encoder-Decoder-attention: masking serves to zero attention outputs where there is padding in the input sentences, to ensure that padding doesn‚Äôt contribute to the self-attention. (Note: since input sequences could be of different lengths they are extended with padding tokens like in most NLP applications so that fixed-length vectors can be input to the Transformer.) Similarly for the Encoder-Decoder attention. In the Decoder Self-attention: masking serves to prevent the decoder from ‚Äòpeeking‚Äô ahead at the rest of the target sentence when predicting the next word. The Decoder processes words in the source sequence and uses them to predict the words in the destination sequence. During training, this is done via Teacher Forcing, where the complete target sequence is fed as Decoder inputs. Therefore, while predicting a word at a certain position, the Decoder has available to it the target words preceding that word as well as the target words following that word. This allows the Decoder to ‚Äòcheat‚Äô by using target words from future ‚Äòtime steps‚Äô. For instance, when predicting ‚ÄòWord 3‚Äô, the Decoder should refer only to the first 3 input words from the target but not the fourth word ‚ÄòKetan‚Äô. Therefore, the Decoder masks out input words that appear later in the sequence. When calculating the Attention Score (refer to the picture earlier showing the calculations) masking is applied to the numerator just before the Softmax. The masked out elements (white squares) are set to negative infinity, so that Softmax turns those values to zero. The last Decoder in the stack passes its output to the Output component which converts it into the final output sentence. The Linear layer projects the Decoder vector into Word Scores, with a score value for each unique word in the target vocabulary, at each position in the sentence. For instance, if our final output sentence has 7 words and the target Spanish vocabulary has 10000 unique words, we generate 10000 score values for each of those 7 words. The score values indicate the likelihood of occurrence for each word in the vocabulary in that position of the sentence. The Softmax layer then turns those scores into probabilities (which add up to 1.0). In each position, we find the index for the word with the highest probability, and then map that index to the corresponding word in the vocabulary. Those words then form the output sequence of the Transformer. During training, we use a loss function such as cross-entropy loss to compare the generated output probability distribution to the target sequence. The probability distribution gives the probability of each word occurring in that position. Let‚Äôs assume our target vocabulary contains just four words. Our goal is to produce a probability distribution that matches our expected target sequence ‚ÄúDe nada END‚Äù. This means that the probability distribution for the first word-position should have a probability of 1 for ‚ÄúDe‚Äù with probabilities for all other words in the vocabulary being 0. Similarly, ‚Äúnada‚Äù and ‚ÄúEND‚Äù should have a probability of 1 for the second and third word-positions respectively. As usual, the loss is used to compute gradients to train the Transformer via backpropagation. Hopefully, this gives you a feel for what goes on inside the Transformer during Training. As we discussed in the previous article, it runs in a loop during Inference but most of the processing remains the same. The Multi-head Attention module is what gives the Transformer its power. In the next article, we will continue our journey and go one step deeper to really understand the details of how Attention is computed. And finally, if you liked this article, you might also enjoy my other series on Audio Deep Learning, Geolocation Machine Learning, and Image Caption architectures. towardsdatascience.com towardsdatascience.com towardsdatascience.com Let‚Äôs keep learning!",1500,17,11,Towards Data Science,2021-01-02,2021
https://towardsdatascience.com/turn-photos-into-cartoons-using-python-bb1a9f578a7e,Turn Photos into Cartoons Using¬†Python,You can give a cartoon effect to a photo by implementing‚Ä¶,5,22,"['Turn Photos into Cartoons Using Python', '1. Load Image', '2. Create Edge Mask', '3. Reduce the Color Palette', '4. Combine Edge Mask with the Colored Image']","As you might know, sketching or creating a cartoon doesn‚Äôt always need to be done manually. Nowadays, many apps can turn your photos into cartoons. But what if I tell you, that you can create your own effect with few lines of code? There is a library called OpenCV which provides a common infrastructure for computer vision applications and has optimized-machine-learning algorithms. It can be used to recognize objects, detect, and produce high-resolution images. In this tutorial, I will show you how to give a cartoon-effect to an image in Python by utilizing OpenCV. I used Google Colab to write and run the code. You can access the full code in Google Colab here To create a cartoon effect, we need to pay attention to two things; edge and color palette. Those are what make the differences between a photo and a cartoon. To adjust that two main components, there are four main steps that we will go through: Before jumping to the main steps, don‚Äôt forget to import the required libraries in your notebook, especially cv2 and NumPy. The first main step is loading the image. Define the read_file function, which includes the cv2_imshow to load our selected image in Google Colab. Call the created function to load the image. I chose the image below to be transformed into a cartoon. Commonly, a cartoon effect emphasizes the thickness of the edge in an image. We can detect the edge in an image by using the cv2.adaptiveThreshold() function. Overall, we can define the egde_mask function as: In that function, we transform the image into grayscale. Then, we reduce the noise of the blurred grayscale image by using cv2.medianBlur. The larger blur value means fewer black noises appear in the image. And then, apply adaptiveThreshold function, and define the line size of the edge. A larger line size means the thicker edges that will be emphasized in the image. After defining the function, call it and see the result. The main difference between a photo and a drawing ‚Äî in terms of color ‚Äî is the number of distinct colors in each of them. A drawing has fewer colors than a photo. Therefore, we use color quantization to reduce the number of colors in the photo. To do color quantization, we apply the K-Means clustering algorithm which is provided by the OpenCV library. To make it easier in the next steps, we can define the color_quantization function as below. We can adjust the k value to determine the number of colors that we want to apply to the image. In this case, I used 9 as the k value for the image. The result is shown below. After doing color quantization, we can reduce the noise in the image by using a bilateral filter. It would give a bit blurred and sharpness-reducing effect to the image. There are three parameters that you can adjust based on your preferences: The final step is combining the edge mask that we created earlier, with the color-processed image. To do so, use the cv2.bitwise_and function. And there it is! We can see the ‚Äúcartoon-version‚Äù of the original photo below. Now you can start playing around with the codes to create your own version of the cartoon effect. Besides adjusting the value in parameters that we used above, you can also add another function from OpenCV to give special effects to your photos. There‚Äôs still a lot of things in the library that we can explore. Happy trying! References:",1800,26,4,Towards Data Science,2021-01-02,2021
https://towardsdatascience.com/creating-an-email-parser-with-python-and-sql-c79cb8771dac,Creating an Email Parser with Python and¬†SQL,Boost your productivity by automatically extracting data‚Ä¶,7,39,"['Creating an Email Parser with Python and SQL', 'Getting started', 'Creating a SQL Database', 'Accessing your Emails in Python', 'Connecting Outlook to Python', 'Parsing Email HTML with Python', 'Uploading data to a SQL database']","You might be wondering what an email parser is, and why you might need one. In short, an email parser is a software that looks for and extracts data from inbound emails and attachments. More importantly, an email parser uses conditional processing to pull the specific data that matters to you. So why does this matter? If you‚Äôve ever spent any time working a regular office job, you‚Äôve probably become intimately familiar with reports, and by extension, copy-pasting lines of text from Microsoft Outlook to Excel or Word. You might even end up doing the same report, week after week. Add in formatting and spellchecking, and this ends up as a huge time drain when you could be focusing on more important tasks. The good news is that you can automate most of this process with Python and SQL. In this post, I‚Äôll cover how to open Outlook emails with Python and extract the body text as HTML. I‚Äôll then cover how to parse this in Python and how to upload the final data to a SQL database. From there, you can write this data to Excel or transform it into a Pandas Dataframe. We‚Äôll be using a few key Python libraries here, namely os, sqlite3 and pywin32. To start off, we‚Äôll first need to decide what we want to extract from our emails. For example, let‚Äôs say we have a bunch of emails that each contain a list of news articles like this: Let‚Äôs then say that we want to extract the header of each bullet point, which includes the title, the publication, media platforms, and URL links. In short, we want to take the entire header of each bullet point, then break it down into four different parts. Our pseudocode so far should look something like this: Before parsing our emails, we‚Äôll first want to set up a SQL database with Python. We‚Äôll do this by establishing a connection to the SQLite database with a connection object that we‚Äôll call db. If it doesn‚Äôt already exist, a new database will be created as emails.db. We can then create tables in our database that our email parser can write to later on. In essence, we‚Äôre creating three tables, where our main table is ‚Äòarticles‚Äô, which has a one-to-many relationship with ‚Äòplatforms‚Äô and ‚Äòlinks‚Äô. In other words, this reflects how one article can have many different platforms and links. You‚Äôll want to move the emails that you want to parse from Outlook to a folder. The simplest method to do this is by dragging and dropping. Next, create a variable storing the folder path of your emails. You can do this manually e.g. folder_path = r‚ÄòC:\Users\Username\EmailFolder‚Äô or with tkinter and os, which will generate a file explorer prompt to select a folder. Here, we‚Äôre using a file input prompt created with tkinter to save our folder path, then normalizing the path with os to remove any redundant separators. We‚Äôll then want to obtain the path headings of each email. We can do this with os.listdir(), which gives a list of all files in the specified directory. This will save the file name of each email in list that we can access later. Next, you‚Äôll want to create an object that will allow us to control Outlook from Python. This is enabled through the pywin32 library that helps to connect Python to Outlook via the Microsoft Outlook Messaging API (MAPI). With this, we can begin to open each item as a HTML object, and use regular expressions i.e. Regex to extract the body text of each email. While conventional wisdom dictates that you shouldn‚Äôt use Regex to parse HTML, we‚Äôre not worried about this here, as we‚Äôre only looking to extract very specific text snippets out of a standard email format (Some commercial email parsers like Parseur are heavily built around Regex). From this point, Regex can be used to narrow down the specific data that you want to extract. This is how the first bullet point of our email might look as HTML: Okay ‚Äî so we can see that there are several key characteristics here, namely that our data exists as a bulleted list or li class=MsoListParagraph. We can use Regex to extract each bullet point. Each bullet point is extracted as a string, and each string is stored in a list. Our first bullet point should look something like this with Regex: To retrieve our title and publication, we can use Regex again. This time, we‚Äôll also use call html.unescape() on our text to help translate our HTML to string e.g. &8211; ‚Üí ‚Äì (a unicode dash). From here, it‚Äôs as simple as splitting our text. We can use split_list = title_pub.split(""‚Äì"") to give us a list: [""New Arrival: Dell G Series Gaming Computers"", ""Tech4tea""]. We can then remove any redundant whitespaces and save each item as a variable. That‚Äôs two down! To get our media platforms, we‚Äôll use a more straightforward method. This will give us a list of publications: [""Online"", ""Facebook"", ""LinkedIn""] Now for the URLs: This will then give us the characters highlighted in green below: Our data so far should look something like this: The final step in this process is to upload each piece of data to our SQL database. We‚Äôll start by uploading our title and publication data. This can be accomplished with the following code: Uploading our links and platforms are a bit more tricky. First, we‚Äôll copy over our primary id from our main table, then iterate over each platform and link individually. The last step here is to commit all these changes to the database. With that done, our email parser is complete! If you‚Äôd like, you can use something like DB Browser to check that the contents of your database have been successfully updated. In case you need it, I‚Äôve uploaded the full code for this on my website and Github.",161,2,7,Towards Data Science,2021-01-02,2021
https://towardsdatascience.com/image-processing-with-python-blurring-and-sharpening-for-beginners-3bcebec0583a,Image Processing with Python‚Ää‚Äî‚ÄäBlurring and Sharpening for Beginners,How do you apply convolution‚Ä¶,1,25,['Image Processing with Python ‚Äî Blurring and Sharpening for Beginners'],"In this article we shall discuss how to apply blurring and sharpening kernels onto images. These basic kernels form the backbone of a lot of more advanced kernel application. In my previous article I discussed the edge detection kernel, but I realized that I only stuck to greyscale images. To act as a helpful guide, I shall discuss how we can go about applying these kernels onto colored images while still retaining the core image. Let‚Äôs get started! As always let us begin by importing the required Python Libraries. For the purposes of this article, we shall use the below image. Now the kernels we shall apply to the image are the Gaussian Blur Kernel and the Sharpen Kernel. You can see how we define their matrixes below. But how do we actually apply these kernels to our image? Well, let us first try by directly convolving them. I have defined the below function to allow us to the kernels iteratively. Note how we set the boundary to fill and fillvalue to 0, this is important to ensure that the output will be a 0 padded matrix of the same size as the original matrix. Oh no, it seems that we have come across a value error. Why is this the case? Remember that when we convolve a matrix with another matrix, the matrices should be of the same dimensions. This means that we cannot apply a 2D convolution to our 3D (because of the color channels) matrix. To solve this we must first convert the image to a greyscale. Now if we run the function, we should get the desired effect. Wonderful! We can now see that the image has been clearly blurred. The below code will show us what happens to the image if we continue to run the gaussian blur convolution to the image. Great! We can clearly see the continued blurring of the image due to the application of our kernel. But what if you needed to blur the image and retain the color? Let us first try to apply the convolutions per color channel. The function actually returns to us the reformed image, we just have to plug it into the show function. Great! It seems that the function worked well. As a fun exercise let us see what happens when we convolve the image 10 times. So this solve our issue right? Well, not really. To see the issue this function has, let us try to sharpen the image. Looks good so far, let us see what the reformed image looks like. The image has been reformed, but we now see that there are some slight distortions. Why is this the case? Remember that the RGB color space implicitly mixes the luminescence of the pixels with the colors. This means that it is practically impossible to apply convolutions to the lighting of an image without changing the colors. So how do we handle this issue? One way to go around this problem is by changing the color space the image. Instead of using the RGB color space, we can make use of the Y‚ÄôUV color space. We do this because the lighting channel in the Y‚ÄôUV space is actually separated from the colors (this is the Y component). For the purposes of this article we shall edit the function to first convert the image into a Y‚ÄôUV color space and then do the required convolutions. We can see that our function now returns an image that is noticeably sharper with none of the color distortions. There are many other ways to tackle this issue with Y‚ÄôUV conversion being only one of them. Remember that the V component of the HSV color space represents almost the same thing. However, the way that the luma component of Y‚ÄôUV space and the value component of the HSV space are slightly different. Let us see what are the consequences of using one over the other. We see that there is some slight improvement of the HSV and Y‚ÄôUV over the original RGB method. For better illustration we can up the amount of iterations from 1 to 2. At 2 iterations the distortions become far more apparent. But it is also very clear that the HSV and Y‚ÄôUV adjusted image are fairing much better than the original RGB adjusted image. These properties should be kept in mind when deciding the best way to apply convolutional kernels onto an image. In Conclusion To summarize, we‚Äôve learned how to conduct blurring and sharpening convolutions to an image. Such techniques are vital for any data scientist working in the field of image processing and computer vision. Very importantly, we learned that simply applying convolutions to the individual RGB channels may not be the best way to go. When working with images, one should always be aware that there are plenty of different kinds of color spaces to work with. Hopefully you found this article helpful and can apply it in your own work.",23,1,7,Towards Data Science,2021-01-02,2021
https://towardsdatascience.com/3-steps-to-get-aws-cloud-practitioner-certified-in-2-weeks-or-less-772178f48249,3 Steps to Get AWS Cloud Practitioner Certified in 2 Weeks or¬†Less,Kickstart 2021 with an¬†AWS‚Ä¶,10,30,"['3 Steps to Get AWS Cloud Practitioner Certified in 2 Weeks', 'Contents', 'Introduction to AWS Certifications', 'Step 1 ‚Äî AWS E-Learning Modules', 'Step 2\u200a‚Äî\u200aAWS Whitepapers and Website', 'Step 3 ‚Äî YouTube Practice Questions', 'Optional Resources', 'About the Examination', 'Conclusion', 'Before you go']","Introduction to AWS CertificationsStep 1: AWS E-learning ModulesStep 2: AWS Whitepapers and WebsiteStep 3: YouTube Practice QuestionsOptional ResourcesAbout the Examination Amazon Web Services (AWS) is one of the most popular cloud platforms out there, and they offer an extensive suite of fully-featured services from data centers globally. In line with these services, AWS offers certifications to help professionals validate these in-demand cloud computing skills. A journey of a thousand miles begins with a single step, and this guide describes how to make that first step in your AWS certification journey, starting with the foundational Cloud Practitioner examination. The purpose of the AWS Certified Cloud Practitioner (CLF-C01) exam is to validate an individual‚Äôs understanding and knowledge of the AWS platform, including cloud architectural principles, cloud economics, available products and services, and other aspects such as security and value proposition. With all that in mind, let‚Äôs get started on the 3‚Äì2‚Äì1 plan for managing the exam: 3 key steps, ‚â§2 weeks, 1 exam attempt. ‚Äî ‚Äî ‚Äî ‚Äî Time needed: 6‚Äì7 days | Cost: Free ‚Äî ‚Äî ‚Äî ‚Äî The online e-learning modules provided by AWS themselves are wonderfully planned and delivered, making the concepts easy to follow and understand. The content is organized into bite-sized chunks that cover the key ideas behind cloud computing and the services offered by AWS. Best of all, it is free! I liked how they used a coffee shop as a simple analogy across all the modules, as that makes the theory much easier to digest and visualize. Go to the AWS Cloud Practitioner Essentials E-Learning page to access the e-learning content. You will need to create an individual Amazon account if you have not previously done so. The game plan for Step 1 is to follow through with each lesson in sequence, and take notes if possible. Although the video playtime is 6 hours in total, you should expect to spend at least twice that amount of time. This is to factor in repeat viewing and revision of the lessons. For people totally new to cloud computing, I would say you should allocate about 2‚Äì3 hours per day for 6 days to have a good understanding of the content. If you already have some baseline knowledge of cloud computing concepts (e.g. exposure to other services like GCP), then the time required will be shorter. Another tip is to watch the videos at a faster speed (e.g. 1.5‚Äì2x), so that the time spent can be optimized. As a token of appreciation for reading this article, I am sharing my notes which I have neatly compiled based on the lessons and transcripts. You can find the notes here on my GitHub page. ‚Äî ‚Äî ‚Äî ‚Äî Time needed: 2‚Äì3 days | Cost: Free ‚Äî ‚Äî ‚Äî ‚Äî After getting a good foundation of the main concepts of AWS services from Step 1, it is time to get exposed to more products and services that AWS has to offer. I found this to be important because the examination tends to test you on the functionality of the wide range of AWS products. You can get a good overview of the suite of services through the whitepapers provided by AWS. Here are the whitepapers which are recommended readings: The whitepapers can be rather lengthy, so I did not actually read all of it in full, as it was too tedious and time-consuming for me. I would say that the main thing is to have a quick glance and identify the services which you have not come across before in the E-Learning modules (Step 1). For the services which are unfamiliar to you, briefly read through the respective sections in the whitepapers to have a basic understanding of what they do. Feel free to read the documents in their entirety if you want to gain a strong understanding of the AWS platform, so as to further increase your confidence and chances of passing the exam at the first attempt. Another resource to get you familiar with the wide range of products is to browse the official AWS website. Just by doing that, you should be able to get a good sense of the available services. ‚Äî ‚Äî ‚Äî ‚Äî Time needed: 2‚Äì3 days | Cost: Free ‚Äî ‚Äî ‚Äî ‚Äî It‚Äôs now time to put your knowledge to the test. There are several resources that offer practice questions at a fee, but I avoided them because YouTube already has plenty of such practice questions available for free. These are the 4 videos I watched as part of my preparation (watched at 2x speed): Make sure you attempt the questions before viewing the answers. If you browse further on YouTube, you will surely come across even more of such videos, so feel free to watch them too. In addition, do check out the official sample questions from AWS here. If you would like a guided online course to get you completely ready in a systematic manner, you can check out some of the paid courses on Udemy. I did not sign up for those, but I had a look on Udemy and felt that these courses appear well-structured, and comes with additional practice questions. Going through these courses will likely make the preparation process slightly longer than 2 weeks, but hey, it‚Äôs not a race, so take more time if you need to. There are already many resources that describe the examination process, so I will not delve into that. You can check out the following to find out more: I will instead share some tips based on my personal experience. In this guide, I shared 3 steps on how to prepare for the AWS Certified Cloud Practitioner exam. As everyone has different learning preferences, the recommendations here are neither definitive nor exhaustive. Nonetheless, it should still provide you with solid preparation to take the exam and pass it on your first attempt. With just the resources listed in this guide (i.e. Steps 1‚Äì3), I was able to score 920/1000 (passing grade is 700/1000) on my first attempt. I am very confident you can achieve that too. Once again, do help yourself with the notes I have compiled from the AWS lessons and transcripts. I welcome you to join me on a data science learning journey! Follow this Medium page and check out my GitHub to stay in the loop of more exciting data science content. Meanwhile, wishing you the best of luck in the examination! towardsdatascience.com towardsdatascience.com towardsdatascience.com",621,6,6,Towards Data Science,2021-01-02,2021
https://towardsdatascience.com/how-to-deploy-your-fastapi-app-on-heroku-for-free-8d4271a4ab9,How to Deploy a FastAPI App on Heroku for¬†Free,How to keep your app active¬†24/7,16,40,"['How to Deploy a FastAPI App on Heroku for Free', 'Introduction', 'Git', 'Setting up Your Heroku Account', 'How to Create the requirement.txt for FastAPI on Heroku', 'runtime.txt', 'Procfile', 'Environment Variables', 'How to Upload to Heroku', 'How to Add a Subdomain', 'How to Keep Your App Active 24/7', 'How to fix a failed cron-job', 'How to Rename Your App', 'How to ssh to Heroku', 'Conclusion', 'References']","Note: The Heroku free versions are no longer available. Please read From Heroku to Deta to Host Python Project for Free for hosting FastAPI free. Heroku is a cloud Platform As A Service (PaaS) supporting Ruby on Rails, Java, Node.js, Scala, Clojure, Python, PHP, and Go. Heroku‚Äôs free version offers 550‚Äì1000 dyno hours per month. Your apps sleep after 30 mins if no one is using your app. Your apps will be always free if your monthly dyno hours is within the limit. The Dynos are isolated, virtualized Linux containers on the Heroku platform. In this article, you will learn how easy it is to set up and run your FastAPI project and making it active 24/7 on Heroku. I‚Äôm currently hosting this FastAPI website at Heroku as a demo. I use Gitstart to automate the Git initialization. From your terminal, it runs git init. Add .gitignore and README file and commit with a message. This creates a new repo in Github, and it will push all the files. Personal accounts are given a base of 550 free dyno hours each month. In addition to these base hours, accounts that verify with a credit card [1] will receive an additional 450 hours added to the monthly free dyno quota. [2] You need to add SSH Keys to Heroku.[3]Install the Heroku CLI [4]Then log in to Heroku from your terminal and create a Heroku app: You can add a custom domain to your Heroku app.[5] Heroku requires the requirement.txt to install your Python packages. pipreqs [6] generates it based on imports in your project. Since pipreqs generates it based on import, you need to add the following to the requirements.txt manually. You need the uvicorn to start an ASGI server on Heroku. You need the aiofiles for your static files. If your project has a form, add the python-multipart. If your project uses a template engine, add the jinja2. Heroku uses the default Python version. In order to use a certain Python version, add runtime.txt with your Python version. For example: Heroku requires the Procfile.[7] If you have the main.py in the app directory, add the following: If you have the main.py in the root directory: Heroku doesn‚Äôt use the .env file. Instead, add values in your .env file to Config Vars. You can find it under the Settings tab. Now you are ready to push your project to Heroku: The heroku open command opens your project in your browser. I am using Cloudflare and this is what I had to do. Select CNAME for the type. Add your subdomain name for the Name. Add your Heroku domain for the Content. Your Heroku domain name is something like, serene-example-4269.herokuapp.com. The image below shows how to add toolbox.mywebsite.com subdomain. After 30min of inactivity, the Heroku app goes idle. You can find your app process using heroic ps. You can find the ‚Äúidle‚Äù in the last line. This is my other app process status showing ‚Äúup‚Äù. cron-job.org [8] and Kaffeine [9] provide a free service. Both visit your URL at a scheduled time. You can create a cronjob by adding your URL and Execution schedule. For your cronjob, you need to select Every 15 minutes. If you set it Every 30 minutes, it may delay 5 to 15 seconds. You can edit or delete your cronjob at ease. Your cron-job may fail telling ‚ÄúResponse data too big‚Äù or ‚ÄúOutput too large‚Äù. The cron-job.org accepts up to 4 kB of response data from your URL/script. To fix it, you can create an endpoint that returns a JSON string rather than an HTML page. Kaffeine pings your Heroku app every 30 minutes so it will never go to sleep. How to remove your app from Kaffeine The link in the website doesn‚Äôt work but if you click the link below it worked for me. http://kaffeine.herokuapp.com/#decaf When you rename your app, don‚Äôt forget to update your cron job. You may want to ssh to Heroku. heroku ps:exec does that. Heroku provides max of 1000 hrs of free hosting for your project which uses under 512MB of RAM usage. You can use your domain name. If you require SSL on custom domains, you need to upgrade to the Hobby tier. cron-job.org and Kaffeine provide a cronjob to visit your URL to avoid sleeping. Get full access to every story on Medium by becoming a member.",423,3,6,Towards Data Science,2021-01-02,2021
https://towardsdatascience.com/pipenv-vs-conda-for-data-scientists-b9a372faf9d9,Pipenv vs Conda (for Data Scientists),"This article compares pipenv and conda as of Jan 2021 using a set of criteria, some of which are most relevant to data scientist",13,68,"['Pipenv vs Conda (for Data Scientists)', 'Introduction', 'Package Availability', 'Dependency resolution', 'Python version', 'Dependency Specification', 'Disk Space', 'Security', 'Longevity', 'Customisation', 'Packaging', 'Miscellaneous', 'Useful Resources']","Python has many tools available for distributing code to developers and does not adhere to ‚ÄúThere should be one ‚Äî and preferably only one ‚Äî obvious way to do it‚Äù. For example Conda+Anaconda is recommended by scipy.org which manages the ubiquitous scipy stack, whilst pipenv+PyPI is recommended by PyPA, the python packaging authority. Which could leave data scientists in a bit of a quandary. This article compares pipenv and conda as of Jan 2021 using the following set of criteria, some of which are more relevant to data scientists: Package Availability Dependency resolutionPython versionDependency SpecificationDisk SpaceSecurityLongevityCustomisationMiscellaneous The article does not recommend one tool over another but should help the reader make a decision based on their needs. The article assumes the reader is already familiar with the python packaging ecosystem, pipenv and conda. For those less familiar, I have also included a list of useful resources at the end of the article. Are packages available in the appropriate format? As stated by Anaconda, ‚Äúover 1500 packages are available in the Anaconda repository, including the most popular data science, machine learning, and AI frameworks. These, along with thousands of additional packages available on Anaconda cloud from channeling including conda-forge and bioconda, can be installed using conda. Despite this large collection of packages, it is still small compared to the 150,000 packages available on PyPI‚Äù. On the other hand, not all packages in PyPI are available as wheels, which is especially problematic for data science libraries which usually require C/C++/Fortran code. Whilst it is possible to install PyPI packages using pip in conda environments, this requires all the sub-dependencies to be pip packages themselves too and can cause headaches so is not recommended. There is usually a delay between packages being available in Anaconda main channel compared to PyPI. For example the delay for pandas seems to be a few weeks. I wanted to check if pipenv+PyPI and conda+Anaconda could provision a data scientist‚Äôs basic tool set: pandas, scikit-learn, sqlalchemy, jupyter, matplotlib and networkx. I used python3.8 because 3.9 came out just recently. Both environments were successfully created in about 3 minutes. Note that I am using Ubuntu WSL1, different platforms might not be as successful in creating the environments. Resolving direct and indirect dependencies Conda To test this criteria I used pandas which has a dependency on numpy. I first attempted to install numpy1.15.3 and pandas using conda, so that the environment has a direct dependency on pandas and numpy and indirect dependency on numpy: Conda is successful at creating an environment and installs pandas1.0.5 which is the last pandas version to support numpy1.15.3. If the package version of an existing environment requires upgrading or downgrading: Conda will ask you before updating the environment: The following packages will be DOWNGRADED: numpy 1.19.2-py37h54aff64_0 ‚Üí 1.15.3-py37h99e49ec_0 numpy-base 1.19.2-py37hfa32c7d_0 ‚Üí 1.15.3-py37h2f8d375_0 pandas 1.2.0-py37ha9443f7_0 ‚Üí 1.0.5-py37h0573a6f_0 Proceed ([y]/n)? Note that it is recommended to specify all packages at the same time to help Conda resolve dependencies. Pipenv I then attempted to install the same packages with pipenv: Pipenv creates an environment using numpy1.19.1, which does not meet my specification. Pipenv determines that there are conflicts, is unable to create a Pipfile.lock and prints the following useful message: ‚úò Locking Failed!There are incompatible versions in the resolved dependencies: numpy==1.15.3 (from -r /tmp/pipenvzq7o52yjrequirements/pipenv-5bf3v15e-constraints.txt (line 3)) numpy>=1.16.5 (from pandas==1.2.0->-r /tmp/pipenvzq7o52yjrequirements/pipenv-5bf3v15e-constraints.txt (line 2)) Pipenv also has the graph and graph-reverse commands which prints the dependency graph and allows users to trace how package depend on each other and helps resolve conflicts. pandas==1.2.0 -numpy [required: >=1.16.5, installed: 1.19.5] -python-dateutil [required: >=2.7.3, installed: 2.8.1] ‚Äî six [required: >=1.5, installed: 1.15.0] -pytz [required: >=2017.3, installed: 2020.5] Note that the pip dependency resolver is going through changes. I used the latest version (20.3.1) but the outcome might vary depending on the pip version. Managing different python versions Conda Conda will treat the python distribution like a package and automatically install any python version that you have directly specified. Moreover when creating a new environment, conda will determine the best python version (if not specified). For example: creates an environment with python3.8.5 and pandas1.1.5 but creates an environment with python3.7.9 which is the last python version to support pandas0.25.0. The install will fail if it requires upgrading/downgrading the python version of an existing environment: but the error message is very helpful: UnsatisfiableError: The following specifications were foundto be incompatible with the existing python installation in your environment:Specifications:- pandas==0.25.0 -> python[version=‚Äô>=3.6,< 3.7.0a0|>=3.7,< 3.8.0a0'] Pipenv Pipenv does not natively install different python versions. It will use the system python (usually stored in /usr/lib) or the base python (usually stored in ~/miniconda3/bin if miniconda is installed) to create new environments. However pipenv can use pyenv to install other python versions if pyenv is installed. You can use pyenv to pre-install python versions, or pipenv will ask you to install a python version if it‚Äôs not already available locally:https://towardsdatascience.com/python-environment-101-1d68bda3094d Unfortunately pipenv+pyenv cannot resolve the best python version, even when creating a environment from scratch. For example: creates an environment with python3.8.5 and pandas1.2.0. Attempting to install pandas0.25.0 where the default pyenv python version is 3.8 stalls: Note that the stalling is probably due to how the requirements for pandas0.25.0 were configured. pip relies on the python_requires attribute to determine if the python version is suitable, which is a recent addition. Attempting to install more recent packages where the python_requires attribute is not met usually fails with a ‚Äúdistribution not found‚Äù error. Note that pipenv will also attempt to install the latest version of a package if unspecified, regardless of the python version. For example attempting to pandas in a python3.5 environment: will fail with the following error message: [pipenv.exceptions.InstallError]: ERROR: Could not find a version that satisfies the requirement pandas==1.1.5[pipenv.exceptions.InstallError]: ERROR: No matching distribution found for pandas==1.1.5 This message is not very helpful and has been raised as an issue with pip. Ensuring a reproducible build that is upgradable Pipenv uses two files to specify dependencies: Pipfile for direct dependencies and Pipfile.lock for both direct and indirect dependencies. Creating an an environment using the Pipfile.lock ensures that exactly the same packages will be installed, including the hash of the package. Creating an environement using the Pipfile gives it the flexibility to upgrade indirect dependencies if required. Pipenv hopes that the Pipfiles will replace requirements.txt in the future (see https://github.com/pypa/pipfile). Conda uses an environment.yaml file to specify both direct and indirect dependencies. Users have to use trial and error when updating their environments. There is a conda-lock library which replicates the Pipfile.lock ability but it is not currently supported by Anaconda. How much space do environments take up? Can sharing help? Python environments used by data scientists tend be large, especially conda environments. For example a conda environment with jupyter and pandas takes up 1.7GB, whilst an equivalent pipenv environment takes up 208MB. Whilst not relevant to most development environments, this may become more important in production, for example when using containers:https://towardsdatascience.com/how-to-shrink-numpy-scipy-pandas-and-matplotlib-for-your-data-product-4ec8d7e86ee4 Because of their large size, data scientists often use a conda environment across multiple exploratory projects, or even across multiple production projects which are part of the same solution:https://stackoverflow.com/questions/55892572/keeping-the-same-shared-virtualenvs-when-switching-from-pyenv-virtualenv-to-pipThe conda environment can be created, activated and used from any location. A pipenv environment is tied to a project repository. Once created, Pipenv saves the pipfiles to the root of the repository. The installed packages are saved to ~/.local/share/.virtualenvs / by default, where pipenv ensures that one environment is created per repo by creating a new directory and appending a hash of the path to the name (i.e. my_project-a3de50). The user must cd to the root of the project repository to activate the environment, but the shell will remain activated even if you leave the directory. It is possible to share an environment across multiple projects by storing the Pipfiles in a separate directory. The user must then remember to cd to the repository to activate and update the environment. How safe are packages to install? The Anaconda main channel https://anaconda.org/anaconda/ is maintained by Anaconda employees and packages go through a strict security check before uploading. In the case of pipenv which uses PyPI, anyone can upload any package and nefarious packages have been found in the past (see https://www.zdnet.com/article/twelve-malicious-python-libraries-found-and-removed-from-pypi/). The same goes with conda-forge although they are developing a process to validate artifacts before they are uploaded to the repository. Work-arounds include: Is conda/pipenv here to stay? How mature is it? Who supports it? Pipenv was first introduced in 2017 by the creator of the popular requests library. Pipenv did not release any new code between Nov 2018-May 2020 which raised some concern about its future:https://medium.com/telnyx-engineering/rip-pipenv-tried-too-hard-do-what-you-need-with-pip-tools-d500edc161d4https://chriswarrick.com/blog/2018/07/17/pipenv-promises-a-lot-delivers-very-little/Pipenv has now been picked up by new developers and is being updated more regularly with monthly releases since May 2020. Conda/Anaconda was created in 2012 by the same team behind scipy.org which manages the scipy stack. Conda is an open source tool but the anaconda repository is hosted by Anaconda Inc., a for-profit organisation. Whilst this means conda/anaconda is unlikely to disappear anytime soon, this has raised concern that Anaconda Inc. might start charging users. They have recently changed their terms of conditions to charge heavy or commercial users which includes mirroring the anaconda repository. Note that the new terms of condition does not apply to the conda-forge channel. What advantages does a custom package manager bring? Conda/Anaconda was created by the python scientific community to solve problems specific to their community, such as non-python dependencies:http://technicaldiscovery.blogspot.com/2013/12/why-i-promote-conda.htmlThis gives it the flexibility and impetus to create products geared for Data Scientists. Conda can distribute non-Python build requirements, such as gcc, which greatly streamlines the process of building other packages on top of the pre-compiled binaries it distributes. Conda can also install R packages. Anaconda developed MKL-powered binary versions of some of the most popular numerical/scientific Python libraries. These have been shown to lead to significant improvements in performance. Whilst MKL optimizations are no longer in production, Anaconda could still develop tools that are only compatible with a conda environment. How is code packaged up? Both conda and pipenv rely on additional tools for packaging code. Both also rely on following ‚Äúrecipes‚Äù depending on whether the code contains non-python code and the target platform. Conda-build is used to create conda packages:https://docs.conda.io/projects/conda-build/en/latest/ PyPA recommends using setuptools to build wheels that can be installed using pipenv. Below is a great overview:https://realpython.com/python-wheels/ Note that python packaging is expected to change a lot in the future with the introduction of pyproject.toml file and PEP518:https://grassfedcode.medium.com/pep-517-and-518-in-plain-english-47208ca8b7a6 Any other factors to consider? A review of the python packaging ecosystemhttps://packaging.python.org/overview/https://towardsdatascience.com/packaging-in-python-tools-and-formats-743ead5f39ee A guide to pipenv https://realpython.com/pipenv-guide/ A guide to conda/Anaconda for data scientists(Whist geared for Windows the theory is relevant to any OS)https://realpython.com/python-windows-machine-learning-setup/ A comparison of conda and piphttps://jakevdp.github.io/blog/2016/08/25/conda-myths-and-misconceptions/https://www.anaconda.com/blog/understanding-conda-and-pip Ensuring a reproducible build, and still be able to quickly change your dependencieshttps://pythonspeed.com/articles/conda-dependency-management/ Options for packaging your Python codehttps://pythonspeed.com/articles/distributing-software/",131,2,9,Towards Data Science,2021-01-02,2021
https://towardsdatascience.com/computational-graphs-in-pytorch-and-tensorflow-c25cc40bdcd1,Computational graphs in PyTorch and TensorFlow,,6,55,"['Computational graphs in PyTorch and TensorFlow', 'Computational Graphs Types[1]', 'Computational Graphs in PyTorch [7]', 'Computational Graphs in TensorFlow', 'Conclusion', 'References']","I had explained about the back-propagation algorithm in Deep Learning context in my earlier article. This is a continuation of that, I recommend you read that article to ensure that you get the maximum benefit from this one. I‚Äôll cover computational graphs in PyTorch and TensorFlow. This is the magic that allows these frameworks to calculate gradients for your neural networks. I‚Äôll start with some introduction to types of computational graphs followed by framework specific details. All the deep learning frameworks rely on the creation of computation graphs for the calculation of gradient values required for the gradient descent optimization. Generally, you have to build the forward propagation graph and the framework takes care of the backward differentiation for you. But before starting with computational graphs in PyTorch, I want to discuss static and dynamic computational graphs. These typically involve two phases as follows. One of the advantages of static graphs is that it allows for powerful offline optimization/scheduling of graphs. This implies that these would be generally faster (the difference may not be significant in every use case and depends on our graph) than dynamic graphs in general. The disadvantage is that handling structured and even variable-sized data is ugly. The graph is defined implicitly (e.g., using operator overloading) as the forward computation is executed. Dynamic graphs have the advantage of being more flexible. The library is less invasive and allows for interleaved construction and evaluation of the graph. The forward computation is written in your favourite programming language with all its features and algorithms. The downside is that there is little time for graph optimization and if the graph does not change, the effort can be wasted. Dynamic graphs are debug friendly. Finding problems in you code is much easier, because it allows line by line execution of the code and you will have access to all the variables. This is definitely a very important feature if you want to use Deep Learning for any real purpose in the industry. PyTorch uses dynamic computational graphs. Tensorflow allows the creation of optimized static graphs and also has eager execution which allows for something similar to dynamic graphs. It is an imperative programming environment that evaluates operations immediately, without building graphs, operations return concrete values instead of constructing a computational graph to run later. Now let‚Äôs look at computational graphs in PyTorch. At its core PyTorch provides two features: Deep learning architectures and their training involve a lot of matrix operations. A Tensor is nothing but an n-dimensional array. For people coming from a Python background, NumPy should ring a bell. It is an extremely powerful and optimized library for matrix operations. However, for deep learning purposes, the matrices are huge and require enormous computational power. A PyTorch Tensor it nothing but an n-dimensional array. The framework provides a lot of functions for operating on these Tensors. But to accelerate the numerical computations for Tensors, PyTorch allows the utilization of GPUs, which can provide speedups of 50x or greater. PyTorch Tensors can also keep track of a computational graph and gradients. In PyTorch, the autograd package provides automatic differentiation to automate the computation of the backward passes in neural networks. The forward pass of your network defines the computational graph; nodes in the graph are Tensors and edges are functions that produced the output Tensors from input Tensors. Back-propagation through this graph then gives the gradients. Every Tensor in PyTorch has a flag: required_grad that allows for fine-grained exclusion of subgraphs from gradient computation and can increase efficiency. If x is a Tensor that has x.requires_grad=True then x.grad is another Tensor holding the gradient of x with respect to some scalar value. As seen from the above example, if there is a single input to an operation that requires gradient, its output will also require gradient. Conversely, only if all inputs don‚Äôt require gradient, the output also won‚Äôt require it. Conceptually, autograd keeps a graph recording of all of the operations that created the data as you execute operations, giving you a directed acyclic graph whose leaves are the input tensors and roots are the output tensors. By tracing this graph from roots to leaves, you can automatically compute the gradients using the chain rule (back-propagation). Internally, autograd represents this graph as a graph of Function objects, which can be apply()-ed to compute the result of evaluating the graph. When computing the forward pass, autograd simultaneously performs the requested computations and builds up a graph representing the function that computes the gradient (the .grad_fn attribute of each torch.Tensor is an entry point into this graph). When the forward pass completed, the graph is evaluated in the backwards pass to compute the gradients. As discussed earlier the computational graphs in PyTorch are dynamic and thus are recreated from scratch at every iteration, and this is exactly what allows for using arbitrary Python control flow statements that can change the overall shape and size of the graph at every iteration. You don‚Äôt have to encode all possible paths before you launch the training ‚Äî what you run is what you differentiate. Every primitive autograd operator is two functions that operate on Tensors. The forward function computes output Tensors from input Tensors. The backward function receives the gradient of the output Tensors with respect to some scalar and computes the gradient of the input Tensors with respect to that same scalar. To summarize, Tensor and Function are interconnected and build up an acyclic graph, that encodes a complete history of the computation. Each tensor has a .grad_fn attribute that references a Function that has created the Tensor (except for Tensors created by the user since their grad_fn is None). If you want to compute the derivatives, you can call .backward() on a Tensor. After the call to the backwards function the gradient values are stored as tensors in grad attribute. These concepts can be represented as following diagram. So for example if you create two Tensors a and b. Followed by c = a/b. The grad_fn of c would be DivBackward which is the backward function for the / operator. And as discussed earlier a collection of these grad_fn makes the backward graph. The forward and backward function are a member of torch.autograd.Function. You can define your own autograd operator by defining a subclass of torch.autograd.Function. is_leaf: All Tensors that have requires_grad which is False are leaf Tensors by convention. For Tensors that have requires_grad with is True, they will be leaf Tensors if they were created by the user. This means that they are not the result of an operation and so grad_fn is None. Only leaf Tensors have their grad populated during a call to backward(). To get grad populated for non-leaf Tensors, you can use retain_grad(). Let us construct the computational graph example used in part 1 of the post and use PyTorch to compute the gradients. The above code constructs the computational graph in PyTorch. Let us look at a few properties of the nodes in the graph. The leaves don‚Äôt have grad_fn but will have gradients. Non leaf nodes have grad_fn but don‚Äôt have gradients. Before the backward() is called there are no grad values. The gradients that we calculated theoretically in the previous post are calculated using PyTorch and shown below. The properties of nodes after backward() call are shown below. As you can see once the graph is built, calculating the gradients in PyTorch is a piece of case. It takes care of the differentiation for you. The jupyter notebook for this tutorial can be found at : https://github.com/msminhas93/ComputationalGraphs This completes the discussion of computational graphs in PyTorch. In the next section let us look at computational graphs in TensorFlow. Tensorflow uses dataflow graph to represent computation in terms of the dependencies between individual operations. This leads to a low-level programming model in which one defines the dataflow graph, then creates a TensorFlow session to run parts of the graph across a set of local and remote devices. An example of a dataflow or computational graph in TensorFlow is shown below. In Tensorflow, any kind of computation is represented as an instance of tf.Graph object. These objects consist of a set of instances of tf.Tensor objects and tf.Operation objects. In Tensorflow, the tf.Tensor objects serve as edges while the tf.Operation serves as nodes which are then added to the tf.Graph instance. In TensorFlow, a tf.Session() object stores the context under which a computation is performed. It is a class for running TensorFlow operations. A Session object encapsulates the environment in which Operation objects are executed, and Tensor objects are evaluated. Now, let us construct the computational graph in Tensorflow using the example used in Part 1 of the post. We start by creating four placeholders. A TensorFlow placeholder is a proxy for a tensor which is fed during session execution. It requires the feed_dict argument to Session.run(), Tensor.eval(), or Operation.run(). Next, we use the TensorFlow operations namely add, log and multiply to construct the example computational graph from the defined placeholders. Once the graph is constructed, the next step is to run it in a Session. Python has a with statement which takes care of opening and closing the Session. In the session scope, we run the tf.gradients function to obtain the required gradients for our example. The output is shown below. TensorFlow has a utility called tensorboard gives you a pictorial representation of the computational graphs with a lot of visualization functionalities. The graph for the previous example is shown below. As can be seen, the graph is the same as the one we constructed in the example picture. The Jupyter Notebook can be found at: https://github.com/msminhas93/ComputationalGraphs Next, let us now look at the timing comparison between the static graphs and the eager execution. We can clearly see the performance difference here. The static graph was faster than the dynamic graph for this example. With this, we reach the end of the ‚ÄúBack-propagation Demystified‚Äù series. The key takeaways are as follows. Finally, here is a comparison of how computational graphs are represented in PyTorch and TensorFlow. I hoped you gained some knowledge by reading this and enjoyed the article. Would love to connect on LinkedIn. [1]http://www.cs.cornell.edu/courses/cs5740/2017sp/lectures/04-nn-compgraph.pdf [2]https://pytorch.org/tutorials/beginner/examples_autograd/tf_two_layer_net.html [3]https://www.tensorflow.org/api_docs/python/tf/Graph [4]https://www.tensorflow.org/guide/intro_to_graphs [5]https://kth.instructure.com/files/1864796/download?download_frd=1 [6]https://jdhao.github.io/2017/11/12/pytorch-computation-graph/ [7]https://towardsdatascience.com/getting-started-with-pytorch-part-1-understanding-how-automatic-differentiation-works-5008282073ec [8]https://pytorch.org/docs/stable/autograd.html",,0,9,Towards Data Science,2021-01-02,2021
https://towardsdatascience.com/making-publication-quality-figures-in-python-part-i-fig-and-axes-d86c3903ad9b,Making publication-quality figures in Python (Part I): Fig and¬†Axes,,7,43,"['Making publication-quality figures in Python (Part I): Fig and Axes', 'Understanding matplotlib global parameters', 'Your canvas (Fig object)', 'Manipulating Ax object', 'Inspecting Ax object', 'Multiple figures on one canvas', 'Continuing Reading']","In this whole series, I will share with you how I usually make publication-quality figures in Python. I want to really convey the ideas of how to gain full control of every element in a python plot. By the end of this tutorial, you will be able to fully understand the philosophy of making figures in Python. Since it will be a huge topic, I decide to break it down to several parts. In this Part I tutorial, I will focus on the very first step ‚Äî understanding the canvas (Fig object) you will be drawing on and the boundary of each figures (Ax object). And in the following tutorials, I will walk you through of how to actually make all kinds of figures you‚Äôve been familiar with (i.e. scatter plot, boxplot, heatmap, etc). All the code and additional information are available at: https://github.com/frankligy/python_visualization_tutorial Let‚Äôs get started! As a side note, there are several python plotting packages, like matplotlib, seaborn, plotly, etc. I would like to demonstrate all the techniques in matplotlib because it is the most low-level library and learning matplotlib can give you a full understanding of all the nitty-gritties. We will touch on seaborn at the end of the tutorial where you can see the benefit of this high-level library. However, there won‚Äôt be shortcut of learning anything, if you want to make publication-quality figures and have the full control of the figures we are making, matplotlib is the best choice as far as I can suggest. First, let‚Äôs load necessary packages, don‚Äôt be scared, I often feel overwhelmed seeing people load ten different packages and I have no idea what they are. You basically load matplotlib package, the weapon we are going to use. Then a sub package pyplot in maplotlib package, this is just for convenience. Finally, we need to use some useful functions from another sub-package ticker, which will be used for us to customize the ticks of the axes. So you see, you really just use matplotlib, nothing complicated! Before we start to draw, I‚Äôd like to introduce some global setting of matplotlib, it a lot of default setting of your figure elements, like font type, font size, tick pads etc. Hence it is great to get a sense of that, again, the goal of this article is to show you how to fully understand every detail. You can access the global parameters as below: As you can see, there are 305 global parameters, let‚Äôs have a peek of them: The full list is available at my github page: https://github.com/frankligy/python_visualization_tutorial/blob/main/rcParams.txt You can tweak any parameters any time by simply change the values of this huge python dictionary, if there are some parameters you want all your following figures to stick with. I found it useful to change this setting like below, the reason for that is when we make publication-quality figure, we sometime will use Adobe Illustrator to do the final formatting, the default font type is not recognizable in Adobe Illustrator, so I would change them to NO. 42. Also, in academic, we prefer using ‚ÄúArial‚Äù font, so I changed it as well. Let‚Äôs create a canvas now: The default figsize is (6.4,4.8) in the unit of inch, remember 6.4 inch is the width and 4.8 inch will be the height. It again can be accessed by: Let‚Äôs see the effect of different figsize, I use Pycharm as my python programming IDE, and it will immediately pop up a canvas window so you can clearly see the differences: Ok, so this is the canvas you are going to draw figures on. Let‚Äôs start to draw a figures on the canvas, each individual figure will be an Ax object, we can create an ax by specifying (left,bottom,width,height) of the figure, it will be clear when you see the result, Let‚Äôs see the values of this tuple, (0.1,0.1) means the left bottom corner of this figure sits on this coordinates (0.1,0.1) of the whole canvas, the whole canvas will be in range (0,1). Then (0.5,0.8) determine the width will be 0.5, and the height will be 0.8. Done! First, I want to get rid of the spines of this figure, I locate ax.spines object and set them invisible. You see, they are all gone, and now you can clearly see, a figure is made up of four spines and ticks and labels. This is the message I want to convey here. Next, we want to play with the x-axis ticks and tick labels, I use tick_params function to set all the parameters to whatever I‚Äôd like to. What happened here? First, I increased the length of each x tick, and the width as well, then I changed the color of the x ticks to red. I increased the pad/gap between x ticks to their corresponding labels, then I changed the label size and label color, finally I rotated the label counter-clockwise by 15 degree. I left Y-axis untouched so you can clear see what I‚Äôve tweaked. Again, I hope you could get full control of every element in a python figure. In addition to change the format, you can change the tick and tick label themself as well, we use set_xticks() function and set_xticklabels function to achieve that, see the effect below: Next, let‚Äôs briefly cover a little advanced topic, what we‚Äôve tried above are all major ticks, what if you want to add minor tick between each major tick interval? We will use set_minor_locator() function, along with the Locator() object we imported from ticker sub package at the beginning. Basically, Locator() determine the location of ticks and Formatter() determine the format of the ticks. Here we demonstrate that in Y-axis instead. MultipleLocator is able to create ticks at the positions that are multiple of certain base, in the above example, every 0.1 unit interval will have a minor tick and every 0.5 unit interval will have a major tick. You can also adjust minor tick parameters using tick_params() function as well, just specify the ‚Äúwhich‚Äù argument to ‚Äúminor‚Äù. There are a bunch of additional classes of Locator() and Formatter(), I would like to refer you to a very well-explained blog: https://jakevdp.github.io/PythonDataScienceHandbook/04.10-customizing-ticks.html Finally, I want to show you one more thing of manipulating Ax object, that is adding grid lines, sometimes this will make our figures look much better. We use grid() function and specify we want grid line (True) based on major ticks on both axis, the opacity will be 0.3. Here I want to introduce a class of very important and useful methods, the figure (or Ax object) we created is basically a python object, we can access all its attributes by definition. Sometimes, we will need those information to get full control of our figure generating process. You can achieve that using get method, you can basically get every element we just played with. For instance, I want to get the x axis ticks and tick labels. We used get_xticks() and get_xticklabels() function and stored the returned objects in two variables c and d. Now we have a look at c and d. Makes sense, right? I hope till now you feel more comfortable with python Fig and Ax objects. I actually didn‚Äôt show you how to make pretty figures, but I gave you the toolkit you need to make whatever figure you‚Äôd like to. We frequently encounter the situation where we want to make multiple figures on one canvas. One way is just to manually create Ax1 object by specifying its position tuple like we did above, then do the same thing for Ax2 object, again and again. Sometimes it‚Äôs a good way to do so. But what if you want to generate, let‚Äôs say 63 figures in a 9 x 7 grid, how can we achieve that? We can use plt.subplots() function, Here I build a 4 x 4 grid, also specify the figsize as we did above. Let‚Äôs see the effect: You see what‚Äôs wrong with this canvas? There are a lot of overlapping between each figure, you can remedy that by either increase the figsize, or decrease the label font size, but I provide a better way to do so. I basically tell python to leave some space between two figures, wspace is the horizontal gap and hspace is the vertical gap. Let‚Äôs see the effect: All right, that‚Äôs all that I have for now. I hope now you have a better sense of using python to design and create your own figures. I will probably add new stuff to this story if I find tips and tricks that are interesting to share in the future. Also, stay tuned for my following tutorials, we will delve into how to make each kind of figures. Don‚Äôt hesitate to ask me questions. All the codes are here: https://github.com/frankligy/python_visualization_tutorial If you like these tutorials, follow me on medium, thank you so much for your support. Connect me on my Twitter or LinkedIn, also please ask me questions about which kind of figure you‚Äôd like to learn how to draw in a succinct fashion, I will respond! Tutorial II: line plots, legends and colors Tutorial III: box plot, bar plot, scatter plot, histogram, heatmap, colormap Tutorial IV: violin plot, dendrogram Tutorial V: Plots in Seaborn (cluster heatmap, pair plot, dist plot, etc)",,0,9,Towards Data Science,2021-01-02,2021
https://towardsdatascience.com/frequentist-and-bayesian-inference-83af2595f172,Frequentist and Bayesian Inference,The two branches of inferential statistics,1,45,['Frequentist and Bayesian Inference'],"Statistics is used for different purposes in different contexts. For example, descriptive statistics is the branch that deals with summarizing an existing sample of data. Descriptive statistics deals with the sample itself ‚Äî there‚Äôs no need for any extrapolation. However, a more challenging problem that we have to address regularly in the real world is using the sample to infer something about the population from which the sample was drawn. For example, let‚Äôs say you rolled a die 10 times, and saw these results: 4 5 2 6 1 5 6 5 3 1 Descriptive statistics can tell you that having been rolled three times, 5 is the ‚Äúmode‚Äù of your sample. But this may not be enough. We may want to ask the question ‚ÄúIs this a fair die?‚Äù and try to answer this question based on the sample that we saw by rolling the die a limited number of times. It‚Äôs the job of inferential statistics to answer these kinds of questions. However, there are two rival branches of inferential statistics that try to answer these questions using different assumptions, theoretical foundations, and to some extent philosophical beliefs: Frequentist statistics and Bayesian statistics. This post aims to explain the two branches and what the primary source of the debate between the two is. Frequentist statistics was developed mainly in the 20th century, and grew to be the dominant statistical paradigm of the time. It continues to be the more popular method used in scientific literature to this day ‚Äî concepts which I described in my previous post like p-values and confidence intervals belong to the frequentist paradigm. At its core, frequentist statistics is about repeatability and gathering more data. The frequentist interpretation of probability is the long-run frequency of repeatable experiments. For example, saying that the probability of a coin landing heads being 0.5 means that if we were to flip the coin enough times, we would see heads 50% of the time. With this definition, you can‚Äôt really define a probability for events that aren‚Äôt repeatable. For instance, before the 2018 FIFA world cup, let‚Äôs say that you read somewhere that Brazil‚Äôs probability of winning the tournament was 14%. This probability cannot really be explained with a purely frequentist interpretation, because the experiment is not repeatable! The 2018 world cup is only going to happen once, and it‚Äôs impossible to repeat the exact experiment with the same exact conditions, players and preceding events. With frequentist inference, any uncertainty in probabilistic estimates that we derive are considered to be due to sampling error alone ‚Äî the differences between the actual population and the sample you ended up drawing. So for frequentists, having large samples solves most problems since your sample becomes closer to the true population distribution. It‚Äôs important to note that frequentists don‚Äôt believe that there‚Äôs any underlying probability distribution for parameters that are being estimated. For example, if you try to estimate the heads probability of a coin, that parameter is a fixed constant that we don‚Äôt know, but it is not inherently probabilistic. We just can‚Äôt be sure about its value since we‚Äôre dealing with a sample instead of the population itself. Confidence intervals are a frequentist concept and may seem to be a contradiction to this idea. For example, we may say the heads probability of a coin p(H) is bounded by some specific values with a 95% probability: L < p(H) < U, where L and U depend on the sample Isn‚Äôt the p(H) probabilistic in this case? It actually isn‚Äôt ‚Äî p(H) is fixed and unknown, and the random variables that are probabilistic are actually the bounds, L and U. L and U are computed based on the sample you drew, so this just says that if you keep drawing samples and keep computing L and U each time, the true, fixed, unknown parameter will be within those bounds 95% of the time. It doesn‚Äôt say that p(H) itself probabilistic! Now, p-values and confidence intervals have formed the backbone of the scientific process across most disciplines for most of the last century. However, one key issue with frequentist inference is the lack of context and the complete dependence on the drawn sample. As I‚Äôve mentioned in my previous post, using p-values blindly can lead to a host of issues, not least of which is a large number of ‚Äúfalse positives‚Äù ‚Äî results that seem to be statistically significant effects, but are not in reality. But how exactly would we bring ‚Äúcontext‚Äù into the problem of inference? That‚Äôs where Bayesian statistics comes in. The name ‚ÄúBayesian inference‚Äù comes from Bayes‚Äô Theorem, which in turn is named after Thomas Bayes, an English statistician from the 18th century. In fact, the key ideas behind Bayes‚Äô theorem were also used independently by the French polymath Pierre-Simon Laplace around the same time. Bayes and Laplace were very different people ‚Äî Bayes was a Presbyterian minister and Laplace is now thought to have been agnostic or an atheist. However, they both did agree on one key point ‚Äî the world is perfect and deterministic. The best illustration for this viewpoint is the famous thought experiment known as Laplace‚Äôs demon. The idea is that if there were an all-knowing entity that knew the positions and velocities of every single particle in the universe, it would be able to know everything that happened in the past and everything that will happen in the future, just using the laws of classical mechanics (and presumably using enough computational power). This demon is an example of scientific determinism. Now, this may all seem a little at odds with the idea of probability itself. If the world is deterministic, where do the uncertain and probability estimates we come up with fit in? To Bayes and Laplace, probability was more due to our imperfect knowledge of the world than any underlying uncertainty in the world itself. So in the Bayesian world view, probability essentially represents our degree of belief in something, which is probably closer to most people‚Äôs intuitive idea of probability. So let‚Äôs get to the core of it. Let‚Äôs say you wanted to evaluate a hypothesis H based on some data D that you sampled in your experiment. The formula for Bayes‚Äô Theorem is stated as: P(H|D) = P(D|H)*P(H) / P(D) P(D) is sort of a normalizing constant, and while there are a lot of interesting ideas around calculating it, it‚Äôs not the focus of this post. So let‚Äôs look at the other terms. P(H|D) ~ P(D|H)*P(H) P(H|D) is the probability you‚Äôre interested in calculating, and is called the posterior. It is the conditional probability of the hypothesis being true, given that you saw this particular data. P(D|H) is called the likelihood, and is the probability of you drawing this data given the hypothesis is true. This term is actually the only one we would be interested in in a frequentist setting. P-values are calculated as the probability of observing data at least this extreme if the hypothesis was true, which would be calculated similarly to how P(D|H) is calculated. One important element of Bayes‚Äô Theorem which is the primary source of contention between Bayesians and Frequentists is the P(H) term ‚Äî the prior. This is the probability we assign to the hypothesis being true, before we see the data. This can be a hard concept to grasp, and for good reason. The probability of the hypothesis is exactly what we‚Äôre trying to calculate right? How can we know what it is before we see the data? This is where you bring ‚Äúcontext‚Äù formally into the problem. The prior represents a combination of your past knowledge and experiences, including any results from previous experiments you or the academic community may have run. At its core, Bayes Theorem is all about updating your beliefs based on the most recent data. After you compute the posterior P(H|D) using Bayes‚Äô Theorem, you can actually use this posterior as your prior in the next experiment! So you keep collecting new data and updating your beliefs each time, with the hope that over time and with enough data, you‚Äôll converge towards the truth. However, you can understand why the frequentists take issue with this idea. Won‚Äôt the prior just represent the experimenter‚Äôs bias? Won‚Äôt we arrive at different results depending on the prior we choose? These are all valid questions, but the prior is a key part of Bayesian inference, and picking a good prior is a whole field of study in itself. We‚Äôll look at an example of a prior in the next section. First, let‚Äôs talk about some of the drawbacks of the Frequentist approach that are addressed by Bayesian inference. One is the issue of false positives. Consider a test that‚Äôs trying to detect a rare disease within a population of 1000 people. The disease only occurs in 1% of the population, meaning 10 people. The test itself is fairly accurate ‚Äî when a person has the disease, it is guaranteed to detect it, and when the person doesn‚Äôt have the disease, it has a 99% chance of correctly identifying that they‚Äôre disease free. If you run this test on the entire population, what would happen? For the 10 people who have the disease, it would correctly identify this. For the 990, who don‚Äôt have the disease, it will correctly identify this for 99% of them, but for 1% of them (9.9, so approximately 10 people), it will misclassify them as having the disease. Now at a glance these seem to be reasonable ‚Äî the test did a pretty good job right? But consider this. If you take the test, and you get a positive result, what is the chance that you actually have the disease? 10 people got a positive result when they actually had the disease, but another 10 got a positive result without having the disease ‚Äî so there‚Äôs only about a 50% chance that a positive result actually means you have the disease! The test had 99% accuracy though, so what went wrong? This is the fundamental issue with detecting anything within a population that has a low incidence rate ‚Äî Frequentist inference is not good at accounting for this low incidence rate. In this example, you can replace ‚Äúthe population‚Äù with all possible experiments that could be run, the ‚Äúdisease‚Äù with experiments that result in an actual positive result of scientific interest, and ‚Äúthe test‚Äù with our standard p-value/statistical significance testing framework. True positive results are likely to be rare, and our statistical significance testing usually allows for a 5% false positive rate (higher than in this example). If we run a similar analysis to the previous example, we again arrive at the conclusion that perhaps a lot of published ‚Äústatistically significant‚Äù scientific research are just false positives. At the end of the day, frequentist inference for the most part just gives you a binary yes/no regarding whether your result is significant or not (although things like confidence intervals can help). If you end up getting a ‚Äúyes‚Äù, what are the chances your result is actually significant considering the rarity of positive results? Frequentist inference doesn‚Äôt try to answer that question. Now, the method by which we arrived at the 50% earlier was actually using Bayes‚Äô theorem! In other words, we calculated: P(disease|positive test) = P(positive test|disease)*P(disease)/P(positive test) The calculation earlier may have seemed very intuitive, and that‚Äôs because Bayes‚Äô theorem is just formalizing a very intuitive idea. So Bayesian inference can help to account for these false positives by taking into account the fact that the underlying incidence rate ‚Äî P(disease) here ‚Äî is low. Frequentist inference doesn‚Äôt attempt to calculate quantities like P(disease|positive test), since in the Frequentist definition of probability you either have the disease or don‚Äôt ‚Äî there‚Äôs no ‚Äúprobability‚Äù involved there. As always, one of the simplest representations of this idea is captured in an XKCD comic: This may be an extreme example, but this gets right to the core of the potential issues with frequentist inference. The chance of the sun exploding spontaneously are so low that it‚Äôs much more likely that the two die came up 6 than that the sun exploded. Since Frequentist inference doesn‚Äôt take the probability of the sun exploding into account (the only data that matters is the die roll), taking a purely Frequentist approach can run into problems like these. And while it may be easy to spot the issue here, in contexts like scientific research it can be much harder to identify that this may be happening in your experiment. So Bayesian inference can be easier to interpret and reason about, since it helps us calculate probabilities that we‚Äôre interested in (that Frequentist inference doesn‚Äôt attempt to calculate). However, it has its own drawbacks, which as mentioned earlier mostly boil down to the choice of prior. In these previous examples, either the prior was clear (the incidence rate of the disease in the population), or we knew that it was low enough to not really matter (probability of the sun exploding). But it‚Äôs not always easy to calculate this prior. For instance, in the case of academic publishing, what would be the prior probability of getting a positive result? It‚Äôs hard even to define what that quantity would mean. Sometimes when it‚Äôs hard to find a good prior, we may use what‚Äôs called an ‚Äúuninformative‚Äù prior. For example, if we‚Äôre trying to evaluate two potential hypotheses based on data but we don‚Äôt know how likely these two are prior to observing the data, we can just place a 50% probability on each hypothesis. This may seem reasonable at a glance, but despite being uninformative, this prior may actually bias your calculations in ways that you may not realize. In the disease example, if you didn‚Äôt know the underlying disease incidence rate and decided to use an uninformative prior that places equal probability on you having or not having the disease (50%) your results would be wildly off. And this, fundamentally, is the problem with Bayesian inference. The prior is supposed to codify subjective belief, but is there really a place for subjective belief in a rigorous scientific process? If we were able to use Bayesian inference to say that the probability of our hypothesis being true is 70%, and someone else conducts the same study with a different prior and arrives at a probability of 40%, how do we determine which result to believe? Who has the ‚Äúbetter‚Äù prior? These are challenging questions that do not necessarily have a good answer. Robustness of your result to the choice of prior is one important aspect of Bayesian approaches for this reason. I believe that part of the appeal of the Bayesian approach is that it‚Äôs a formal representation of a very fundamentally human thought process. The idea of having a pre-existing belief, and then updating that belief based on the new data ‚Äî there‚Äôs something very natural about it. If our prior beliefs are very strong, we may not change our views irrespective of how convincing the data is (like the sun exploding example). But if we‚Äôre on the fence, then updating our beliefs based on new data can certainly bring us closer to the truth. We may not apply Bayes‚Äô Theorem explicitly by calculating the prior and posterior terms every day, but we do use it in spirit in many aspects of our lives. This is also part of the reason its result turned out to be so intuitive in the disease example. Compare this to the p-values used in Frequentist inference ‚Äî they‚Äôre anything but intuitive, and we saw that taking a strictly Frequentist approach can lead to trouble. At the same time, bringing this notion of subjectivity into the scientific process seems a little flawed to start with, even if quantifying your subjectivity is the whole point. How can science progress if we‚Äôre bickering over who has the right prior? And at the same time, if the reproducibility crisis in science was caused due to improperly applying the ideas of Frequentist inference, switching to a different framework may not help to solve all the problems. If you can misuse Frequentist inference to arrive at a result that you want, you can certainly do the same for Bayesian inference. The two approaches may be better suited to different problems, and there are a number of hybrid techniques being considered that combine the best aspects of both frameworks. So there is no right answer, and as with most things, if there is one it‚Äôs probably somewhere in the middle. [1] Frequentist and Bayesian Approaches in Statistics [2] Comparison of frequentist and Bayesian inference [3] The Signal and the Noise [4] Bayesian vs Frequentist Approach [5] Probability concepts explained: Bayesian inference for parameter estimation.",252,4,11,Towards Data Science,2021-01-02,2021
https://towardsdatascience.com/5-tips-on-how-to-land-machine-learning-jobs-8eb5c1c3ee95,5 Tips on How to Land Machine Learning¬†Jobs,From a FAANG¬†MLE,8,28,"['5 Tips on How to Land Machine Learning Jobs', '1. Get Acquainted With Machine Learning', '2. Build a Portfolio for Machine Learning Job Applications: Create a Presence on Github and Kaggle', '3. Improve your Coding Skills', '4. Understand How Big Systems Work', '5. How to Start Applying for Machine Learning Jobs', 'Conclusion', 'Continue Learning']","Machine learning is a growing field getting a lot of attention, but getting machine learning jobs is still very difficult. Landing an engineering role at a big company means knowing not just Data Science, but also things like programming and system design. More often than not, there‚Äôs a lot of research and learning involved to prepare for applying for a new position. When I was preparing for my machine learning job interviews, I started preparing two months prior to the interviews. That‚Äôs when I really understood what I needed for the data science and machine learning positions I wanted. Fortunately, many recruiters allow and encourage you to take the time to prepare. However, this leaves one important question: exactly how do you prepare for your role? In this article, I will share some strategies for getting a machine learning job, from understanding the market and building a portfolio to skill improvement and the application process itself. Every machine learning job is different, and each will have a different focus. Some focus on machine learning, others on machine learning pipelines, others on big data, and others on deep learning, for example. That said, the central part of any machine learning engineer‚Äôs (MLE) job is to do machine learning. So, even before applying for any MLE role, make sure you know enough about ML first. Get hands-on with a variety of machine learning projects. Build your own basic systems to understand how they work. Learn about big data platforms like Spark and deep learning libraries like Pytorch. If you need more resources to get your head around all of this, I have a dedicated blog to help. Here‚Äôs a good basic goal to let you know you are well prepared: By the end of your ML preparation, you should be able to convert a business problem to a machine learning system. You should ideally be able to design a system end-to-end, which means data collection, exploratory data analysis, feature engineering, model evaluation, model testing, and deployment. A significant challenge when it comes to job applications for machine learning engineer positions is simply getting an interview. So, how can a company find you? How can you make yourself stand out? One answer is to work on creating and completing projects with your skillset. Try out lots of new toy projects, and use resources like Kaggle for inspiration. Participating in discussion forums is another avenue with multiple benefits; you get to learn from and discuss with others while marketing yourself. Be creative and proactive where possible. Building your profile on GitHub can really help. Write lots of code and solve a variety of problems. It can be hard to find these on your own, but taking part in Kaggle competitions is a really great place to start. Working on programming projects is another option for building out your portfolio. When I was starting out, I worked on whatever I felt like, and whatever interested me. For a time I tried to create some games myself, but now I often try to understand research papers by implementing their systems. It‚Äôs one thing to understand the theory, but it‚Äôs another to write code and implement systems. When you apply for a machine learning job, you‚Äôll want to make sure you can do both. A lot of companies have multiple coding rounds as part of their selection process. This is by far the most crucial part of your preparation, because even a machine learning engineer is still an engineer in the end. While it may seem a suboptimal process for finding the right person, it is still a part of the current structure, so if you want to work as an MLE, you‚Äôll need to understand the various data structures and algorithms related to a particular field of work. To help, here are some recommendations and resources. For understanding the ground-level basics of data structures, the book Cracking the Coding Interview by Gayle Laakmann McDowell contains a number of tips for preparation. It‚Äôs a concise book with a just-right amount of information for cracking coding interviews. Each data structure is explained in 2‚Äì3 pages along with practice questions for deeper understanding. I also recommend creating a list of topics you need to prepare for given your preferences, experience, etc. For example, my list looked like this: Data Structures: Array, Stacks/Queues, Dictionary, Trees, Heaps, Graphs. Algorithms: Sorting, Divide-and-Conquer, Dynamic Programming/memoization, Recursion, Binary Search, Breadth-First Search/Depth First Search, Tree traversals. Ideally, by the end of your preparation, you will have read up on the necessary topics, and solved some problems of easy and medium-level difficulties in a place like Leetcode. I also recommend the Algorithm Specialization Course on Coursera by UCSanDiego. I‚Äôve also written some basic-level posts on these topics with simple explanations. You can find them below. Working at a company essentially means working on creating systems end to end while keeping in mind factors such as latency, maintainability and scalability. For this reason, many companies include system design as part of their interview process. They want to gauge how well you can understand and potentially help to improve their own systems. To this end, example problems might run along the lines of: While these questions may seem daunting, they‚Äôre quite open-ended when you start preparing for them. Also, remember that there are no wrong answers. The internet is full of good resources for this kind of preparation, but I want to mention two specifically: In the end, what‚Äôs most important is to understand how a particular system works on the most basic level, how it is set up to mitigate various failure points, and how the system works for a large number of users. It‚Äôs from these building blocks that you will really show your system design abilities. So you‚Äôre confident with data science, you understand data structures, you know how various systems work, and you‚Äôve developed a great portfolio. How should you start applying for jobs? This is a fundamental question that is often not answered in any real detail. So, if I were advising any of my friends on how to start looking for a job, I would give them the following flow: This article is a list of how I prepared for interviews for machine learning jobs and what I did to research and prepare for my job at present. As I said at the start, it‚Äôs not an easy process, so it‚Äôs important to approach it with a mind for time and patience. That said, it‚Äôs very important not to lose hope! Focus on learning and improving a little each day, and understand that there are no shortcuts here, everyone walks the same path, so you just have to keep going. After all, it is only through a series of small steps, one after the other, that you can cover long distances. So stay determined and carry on. Your results will come. Also if you want to know about my own interview experience ‚Äî See it Here. If you want to read up more on Algorithms and Data Structures in a more structured way, here is an Algorithm Specialization on Coursera by UCSanDiego. I sort of audited this course while preparing. Thanks for the read. I am going to be writing more beginner-friendly posts in the future too. Follow me up at Medium or Subscribe to my blog to be informed about them. Also, a small disclaimer ‚Äî There might be some affiliate links in this post to relevant resources, as sharing knowledge is never a bad idea.",255,1,8,Towards Data Science,2021-01-02,2021
https://towardsdatascience.com/how-to-do-logistic-regression-in-r-456e9cfec7cd,How to do Logistic Regression in¬†R,"Interpret, Predict, and Evaluate Logistic Regression with‚Ä¶",1,35,['How to do Logistic Regression in R'],"Introduction Logistic regression is one of the most popular forms of the generalized linear model. It comes in handy if you want to predict a binary outcome from a set of continuous and/or categorical predictor variables. In this article, I will discuss an overview on how to use Logistic Regression in R with an example dataset. We will use infidelity data as our example dataset, known as Fair‚Äôs Affairs, which is based on a cross-sectional survey conducted by Psychology Today in 1969 and is described in Greene (2003) and Fair (1978). This data contains 9 variables collected on 601 respondents which hold information such as how often they have affairs during the past years, as well as their age, gender, education, years married, have children (yes/no), how religious they are (on a 5-point scale from 1=anti to 5=very), occupation (7-point classification), and a self-rating on happiness toward their marriage (from 1=very unhappy to 5=very happy). The figure below shows a few observations to give you an overview of the data. Applying Logistic Regression, we can find which factors contributed the most to infidelity. Then, you can use the model to check which one between you and your partner that more likely to have an affair or not üòú But, before that, we will run through some descriptive statistics with the code below to get a better understanding of our data. From the summary above, we can see that there are 286 male respondents (representing 48% of the overall respondents), 430 respondents had children (representing 72% of the overall respondents), and average age for our respondents was 32.5 years old. In addition, we find that 451 respondents claimed not engaging in an affair in the past year. It means 25% of our respondents has an affair with the largest number reported was 12. In conclusion, we can say that 6% of respondents has 1 affair per month üòè. As we are interested in the binary outcome for our response variable (had an affair/didn‚Äôt have an affair). We can transform affairs into abinary variable called ynaffair with the following code. Fit the model with Logistic Regression Now, we can execute the logistic regression to measure the relationship between response variable (affair) and explanatory variables (age, gender, education, occupation, children, self-rating, etc) in R. If we observe the Pr(>|z|) or p-values for the regression coefficients, then we find that gender, presence of children, education, and occupation do not have a significant contribution to our response variable. Therefore, we can try to fit a second model by including only significant variables such as age, years married, religiousness, and rating to fit the data instead. For the second model, we can see that p-values for each regression coefficient is statistically significant. Then, we may run chi-square test with anova function in R to compared between first and second model. We will see which model that explain our response variable better. The output above displays nonsignificant chi-square value with p-values= 0.21. It means that the second model with only four predictors fits as well as the full model with nine predictors. It supports our initial belief that gender, children, education, and occupation don‚Äôt add any contribution to predict infidelity (our response variable). Thus, we will continue the analysis with the second model as it is easier to do our interpretations on the simpler model. Interpret the model parameters Based on the regression coefficients from the second model, we are seeing that the odds of having affairs increase with years married and decrease with age, religiousness, and happiness self-rating. We can observe it based on the positive or negative sign from each regression coefficient. In conclusion, we might say the longer you are married, then the more likely you will have an affair. On the other hand, the happier you are in the marriage, then the less likely you will have an affair. Next, we want to know the impact value of each of these variables towards affair. First, we need to remember that logistic regression modeled the response variable to log(odds) that Y = 1. It implies the regression coefficients allow the change in log(odds) in the return for a unit change in the predictor variable, holding all other predictor variables constant. Since log(odds) are hard to interpret, we will transform it by exponentiating the outcome as follow We observe that the odds of having affair are increased by a factor of 1.106 for a one-year increase in years married (holding age, religiousness, and happiness rating constant). On the contrary, the odds of having affair are multiplied by a factor of 0.965 for every year increase in age. It means the chance of having an affair drop by -3.5% every time someone gets older. Furthermore, the change in the odds of the higher value on the response variable for an n unit change in a predictor variable is exp(Œ≤j)^n. Then, a 15-year increase would increase the odds by a factor of 1.106¬π‚Åµ‚âà4.5, holding the other predictor variables constant. Predict the outcome using new data In this section, we are using the model that we built to predict the outcome for the new data. The first step, we will make a new data containing the values of predictor variables we‚Äôre interested in. The second step, we will apply the predict() function in R to estimate the probabilities of the outcome event following the values from the new data. Consider new data below where we have 5 new respondents with different self-rating, holding other variables set to the average of overall data. Then, we apply the prediction function to get the probabilities of having affair for these new respondents. Clearly, we notice that chance of having affair declining from 0.53 when marriage is rated 1=‚Äùvery unhappy‚Äù to 0.15 when the marriage is rated 5=‚Äùvery happy‚Äù (holding other predictor variables constant). It indicates the unhappy couple are three times more likely to have an affair compared to the happy one. Let‚Äôs create another new data to observe the impact of age toward infidelity Here, we see that as age increases from 17 to 57, the probability of having affair declining from 0.34 to 0.11, holding the other variables constant. If you are interested to explore the impact of other predictor variables or to predict other new data, then you can use this approach to analyze it further. Evaluate overdispersion In logistic regression, we need to check the expected variance for data drawn from a binomial distribution œÉ2 = nœÄ(1 ‚àí œÄ), where n is the number of observations and œÄ is the probability of belonging to the Y = 1 group. Overdispersion occurs when data admit more variability than expected under the assumed distribution. If overdispersion is present in a dataset, the estimated standard errors and test statistics the overall goodness-of-fit will be distorted and adjustments must be made. One of the solutions, we need to use the quasibinomial distribution rather than the binomial distribution for glm() function in R. There are two ways to verify if we have an overdispersion issue or not: The first method, we can check overdispersion by dividing the residual deviance with the residual degrees of freedom of our binomial model. If the ratio considerably larger than 1, then it indicates that we have an overdispersion issue. Calculating this ratio using our data example, we find that the ratio is close to 1. It means no overdispersion problem on our model. The second method, we are using two models fit to check overdispersion. Basically, we will fit the logistic regression using two different models using different distributions. Then, we check if there‚Äôs a statistical evidence that the expected variance of the two models is significantly different. We find that p-value =0.34 is clearly not significant (p > 0.05), strengthening our belief that overdispersion isn‚Äôt a problem on our model I hope you find this article is useful and kindly share it with others Cheers, Michaelino Mervisiano",135,1,9,Towards Data Science,2021-01-02,2021
https://towardsdatascience.com/10-examples-to-master-python-dictionary-comprehensions-7aaa536f5960,10 Examples to Master Python Dictionary Comprehensions,A comprehensive practical guide,1,29,['10 Examples to Master Python Dictionary Comprehensions'],"A dictionary is an unordered collection of key-value pairs. Each entry has a key and value. A dictionary can be considered as a list with special index. The keys must be unique and immutable. So we can use strings, numbers (int or float), or tuples as keys. Values can be of any type. In this article, we will focus on dictionary comprehension which is a method to create dictionaries using iterables. The logic is the same as list comprehension but the syntax is different due to the structure of dictionaries. In order to see the similarity between a list and dictionary comprehension, I will create both a list and dictionary comprehension in the first two examples. We have an iterable which is a list named ‚Äúwords‚Äù. In the list comprehension, we create a list that contains the length of the words. In the dictionary comprehension, we need to specify both keys and values based on the iteration. The returned dictionary contains the words as keys and their length as values. The basic syntax for list and dictionary comprehension are: For this example, we will repeat the task in the first example with an additional condition. Both list and dictionary comprehensions accept if/else conditional statements. The returned variables only contain the words longer than 5 characters. In this example, we will slightly increase the complexity of the conditional statement. We implement an if/else conditional in the dictionary comprehension. If the length is greater than 5, the value becomes the length. Otherwise, we assign the word ‚Äòshort‚Äô as the value. What makes comprehensions appealing is their one liner syntax. It looks quite simple and easier to understand than the equivalent for loops. For instance, the equivalent for loop of the comprehension above is: We can iterate over two iterables in a dictionary comprehension. Key-value pairs are created by iterating over separate lists for keys and values. The zip function returns an iterable of tuples by combining the items from each list. We can also put a condition on the values when iterating over a list of tuples. We can also apply transformations on key-value pairs. Both keys and values are modified using simple Python methods. We can access the key-value pairs in a dictionary by using the items method. We can use the items of an existing dictionary as iterable in a dictionary comprehension. It allows us to create dictionaries based on existing dictionaries and modify both keys and values. The enumerate function of Python can be used to create an iterable of tuples based on a list. Each tuple contains the items in the list with incrementing integer values. We can use the enumerate function in a dictionary comprehension. If you just want to create a dictionary based on a list of tuples without any modification on the values, you do not need to use a comprehension. The dict function will do the job. This example contains a slightly more complicated conditionals than the previous ones. Consider we have the following dictionary and list. We want to create a new dictionary using the list and dictionary defined above. The keys of the new dictionary will be the elements in the list so we will iterate over the elements in list. If the element is also in the dictionary, the value will be the values of that key in the dictionary. Otherwise, the value will be the length of the key. The word artificial is not in the dictionary so its value is the length of the word. The word data is in the dictionary so its value is taken from the dictionary. The keys of a dictionary must be immutable so tuples can be used as keys. Dictionary comprehensions allow for generating keys of tuples by implemented nested loops. Each pair of items in the lists is a key in the dictionary. The value is the product of the items in keys. The equivalent for loop syntax: Dictionaries are very important data structures in Python and used in many cases. The examples we did in this post will cover most of what you need to know about dictionary comprehensions. They will make you feel comfortable when working with and creating new dictionaries. Thank you for reading. Please let me know if you have any feedback.",249,1,5,Towards Data Science,2021-01-02,2021
https://towardsdatascience.com/the-simple-ways-to-refactor-terrible-code-983f563964fc,The Simple Ways to Refactor Terrible¬†Code,It‚Äôs time to overcome the fear of messy legacy code¬†and‚Ä¶,7,35,"['The Simple Ways to Refactor Terrible Code', 'The Easiest Improvements', 'Low Hanging Fruit', 'Digging Deeper', 'Legacy Code', 'Leverage Tooling', 'Conclusion']","A lot of people try to avoid refactoring, whether it‚Äôs because they are scared of breaking their code, because they don‚Äôt see immediate benefit, or they just pretend that there‚Äôs no time for it. In reality though, refactoring can be extremely beneficial both short-term and long-term and there‚Äôs a lot of reasons to spend some time on it. The most commonly mentioned ones would probably be finding bugs, making software easier to understand or improving its design. The real reasons in my opinion, though, that should push you to mess with your (or maybe legacy) terrible code should be that refactoring helps us lose fear from our own code and the helps us avoid the feeling of shame when we have to show our code to somebody else. So, in this article we will go over some easy ways to refactor and considerably improve your code without risking breaking everything at the same time, so that you don‚Äôt have to be scared of your own code or be ashamed to show it to your colleagues. To ease you into the refactoring of the feared codebase we will start with simple, yet very significant improvements that literary cannot break anything ‚Äî first one being reformatting of code. There are couple of ways how to approach fixing code formatting. You can either fix things like wrong indentation or missing {} around code-blocks as you go or you can take little more systematic approach. The more systematic approach would constitute creating a style-guide and then applying it (and ideally enforcing it). I said that we should create a style-guide, but for most programming languages there already is a set of standard conventions such as PEP8 for Python or Google Java Style Guide for Java, which you can adopt and use tools or IDE plugins to help you find and fix all formatting issues. Regardless of which approach you choose, it‚Äôs important to (carefully) choose one style and stick to it and also make sure everybody in the team follows same guide to keep things consistent. Another safe area for refactoring is comments. It‚Äôs good to have them as long as they‚Äôre relevant, up-to-date and useful. Most of the time though, you will find a lot of comments that should be removed. Those can be redundant comments or docs that describe something self-explanatory, commented-out code or obsolete comments that should be updated (or removed if not relevant anymore). Of these three I really want to highlight the commented-out code, because a lot of people will say: ‚ÄúBut I might need it later!‚Äù ‚Äî no you won‚Äôt, stop lying to yourself, just delete it now and if you will actually need it in the future, then you‚Äôll be able to find it in git history. When it comes to comments, just remember that comments do not make up for bad code. Last thing for this section is dead code. It creates clutter and the longer you leave it in the codebase the harder it becomes to remove because no one knows what it does or whether it‚Äôs needed or not. Nowadays, any decent IDE will tell you about unreachable dead code so you don‚Äôt need to be scared about removing it, so just get rid of it, there‚Äôs no reason to keep it in there. Oftentimes it‚Äôs easy to find things to refactor, like with the simple improvements shown above. If you however need a few examples of crappy code to look for, then here‚Äôs a list: If you omit braces like this you might save one line of code, but really you‚Äôre just setting yourself up for some nasty, long debugging. On top of that, if you need to set breakpoint in such one-liners, you will find out that you can‚Äôt really put the breakpoints in properly because you can only set them on whole line and not just the part in a conditional or loop. The most basic rule: Make only tiny changes and retest. Now that you went through some of the code and have little more confidence modifying it, you might want to dig deeper and look for more specific flaws in the code. One of the flaws you will definitely find will be duplicate code. Simple rule of thumb with code duplication is that if you repeat same piece of code 3 times, it‚Äôs time extract it to separate function or variable. Another common flaw might be excessive use of boolean parameters. Passing boolean parameters to a function might indicate that this function is doing more than one thing and therefore should be split into (at least) two smaller ones. Speaking of function parameters ‚Äî you might sometime encounter a bad practice of modifying input parameters. Arguments are function input and that‚Äôs it ‚Äî use them as input and don‚Äôt change their value. If you see this flaw in a code it‚Äôs best to copy the value into new variable and avoid modifying the parameter. Last issue related to parameters that you might want to look out for are functions with too many parameters. Up to 3 parameters are usually fine, but seeing any more than that should raise a suspicion. You should inspect such function and make decision whether it‚Äôs OK to use so many arguments or whether it‚Äôs better to break up and/or simplify the function. Moving on to variables, there are 2 basic issues that you might encounter, first being redundant temporary variables. This would look something like this: Using this kind of variable is not useful in any way. It makes the code longer and doesn‚Äôt explain or improve readability in any way so we should remove it and inline it like so: The other common issue would be opposite of the above ‚Äî that is ‚Äî code lacking variables, for example: This piece of code ‚Äî unlike the previous issue ‚Äî would benefit from a few extra variables that would help to explain what the code does. It would also simplify debugging, as it would be easier to set breakpoints and watch for each variable. Refactored version can look for example like this: Last candidate for refactoring in this section is very easy to spot ‚Äî it is complex/composite conditional. We‚Äôve all seen those huge multi-line conditionals with a few && and || grouped together. They're ugly, hard to parse, hard to debug and hard to modify. The simplest way to get rid of it - or rather abstract it - is to extract it into separate checking function. Example of that looks like so: To be able to effectively refactor our code we need to have a decent test coverage, otherwise we‚Äôre risking breaking the existing functionality. In previous sections I kinda assumed that we have tests and therefore can modify code and run tests suite to verify that everything is OK. But how do we refactor code that doesn‚Äôt have proper test coverage ‚Äî or in other words how do we refactor Legacy Code? In cases where we can‚Äôt rely on tests, I recommend only messing with what has to be changed, which would be code that has bugs that need fixing or part of codebase where we‚Äôre adding new feature. In these areas start by writing tests for existing legacy code and only then proceed with any code modifications. I would personally start with first refactoring the old code to familiarize myself with it and to make it more ‚Äúwelcoming‚Äù to the necessary changes. From this point you can proceed as with any other code that needs refactoring ‚Äî take it one step at the time, start with small simple changes and retest frequently. Refactoring and in general taking good care of your code can be time-consuming, which can be problem on projects with tight schedule or when deadlines are approaching. These are the times when people throw all the good practices out of the window and try to ‚Äújust ship it‚Äù as soon as possible. To alleviate this pressure and make it easier to keep refactoring and improving the code even when there seems to be no time for it, try to use tools that can do some of the work for you. These can be linters like pylint, code checkers like Checkstyle or code coverage tools like Sonarqube. Speaking about code coverage and tests ‚Äî also make sure your test suite is fast, otherwise you will end up dropping the slow tests or not running the suite at all. In ideal world, all the above should be automated using some kind of CI/CD pipeline, which would run code checks with mandatory quality gates, report any code style inconsistencies, any possible bugs or issues before it even gets build and deployed. This article is not an exhaustive list of ‚Äúall things refactoring‚Äù. For more extensive list and examples you should definitely check out books like Clean Code (especially chapter Smells and Heuristics) by uncle Bob or book called Refactoring by Martin Fowler. Regardless of which guides or books on this topic you decide to follow, it eventually all boils down to a few simple rules. Always try to leave the code little cleaner then before ‚Äî this way you can slowly but surely refactor and improve overall quality without too much effort and time spend. At the same time don‚Äôt put off refactoring because doing it later usually mean never, which ends up producing terrible mess of a legacy code. This article was originally posted at martinheinz.dev towardsdatascience.com towardsdatascience.com towardsdatascience.com",116,1,9,Towards Data Science,2021-01-02,2021
https://towardsdatascience.com/raw-image-processing-in-python-238e5d582761,Raw Image Processing in¬†Python,Preprocessing raw images for machine learning pipelines,6,13,"['Raw Image Processing in Python', 'What exactly is a raw image?', 'How does an image sensor work?', 'How to obtain the color channels from this image', 'What does a raw image contain?', 'Now, we are going to discuss some steps in detail along with the python code:']","Almost all modern-day cameras capture raw format images and process them in a format commonly known as sRGB, suitable for humans to see. However, one might wonder what all the techniques are used to convert the raw images into sRGB format are? Why was it necessary? Also, one might wonder how to use raw images or process them in a certain manner to get better performance on some machine learning tasks. This article attempts to answer all such questions in addition to step-by-step python code for each process. Most of the filters on social media apps such as Snapchat, Instagram, etc., use machine learning. The machine learning algorithm behind those filters uses raw images to process the filter‚Äôs image to give real-time results. So, it becomes increasingly important to know what raw image and how it is processed by the camera while designing an algorithm that uses raw images. A raw image can be defined as a minimally processed image captured by a camera. It is yet to be processed by the software methods to process background noise, contrast, black level, etc. The raw image is unpleasant to the human eye in most cases and needed to be processed to be pleasant to see. How is a raw image is captured in a camera, and how does the camera sensor work? The image sensor can be considered a circuit consisting of a surface used to capture the camera‚Äôs shutter‚Äôs electromagnetic waves or when the sensor is exposed to light. The sensor‚Äôs surface captures the intensity of the electromagnetic waves, aka light, that is incident on the surface at the time of capture. The surface can be considered a 2D array in which each element stores the light incident‚Äôs intensity. But, by storing only the light‚Äôs intensities, the sensor cannot comprehend the colors in the light. So, how does the sensor detect the colors in the scene? For detecting the color in a sensor, various techniques are used; one of the most common and most widely used is the Bayer-filter sensor and discussed here. A Bayer filter is used to map the incoming electromagnetic signal into the RGB space by using a filtering technique. The incident light is filtered into Red, Green, and Blue colors using a wavelength filter before the light hits the sensor. Using this technique, the intensities of a particular color (in this case, red, green, and blue) can be known. The red, green, and blue intensities are stored alternately in the Bayer filter, as shown in the figure. The are other filter patterns used on some cameras, but the Bayer filter pattern is the most widely used one. A raw image a 2D array that consists of information about the light intensities at the various wavelength/colors. To obtain a color channel, we need to separate pixels of each color and combine them to make an image. However, one can easily see that the number of green pixels is double the color pixels. In this case, the value of adjacent green pixels is averaged to obtain a single value. Hence, for a raw image of H x W size, the final RGB image obtained is H/2 x W/2 x 3. The raw image file generally contains the image as a 2D array recorded on the image sensor after being passed from the Bayer filter. The file contains a large amount of metadata about the camera, aperture, lighting condition, etc. in the file, which helps during the image‚Äôs postprocessing. Some common metadata types are the black level, white level, orientation, color space transform, etc., which are discussed in this article. All these steps need to be done on the image to convert it into the required format to maintain quality. The black level is defined as the intensity of the least intense/dark part of the image. It is necessary to calibrate the image‚Äôs black level during postprocessing to obtain the perfect black pixels that are not present in the original raw images. Various algorithms are used to correct the black level in the images and are beyond this article‚Äôs scope. In some cameras, the image is stored vertically inverted, so the metadata‚Äôs orientation information helps correct the image in such cases. The lens in the camera projects the image into the sensor in an inverted form. Sometimes, it is also left-right flipped. The lens‚Äôs orientation effect is generally corrected internally within the camera and doesn‚Äôt need to be corrected during postprocessing. This is mostly the last step in any image processing pipeline. The processed image is transformed to the required color space, such as sRGB, YCrCb, Grayscale, etc., before being stored in the disk. The most commonly used color space is the sRGb color space. After performing the color space transformation, the images are stored on the disk in the form .png, .jpeg, etc., image storing formats. Camera pipelines are much more complex than the one we have discussed here, but the details discussed in this article are more than sufficient to start using raw image data in a machine learning pipeline. Feel free to ask questions in the comment section.",102,3,5,Towards Data Science,2021-01-02,2021
https://towardsdatascience.com/predicting-a-failure-in-scanias-air-pressure-system-aps-c260bcc4d038,"<strong class=""markup--strong markup--h3-strong"">Predicting a Failure in Scania‚Äôs Air Pressure¬†System</strong>",Improving Maintenance Costs using Machine¬†Learning,21,67,"['Predicting a Failure in Scania‚Äôs Air Pressure System', 'Content:', 'ML Formulation', 'Business Constraints', 'Dataset Overview', 'Performance Metric', 'Literature Review', 'First Cut Approach', 'Getting Started', 'Removing Single Valued Features', 'Handling Missing Values', 'Separating Features for Analysis', 'Histogram Feature Analysis', 'Numerical Feature Analysis', 'Summarizing our Exploratory Data Analysis', 'Preparing Our Data ( Standardizing + SMOTE + UnderSampling)', 'Experimenting with Classical ML models', 'Deployment on Local Server using Flask API:', 'Conclusion:', 'Future Scope', 'References']","The Air Pressure System (APS) is an essential part of a heavy duty vehicle, where compressed air causes the piston to apply pressure on the brake pads to slow down the vehicle. The advantages of having an APS instead of a hydraulic setup is the easy availability and sustainability of air from nature. The dataset consists of data collected from heavy Scania trucks in everyday usage. These are failure cases of the trucks during operation, and our task is to predict whether a given failure is caused due to a specific component of the Air Pressure System. This may help in avoiding failure during truck operation and thereby reducing maintenance cost. The data can be found at: https://archive.ics.uci.edu/ml/datasets/APS+Failure+at+Scania+Trucks This is a Binary Classification problem where the positive class tells us that the failure was due to a specific component of the APS, whereas, the negative class tells us that the failure has nothing to do with that component. Therefore, given a new data point (sensor information), we can build an ML model that would tell us if the failure was due to the truck‚Äôs APS or not. The training dataset consists of 60,000 data points and 171 features, of which one is the class label. The features are a combination of numerical data and histogram bins data. The feature names are kept anonymized for proprietary reasons. 59,000 data points belong to the negative class and the remaining 1,000 belong to the positive class. This tells us that we are dealing with a highly imbalanced dataset and is usually the type of data we can expect in a real world scenario. Another problem observed is that a large part of the data is missing. In extreme cases, some instances have 80% of the values missing. The dataset is classified as Missing Completely At Random (MCAR), as there is no relationship whether a data point is missing and any value in the dataset is missing or observed. Therefore, we have to find ways to resolve these issues by feature engineering methods. We will be using Macro-F1 Score as our performance metric for this project. Macro F1 score takes in to account the F1 scores of each class. It may be beneficial in showing us the performance of our model based on the number of correctly classified points for both classes. This is useful because the cost of misclassification is very high since an APS failure which is not detected can lead to failure of the truck during operation and increase in maintenance cost. Cerqueira, V√≠tor, et al. ‚ÄúCombining Boosted Trees with Metafeature Engineering for Predictive Maintenance.‚Äù International Symposium on Intelligent Data Analysis. Springer, Cham, 2016. This paper mentions that the authors‚Äô approach to this problem consists of 4 steps. (i) A filter that excludes a subset of features and data points based on the number of missing values; (ii) A metafeature engineering procedure used to create new features based on existing information; (iii) a biased sampling method to deal with class imbalance problem (SMOTE); and (iv) use of boosted trees for classification. Features having a high percentage of missing values were removed. During their analysis, they found that some features had an extremity of 80% data missing, and 8 out of 170 features had more than 50% missing values. After removing the said features, it was seen that there were duplicate data points, indicating that the removed features have a little effect in getting a good score. They mentioned that they are treating the problem as an Anomaly Detection problem since the positive class of the data are characterized by rare events in the domain. They used BoxPlot Analysis (for each feature, compare each value to the typical value found in that feature), Local Outlier Factor (compare data point to it‚Äôs local neighborhood through density estimation) and Hierarchical Agglomerative Clustering (each step merges two similar group, and the last observation that are merged might be an outlier) for their metafeature engineering. SMOTE is a method of duplicating the data points of the minority class of the imbalanced dataset, to balance it out. The use of SMOTE + MetaFeature Engineering with XGBOOST library was seen to give the best result. This paper gives us an idea creating new features using Boxplot Analysis, LOF and hierarchical agglomerative clustering. It also shows us a use case of SMOTE and the results of all these feature engineering techniques combined with using a GBDT model. Costa, Camila Ferreira, and Mario A. Nascimento. ‚ÄúIDA 2016 Industrial Challenge: Using Machine Learning for Predicting Failures.‚Äù International Symposium on Intelligent Data Analysis. Springer, Cham, 2016. This paper is the winning solution of the challenge. The authors tried different algorithms, namely, Logistic Regression, K-NN, SVM, Decision Trees and Random Forests to solve the problem. They handled the missing data by implementing the Soft-Impute Algorithm. It is a large scale matrix completion algorithm that replaces missing values with current guesses and solves an optimization problem. The imbalance data was handled by setting a high threshold (cut-off) value, meaning the model will predict a negative class only if it is extremely sure. The final result showed that Random Forest performed the best, giving a Total Cost (given metric) that was 92.56% lesser than their baseline model. The KNN model was the second best classifier with 90.84% improvement, and the Logistic Regression model worked well with 88.72% improvement. The SVM with RBF Kernel improved the total cost by just 86.36%. This paper compares the performance of different models, on data whose missing values were imputed using more complex algorithms rather than simple mean/median. We see that Ensemble models will work well to tackle this problem, and a high threshold value plays a key role in handling imbalanced data. First off, let‚Äôs import the required packages and read our training data. The dataset consists of 171 features, including the class label. Also, in the class label attribute, we will replace ‚Äòneg‚Äô with 0 and ‚Äòpos‚Äô with 1. The class distribution graph shows a serious case of data imbalance, since out of the total 60,000 training points, about 59,000 points belong to the negative class and just 1,000 points belong to the positive class. We can choose to up-sample the minority class data points, or use a modified classifier to tackle this problem. Also, the percentage of missing data is significantly high in some features (As high as 82% in a feature). Out of the available features, the ones that have the same value for all data points do not hold much importance in improving performance of our model. Hence, we can discard those features. We can remove the features that have standard deviation = 0. One of the features, (‚Äòcd_000') is seen to have a constant value for all data points. We may remove this feature. It is always a good practice to identify and replace missing values for each column in your input data prior to modeling your prediction task. This is called missing data imputation, or imputing for short. We can perform some basic handling of missing data in the following manner: 128 features have less than 5% of it‚Äôs values missing, hence we drop the rows consisting of missing values for these features(4027 rows). 7 features (‚Äòbr_000‚Äô, ‚Äòbq_000‚Äô, ‚Äòbp_000‚Äô, ‚Äòbo_000‚Äô, ‚Äòab_000‚Äô, ‚Äòcr_000‚Äô, ‚Äòbn_000') had more than 70% of it‚Äôs values missing. These features have been removed. The class label has then been separated from our dataset, leaving us with a dataset of shape (55973,162). 14 features had 5% to 15% of it‚Äôs values missing and are passed through sklearn‚Äôs SimpleImputer and the missing values are imputed using ‚Äòmedian‚Äô. Following which, for features having 15% to 70% missing values, we will perform an Iterative model based imputation technique called MICE. At each step, a feature with missing values is designated as output y and the other feature columns are treated as inputs X. A regressor (we have used Ridge Regressor) is fit on (X, y) for known y. Then, the regressor is used to predict the missing values of y. This is done for each feature in an iterative fashion, and then is repeated for max_iter (10 as default) imputation rounds. The results of the final imputation round are returned. All the above models are saved and the preprocessing steps are performed on the test dataset. It was given to us that certain features are histogram bin information, and the prefix (letter before the ‚Äò _ ‚Äò) is the Identifier and the suffix is the bin_id (Identifier_Bin). To find the features that are contain histogram bin information, we know that all features from a single histogram have the same prefix. We can see that there are 7 sets of features having 10 bins each. In other words, there are 7 histograms divided into 10 bins each. eg: Identifier ‚Äòag‚Äô consists of ag_000, ag_001, ag_002, ag_003, ag_004, ag_005, ag_006, ag_007, ag_008 and ag_009. The Histogram Identifiers are: [‚Äòag‚Äô, ‚Äòay‚Äô, ‚Äòaz‚Äô, ‚Äòba‚Äô, ‚Äòcn‚Äô, ‚Äòcs‚Äô, ‚Äòee‚Äô]. We will select the top features from both the datasets using the complete imputed set. But the Analysis will be performed on the data having missing values. We will perform EDA on the top 15 features of the histogram dataset. For selecting the features, we will perform Recursive Feature Elimination, using Random Forest Classifier The top 15 features are : The PDF, CDF and Box plots of each of these features to try to understand the distribution of our data. The observations made are as follows: Plots of features ag_003, ay_008, ba_002, ba_003, ba_004, cn_004, cs_002, cs_004, ee_003 and ee_005 show us that the Lower values of the features indicate no failure in the APS component. A higher value clearly indicates an APS component failure Around 99% values of feature ag_001 and ay_005, where there is no failure in the APS component, are 0. We can say that in these top features, a higher value may indicate a failure in the truck‚Äôs Air Pressure System But, there are few cases when the values are higher than usual, but still do not lead to APS failure. Example: Feature ee_005 Taking into consideration how each feature is correlated with the target variable (‚Äòclass‚Äô), we can observe that feature ‚Äòay_005‚Äô is the most uncorrelated feature among our top attributes. We can perform further Bivariate Analysis on how the other top features vary w.r.t feature ‚Äòay_005‚Äô. ag_002, ag_001, cn_000: It can be seen from the scatter plot that for any value of the other top features, there is failure in the APS component (class label = 1) when the value in feature ‚Äòay_005‚Äô is nearly 0. We will perform EDA on the top 15 features of the histogram dataset. For selecting the features, we will perform Recursive Feature Elimination, using Random Forest Classifier The top 15 features are : The PDF, CDF and Box plots of each of these features to try to understand the distribution of our data. The observations made are as follows: aa_000 : If there is no failure in the APS (class label = 0), about 95% of the points have a value below 0.1x1e6. A higher value than that usually indicates a failure in the APS component. al_000, am_000 : The values of instances of failure and non-failure of the APS component are not clearly seperable in this feature. Although points of the failure cases do have a slightly higher value. ap_000, aq_000, bj_000, bu_000 : Instances of failure have a higher value, compared to non-failure cases. But there are few instances of non-failure of the APS component, that see higher values in this feature. In all features, except dg_000, cj_000, am_0 and al_000, the higher values in the features usually indicate failure in APS component. But due to the Imbalanced nature of the data this may not be certain. Taking into consideration how each feature is correlated with the target variable (‚Äòclass‚Äô), we can observe that feature ‚Äòdx_000‚Äô is the most uncorrelated feature among our top attributes. We can perform further Bivariate Analysis on how the other top features vary w.r.t feature ‚Äòdx_000‚Äô. The main observation in all plots here is that for any value of the remaining features, if the feature ‚Äòdx_000‚Äô has a low value ( nearly 0 ), it MAY INDICATE that there is a failure in the APS component (class label=1). Standardizing a vector most often means subtracting a measure of location and dividing by a measure of scale. For example, if the vector contains random values with a Gaussian distribution, you might subtract the mean and divide by the standard deviation, thereby obtaining a ‚Äústandard normal‚Äù random variable with mean 0 and standard deviation 1. We will scale our data using sklearn‚Äôs MinMaxScaler. A problem with imbalanced classification is that there are too few examples of the minority class for a model to effectively learn the decision boundary. One way to solve this problem is to oversample the examples in the minority class. The combination of SMOTE and under-sampling performs better than plain under-sampling. Finally, we have 33,226 points belonging to negative class and 16,613 points belonging to the positive class. We will pass our scaled dataset through the linear models (Logistic Regression and Support Vector Machines). Now that we have prepared our performed our EDA, data-preprocessing and feature engineering, let‚Äôs move on to modelling. We will pass our data through various models, perform hyperparameter tuning, and evaluate each of them based on our performance metric (Macro-F1 Score) and Confusion Matrix. The different models that we will be trying out here are Logistic Regression, Support Vector Machines, Naive Bayes, Decision Trees, Random Forest, Gradient Boosted Decision Trees, Adaboost Classifier and a Custom Ensemble. As a Baseline Model, we will predict all class labels to be 0 (majority class) and calculate the F1 score for the same. We can use sklearn‚Äôs DummyClassifier to obtain our baseline results. For our Custom Ensemble: We can use Decision Trees as our base model and GBDT as the metamodel. This is a custom implemented model. After performing hyperparameter tuning and experimenting various models, we see that the Gradient Boosted Decision Trees works best as it gets the highest Macro-F1 Score (as seen below). The model can be deployed on our local server using Flask API. Code for the same includes loading the required models, creating Pandas Dataframe from the .csv file taken from the specified path, and storing the final output in a .csv file at the output directory. The HTML code for the same is given below On running the above code, the html page on our local server would look something like this, where you can specify the path to your input file and output directory: The output directory would consist of a .csv file (consisting of timestamp) which consists of the preprocessed dataset along with the model predictions. For a clearer picture, you can view this video, which demonstrates the complete process: To sum it all up, we first remove features that have maximum amount of missing values, then impute the missing values from the remaining features using a combination of Median and MICE Imputation methods and then pass this preprocessed dataset through our trained Gradient Boosted Decision Trees model. The results achieved were fairly good and have been deployed. I hope this project gives you a fair idea on how to approach any data science project especially if you‚Äôre just starting out :) You can view the entire Code at my Github. And feel free to contact me through LinkedIn or Twitter.",22,2,16,Towards Data Science,2021-01-02,2021
https://towardsdatascience.com/hands-on-review-byol-bootstrap-your-own-latent-67e4c5744e1b,Hands on Review: BYOL(Bootstrap Your Own¬†Latent),"Lately, Self-supervised learning methods have¬†become‚Ä¶",1,20,['First Hand Review: BYOL(Bootstrap Your Own Latent)'],"Lately, Self-supervised learning methods have become the cornerstone for unsupervised visual representation learning. One such method Bootstrap Your Own Latent(BYOL) which is introduced recently is reviewed in this post. I have already covered other interesting self-supervised learning methods based on contrastive learning that came before BYOL such as SimCLR, MoCo, etc. in another post thoroughly and should be considered for understanding the fundamentals of this post. Please find them here. Unlike other contrastive learning methods, BYOL achieves state-of-the-art performance without using any negative samples. Fundamentally, like a siamese network, BYOL uses two same encoder networks referred to as online and target network for obtaining representations and reduces the contrastive loss between the two representations. Network Architecture The architecture of the BYOL network is shown below. Œ∏ and œµ represent online and target network parameters respectively and f_Œ∏ and f_œµ are online and target encoders respectively. Target network weights are slowly moving average of the online network weights i.e. Idea is to train the online network f_Œ∏ in the first step and use those learned representations for downstream tasks and fine-tune them further using labelled data in the second step. The first step i.e. BYOL could be summarized in the following 5 straightforward steps. Mathematically, Contrastive loss is computed as mean squared error between q_Œ∏(z_Œ∏) and z‚Äô_œµ. Before computing the mean squared error, the labels z‚Äô_œµ and targets q_Œ∏(z_Œ∏) are L2-normalized. The equation is, z`_œµ bar , is the L2 normalized z`_œµ and q_Œ∏(z_Œ∏) bar is L2 normalized q_Œ∏(z_Œ∏). Why BYOL? The First question is, why and where one should use BYOL? BYOL method helps in learning useful representations for a variety of downstream computer vision tasks such as object recognition, object detection, semantic segmentation, etc. Once these representations are learned in BYOL way, they could be used with any standard object classification model such as Resnet, VGGnet, or any semantic segmentation network such as FCN8s, deeplabv3, etc or any other task-specific network and it gets to a better result than training these networks from scratch. This is the major reason behind the popularity of BYOL. The below graph shows that the BYOL representations learned using Imagenet images beats all previous unsupervised learning methods and achieves classification accuracy of 74.1% with Resnet50 under linear evaluation protocol. In case you are not sure about Linear evaluation protocol, it is described in my last post in detail. The power of BYOL is leveraged more efficiently in dense prediction tasks where generally only a few labels are available due to the complex and costly task of data labelling. When BYOL is used for one such task namely semantic segmentation using cityscapes dataset with FCN8s network along with Resnet50 backbone, it outperforms the version of the network trained from scratch i.e. with random weights. The below graph compares the performance of 3 main networks on the cityscapes dataset. The below graph clearly shows that the BYOL significantly helps in learning useful representations for this task and hints that it should be considered as a pre-training step for other computer vision industrial applications where Imagenet weights could not be used due to licensing regulations and lots of unlabelled data is present for unsupervised training. Implementation Details For Image augmentations, the following set of augmentations are used. First, a random crop is selected from the image and resized to 224x224. Then random horizontal flip is applied, followed by random color distortion and random grayscale conversion. Random color distortion consists of a random sequence of brightness, contrast, saturation, hue adjustments. The following code snippet implements the BYOL augmentation pipeline in PyTorch.. In the actual BYOL implementations, Resnet50 is used as an encoder network. For the projection MLP, the 2048 dimensional feature vector is projected onto 4096-dimensional vector space first with Batch norm followed by ReLU non-linear activation and then it is reduced to the 256-dimensional feature vector. The same architecture is used for the predictor network. Below PyTorch snippet implements the Resnet50 based BYOL network, but it could also be used in conjunction with any arbitrary encoder network such as VGG, InceptionNet, etc. without any significant change. Why BYOL works the way it works Another interesting fact is, although a collapsed solution exists for the task curated for BYOL, the model avoids it safely and the actual reason for it is unknown. Collapsed solution means, the model might get away by learning a constant vector for any view of any image and gets to zero loss, but it does not happen. The authors of the original paper[1], conjecture that it might be due to the complex network(Deep Resnet with skip connections) used in the backbone, the model never gets to the straightforward collapsed solution. But in another recent paper SimSiam[2] Chen, Xineli and He, found out it is not the complex network architecture but the ‚Äústop-gradient‚Äù operation that makes the model to avoid the collapsed representations. ‚Äústop-gradient‚Äù means that the network never gets to update the weights of the target network directly through gradients and hence never gets to the collapsed solution. They also show that there isn‚Äôt any need for a momentum target network to avoid collapsed representation but it certainly gives better representations for downstream tasks if used. That was the quick summary of BYOL along with code in PyTorch. For full implementation, this GitHub repo https://github.com/nilesh0109/self-supervised-sem-seg could be referred. Below is the list of references used in this post.",484,0,6,Towards Data Science,2021-01-02,2021
https://towardsdatascience.com/the-case-for-using-timeseries-databases-c060a8afe727,The Case for Using Timeseries Databases,A brief introduction to¬†the‚Ä¶,3,28,"['The Case for Using Timeseries Databases', 'Time-series Databases', 'Conclusion']","This article was last updated on 1 September 2021. A plethora of new databases have evolved from relational databases based on specific business requirements and use-cases. From in-memory key-value stores to graph databases, from geospatial databases to time-series databases. All of these different types of databases serve a specific use where the general solution of using a relational database isn‚Äôt very efficient. Although there are a lot of different types of databases, here we‚Äôre going to look at time-series databases ‚Äî the databases required to handle time-series data. Data that consists of successive measurements of something over a time interval is time series data. With the modernization of financial trading and IoT's advent, the need for time-series databases is evident. Stock and cryptocurrency prices change every second. To measure this changing data and to perform analysis on that data, we need an efficient way of storing and retrieving data. With the unprecedented penetration of IoT devices in our lives, the data generated by IoT devices is increasing every day. Be it the diagnostics of your car, the temperature readings from your house, the GPS location of your dog who got lost, IoT devices are everywhere. IoT devices are made to do one thing and one thing only. Capture information through sensors on the device and send it to the server for storage. As the existing communication protocols were too complex for this kind of lightweight, high-frequency data, streaming data, MQTT was developed to solve messaging for IoT. Timeseries data can be of two types ‚Äî regular (usually measurement-based) and irregular (usually event-based). But time-series data is not limited to IoT; it penetrates the whole of the internet too. Capturing trends on search engine queries, hashtags, the virality of social media posts, and so on also generate time-series data. It doesn‚Äôt end there. Logging and auditing for security and compliance in a world driven by software are essential. All such data can also be categorized as time-series data. Timeseries databases are designed specifically to deal with the problems that arise from capturing, storing, and analyzing time-series data from one or more of the aforementioned sources. So, for the sake of simplicity, let‚Äôs define time-series data as data that has Given this information, a time-series database should be able to store a large amount of data with the capability of large-scale record scans, data analysis, and data lifecycle management. As mentioned earlier, traditional transactional databases, although you can use them to store, retrieve, and process time-series data, but that wouldn‚Äôt make the best use of the resources available. Specific problems require specific solutions. Now, as companies have realized this fact, they have started using specialized databases for solving specific problems. This brings back to what I started this post talking about. Timeseries databases, amongst all other databases, have seen a higher adoption rate in the last 2 years (data as of December 2020). The main reasons for this ~2.5 times hike in the usage of time-series databases can be attributed to the convergence of cloud & data technologies along with the ability to capture data from places where it wasn‚Äôt common to capture data from earlier, i.e., the engine of a car, your refrigerator, location data of billions of devices, and so on. Apart from the new sources, companies have also realized that some of the older sources weren‚Äôt really suited for transactional databases after all. All of this has contributed to the wider adoption of time-series databases. With the existence of time-series databases justified, let‚Äôs look into what are the different options you can go for if you want to try out time-series databases. A fuller list of time-series databases can be found on the DB-engines website. I‚Äôll talk about three of them. Marketed as PostgreSQL for Time-series, it catches your attention really quickly. PostgreSQL for anything is a compliment, by default. With new architectural constructs like hypertables and chunks, TimescaleDB boasts over 15x improvements in inserts and a substantial improvement in query performance. Read more about that here. Although there are no fully integrated solutions for TimescaleDB in the cloud with major cloud providers, just like most other time-series databases, TimescaleDB can be run seamlessly on all of them. For instance, if your infrastructure is in AWS and you don‚Äôt want to run your TimescaleDB instance in Timescale Cloud, you can either use EC2 instances to install an official TimescaleDB AMI, or you can use the AWS Elastic Kubernetes Services using the official helm charts. In the following video, Mike Freedman is talking about the need for time-series databases and how they architected TimescaleDB around PostgreSQL. Unlike TimescaleDB, which had its inspiration in PostgreSQL ‚Äî a relational database, this one is a NoSQL time-series database written from scratch. While TimescaleDB had the advantage of standing on the shoulders of widely accepted and admired relational databases, InfluxDB took a different path. InfluxDB is one of the top time-series databases but, according to TimescaleDB‚Äôs study, it fails to beat TimescaleDB in a number of areas. If you are up for an interesting read and want to install both these databases on your system to find out for yourself, head over to this interesting comparison published just today by Oleksander Bausk on his blog. bausk.dev Having said that, InfluxDB has a great set of features. Apart from the query languages InfluxQL and Flux, InfluxDB has also developed a clean, lightweight, text-based protocol that is used to write points to the database. To their credit, this has seen adoption from other time-series databases like QuestDB. Like TimescaleDB, InfluxDB also offers a cloud solution out of the box, but you can still decide to run InfluxDB on one of the cloud platforms. For instance, if you run it on AWS, you‚Äôll have native support for CloudWatch metrics, Grafana, RDS, Kinesis, and so on. All in all, a very good database. As it is fairly new, it is hard to tell how well it will compete with the more relational database-based time-series databases. A recent addition to the list of time-series databases, QuestDB has come out of one of the latest batches of YCombinator. Some of the main differentiators for QuestDB are columnar storage, low memory footprint, the use of the relational model for time-series, and scalable schemaless ingestion. Similar to most time-series databases, QuestDB also offers cloud deployment options on AWS using official AMIs and Kubernetes Helm Charts. QuestDB has also adopted InfluxDB Line Protocol for ingestion without worrying about changing the schema with the changing structure of data. Being a columnar database, QuestDB handles the creation of new columns seamlessly, and hence, supports schemaless ingestion. I recently wrote about that in another post. towardsdatascience.com Although in its early days, with almost full ANSI SQL support along with some additions to the SQL dialect, QuestDB has created a bunch of completely unique features that make it a viable alternative, possibly better than some of the other major databases available in the market. Although there are several other databases, I‚Äôve only talked about these three for now. The shift to time-series databases for time-series data is evident from publicly available data. More and more companies will start using time-series databases as part of their database stack, not necessarily replacing relational databases, but adding to their data capabilities. This is the reason why this year will be particularly exciting for not just time-series databases but all other specialized databases coming up in the market.",127,0,6,Towards Data Science,2021-01-02,2021
https://towardsdatascience.com/business-kpis-forecasting-with-python-using-prophet-part-1-1b8a97d45e3c,Forecasting Business KPIs With Python Using Prophet-Part 1,Learn how to add value to your business by forecasting future performance with¬†Prophet.,9,53,"['Forecasting Business KPIs With Python Using Prophet-Part 1', 'Introduction', 'Organization Of The Tutorial', '1. Importing Packages & Dataset', '2. Fitting a Prophet Model', '3. Visualizing Forecast Results', '4. A Quick Model Evaluation', 'Conclusion', 'You May Also Like']","Update: Many of you contacted me asking for valuable resources to learn more about time series forecasting with Python. Below I share 2 courses that I personally took and that would strongly recommend to expand your knowledge on the topic: **USE CODE JULY75 FOR A 75% DISCOUNT ON UDACITY COURSES** Hope you‚Äôll find them useful too! Now enjoy the article :D Picture this: you are a data analyst or business intelligence analyst with experience building KPIs, reporting and extracting insights on these metrics, but with little to no experience working on predictive models. At the same time your company is not just willing to track performance retroactively, but also in need for a strategic or dynamic forecast, but it turns out that there are no data scientists around the corner with a similar background. Your manager approaches you, claiming you will be perfect for the job as you have just the right background and skillset to create a simple model to forecast business KPIs, with a reminder that the forecast is due in one week‚Ä¶ Is this scenario unusual in fast-paced companies? Not really: for a data analyst to be required to work on predictive models is as likely as for a data scientist to cover data engineering tasks (like extraction, cleansing and manipulation) every now and then. Should you panic? Nope, keep it together: if you are familiar with Python or R, then FB Prophet can help you and your team to implement a simpler time series modelling approach, able to produce reliable forecasts for planning and goal setting across your business. In more detail, on its open-source projects web page, Facebook states that: ‚ÄúProphet is a procedure for forecasting time series data based on an additive model where non-linear trends are fit with yearly, weekly, and daily seasonality, plus holiday effects. It works best with time series that have strong seasonal effects and several seasons of historical data. Prophet is robust to missing data and shifts in the trend, and typically handles outliers well.‚Äù It sounds like this tool is a panacea! Then, let‚Äôs learn how to create value with Prophet in a real business context. In this tutorial we will put Prophet to the test by implementing a model to predict the following two KPIs: The tutorial will be divided into three parts: PART I(this article) is focused on how to put together an initial dataset, that will not only include the KPIs to be predicted, but also the date flags derived from business knowledge that will be used to improve the forecast in Part II. I then show how, with just a few lines of code, you can build straightforward model in Prophet. The article will also demonstrate how to visualize your predictions efficiently and neatly, by using a combination of matplotliband fbprophet built-in plotting functionalities. PART II will be dedicated to improve the original predictive model by adding tailored seasonality, holidays and by modifying some more advanced parameters. In particular, I will show how to programmatically use date flags in the original dataset to create a ‚Äúholidays‚Äù dataframe to be passed as an input for the model and elaborate on how wisely tweaking the Fourier order could enhance predictions accuracy. towardsdatascience.com PART III will be the final post of this tutorial, where I will describe how to perform a quick model evaluation using numpy or a more advanced one using the evaluation measures in statsmodels or the cross validation module natively embedded in the fbprophet package. At the end of this series, you will have a thorough understanding of how to use Prophet to forecast any key performance indicators and these skills will help you add even more value to your business. With these premises, now is finally time to start coding! The full notebook and the dataset for PART I are available here. First let‚Äôs import the packages we are going to use. This assumes that fbprophet is already successfully installed in your preferred environment: Since this article will also focus on how to properly visualize forecasts, I am going to use ‚Äúseaborn‚Äù style for matplotlib, as I find it being rather neat. If you wish to use a different one, run the following command that displays the full list of available styles: The dataset that includes the KPIs we wish to predict, has been obtained using a SQL query to aggregate the two metrics of interest (sales_num , sales_value_gbp) at the daily level and then to generate four date flags (fl_last_working_day , fl_first_working_day , fl_last_friday and fl_new_year ). The observed metrics are available for the period 2017‚Äì01‚Äì01 to 2020‚Äì11‚Äì30 but the dataset also includes dates for the entire period we wish to forecast (that runs until 2021‚Äì12‚Äì31). Displaying the first 5 rows leads to this result: As you can see all the four date flags are binary variables that can either get value 0 or 1. For example the fl_new_year is equal to 1 on the first day of each year and 0 elsewhere, whereas the fl_last_friday is equal to 1 on the last Friday of each month and 0 elsewhere and so on with the other flags‚Ä¶These flags will be used to introduce specific seasonal effects to improve the accuracy of our model in PART II, but for now let‚Äôs just plot our two KPIs and see how they look like: It is clear that both sales_num and sales_value_gbp have been growing steadily over the last three years and that they present a sharp seasonal effect around the Christmas period (despite it is less marked for Sales Value (¬£)). If we then zoom in and plot sales_num just for 2019, it appears clear that there are multiple seasonal components to take into account: Now that we understand both metrics a little bit better, let‚Äôs see how to predict their daily value until 2021‚Äì12‚Äì31. First of all, it is handy to define a number of dates we are going to use extensively in our forecasting exercise: The cutoff_date is the last date of the period that will be used to train the model ( 2017‚Äì01‚Äì01 to 2020‚Äì10‚Äì31 ), whereas the test_end_date is the last date of the period that will be used to assess the accuracy of the model (2020‚Äì11‚Äì01to 2020‚Äì11‚Äì30).In effect, as you may have noticed, the testing period overlaps at the very beginning with the actual forecast (that will instead run between 2020‚Äì11‚Äì01 to 2021‚Äì12‚Äì31) and this will allow us to compare actual values to predicted values. One last thing to define, is the number of days in the future we wish to predict (days_to_forecast) and this can easily be achieved by passing forecast_end_date and forecast_start_date to pd.timedelta().days. Next, it‚Äôs time to include the metrics we wish to forecast in a list: and then to create a dataset to train our model for each KPI in the kpis list. Note that df_train should only include two columns (the observation date and the single KPI that is selected from the list by the for loop at any given time) and that nan values should either be filtered out or replaced with zeros, depending on the specific use case. Moreover, fbprophet requires for the columns in the train dataset to be renamed according to a convention where the observation date becomes ds and the metric to model becomes y : Everything is in place now to create our model and fit it using the training dataset. This can easily be achieved by adding the following code to the loop: If this is the first time for you using Prophet, you may be wondering the meaning of the parameters specified in the model. To understand how they should be properly used, it‚Äôs important to highlight that the Prophet() forecaster function already comes with the following default arguments (in a Jupyter Notebook, they can be displayed by placing the cursor in the middle of the parenthesis and then typing shift + tab + tab): This means that using a combination of data exploration and business knowledge we were able to assess that our model should be built by taking into account: Now that we know how to tweak basic parameters, we can finally predict the future values of our KPIs. In order to do that, the first step is to create a dataset including observed data as well as future dates. In ours case days_to_forecast corresponds to a period of 426 days in the future: Then it‚Äôs time to run model.predict() on future_data to obtain the final forecast. As you can see, we only select the four most relevant columns (namely observed date ds , predicted value yhat , uncertainty interval lower yhat_lower and upper yhat_upper bounds and the underlaying trend ): When displayed, the first and the last five rows of the forecast dataset for sales_num looks like: *Note that while predicting multiple KPIs with a loop, the last forecast available in this dataset will be the one computed for the last metric in the kpis list. You may have noticed that Prophet provided estimated values for both observed dates and future dates, meaning that forecast includes a prediction for every single day from 2017‚Äì01‚Äì01 to 2021‚Äì12‚Äì31. This is a very relevant detail to keep in mind, particularly for the next section, where we will visualize the forecasts. For the time being, we wish instead to keep the original observed value in the df dataframe and replace NaN values with predicted values. This can be achieved with this line of code: In this section, I am going to present three types of visualizations I find particularly useful while assessing the performance of a model built in Prophet. In effect, plotting observed and predicted values makes comparing models (that use different parameters) much more straightforward and intuitive. While working with fbprophet, you can simply plot a forecast by running: Despite the plotting tool embedded in the package may work well for a very high level analysis, it is quite simplistic and difficult to tweak. These limitations make it hard to sell on the workplace. However, an interesting chart you can build directly with Prophet, is the one displaying the trend changepoints over time: In the plot above, the black dots indicate the observed values, whereas the blue line represent the predicted values. As mentioned, predicted values are calculated for the entire dataset when model.predict(future) data is run. The number of displayed changepoints and the shape of the piecewise underlying trend will change depending on the value assigned to the changepoint_range (default is 0.8) and changepoint_prior_scale (default is 0.05) parameters as they influence directly the flexibility of the model against trend changepoints. A more intuitive way to compare predicted values against actual (observed) values would be to overlap both time series using different colors and opacities. To achieve that, we need a dataframe where actual and predicted values are saved in different columns: This means that combined_df should be created before replacing NaN values with forecast values for future dates in df that has previously been obtained as follows: That is because we will use matplotlibto overlap two different columns as shown below: We can now clearly see that despite the current model was able to fit the underlying trend pretty well, it still behaves poorly during peak days or particular holidays (like the Christmas period). In PART II we will learn how to implement model that includes holidays to make it much more accurate. At some point, you may wish to present this model to your colleagues and the visualization above could still look rather confusing to a less technical audience. It would probably be better to completely separate the two time series, showing actuals first and predicted values after, including the uncertainty interval: It must be highlighted that actuals and predicted values still overlap in between 2020‚Äì11‚Äì01 and 2020‚Äì11‚Äì30 that, you would remember, is the one-month period selected to test the accuracy of the model. In the last section of this tutorial, we will use the Mean Absolute Percentage Error (MAPE) to compute the model performance with a single value. There are many other metrics that can be used to assess the quality of a predictive model (PART III will cover the topic in depth) but for the time being a single matric is probably more intuitive. To compute MAPE, we first create a combined_df_test that uses the combined_df as an input but just for the month of November 2020. Then we use numpy to write our own formula for mape: Running the code above, we get a MAPE of 6.73% indicating that across the predicted points, the forecast is on average 6.73% off against actual values. This is already a pretty good result for a semi-out-of-the-box model, but we will try to noticeably lower MAPE in PART II. This is the end of the first tutorial about business KPIs forecasting with Prophet, I hope you have enjoyed it! If you followed along, you should have enough material to start creating value for your business by building a model to predict future performance. But we are not done here yet: in PART II you will learn how to build a more complex model using your business knowledge to pass special events or seasonal components as model variables, whereas in PART III you will learn how to employ cross validation and other evaluation metrics to choose among different models. So I see you there and keep learning! A note for the reader: This post includes affiliate links for which I may make a small commission at no extra cost to you should, you make a purchase. towardsdatascience.com medium.com towardsdatascience.com towardsdatascience.com",,0,13,Towards Data Science,2021-01-02,2021
https://towardsdatascience.com/5-common-resume-mistakes-831e8b3e360d,5 Common Resume¬†Mistakes,From reviewing hundreds of data science¬†resumes,7,16,"['5 Common Resume Mistakes', '1. Using a non-standard resume template', '2. Adding too much fluff', '3. Listing Skills When You Shouldn‚Äôt', '4. Not Making Room for Non-Standard Experience', '5. Relying Too Much on School Projects', '6. Bonus: A Basically Empty GitHub']","I remember graduating from school, putting together a resume, and constantly wondering if my resume was ‚Äúgood.‚Äù It can be so hard to know what makes a resume good, but after reading hundreds of data science resumes, I wanted to point out 5 mistakes I see all the time. If you can avoid these mistakes, you will have made great progress in getting your resume to a good spot. So ‚Äî here are 5 common resume mistakes: I‚Äôve found that it is increasingly common for people to try to make their resume different. Things like skill charts or personal touches, while they look cool, are often distracting. Business Insider did a comparison of 2 resume formats and tracked recruiters‚Äô eye movements. Here are the results: The resume on the right, with a more standard and clear format, was more thoroughly observed. Recruiters have to analyze hundreds of resumes. You want your resume to feel as familiar as possible so recruiters can find the information they are looking for quickly and efficiently. When I graduated, I had minimal experience, and I remember feeling like I had to do everything I could to make my resume feel ‚Äúfull.‚Äù I guess my hope was I would somehow look more experienced if my resume was denser. I now realize that was a mistake. Again ‚Äî recruiters and hiring managers don‚Äôt have a lot of time. They don‚Äôt want to read about unrelated jobs or maybe less inspiring job experience. You should make sure your resume focuses as clearly as possible on what you are most proud of ‚Äî what you think makes you the perfect fit for your job. Too often, I finish reading resumes and nothing unique really stood out to me. Make sure you remove the fluff from your resume so that even when read very quickly it is evident to a hiring manager what makes you a fantastic fit for the role. In my opinion, this means you should never have a resume that is longer than 1 page. Even with decades of experience, I think people can summarize what really needs to be known in a single page. I get it ‚Äî you took a class and did some C++ for a project. The job mentioned C++ as a beneficial skill to have, so why not add it to your resume? Why not add all the programming languages you have ever programmed ‚ÄúHello World‚Äù in? While often not that extreme, be careful making a laundry list of technical skills for which you have a very basic knowledge. Anything listed on your resume is free game for questions and it looks pretty bad when your C++ knowledge turns out to be limited to a few lines of code. So, try to be honest with yourself and only add skills for which you feel decently proficient. This isn‚Äôt to say you have to be an expert in everything you list, but you should be comfortable fielding interview questions related to your skills. Sometimes when interviewing candidates experiences come out that are super relevant, but were not on their resume. Usually, this is because the experience didn‚Äôt fit well into previous work experience or education. For example, maybe a side-project or open-source contributions. Make sure you make room for the experiences you feel have prepared you to be a great fit for the job. As mentioned in point 2, remove the fluff, and make room for these non-standard experiences you have. They will hopefully make you stand out among all the other resumes. School projects are awesome and can help you learn a ton. Unfortunately, they tend to pretty boiler-plate and the real downside is that many other people have done them as well. If you have a bunch of school projects listed on your resume that you know many other people have done as well, it‚Äôs likely recruiters have seen them before, and basically just glaze over them when reading your resume. A strong GitHub account is an amazing thing to list on your resume. I love looking at candidates‚Äô GitHub profiles because I can instantly see examples of code and projects they have worked on. I strongly recommend people find time to have a few projects on GitHub they are proud of ‚Äî it can go a long way to help you stand out. That being said, listing a basically empty GitHub account isn‚Äôt a great idea. If you have one push for your one school project, that actually isn‚Äôt very much code, and might not be code you‚Äôre proud of, maybe skip putting your GitHub on your resume. If you‚Äôre GitHub is empty, definitely skip it (you would be surprised how often I see this). I hope this was helpful for you to improve your resumes! Get your Free Guide to creating an amazing data science project.",261,1,4,Towards Data Science,2021-01-02,2021
https://towardsdatascience.com/measuring-enhancing-image-quality-attributes-234b0f250e10,Measuring & enhancing image quality attributes,Before starting our discussion about measuring or‚Ä¶,3,30,"['Measuring and enhancing image quality attributes', 'Measuring attributes', 'Enhancing attributes']","Before starting our discussion about measuring or enhancing image quality attributes, we have to first properly introduce them. For this, I‚Äôve taken inspiration from the book Camera Image Quality Benchmarking, which describes in great detail the attributes that I will be speaking about here. It‚Äôs important to note that, although the attributes described in the book are camera attributes, our discussion is centered around image attributes. Fortunately, a couple of camera attributes can be used as image attributes as well. Usually referring to the time of exposure, which is a property of the camera that affects the amount of light in an image. The corresponding image attribute is actually the brightness. There are multiple ways to compute the brightness or an equivalent measure: If we do the average for all the pixels, we can obtain a measure of perceived brightness. Also, by splitting the resulting value into five pieces (because the min is 0 and the max is 255) we can define a scale: (Very dark, dark, Normal, Bright, Very Bright). High-dynamic-range imaging (HDRI or HDR) is a technique used in imaging and photography to reproduce a greater dynamic range of luminosity than is possible with standard digital imaging or photographic techniques. While the human eye can adjust to a wide range of light conditions, most imaging devices use 8-bits per channel, so we are limited to only 256 levels. HDR imaging works with images that use more than 8 bits per channel (usually 32-bit float values), allowing a much wider dynamic range. What is tone mapping? There are different ways to obtain HDR images, but the most common one is to use photographs of the scene taken with different exposure values. To combine these exposures it is useful to know your camera‚Äôs response function and there are algorithms to estimate it. After the HDR image has been merged, it has to be converted back to 8-bit to view it on usual displays. This process is called tone mapping. Measuring if an image is well tone-mapped From the above definition, I propose (so it‚Äôs possible it‚Äôs totally wrong) the following procedure for measuring tone mapping. The intuition behind this comes from the way histograms look when the images are not properly tone mapped. Most of the time they look like this: They are either too dark (shadow clipping), too bright (highlights clipping), or with both (for example a dark bathroom with the blitz visible in the mirror or a photo of a light pole in the middle of the night). In contrast, a well tone mapped image looks like this: Based on this, I propose (so take it with a grain of salt) a scoring method that tries to take into account the things described above. The score will be between [0, 1], 0 meaning the image is not correctly tone-mapped, and 1 that it is correctly tone-mapped. Besides the saturation effect, a poorly tone-mapped image might also be an image that has the majority of the brightness values in a tight interval (small variance => fewer available tones). 4. We define a parabolic penalizing probability distribution, that‚Äôs 0 in 0 and 1 with a maximum in 1/2 (This should be pretty fine as long as we penalize the extremes ‚Äî thus low scores <=> the majority of the brightness was concentrated in the head and tail of the distribution). (Note: This is actually a simple example of a Bernoulli distribution, a good thing to use as a prior probability distribution). 5. Next, we can define the ‚Äúpenalized‚Äù brightness probability distribution as ‚Äã. The only thing left is to properly limit this product‚Ä¶between 0 and 1. The first part is already solved‚Ä¶the minimum of this product, for all the values of ‚Äã is 0. That‚Äôs because we can define a black and white image that‚Äôs with the following probability distribution: We can see because f is not 0 only at 0 and 255 the sum over all the pixels in the example image will be 0‚Äã. Any other configuration would result in a sum that‚Äôs greater than 0. To make the sum at most 1 we can use a high school trick, via the CBS inequality. In general: In our case, that would be: If we divide the left part with the right part we finally get a score that‚Äôs between 0 and 1. Thus the final form of the first term is: I don‚Äôt know why, but it resembles a lot with Pearson‚Äôs correlation factor‚Ä¶ ü§î The next term I would simply define as: In the end, we get the following tone mapping score: Now, let‚Äôs see some code as well: Because, the blurred image‚Äôs edge is smoothed, so the variance is small. It's a one-liner in OpenCV, simply code üé®: (https://stackoverflow.com/questions/48319918/whats-the-theory-behind-computing-variance-of-an-image). The OpenCV docs have a nice tutorial on this, High Dynamic Range (HDR). For brevity‚Äôs sake, I‚Äôm putting here only the results obtained with Debevec‚Äôs algorithm (http://www.pauldebevec.com/Research/HDR/debevec-siggraph97.pdf). The end result: Finding flares reduces to the problem of finding very bright regions in the image. I haven‚Äôt found a specific method of finding if an image has a flare, only for correcting one: The method is called CLAHE (Contrast Limited Adaptive Histogram Equalization). Before speaking about CLAHE, it‚Äôs good to know why Histogram Equalization does NOT work: While the background contrast has improved after histogram equalization, the face of the statue became too bright. Because of this, a local version is preferred and thus, adaptive histogram equalization is used. In this, the image is divided into small blocks called ‚Äútiles‚Äù (tile size is 8x8 by default in OpenCV). Then each of these blocks is histogram equalized as usual. So in a small area, a histogram would confine to a small region (unless there is noise). If the noise is there, it will be amplified. To avoid this, contrast limiting is applied. More on histogram equalization on the OpenCV docs (https://docs.opencv.org/3.1.0/d5/daf/tutorial_py_histogram_equalization.html).",19,0,8,Towards Data Science,2021-01-02,2021
https://towardsdatascience.com/setting-up-python-environment-using-github-actions-9a81936be5c9,Setting up python environment using Github¬†actions.,This article aims to automate the machine learning‚Ä¶,7,29,"['How to Configure Github Actions the Easy Way.', 'Introduction', 'Prerequisites:', ""What are GitHub's actions?"", 'Set-up the yml file to use a specific version of python', 'Testing: pushing code to the Github repo', 'Conclusions and takeaways']","In my previous article, I walked you through a step by step tutorial to correctly setting up the python3 environment using pyenv on your local machine. However, some issues may arise in some circumstances due to some conflicts resulting from pull requests that use different python versions or even missing modules. This tutorial will demonstrate the steps to build a standard workflow for any project utilizing Github actions plugins. Configuring Github actions in the project CI folder would protect the codebase pull requests that don‚Äôt meet the code standards regarding code formatting, syntax error, and version conflict. So, let‚Äôs get started! Github actions are plugins that automate and manage the project workflow complete development life cycle. It is similar to Circleci, Travis, and Gitlab in functionality; however, it is free if the project repository is public. Workflows for GitHub actions are a series of steps or directions written in the yml file that run every time you push to a specific branch on Github. Setting up a workflow is a timesaver in the long run, where you can have peace of mind during pushing/merging code. The workflow could be customized to check against different tests stated as steps on the yml file. These checks could be using the correct version of the coding language, install dependence, or even check against code formatting and syntax error. This way, you could guarantee to be less anxious about merging code into the main branch ‚Äî at least nothing would break! You might have multiple workflows, and each one might have multiple jobs. The workflow is triggered every time, pushing to a specific branch. In this guide, I will use the following: Github actions could be configured either on existing repos or creating one from scratch. For the sake of simplicity, I would start a new one from scratch; navigate to GitHub, and create a new repo and name it; I will name it github_actions . Then clone the repo into your local machine and create a GitHub workflow action file on .github/workflows/main.yml ‚Äî this where the steps would be defined. You could use the github workflows template by heading to the action tap and trigger the yml file. You might include a requirements.txt file by running the following command in the terminal: Each workflow might have a bunch of jobs, and each job contains several steps to execute. I am using the pyenv-action action developed by Gabrial. This action would do the following: Open the main.yml file and start configuring pyenv and pick the right python version. The following snippet is installing pyenv using the pyenv-action version 7, then upgrading the pip after picking version 3.8.6 and set it as the default. Following that, create a virtual environment with the default version and install project dependencies. It has a successful build, and a green health check would appear associated with the commit id, as demonstrated above. It is the same as the previous step; however, we will define the Postgres database connection in the services section using the Postgres docker image. Some health checks were configured to wait until Postgres started. Finally, port 5432 was mapped from the service container to the host. The db_service service container has a successful build üòè. Utilizing pylint would check every code snippet pushed to the master branch against errors and enforce a coding standard. Pylint is a tool that checks for errors in Python code, tries to enforce a coding standard and looks for code smells. It can also look for certain type errors, it can recommend suggestions about how particular blocks can be refactored and can offer you details about the code‚Äôs complexity. ‚Äî pylint 2.6.1-dev1 We could install pylint after installing the requirements and run pylint as a next step The final main.yml file might look like the following: Now it is time to add some code and try to push it to the GitHub repository. I started by creating a demo file and developed a method for generating random sampling from an input array of distinct elements and size. All subsets should be equally likely. The next step would be committing and pushing the Github repository changes using a pull request or direct push to master. Once the changes are made to the remote, different tests would be run against the changes before merging the changes into master. As shown in the above image, there was an error generating because of inadequate formatting according to the pylint package; some of the errors were because I didn‚Äôt follow the snake_case naming conventions. As you can see, the pushed code had a rate of 4.29/10. If you want to ignore the pylint checks for docstring, you could create a newfile named .pylint and add the following snippet into it to ignore docstring module. [MASTER]disable= C0114, # missing-module-docstring Now commit and push the Github repository changes after creating the pylint file, the build should be successful, and a healthy green check would appear near the repo üòé. Github action is a powerful plugin that empowers the process of automating the machine learning development cycle. You could add more steps and jobs to the workflow, including deployment to different platforms, such as Iaas, Paas, Saas, and onsite premises. Also, it allows you to use docker containers as well as Kubernetes. There are plenty of GitHub-actions built by the community‚Äôs marketplace; you can even create your customized action if you have a programming background or follow the documentation. Some of the references that helped me to configure this workflow are: Finally, I hope this provided a comprehensive guide to utilizing the GitHub actions to automate machine learning projects. If you followed along and have a question or suggestion, please post them in the comment section below; I would be more than happy to help. Thanks for reading and happy learningüëç",149,0,6,Towards Data Science,2021-01-02,2021
https://towardsdatascience.com/online-masters-in-data-science-worth-taking-the-plunge-ec5c2a53048d,Online Master‚Äôs in Data Science‚Ää‚Äî‚ÄäWorth taking the¬†plunge?,How long it takes per course and¬†task‚Ä¶,7,20,"['Online Master‚Äôs in Data Science ‚Äî Worth taking the plunge?', 'Background', 'Data', 'Results', 'Discussion', 'Conclusion', 'Appendix / References']","In short, I took an online Master of Computer Science in Data Science offered through the University of Illinois part time over 3 years. I required a total of 8 courses to graduate and I took one course per semester (3 per year). Each course had lectures and assignments while some had final exams, mid terms, projects and quizzes. I‚Äôll be giving my own personal experience completing the degree. If you are considering pursuing an online Master of Computer Science in Data Science and would like to know how I found it, feel free to ask in the comments ‚Äî I would be happy to answer best I can. Regardless, I found the program VERY helpful üëç and I learned a great deal. It really is what you make of it in terms of how deep you want to go into a topic. The only caveat Ô∏èI would mention is that if you want to get into deep learning specifically or you have a very specific technical position in mind then I would suggest going into a more tailored boot camp type of program and not doing a formal master‚Äôs degree‚ö†Ô∏è. The reason I suggest that is because this degree did not go into great detail on the state of the art of neural networks but just covered the basics, most of which you could find in a good tutorial online. However, if you want to know the foundational stats and algorithms behind these concepts that you will use throughout your whole career then a program like this is what you are looking for. The data used for this analysis was gathered by my fellow student manually. He carefully documented how long it took him to complete each assignment, lecture, exam, mid term, quiz and project throughout his whole journey. So huge kudos to Paul for collecting all this! üôå. I have his consent to publish the results. Overall, the degree took approximately 1,133 hours to complete. Eight courses were required, and I assumed a 12 week term in calculating the hours per week. Here is the same data broken down by Task Type. Not all courses had final exams, mid-terms, projects or quizzes. However, all courses had assignments and lectures. Plotting hours spent doing assignments versus hours spent in lecture with a 30th percentile horizontal line we see that most of courses had over 20 total hours of lectures. Distributed Systems took close to 40 hours of lecture time while Data Cleaning took less than 10 true hours of lecture. This will be discussed later. Conversely, with an 80th percentile vertical line we see that the majority of courses had less than 90 total hours of assignments. Applied Machine Learning course is a clear outlier üìä. That specific course had close to 170 hours of assignments. Why Applied Machine Learning had significantly more time spent on assignments will be discussed later. For fun, I trained a clustering algorithm where the number of clusters does not need to be specified like OPTICS or DBSCAN and got the chart below. Each colour dot corresponds to a specific cluster. Applied Machine Learning (AML) and Practical Statistical Learning (PSL) seem to be in their own clusters. Firstly, the disparity between time spent completing certain courses, for example Data Cleaning versus Practical Statistical Learning (PSL) is directly related to the complexity of the content. Specifically, Data Cleaning mainly covered theories such as data flows, database design and RegEx which are generally known by folks in the analytics profession already. Therefore, most of the videos can be watched on 2x speed which is why the hours spent in lecture is comparatively low. It does not mean that there are only 10 hours of lecture üòå. PSL went into the guts of how specific clustering, classification and regression models worked. These more complex concepts called for re-watching of the lectures multiple times to fully understand grasp the topics to be able to earn 100% on the quizzes. Moreover, there were four projects in PSL in addition to all the other assignments and quizzes. üí™ Both courses required skills in either R or Python however they covered totally different areas. Secondly, we saw that Applied Machine Learning course required a VERY high time investment on assignments. There was no final, no quizzes and no project just A LOT of assignments. Personally, I LOVED this course because each assignment felt like a mini-Project and you had no idea how you were going to figure it out in time üòÖ. But in the end you did and it was great! Thirdly, I consider Data Visualization and Data cleaning the ‚Äúeasy‚Äù courses. You could take two or three other courses in the same semester while working full time and not go completely insane üò§. I did not do that but many of my other classmates did. If you are considering pursuing an online Master of Computer Science / Data Science degree then I would highly recommend it. This degree program was very technical compared to similar programs that include law, privacy and research design courses in their curriculum. I feel that a more technical degree translates better to an online format compared to master‚Äôs degrees that focus more on social/qualitative courses for obvious reasons. The degree was still a lot of work as you can see from the hours spent üò≠. The silver lining is that not all courses take up every waking moment of your life to complete. There are ‚Äúeasy‚Äù üò∫ones and there are killer üíÄones, as long as you know what you are getting yourself into you can come out at the end of it a better person. Lastly, I want to also mention that you should take into account the opportunity cost of doing other things for those 1,133 hours you will be spending on your master‚Äôs degree. The good thing with an online technical masters compared to an MBA style masters is that they are typically significantly cheaper üí∞. I hope these data points provide you with a better idea of how long it takes to complete a master‚Äôs degree. Please spend some time digesting the visuals above, there are lot of insights to gain just from the bar chart. [1] Paul Nel. Some Stats for the MCS-DS Program at UIUC. (2019) [2] GitHub with PBIX that includes aggregated and cleaned data. https://github.com/mattbitter/Medium_MatthewBitter_Grad_Article",34,4,6,Towards Data Science,2021-01-02,2021
https://towardsdatascience.com/9-examples-to-master-seaborn-grids-1935bb38014c,9 Examples to Master Seaborn¬†Grids,Practical guide for FacetGrid and¬†PairGrid,1,33,['9 Examples to Master Seaborn Grids'],"Data visualization is a fundamental piece of data analysis. It helps us better understand the relationships in the data and explore the underlying structure. We can create more informative visualizations by combining multiple plots in the same figure. There are many ways to create multi-plot visualizations. Seaborn library makes it simple and straightforward to generate such plots using the FacetGrid and PairGrid classes. In this article, we will go over 9 examples to practice how to use these function. We will start with very basic ones and steadily increase the complexity. For the examples, we will be using a customer churn dataset available on Kaggle. We start with importing the necessary libraries. The next step is to read the dataset into a Pandas dataframe. I have selected some of the columns in the original dataset and also done some filtering and sampling for demonstration purposes. FacetGrid is a grid of subplots which allows for transferring the structure of the dataset to the subplots. Row, col, and hue parameters can be considered as the three dimensions of FacetGrid objects. We first create a FacetGrid object and then map the data. Let‚Äôs start with a very simple example of creating the structure of a FacetGrid. We now have an empty plot that only represents the structure. Seaborn generates the structure based on the values in the columns passed to the col and row parameters. Since the attrition flag column has two unique values, a grid with two columns is returned. The height and aspect parameters adjust the size of subplots. Once we have a FacetGrid object, we can map data to it. The map method takes a plotting function and variables to plot as argument. The grid above shows the distribution of the customer age column using a histogram. The data points are separated according to the categories in attrition flag column. The plotting function passed to the map method does not have to be a Seaborn function. We can also use matplotlib functions. For instance, the plot above can be created with ‚Äúplt.hist‚Äù. Note: Seaborn also accepts custom functions to use for mapping. However, there are certain rules you must follow when creating them. We have only used the col parameter so far. We can use the row and hue parameters to add more dimensions. We have a grid of scatter plots that show the relationship between two numerical columns. We are able to demonstrate relationship separately for categories in the attrition flag, gender, and marital status columns. When we use the hue parameter, the legend should also be added using the add_legend function. We can specify the order of the categories represented by the subplots. The row_order and col_order parameters can be used to order. As you can also notice from the plots, the categories are sorted by size. We have used the value_counts function of Pandas to generate an order for the categories. PairGrid generates a grid of plots that visualize the pairwise relationships of variables. For instance, we can create a grid of scatter plots between some numerical variables. In the previous example, the plots on the diagonal are useless because they show a scatter plot of a variable with itself. In order to make the grid more informative, we can plot histograms of variables on the diagonal. We pass separate functions by using the map_diag and map_offdiag methods. It is better than the previous one as we also get an overview of the distribution of each variable. The PairGrid also supports the hue parameter so we can separate the data points in the scatter plot based on a categorical variable. Another useful parameter is the var parameter. In the previous examples for PairGrid, we used a subset of the dataframe so it only included the columns to be plotted. We can also pass a list of columns to be plotted to the var parameter. In a PairGrid, the plots on the upper and lower side of diagonal are mirror images. Thus, we have the same plot from a different perspective. We have the option to have different kind of plots on the upper and lower side of the diagonal. The map_upper and map_lower functions are used to generate different kinds of plots for the upper and lower side. On the upper side, we have the scatter plots. The diagonal contains the histograms of each variable. On the lower side, we have two-dimensional histograms. We have practiced how to create multi-plot visualizations with the FacetGrid and PairGrid of Seaborn. They are very useful tools for exploratory data analysis. The way Seaborn generates these plots makes them simple and easy to understand. It is important to understand the differences between a FacetGrid and PairGrid. In a FacetGrid, each subplot represents the same relationship but under different conditions. For instance, we can have a scatter plot of two variables in a FacetGrid and separate the data points based on the categories of another variables. In the PairGrid, each plot shows a different relationship. For instance, when we create a PairGrid of scatter plots with three variables, each subplot represents a different pairwise relationship. There are many more features that can be added on FacetGrid and PairGrid objects in order to enrich both the functionality and appearance. Once you are comfortable with the basic ones, you can create more detailed grids. Thank you for reading. Please let me know if you have any feedback.",78,1,6,Towards Data Science,2021-01-02,2021
https://towardsdatascience.com/building-data-pipelines-using-r-d9883cbc15c6,Building Data Pipelines using¬†R,An example of how to consume data files in R using a data pipeline‚Ä¶,9,48,"['Building Data Pipelines using R', 'The dataset', 'Importing it into R', 'Starting with Column Names', 'Automating Filtering', 'Correcting the Year Column', 'Automating Column Selection', 'Converting our data to Numeric', 'Conclusion']","If you work as a data analyst, the probability that you‚Äôve came across a dataset that caused you a lot of trouble due to it‚Äôs size or complexity is high. Most data analysts today rely on a combination of several visualization and spreadsheet tools that help them make sense of the data around them, but the ‚Äúcurse of scattered files‚Äù still stands ‚Äîparticularly in big companies. But, as we leave behind the first two decades of the millenium, we witness a huge growth in the creation of new data sources ‚Äî not only data analysts need to make sense of data that is produced within the organization (and with organizations trying to be more data-savyy, the amount of data produced and stored grows exponentially) but sometimes they are asked to make sense of external data, extraneous to the company. This diversity asks for new ways to approach new problems that are not solvable with old tools. Sure, Microsoft Excel is one of the best tools to analyze data due to it‚Äôs democratization and usability, but as soon as you pass a certain amount of rows your ability to gather insights and make sense of raw data gets pretty limited. Analyzing large files with Excel has two main problems: It‚Äôs not that Excel is a bad tool (I really think it is great), it‚Äôs just that it‚Äôs not tailored for large files ‚Äî and yet, most people, particularly in large companies, still rely on it to do every kind of data tasks. Data visualization and self service Business Intelligence tools have been tackling this problem in some way ‚Äî but to have the tools need you are overly dependent on budget or politics. Luckily, the adoption of open source tools surged during the past decade (particularly in the data analytics space where R and Python shine) as, regardless of having a huge community, they have a smooth learning curve and are pretty accessible for most people. Being able to code in these systems may enable analysts to improve their overall productivity and capacity of analyzing data ‚Äî but, sometimes they get frustrated when they can‚Äôt load spreadsheet files properly into R or Python and end up sticking with spreadsheet software. In this article we are going to explore how we can build and think of functions that prepare our spreadsheets files for analysis ‚Äî the flow of the pipeline is the same that I teach on my R Programming Course on Udemy We are going to use the FBI Crime Data Set that has this general look: As we can see, we have several columns that convey almost the same information such as nominal values and rates. If we try to ingest this data directly into R we are probably going to get some weird stuff due to some additional information we have on the columns (references, for example) as any tool will have a difficult time to target the ‚Äúreal data‚Äù in this spreadsheet (and remember that Excel should have this information as it is a user-facing tool). As an example, the footnote will probably be included as a row somewhere in your data ‚Äî this makes your data really hard to analyze ‚Äî but you can fix most of these problems with a bit of data wrangling skills! To load this Excel file into R we will rely on the readxl (https://readxl.tidyverse.org/) R library of the tidyverse package: Looking at the head of the file that we just loaded: Ugh, look at that! Our first two rows are probably useless and the third row contains our column names. We also have some problems with some Years (look at row index #8 with the year 20015 ‚Äî this happened as the superscript 5 was read as a normal number). The bottom of the table does not look good, also: Some of the descriptions and metadata we had on the excel sheet ended up being read as rows ‚Äî this metadata is useful for someone that is looking to the Excel file but not for someone that is trying to analyze this data consistently on a data analysis tool. These are common data wrangling problems that data scientists and analysts face each time they want to analyze a new data set ‚Äî particularly from a less structured source. We have some work on our hands, so let‚Äôs get this done! This one is pretty easy ‚Äî our column names are on the third row, so we can just set them up using the colnames function: Our general table gets this look: To build interesting and robust data pipelines we have to come up with systematic rules that can convey future changes in the possible files that we will be passing through our pipeline. We could definitely subset the years by using a vector to subset the first column in the dataset and looking at the values 1997, 1998‚Ä¶ 2016 (the latest year we have on this file) but what if someone gives us the file with data until 2017? or a file that contains 1996? If we had our years hard-coded into the pipeline, we would be building hard-coded rules ‚Äî these are never a good option because they only apply to this file and this file only (or a file with exactly the same structure). With a new file that would contain data for more than 20 years (the ones that are available in our table) your pipeline would not be able to filter them ‚Äî so by looking at the file below, can you come up with some rule that make it possible to subset the table, no matter the number of rows with yearly information? One nice rule we can come up with is to only get the rows that have a value that can be converted to numeric in the first column. This is an example of a good type of a ‚Äúdata pipeline rule‚Äù as it prevents errors or loss of information in the future. If we try to convert the first column of this dataset into a numeric type, elements that are not numbers will be returned as NA‚Äôs (Not available) ‚Äî we can then filter those NA‚Äôs out, efficiently retaining the rows that we are interested in‚Äî we will rely on sapply to convert our first column into numeric ‚Äî we will create a new column: Our new converted_index column will have the following look: Notice how the NA‚Äôs were introduced by coercion when R couldn‚Äôt convert our column to numeric . Filtering these rows out will give us the following table ‚Äî short summary of the first 11 rows and first 9 columns: How did we filter these NA‚Äôs out? We were able to achieve that with the following code: With this rule we select every row of our dataframe where our last column is not NA ‚Äî as our converted_index column was created inside the pipeline it would also be safe to do the following (which yields the same result): In this case, having the column converted_index hard-coded would not be problematic ‚Äî as the converted index was created inside our pipeline and the rules that created that column are flexible to the input, there‚Äôs less risk. We still need to get rid of the weird values like 20015 ‚Äî this happened because R read the superscript 5 that points to a reference in the excel file (and that makes sense, in that system) as a number. Luckily, the Year Column is still a character so we are able to apply substring directly and only retrieve the first 4 ‚Äúletters‚Äù (they are numbers but R treats them as a string because of the type of the data) of each year: Notice how I am rewriting the Year column with only the first 4 digits of each Year ‚Äî the extra digits that were wrongly assumed by R as part of the Year are left out, as we want. We are able to get the following data frame: As we have seen columns convey the same information (rate is just a division of the original column by a million inhabitants) ‚Äî as we have done with row filtering, we can also do some automated column filtering. Can you think of a rule that will be able to select the non-rate columns in the table, without relying on hard-coded rules? Let‚Äôs inspect the names of our columns with: This is the output: Looking closely, every column that has rate in its name should be removed from our table. Instead of hardcoding the names or indexes of the columns we can rely on the grepl function. Using: This function will returns us a vector with TRUE and FALSE ‚Äî TRUE in the indexes where the column name contains ‚Äúrate‚Äù and FALSE otherwise: We can now rely on this vector to filter out these columns of our data frame ‚Äî using indexers. We will also rewrite our original object: We‚Äôre almost done! Let‚Äôs look at our crime_data_filter structure: Most of our columns are still characters(a problem that migrated since the beginning of the pipeline as we had blank cells in the excel that made R assume this was a character column ) even if we only see numbers in it! Let‚Äôs convert these columns to numeric ‚Äî we can‚Äôt apply as.numeric directly to our object (we could only do this to Vectors, Matrixes or Arrays) ‚Äî we need to rely on our apply family of functions! As sapply returns a list, we will wrap this function on a data.frame function to get a data frame. Now that we have a really clean table and able to be analyzed ‚Äî we can encapsulate every instruction we have done in a function that can be reused: In conclusion, knowing how to build data pipelines is an essential task of analyzing data in systems that rely on code ‚Äî particularly in a world where more and more professionals are acessing data using R and Python, it‚Äôs incredibly important to understand how to build error-proof rules in reading data files. This lecture is taken from my R Programming course available on the Udemy platform ‚Äî the course is suitable for beginners and people that want to learn the fundamentals of R Programming. The course also contains more than 50 coding exercises that enables you to practice as you learn new concepts.",53,2,9,Towards Data Science,2021-01-02,2021
https://towardsdatascience.com/manifold-clustering-in-the-embedding-space-using-umap-and-gmm-dbab26a9efba,Manifold clustering in the embedding space using UMAP and¬†GMM,How to reduce the dimensionality of‚Ä¶,11,36,"['Manifold clustering in the embedding space using UMAP and GMM', 'Dimensionality reduction via Uniform Manifold Approximation and Projection (UMAP)', 'Clustering with Gaussian Mixture Model (GMM)', 'GMM parameters', 'Model selection using BIC and Silhouette scores', 'Visualizing the clusters', 'Comparison with the COCO taxonomy', 'Predicting COCO annotations via supervised learning', 'Clusters similarity and consistency', 'Pictures in the cluster', 'Conclusions']","In the previous article Extracting rich embedding features from pictures using PyTorch and ResNeXt-WSL we have seen how to represent pictures into a multi-dimensional numerical embedding space. We have also seen the effectiveness of the embedding space to represent similar pictures closely to each other. In this tutorial, we will see a few clustering techniques that are suitable for discovering and identifying the manifolds in our dataset. Moreover, the proposed clustering technique is also motivated by the ‚Äúdocument embedding averaging‚Äù that will be described in the next article. Dimensionality reduction is not just used for data visualization, but it is a fundamental step for clustering algorithms due to the ‚Äú curse of dimensionality ‚Äú. In other words, if the number of dimensions increases then most of the points will start to look as similar and as different from each other across at least a few of those dimensions. The effect is that there is no clear structure to follow resulting in a random grouping of the data points. The unsupervised dimensionality reduction techniques are divided into two families: Linear Projection and Manifold Learning. The main difference of manifold learning with linear projections (e.g. PCA, SVD) is that it can handle non-linear relationships in the data and it is very effective for clustering groups of similar data points preserving their relative proximities. For our purposes and given the nature of our data (the embedding space generated by a deep convolutional neural network), we do not consider any linear projection technique as suitable. In the manifold learning family, there are two main competitors: t-SNE and UMAP. t-SNE vs UMAP 3d projection of COCO detection 2017 pictures colored by supercategory Uniform Manifold Approximation and Projection (UMAP) is a general-purpose manifold learning and dimension reduction algorithm.T-distributed Stochastic Neighbour Embedding (t-SNE) is an algorithm that generates a low-dimensional graph trying to keep similar instances close and dissimilar instances apart. They sound similar, and in fact from a lot of aspects they are. Nevertheless, let‚Äôs summarize a few reasons to prefer UMAP over t-SNE for clustering purposes: For a more comprehensive comparison of t-SNE vs. UMAP please refer to the following article: How exactly UMAP works. For the reasons discussed above, we can conclude that t-SNE is a great visualization tool but UMAP is a more suitable technique for clustering purposes in the case of manifold structures. GMM is a probabilistic model that assumes all the data points are generated from a mixture of a finite number of Gaussian distributions with unknown parameters. It can be seen as a generalization of the more popular k-means model. The advantage of using GMM over k-means is that it can represent clusters of different sizes and shapes based on a parametric covariance matrix.In k-means the clusters are spherical over all of the dimensions and share the same diameter. This is a big limitation when considering a manifold feature space as is the case of transforming a deep convolutional neural network embedding space with UMAP. Another distinction is in the interpretation of the clustering output. k-means divide the space into voronoi cells and hard assign each point to the cluster of the closest centroid. GMM, on the other hand, gives us an interpretable output modeling the probability that each data point belong to each cluster. The latter is a desired probability for dealing with fuzzy situations in presence of overlapping clusters or outliers. As with k-means, also GMM requires the number of clusters k to be specified. Moreover, in order for GMM to be able to model arbitrary elliptic shapes in the feature space, the covariance matrix should be ‚Äúfull‚Äù. The problem with ‚Äúfull‚Äù GMM models is that the degrees of freedom increase quadratically with the dimension of the feature space, risking to overfit the data. There are a few constrained versions of GMM that impose certain properties to the covariance matrix., namely: spherical, diagonal, and tied. We are now left with two major parameters to tune: the number of clusters k and the covariance type among the 4 options listed above. Likely, since GMM is a probabilistic model we can calculate the Bayesian Information Criterion (BIC) that is a statistics calculated as the sum of the negative log-likelihood of the model and a penalty term that is a function of the number of data samples and the number of free parameters of the model. The smaller the BIC value the more preferable is the model. Nonetheless, searching for the minimum BIC score may suggest selecting a model with a lot of clusters in front of tiny decreases of the score. That is why a preferred approach is to identify the elbow of the curve that corresponds to the minimum of the second derivative. The BIC score is comparable among different clustering outputs only if they are representing the same points in the same feature space. That is, we cannot compare data points reduced via PCA with data points reduced via UMAP. Another technique is the Silhouette score that is an empirical method measuring the consistency of the clusters by comparing how much a point is similar to its cluster (cohesion) compared to the other clusters (separation). The silhouette ranges from ‚àí1 to +1, where a high value indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters. If most objects have a high value, then the clustering configuration is appropriate, otherwise the clustering configuration may have too many or too few clusters. If we plot both the BIC and Silhouette curves as function of the number of clusters k for the 4 different covariance types we obtain the following graph: The dot points correspond, respectively, to the elbow and the maximum of the BIC and Silhouette curves. We can conclude that the ideal number of clusters should be between 30 and 50. In terms of the covariance type, the tied type minimizes the BIC while there is not strong evidence of worsening results in the Silhouette curve. The lower BIC score can be explained by the good trade-off between low model complexity and the high likelihood of the points. Moreover, given the nature of the feature space, it does make sense to consider manifolds of irregular but similar shapes. The selected configuration for us will be tied covariance type and 40 clusters. In order to visualize the clusters we will re-use the 3D projections that were calculated in Extracting rich embedding features from pictures using PyTorch and ResNeXt-WSL but we will color the points based on the assigned clusters rather than the COCO super categories of the pictures. In order to measure the predictive power of the embedding space, let‚Äôs try to predict the COCO annotations. We can train a random forest classifier with default parameters on a multi-labels task. Since each picture can have none, one, or many categories annotated, the task consists of predicting whether a given label is present or not in the picture. I have used 4000 pictures for training and 1000 for the test stratifying on the most frequent label in each picture. The multilabel accuracy ( exact subset of labels match as defined in scikit-learn) would be 16.7% which is not bad considering the high cardinality of the task. If we micro-average all of the label predictions we can perform a binary evaluation: We have achieved 89% precision and 31% recall on all of the possible picture annotations, not bad and not great. We should consider that we only trained on a very small sample of pictures and a few labels had very few occurrences. Nonetheless, the purpose of this tutorial is not to predict COCO categories but rather to show the effectiveness of the embedding features in identifying correct manifolds. Since we have grouped the COCO pictures into 40 unsupervised clusters, let‚Äôs compare our grouping with the categories provided in the COCO taxonomy. We can use the Adjusted Rand Index that is a measure of similarity between two clustering outputs. The higher the score, the higher the consistency between the two groupings. We obtained the following results: As we could have already observed from the 3D projections, we can conclude that the discovered topicality defined by the manifolds in the data does not match the COCO taxonomy. What topic is each cluster representing then? Let‚Äôs print the pictures closest to the centroid in a few sample clusters. In this tutorial, we have learned how to cluster pictures in their latent embedding space. We first have used UMAP for isolating manifolds and projecting them into a lower-dimensional space. We then used GMM for discovering the high-density areas in the UMAP space. The BIC elbow and Silhouette techniques were used to find the ideal number of clusters as well as the constraints to the covariance matrix. Through the AdjustedRand test, we demonstrated that the data is intrinsically organized into major topics that do not match with the COCO taxonomy. For instance, we found clusters for horses, bears, towers, watersports, people‚Äôs dining, and more. The presented methodology can be used to cluster any dataset that presents high-dimensional manifolds, not just pictures. It is in general suitable for embeddings produced by neural network models. If instead of using a pre-trained network, you are training your own, you may want to consider a small dimensionality (below 50) such that you may not need any dimensionality reduction before clustering. Other clustering algorithms that work with any kind of shapes, and are not constrained by the Gaussian mixture assumption, are the hierarchical density-based models such as HDBSCAN. Stay tuned for the next article on how to exploit embedding features and the manifolds clusters to average and represent collections of datapoints (documents) into the same latent space. You can find the code and the notebooks at https://github.com/gm-spacagna/docem. If you want to learn more about tuning techniques for data clustering algorithms you can read those articles: Data Clustering? don‚Äôt worry about the algorithm and A Distributed Genetic Evolutionary Tuning for Data Clustering: Part 1. Originally published at https://datasciencevademecum.com on January 2, 2021.",17,1,9,Towards Data Science,2021-01-02,2021
https://towardsdatascience.com/the-quickest-way-to-build-dashboards-for-machine-learning-models-ec769825070d,The quickest way to build Dashboards for Machine Learning¬†models,,4,17,"['The quickest way to build Dashboards for Machine Learning models', 'Introduction', 'Implementation', 'Deploying']","Using explainerdashboard library for quickly building interactive dashboards for analyzing and explaining the predictions and workings of machine learning models Most of the data scientist‚Äôs job revolves around collecting data and creating machine learning models for business problems. While building complex machine learning models is difficult, conveying the predictions of trained machine learning models to stake holder‚Äôs with no technical background is even more cumbersome. This is where python packages like Tabpy and explainerdashboard come in handy. In my previous article, I explained how to integrate trained machine learning models directly with tableau using the Tabpy package. towardsdatascience.com In this article, I am going to discuss implementing an excellent python package called explainerdashboard. The explainerdashboard package helps analyzing machine learning models by creating interactive visual dashboards. Beginners that are trying to break into data science or experienced data science professionals looking to better understand different attributes of their model can use this package and it just takes 2 lines of extra code to create basic dashboards. Just like most other python packages, this package can be installed by using the command pip install explainerdashboard. The famous Titanic dataset is used here to showcase the features of the random forest classification model in an interactive dashboard format. After creating the machine learning model, using Randomforestclassifier from the sci-kit package. We can customize the features we want to display in the dashboard using the classifierexplainer . The below image is the sample dashboard we get after starting the dashboard using the code line ExplainerDashboard(name).run() To run the dashboard in the inline notebooks like the jupyter notebook we can use inlineexplainer Now, we can analyze the feature importance and individual decision tree‚Äôs with custom metrics in the dashboard to better understand the performance of the ML model. More information about creating custom dashboards can be found in the official documentation of explainerdashboard here. After analyzing the machine learning models, deploying them into production is also very easy. It is recommended in the official documentation to use robust production servers like gunicorn or waitress to deploy the dashboard instead of the built-in flask development server. Install gunicorn or waitress using the command pip install gunicorn or pip install waitress. Before starting the dashboard and deploying the model, we should save our explainer model to the disk this can be done by dumping the model. After saving the model to the disk, we can load the explainer model from the file and then start the dashboard, and then we can expose the flask server as app: Here, I saved the file as flaskserver.py Now we can start the gunicorn server by using the following command: The code and images used in this article are taken from the official documentation and Github repository of the library. I hope that‚Äôs useful! Thank you for reading my article. If you have any questions regarding this article or want to connect and talk, feel free to direct message me on LinkedIn. I will be more than happy to connect with you and help in any way I can.",215,3,3,Towards Data Science,2021-01-02,2021
https://towardsdatascience.com/how-to-become-a-data-scientist-in-2021-9e0535924d45,How to Become a Data Scientist in¬†2021,Complete Guide to get you¬†Hired,12,70,"['How to Become a Data Scientist in 2021', '1. Should you become a Data Scientist?', '2. Your desired role in Data Science', '3. Qualification Check', '4. Level up your Data Science Skills', '5. Work on Projects', '6. Build up your Online Presence', '7. LinkedIn and Networking', 'LinkedIn', 'Networking', '8. Build your Resume', '9. Get Hired']","If you aspire to become a data scientist in 2021, you‚Äôve come to the right place. In this massive in-depth guide, I will walk you through the step-by-step process I took to become a data scientist. Of course, becoming one from scratch takes determination, motivation, and lots of self-discipline. If you think you are up for the challenge, read on. Content The very first question you should ask, why a Data Scientist? The hype around the topic of data science is very real but that should not be the only reason you are coming into this field. The fact is, becoming a Data Scientist is no easy feat. A Data Scientist requires a certain knack for looking at data and it involves lots of experimentation, research, critical and analytical thinking. The media often portrait data science as the cool kids next door, working on amazing technology like AI but behind every buzz word are hours of hard work and perseverance. If you think you have done your fair share of research and decided that Data Scientist is right for you, here is the good news. The best time to get into data science is NOW. Other than the fact that; Data Science has also matured substantially in recent years with its potentials greatly recognized by most industries. Areas like healthcare, finance, agriculture, retail, defense have already seen their operation impacted by AI and there is only more to come. This means that equipping yourself with data science skills is valuable to your employers, no matter the industry. Furthermore, learning data science has never been easier! The immense interest in data science has led to an explosion in the amount of data science resources, benefiting both beginners and practitioners in the field. Video lectures, blog articles, and e-books, a wide variation of teaching materials are publicly available for learners. To add on, offline in-person lessons are also in-demand right now and I will introduce them in the next few sections. So should you become a Data Scientist? If you believe in the potential of Data Science, are motivated and driven, I will say go for it. Even though I generalize everything as Data Scientists, understand that there are multiple roles in Data Science. Specifics of each role are beyond the scope of this guide but this blog post summarized it nicely. Why is this important? The role you chose decides the skills you need to hone. A Business Analyst concerns themselves with product details and business knowledge while a Machine Learning Specialist focuses on model building. Knowing which role you wish to specialize in helps to better plan your learning journey. One piece of advice is to leverage any prior experience you may have. This way, you can make yourself more desirable to recruiters as you possess domain-specific data analytics skillsets. Do some soul-searching at this stage and find the role you really want. Although this guide focuses primarily on Data Scientist, many of it still applies to any other roles. Check what now? This is the time to assess your past qualifications and see where should you start your data science learning journey. Doesn‚Äôt matter if it‚Äôs a fresh start, but it‚Äôs important to know where. Basically, you should try to check off these fundamental skills needed to become a data scientist; Many of you will be doubting yourself at this stage and question if your qualification is able to build a career in data science. I can and I will assure you that you definitely can. Apart from the more academic work such as an AI Research Scientist, you certainly do not need a Ph.D. or even a higher degree to excel in data science. Once you figure out where to start, it‚Äôs time to put that into action. There are typically 3 ways you can go about learning Data Science. These are ranked according to the ability to personalize your learning with 1 as the least customizable to 3 the most flexible. There is no one perfect path, just the one most suitable for you. Bootcamps are basically intensive on-campus training that ranges from a few weeks to a few months. They aim to cover as much content as possible in short duration so you would expect a very steep learning curve. Time is also a factor as these often require massive time commitment during the course. As such, this option is only recommended for full-time learners who are able to commit to their schedules. In addition, this is also the least personalized option on this list. Mainly because most boot camps assume zero knowledge and attempt to teach everything from scratch, so there is no need to customize your experience as everyone starts equal. A bonus point of boot camps is the opportunity to connect with like-minded individuals. Your fellow learners will be as motivated as you are and everyone has a common goal. Take this time to network with others and build your data science circle. Graduate Degrees usually span from 1‚Äì2 years for a Master‚Äôs degree to 3‚Äì4 years for a Ph.D. These are specialized postgraduate programs with a validated curriculum for the specialization you chose. Some examples are the MIT Master of Business Analytics and Master of Science in Data Science from Columbia University. Furthermore, these programs provide some degree of personalization where you can choose the course you wish to pursue. You will be able to take charge of your learning and decide the specialization you want. However, stating the obvious here, this is also the most expensive option on this list and you have to take that into your consideration. Enrolling in a graduate degree program is often a huge commitment, so make sure you have done your research. MOOCs have come a long way since it first started. Recognized universities are releasing some of their modules as MOOCs and even major tech companies like Google have come up with their own online courses. Gone are the times where MOOCs were being frowned upon and companies are becoming more receptive to self-learners. Using the right resources and the right learning path, anyone can build their arsenals of data science skills using online courses. In addition, there has been a shift from on-campus teaching to online-style delivery. For people who wanted the flexibility of online courses but the credibility of a university, edX or Coursera have partnered with major Universities to provide their Master‚Äôs programs on their platforms. This has brought MOOCs to a new height and changed the definition of what constitutes an online course. No matter which path you chose, leveling up your data science skills is just the first step of becoming a Data Scientist. While courses tell employers you know data science, projects show them your ability as a Data Scientist. Doing projects not only boosts your resume but also helps to build up your technical skills as a Data Scientist. Kaggle, the best data science competition platform. Even though Kaggle is known for its competition, working on Kaggle can be a project in itself. Competition datasets are often real data provided by companies with the intention to tap on the strength of the community to solve their business problem. During the competition, you will go through the whole process of data collection -> data processing -> modeling -> evaluation -> and optimization, similar to any real-life data science project. If you are new to Kaggle, competitions are available in a few categories. For starters, there are playground or knowledge competitions that test your fundamentals while featured competitions are available for Data Scientists to compete and win prizes. If possible, work on active featured competitions as ranking yourself against the leaderboard is a good experience and a great addition to your resume. However, the major drawback is that datasets from Kaggle are pre-cleaned. Basic pre-processing of data was already done by the submitting companies before hosting the competitions in Kaggle, reducing much of your workload. If you have not heard, Data Scientists typically spend upwards of 70% of their time processing data while only 30% is left for modeling. I also recommend you to go out there and work on some self-initiated projects. It does not have to be game-changing or life savings. Just novel and innovative will do the trick. If you know the industry you wish to work in, working on projects relevant to the industry can be a very good start. Find freely available datasets or scrap a website, go through the process of experimentation, and experience what it is like to be a Data Scientist. There is no better advice than having your own pet projects that you can proudly present to the world (and recruiters). What do you mean by online presence? Do I need to be an influencer to be a Data Scientist? Not quite, but close. Having an online presence has become increasingly important in the tech world including Data Science. For self-taught learners like us, an online presence helps to validate our work and qualification in data science. This validation comes in the form of the followers, comments, and peer-reviews you have in the online space. However, this is not any social media followers or engagement, I refer to people who read and find your articles interesting, people who share the same thoughts or issues as you in the field, or even people who get inspired by your ideas and use your projects. Yes, the easiest way to build up your online presence is to write your thoughts and share your work. Medium is an online publishing platform where writers can write and share their articles at no cost at all. You do not need a domain name or a hosting server to post your content online. Just sign up for an account, build your profile, and write away. Medium has also become the preferred platform for publishing data science articles making it the best choice for you to start (Why do you think you find this article here). So what to write? It can be anything or everything. The main goal is to pen down your thoughts, your journey, and appeal to people like you. Some blog ideas; Oh yes, Github. You do not truly belong to the technology sector if you don‚Äôt know GitHub. GitHub is a development platform where users upload their open-source codes in the form of a repository (repo) to manage or share their projects. Furthermore, It has great build-in version control functions that benefit both software engineers and data scientists and allow collaboration between professionals. Hence, starting a GitHub account goes a long way to getting noticed in the tech space. Some potential employers might even ask for your GitHub to assess your projects, codes, and technical competency! So what are some of the things you can do with GitHub? Never underestimate the power of networking. When walking into unknown territory, having offered a helping hand is rarely a bad thing. This is where LinkedIn excels as a social platform. LinkedIn allows users to build their professional profiles and connect people with similar backgrounds. You can find people in similar spaces, send them a personalized message, and build meaningful connections all in a single platform. Here are 4 tips to help you optimize your LinkedIn profile. Firstly, when logged on to LinkedIn, click on ‚Äòview profile‚Äò to edit your profile. A profile picture and a cover image are used to represent yourself on LinkedIn. This is the first thing people see before reading your profile and a good first impression counts. Your profile is segregated into sections where each section tells something about you. The most important sections are your experience, education, certifications, and projects. To start, click on the ‚ÄòAdd profile section‚Äò and add these sections individually to your profile. You can find ‚ÄòExperience‚Äò, ‚ÄòEducation‚Äò and Certifications under Background while ‚ÄòProjects‚Äò can be found under Accomplishments. Flaunt your achievements, this is your profile and you should be proud of what you have achieved. Nothing is too insignificant, give yourself the credit you deserved. With that said, your profile should remain factual. Honesty is the best policy. Next, personalize your intro by pressing the pencil symbol under your cover image. Change your headline to reflect your current status. It can be your current role in the company, current education, or even a general statement about yourself. For example, Anything that describes you really. The headline will appear alongside your profile picture so here is also where your first impression goes. Make it count. After completing your profile, the last step is to let recruiters know that you are open to opportunities. Head back to your profile page, click on the ‚ÄòAdd profile section‚Äò and find ‚ÄòLooking for job opportunities‚Äò under Intro. Once set up, it will appear on your profile but only visible to recruiters. Now that you are done, pull out your contacts or head to the suggested networks and start connecting. Happy linkeding. LinkedIn is hardly the only way to network. In fact, connecting with others in-person leaves a more impactful impression and certainly more meaningful connections. Join your local data science community, attend meet-ups, or attend some data science conferences. Even if you are using LinkedIn, do not miss out on these channels of networking. All are great avenues to meet people, gather ideas, and who knows, you might find your next boss in the mix. Finally, you are ready to get a job in Data Science. Or is it? There is just one last hurdle ‚Äî getting your resume noticed by employers. Recruiters and employers get hundreds of resumes per job opening, so how do you make your resume stand out from the rest and not get filtered to its demise. Other than your name, make sure to add your social and online profiles. This is to allow recruiters to find you easily if they want to. If you do not have any data science-related experience, we recommend prioritizing your data science projects before experience. This way, it will be kept relevant to the job scope for whoever reads it. List down your top 3 projects starting from the most impactful. Try to have some variations in your projects to show a wide range of competencies. For each project; The Experience section will come next, list your 2‚Äì3 most relevant and recent working experiences. Here is where your blogging experience can come in handy if you do not have any tech experience. It might not seem much but it will definitely show your enthusiasm and motivation in the area of data science. As for other experiences, keep them relevant to the employers. Even if your experience is not related to data science, rephrase it to advertise your soft skills such as communication, leadership, or time management. All of which is as important as your technical skills for a data scientist. Be strategic and relevant in this section. Although you can pretty much write anything here, keep it to the most important few especially in the ‚ÄòCertification & Course‚Äò section. Unless your certifications are widely recognized, most employers do not care what MOOCs you have taken. Only state those that are popular or from a highly credible university. As for technical skills, do not list all skills you think you know. Be strategic. Use the company‚Äôs Job Description as a reference and write those that are required for the role you are applying for. Try to keep your resume within one page, the most two. Concise is key and personalize each resume to the role you are applying before you hit the send button. Now you are ready to be hired as a data scientist, go ahead send your resume and hope for the best. As a self-taught data scientist, my advice to those walking the same path is to persevere. Data science is never easy and so is getting hired to be one. Assess your ability and do not be afraid to start lower. Don‚Äôt wait, be motivated, get working, and join me in this exciting career of a Data Scientist.",195,3,13,Towards Data Science,2021-01-02,2021
https://towardsdatascience.com/broadcasting-in-numpy-58856f926d73,Broadcasting in¬†NumPy,Broadcasting is an operation of matching the dimensions of differently shaped‚Ä¶,1,18,['Broadcasting in NumPy'],"Broadcasting is an operation of matching the dimensions of differently shaped arrays in order to be able to perform further operations on those arrays (eg per-element arithmetic). A very powerful feature, but the documentation in somewhat insufficient. Usually people learn about it from the error messages when the array sizes don‚Äôt match each other. The simplest example of broadcasting is the multiplication of an n-dimensional array by a scalar, which evidently means per-element multiplication of its elements by the scalar value: Broadcasting is akin to the well known ‚Äútype promotion‚Äù: in most languages when adding an integer and a float, the integer is auto converted to the float type first. In 2D (eg dividing a matrix by a vector) the broadcasting is somewhat trickier since the result of the operation depends on the particular shapes of the operands: All of those shape changes (digits in gray) happen on-the-fly, without taking extra memory (aka views). In 3D and above the broadcasting is even less intuitive and using it requires knowledge of the broadcasting rules in their generic form. They have been formulated in slightly different ways in different sources and the exact wording has been changed in time. I find the three-step rules from the Python Data Science Handbook¬π to be the most comprehensive: Rule 1: If the two arrays differ in their number of dimensions, the shape of the one with fewer dimensions is padded with ones on its leading (left) side. Rule 2: If the shape of the two arrays does not match in any dimension, the array with shape equal to 1 in that dimension is stretched to match the other shape. Rule 3: If in any dimension the sizes disagree and neither is equal to 1, an error is raised. Stretching means that the corresponding layers are replicated: Converting a color image to grayscale¬≤ is a nice example of 3D broadcasting. According to the formula for the sRGB color space one can get luminance through a linear combinations of the image color channels: Using broadcasting this is written as simple as where im is an image with the standard order of indices eg if im.shape == (1080, 1920, 3) then gray.shape == (1080, 1920) where 1080 is height of the image, 1920 is its width and 3 is the number of color planes (RGB). Complete example: There is a specialized function np.broadcast¬≥ which returns a generator object though broadcasting is rarely done explicitly (as looping would be performed in python which is slower than in C). Usually it happens ‚Äòbehind the scene‚Äô. But broadcasting is not limited to arithmetic operations. See more usage examples in my article ‚ÄúNumpy Illustrated‚Å¥‚Äù Broadcasting was initially introduced in the library called Numeric‚Åµ, the predecessor of NumPy, somewhere around 1995‚Äì1999, adopted by PyTorch, TensorFlow, Keras and so on. Even Matlab added it in 2016b thanks of the users who have ‚Äúasked for this behavior over the years‚Äù.",69,0,4,Towards Data Science,2021-01-02,2021
https://towardsdatascience.com/introduction-to-generative-networks-e33c18a660dd,Introduction To Generative Networks,What are GANs and How they¬†work?,1,46,['Introduction To Generative Networks'],"GANs are the most exciting discovery in the field of machine learning in the last decade according to several prominent experts of the domain. The idea of GANs was first introduced in 2014 by an article by Ian J Goodfellow and has been very popular since its discovery. GANs have been a large improvement in the field of generative networks. Previously, the generative work was done using variational autoencoders, which worked on a policy of reparameterization trick and was based on a random number generation from feature distributions. This caused VAEs to create non-realistic images. GANs work much better in that aspect. In this article, we will see the idea behind generative networks, types of generative networks, and their workings Let‚Äôs understand the idea with a simple example. Let‚Äôs say we have RGB images of puppies of dimension 100 x 100. So, we will have 100x100x3= 30000 different pixels. we multiply 3 as an RGB has 3 channels in the image. Now, if we flatten the image, we will get a vector of 30000 dimensions. The image can be represented as a point vector in a 30000-dimensional vector space. Similarly, if we plot all the images in the puppy images dataset as point vectors on that vector space, we will get an entire distribution of the images. So, from the distribution, we can get a clear idea, which points in the 30000-dimensional vector space can represent a puppy. From the above diagram, we can get an idea that the blue points distribution is the distribution of the puppy images of the dataset. Now, if we can pick any point vector from this vector space, we will be able to obtain a probability of that point to be an image of a puppy using the given distribution. As the blue points represent all the points in the dataset that has been used to create the distribution, the sum of the probabilities of all the points to represent a puppy must be equal to 1. In simpler words, we are creating a probability density distribution using the samples in the dataset. Now, if we consider point 1 and point 2, point 1 has a much higher probability to represent a puppy compared to point 2, which is pretty evident from the diagram. So, to generate an image that is not present in our original dataset all we have to do is randomly sample a vector point from the probability distribution created by the given dataset. The problem with the above approach is that the distribution given is too complex to sample from. The above image is a simplified representation but in the actual cases, we have a huge huge number of dimensions and the distribution becomes too complex. The reason for not being able to sample is that we can‚Äôt really obtain the distribution function of the given distribution and it is impossible to generate such a complex random variable. To solve this issue, we use transformations. In this method, we generate a random variable to create a simple uniform random distribution. We then transform this simple distribution using a complex function to convert our simple distribution into our required complex distribution. Here we know the distribution function and can randomly sample from it. We have found a way to create the complex distribution given by the dataset, now comes the implementational challenges. To transform the signal we actually need the complex distribution function which of course we don't have. What we have, are the data samples that we can plot to obtain the distribution. So, we work with two distributions, one is the N-dimensional complex one obtained by plotting the samples we have, and another, we create, by generating N uniform random variables (one for each dimension). We have two distributions now, both of N dimensions. We can generate simple random uniform variables using a pseudorandom number generator. It generates a sequence of numbers that approximates a random distribution between 0 and 1. As we can see above, we created the N variable uniform distribution and transform it using a complex function to be similar to the given complex distribution. So, we can just plot the points in the dataset as samples to obtain the distribution and bring two distributions closer to make them similar as shown in the last diagram. Thus we can create a distribution and transform it as the given complex distribution and sample from it to generate the required outputs. One thing to note is, after creating the new uniform random distribution, we upsample, so that we can compare and transform to approximate the original complex distribution with the newly created one. So, we have learned till now that our actual task is to formulate a complex function that we can use to transform our created simple normal uniform distribution into a given complex distribution. We know we can use neural networks for the formulation of the function of the transformation. Neural networks use a degree of non-linearity which makes it possible for neural networks to devise any required complex function. So, the Neural Network acts as the transforming function. Now, there are two types of generative network architectures possible depending on the procedure they use to perform the task. So, let's see their workings. Generative Matching networks is a direct approach to this problem. It simply tries to minimize the distance between the generated distribution and the actual complex distribution. It picks some random samples, generates the distribution calculates the difference between the generated distribution and the actual distribution after each iteration. The difference serves as the error which is backpropagated through the model and the model parameters are updated using gradient descent. Again, we obtain the generated distribution and continue the above process. The above diagram represents GMN. Now, as at every iteration we match the real distribution with the generated distribution at each stage to generate the error, we call it the Generative Matching Network. For every neural network to work, we need a loss function, which the network minimizes. From our discussion above, it is pretty evident for us that the loss function, in this case, is the difference or distance between the generated distribution and the real distribution. Now, for this purpose, we may have used any loss functions, like KL divergence which calculates the difference between two given distributions, but Maximum Mean Discrepancy (MMD) is used as the loss function in the case of GMT. The MMD defines a distance between two probability distributions that can be computed (estimated) based on samples of these distributions. The details about MMD can be found here. The main target of our Neural Network is to minimize the MMD error or loss function. One thing to note is that the input to the network is randomly generated N-Dimensional point vectors regarded as noise in these circumstances. We do not use GMNs that much because they are hard to set up and train. Let‚Äôs uncover the most used generative network GANs and dive into the concepts. General Adversarial Networks is said to be an indirect approach to the problem. GANs train the generator network to do a task that in turn reduces the difference between the original and generated distributions. Here the task is to increase the error of a discriminator model. So, as we are not directly working on the actual motive, we call them ‚ÄúAdversarial‚Äù Networks. Now, let‚Äôs elaborate on the workings of GANs. GANs have two neural networks, a generator, and a discriminator. The generator generates vector points and the function of the discriminator is to discriminate between the generated points and the real data points. So, the discriminator tries to decrease the classification error by identifying the generated data correctly while the generator tries to increase the classification error by generating better data points. The idea behind this network is based on a concept of equilibrium in game theory. This is called the Nash equilibrium. In game theory, the Nash equilibrium, named after the mathematician John Forbes Nash Jr., is the most common way to define the solution of a non-cooperative game involving two or more players. In a Nash equilibrium, each player is assumed to know the equilibrium strategies of the other players and no player has anything to gain by changing only his own strategy.-Wikipedia. Two models, the discriminator, and generator are trained simultaneously to beat each other and attain Nash Equilibrium in a two-player game. We can understand, in real-world two-player games, the action of one player affects another player's moves. Similarly, here the target of the generator shifts according to the discriminator‚Äôs generated error. It will be very hard for the generator to do its work if the target or the discriminator error is varying or moving. So, to avoid this situation, the discriminator is not trained during the training of the generator and vice versa. But it is important for the two models to learn together for them to converge. So, first, we train the discriminator and then we move on to train the generator. The above image shows a GAN model. It is a design based on the original by Google Developers. As we can see the generator is dependent on the discriminator. Now, the discriminator loss is only backpropagated to the Discriminator network. The Generator loss is backpropagated through the Discriminator to reach the Generator. The weights of the discriminator are frozen during the training of the generator so that they do not get updated due to the backpropagation during generator training. The procedure: The above steps show the procedure for the training of a GAN in one epoch. Initially, the discriminator performs in a superb fashion as the generator is not at all trained and the generated output is not anywhere close to the actual instances. For any network to train, we know, we need a loss function, which will be minimized by the Discriminator network and the Generator network to learn. The loss function for GAN is proposed to be Minimax loss in the introductory paper. The minimax loss is again developed from the minimax algorithm from game theory. I will try to give an example. Say, two players play a game and compete. The game theory states the whole game and every possible move of both the players at every stage of the game can be clearly represented by an n-ary tree structure. The value of n depends on the type of game and move. This is called a game tree. Now, we can score a player move‚Äôs according to the potential of the move to win the game. This is called utility. So, the one who makes the best moves has the maximum utility and he/she wins. The utility is nothing but a way to interpret the probable chance of winning. We can see that, to win, a player must maximize his own utility and minimize the opponent‚Äôs utility. But for this we must find out the best moves the opponent can make, so we will need to find their future maximum utility moves also. Thus, the algorithm needs us to make recursive calls to maximize and minimize. So, the algorithm is called the Minimax algorithm. You can read more about this algorithm here. The above equation shows the Discriminator Loss Function. E(x) is the expectation of a probabilistic sum. D(x) is the probability of the discriminator to predict the instance x as real. G(x) is the generated output, for random noise x. z is the given random noise. In this equation, the first part corresponds to the real instances. x is real, so, here D(x) needs to be 1 or closer to 1 for the discriminator to work better. We need the value of the first part to increase. Again, G(z) is fake, so we need D(G(z)) to be low, so, we try to increase (1- D(G(z))). The loss is similar to a cross-entropy loss. This is the discriminator loss. The Discriminator tries to maximize the above-given loss function, conversely, it tries to minimize the negative of it. The generator works in the opposite direction. So, it tries to minimize the given loss function. The first part is not affected by the generator. So, it only affects the second part (1-D(G(z))). The generator minimizes (1-D(G(z))) or it maximizes, D(G(z)). So, it wants the discriminator to predict a high probability of being a real instance, for the generated instances. GANs face some challenges in their applications and design. Let‚Äôs talk about them. The above problems proved that we must modify the loss function in order to remove the vanishing gradient challenge. Let‚Äôs look at the modified loss functions: In statistics, the earth mover‚Äôs distance (EMD) is a measure of the distance between two probability distributions over a region D. In mathematics, this is known as the Wasserstein metric. Informally, if the distributions are interpreted as two different ways of piling up a certain amount of dirt over the region D, the EMD is the minimum cost of turning one pile into the other; where the cost is assumed to be amount of dirt moved times the distance by which it is moved. Earth Mover‚Äôs Distance can be formulated and solved as a transportation problem. Suppose that several suppliers, each with a given amount of goods, are required to supply several consumers, each with a given limited capacity. For each supplier-consumer pair, the cost of transporting a single unit of goods is given. The transportation problem is then to find a least-expensive flow of goods from the suppliers to the consumers that satisfies the consumers‚Äô demand. ‚Äî Wikipedia So, the earth mover‚Äôs distance basically gives the least cost required to move the points of one distribution to match another distribution. The GAN using this loss is called Wasserstein GAN or WGAN. Here, the discriminator does not try to classify an instance as generated or real. The discriminator assigns a score. The discriminator tries to assign a larger score for the real instances, than the fake instances. There are no cutoffs as such. Just the number assigned to real instances are much larger. Now, as the discriminator doesn‚Äôt classify, it is called a critic. The loss is called critic loss. The generator tries to increase the score for fake instances. Critic Loss: D(x) ‚Äî D(G(z)) Generator Loss: D(G(z)) The D(x) is the critic‚Äôs output value for x instance. As we can see from the critic‚Äôs loss the critic tries to increase the difference between the values output for real and fake instances. So, it maximizes this function or minimizes its negation. Similarly, the generator tries to increase or maximize the output value given by the discriminator to the generated instances. The values are not bound between 0 and 1, and has large values, so free from the vanishing gradient problem. In this article, we have seen an introduction to generative networks and focussed on GAN. I hope this helps.",13,0,13,Towards Data Science,2021-01-02,2021
https://towardsdatascience.com/meta-policy-gradients-a-survey-78dc742d395d,Meta-Policy Gradients: A¬†Survey,Automated Hyperparameter‚Ä¶,7,46,"['Meta-Policy Gradients: A Survey', 'Disentangling Meta-Algorithm & RL Agent ‚û∞', 'Background: 2nd-Order Policy Gradients ‚àá¬≤', 'MPGs for Online Deep RL Hyperparameter Tuning üí®', 'MPGs for Offline Discovery of Objectives üìñ', 'MPGs for Online Discovery of Objectives üîõ', 'Open Research Questions ?']","Most learning curves plateau. After an initial absorption of statistical regularities, the system saturates and we reach the limits of hand-crafted learning rules and inductive biases. In the worst case, we start to overfit. But what if the learning system could critique its own learning behaviour? In a fully self-referential fashion. Learning to learn‚Ä¶ how to learn how to learn. Introspection and the recursive bootstrapping of previous learning experiences ‚Äî is this the key to intelligence? If this sounds familiar to you, you might have had the pleasure of listening to J√ºrgen Schmidhuber. ‚ÄòEvery really self-referential evolving system should accelerate its evolution.‚Äô ‚Äî Schmidhuber (1987, p.45) Back in the 90s these ideas were visionary ‚Äî but highly impractical. But with the advent of scalable automatic differentiation toolboxes, we are moving closer towards meta-meta-‚Ä¶ learning systems. In this post we review a set of novel Reinforcement Learning (RL) algorithms, which allow us to automate much of the ‚Äòmanual‚Äô RL design work. They come by the name of meta-policy gradients (MPG) and can tune almost all differentiable ingredients of the RL pipeline via higher-order gradients. Meta-learning or ‚Äölearning to learn‚Äô aims to automatically discover learning algorithms by decomposing the overall learning process into two stages: An inner-loop unfolding of an agents‚Äô ‚Äòlifetime‚Äô and an outer loop ‚Äöintrospection‚Äô of the experience. This general two-phase paradigm can come in many different flavours, here are only a few: But most of these traditional meta-learning algorithms have a hard time separating the meta-learned algorithm from the agent itself. The meta-learned algorithm is hard to interpret and limited in its meta-test generalization capabilities. In RL¬≤ for example, the recurrent weight dynamics encode the learning algorithm by modulating effective policy. In MAML, the meta-learned initialization is inherently entangled with the network architecture of the agent. Meta-policy gradient methods, on the other hand, aim to overcome these limitations by optimizing the meta-level to provide the lower level with an objective, which maximizes the subsequent learning progress. This can range from solely optimizing single parameters to learning a black-box neural net-parametrized inner-loop objective. But what is the outer-loop objective? Most commonly it is chosen to be the Reinforcement Learning problem itself and we use pseudo-gradients resulting from a REINFORCE estimator (Williams, 1992). Hence, the name ‚Äî meta-policy gradients. Next, we introduce the required mathematical background following Xu et al. (2018). Let‚Äôs assume we want to train an agent parameterized by Œ∏ (e.g. a policy/value network). The standard learning loop (as shown in figure 1) would repeatedly perform 3 steps: Now let‚Äôs assume that our update function explicitly depends on a set of meta-parameters Œ∑ (e.g. the discount factor Œ≥): Importantly, the update function is assumed to be differentiable with respect to Œ∑. We can then ask the following questions: How does the performance after the update depend on the meta-parameters used to perform the update? How should we choose Œ∑ for the next update given the current state of our agent? In order to answer this question, we need a meta-objective to evaluate a candidate Œ∑? One way to do so is to perform online cross-validation by (Sutton, 1992) using another hold-out trajectory ùúè‚Äô, collected from the post-update policy. For simplicity, we can assume that the meta-objective ¬ØL, with which we evaluate Œ∑ on ùúè‚Äô, is the same as the inner loop objective L (e.g. some REINFORCE variant). Hence, we want to estimate the meta-gradient: Using the chain rule, we can re-write the derivative of the updated parameters Œ∏‚Äô with respect to Œ∑: The dependence of the updated parameters on Œ∑ boils down to a dynamical system: The parameters of this additive sequence and the respective Jacobians can be accumulated online as we unroll the network. This in turn provides us with all necessary ingredients to perform an outer loop gradient descent update to improve the hyperparameters Œ∑: Note that we have only considered the effect of the meta-parameters on the performance after a single update. Considering multiple steps is conceptually straight forward, but the math requires us to propagate gradients √† la backpropagation through time. We can use a fixed set of K steps and automatic differentiation toolboxes to do the gradient bookkeeping. The full meta-policy gradient procedure then boils down to repeating 3 essential steps (see figure 2): There are multiple resulting computational considerations: First, online cross-validation does not imply a decrease in sample efficiency. Instead of discarding ùúè‚Äô after meta-evaluation, we can simply reuse it for step 1 of the next procedure iteration. Second, the derivative of the update f with respect to the meta-parameters is expensive to compute. This is due to us having to to perform ‚Äòunrolled‚Äô optimization through a (multi-step) inner-loop optimization procedure. There is a set of heuristic solutions which result in a semi-/pseudo-meta policy gradient update: And finally, the memory complexity of MPG increases with the ‚Äòmeta-trajectory length‚Äô K. Choosing a small K introduces a truncation bias, while larger K may suffer from high variance. Furthermore, the trace of z required to compute the meta-policy gradient may be tough to fit into memory (depending on the dimensionality of Œ∏ and K). Now that we have introduced the formal MPG framework, let‚Äôs see how we can exploit it. The RL problem postulates an artificial agent, who maximizes its expected discounted cumulative reward. This reward is sampled from a stochastic and unkwown reward function. Hence, there is no simple and explicit differentiable objective as in the supervised learning case. Instead, we have to come up with a proxy objective such as the Mean-Squared Bellman Error or a Policy Gradient objective. These objectives come with their own hyperparameters, which often are non-trivial to tune and can require dynamic scheduling. MPG methods promise to discover adaptive objectives capable of providing the best possible proxy ‚Äî at any given time of the learning procedure. Throughout the next section we will focus on how the MPG setup can improve the control problem using an actor-critic (AC) objective. We cover both the standard AC setup (Xu et al., 2018) as well as the IMPALA architecture (Zahavy et al., 2020). Xu et al. (2018) applied the MPG framework to the return function G_Œ∑(ùúè_t) used throughout value-based RL and policy gradient baseline corrections. The meta-parameters, Œ∑ = {Œ≥, Œª}, control the bias-variance trade-off in RL. How much do we rely on Monte Carlo estimation and how much on a bootstrap estimate of future returns? G defines the critic target and baseline corrects the REINFORCE term: where g rescales the effective learning rate of the critic and entropy loss terms. The inner loop update f for a fixed set of hyperparameters Œ∑ then boils down to: The next ingredient for our meta-policy gradient is the derivative of the update operation with respect to the meta-parameters Œ∑: Finally, the derivative of the online-validation loss wrt. the updated parameters is given by: We can now stitch all of these together, store our trace and update the meta-parameters Œ≥, Œª. Great! But there remains a fundamental challenge: Our inner-loop function approximators are shooting after a moving target. How can we learn values if the nature of the targets changes with each meta-parameter update? Here are a few solution ideas: After the initial paper by Xu et al. (2018), this approach was extended to many more hyperparameters. Here is a list of references, which applies MPGs to a vast set of fundamental RL design choices: While these projects mainly focus on tuning small sets of hyperparameters, ultimately we would like to automatically tune the entire RL pipeline. Zahavy et al. (2020) make a big step into this direction by optimizing almost all hyperparameters of the IMPALA (Espeholt et al., 2018) architecture. But that‚Äôs not all: They also adapt the network architecture and objective to maximally utilize the power of MPGs. But let‚Äôs first take a step back: IMPALA is a large-scale off-policy actor-critic algorithm, which allows for high data throughput using the distributed actor-learner framework. Many workers asynchronously collect trajectories, which are then sent to a centralized learner. This learner processes the experience and performs gradient updates. There is one caveat: The trajectories are most likely not collected using the most recent policy. Therefore, the algorithm is not entirely on-policy and gradients (estimated from outdated policy rollouts) will become ‚Äòstale‚Äô. IMPALA corrects for this covariate shift by using a procedure called v-trace. V-trace combats the off-policy nature of transitions with a modified importance sampling (IS) ratio, which explicitly controls the variance of the gradient estimators and contraction speed: where œÅ controls the nature of the target value function by acting as a discount scaling. c, on the other hand, performs a form of trace cutting (√† la Retrace, Munos et al., 2016). The clipping threshold constrains the variance of the product of importance weights: So where do MPGs come into play? The STAC algorithm (Zahavy et al., 2020) then aims to learn the degree of correction by utilizing a convex-combination version of the V-trace parameters: The meta-objective can then be differentiated with respect to Œ± and can smoothly interpolate between the fixed point of our approximate Bellman iteration for the target policy œÄ and the behaviour policy ¬µ. A low Œ±_c puts more weight on IS, which in turn implies more contraction but also high variance. A large Œ±_c, on the other hand, emphasizes v-trace, which leads to less contraction but also lower variance. To stabilize the impact of the outer loop non-stationarity, Zahavy et al. (2020) modify the meta-objective and add a KL regularizer similar to TRPO: In order to assure the proper scaling between the inner and outer loss magnitude the authors propose to sigmoid-squash and scale the loss coefficients. The second innovation of Zahavy et al. (2020) is to add a set of auxiliary output heads on top of the shared torso corresponding different policy and critic parameterization with their own meta-parameters. The meta-controller can then control the gradient flow into the torso for potentially different timescales. Only the first of the heads is used to generate the training trajectories, while the others act as implicit regularizers. The paper shows that on ATARI the MPG improvements increase, the more parameters we optimize with the MPG setup (panel B in figure 3). Empirically a high discount and value loss coefficient in the outer loop appear to perform better (panel A in figure 3). Finally, when examining the online optimized meta-parameter schedules on the James Bond ATARI game, we can see significant variation across the three heads and strong non-linear dynamics over the course of training (panel C in figure 3). The RL problem is inherently misspecified. The agent is tasked with maximizing an expected sum of discounted rewards, without having access to the underlying reward function. In order to overcome this fundamental problem we define surrogate objectives. But it is safe to assume that for us humans the objective function changes over the course of our lives. Potentially making learning easier. Might it potentially even be possible to completely discard the ingenious legacy of Richard Bellman and to learn what to predict in order to best solve the RL problem? The ideas introduced in the following sections extend MPGs to not only optimize specific hyperparameters but to meta-optimize an entire parametrized surrogate objective function, which serves the purpose of providing the best learning curriculum to the RL agent. The first set of methods are offline and meta-learned across a large number of training inner-loops and different MDPs. Meta-learning a parametrized loss function L_œÜ, has the advantage of potentially being more robust when varying the task distribution. The implicit characterization of the optimization problem allows for generalization beyond the meta-training distribution and tackles the core motivating problem: To disentangle learned learning algorithm from the learning agent. Computing higher-order gradients is expensive and truncating the unrolled optimization has the unwanted effect of introducing bias in the gradient estimates. In order to overcome this technical difficulty, Houthooft et al. (2018) propose to use evolutionary strategies for gradient estimation using a population of meta-loss parametrizations. The gradient of a (potentially non-differentiable) function L is approximated using a population-based Monte Carlo estimator, e.g. via antithetic sampling: where œµ is multivariate Gaussian variable. P denotes the evaluated population size, œÉ controls the perturbation variance and Œ≤ effectively rescales the learning rate. The individual function/network/agent evaluations can be collected in parallel. Houthooft et al. (2018) introduce a parametrization of the meta-loss, which uses temporal convolutions of the agents‚Äô previous experiences. It takes into account the most recent history (see figure 4). The extended memory can help incentivize structured exploration in problems with long time horizons. Furthermore, the additional input may allow the loss to perform environment/task identification and to tail its target to a specific application. Furthermore, the meta-loss is fed a vector of ones, which can act as an explicit memory unit. During the inner loop optimization the agents uses the meta-loss to optimize their policy network parameters using standard SGD. The total loss of the inner loop is given by a convex combination of the parametrized loss and a standard PPO loss. A set of MuJoCo experiments reveal that EPG outperforms a plain PPO objective and that the resulting gradients are related but different (correlation of around 0.5). Interestingly, the authors find that the EPG loss has learned an adaptive gradient rescaling, which is reminiscent of policy update smoothness regularization ideas explicitly enforced in TRPO and PPO (see figure 4). One has to note that this comes at the substantial costs of an evolutionary outer-loop of meta-loss optimization. Finally, the authors show that one can combine the method with learned policy initializations such as MAML and that the loss does generalize to longer training horizons, different architectures as well as new tasks. The general idea of an offline learned loss was afterwards extended by Chebotar et al. (2019), who use gradient-based meta-optimization and explicitly condition the meta-loss on additional task and context information. Interestingly, the authors show that the pre-conditioning can help shape the loss landscape for enhanced smoothness in the inner loop optimization. While the previous work mainly considered on-policy RL agents, Kirsch et al. (2020) extend meta-policy gradients to the off-policy setting, which allows to leverage replay buffers. As in ‚Äòshallow‚Äô non-meta RL this can lead to sample efficiency, since transitions are re-used multiple times to construct gradients. Conceptually, the proposed MetaGenRL framework builds on the off-policy continuous control algorithm DDPG (Lillicrap et al., 2015) and its extension TD3(Fujimoto et al., 2018): ‚ÄòOur key insight is that a differentiable critic Q_Œ∏: S x A ‚Üí R can be used to measure the effect of locally changing the objective function parameters Œ± based on the quality of the corresponding policy gradients.‚Äô ‚Äî Kirsch et al. (2020, p.3) So what does this mean on an algorithmic level? Standard DDPG alternates between minimizing a Mean-Squared Bellman Error of the critic Q_Œ∏ and the improvement of the policy using the deterministic gradient of the value function wrt. the policy parameters. In MetaGenRL an additional intermediate layer of transformation is added: The critic is now tasked to refine the objective function L_Œ±, which then in turn is used to construct policy gradients. In practice, L is parametrized by a small LSTM, which processes the trajectory data in reverse order. This allows the learned objective to evaluate a specific transition based on future experiences within an episode and to emulate a form of pseudo-bootstrapping. The authors benchmark on MuJoCo and against traditional meta-learning such as RL¬≤ and EPG. They find that MetaGenRL is not only more sample efficient and also allows for generalization to completely novel environments. Futhermore, they perform a set of ablation studies that show that the meta-objective performs well even without the timestamp input, but requires a value estimate input to induce learning progress. Learned Policy Gradients (LPG, Oh et al., 2020) extend upon MetaGenRL in several aspects. Instead of relying on the notion of a critic, they overcome hand-crafted semantics by learning their own characterization of what might be important to predict on a set of representative Markov Decision Processes. They do not enforce any meaning on the agent‚Äôs vector-valued predictions. Instead the meta-learner decides what has to be predicted and thereby discovers its own update rules. The LPG framework proposes to use a recurrent neural network to emit inner loop optimization targets (see figure 6). œÄ-hat denotes a policy-adjustment target, while y-hat represents the output of a categorical distribution related to a specific state. Again, the RNN processes an episode in reverse order and is trained by performing gradient descent on a sequence of inner-loop gradient descent updates resulting from the previous objective parameterization. But unlike MetaGenRL it does not rely upon the notion of a value function for bootstrapping. y-hat is free to learn its own semantics. The LPG targets can then be used to construct an inner loop update based on a combination of cross-entropy and KL divergence terms: The KL loss term in the inner loop is reminiscent of ideas in Distributional RL such as C51 (Bellemare et al., 2017) and allows for a more detailed learning signal/belief estimate than the expected value. The outer loop loss is similar to the previously introduced objectives but also includes an additional set of regularizers, which aim to ensure stability throughout the optimization procedure wrt. to Œ∑. In figure 7 panel A we can see how the different dimensions can encode different value-related estimates similar to classic value function bootstrapping. Something that has left quite the impression on me, is the result depicted in figure 7 panel C: The authors show that it is possible to learn a target-providing RNN on a set of toy MDPs (gridworlds and delayed chain MDPs), which is capable of generalizing to ATARI almost as well as the substantially hand-tuned DQN objective with reward clipping and all the extra sauce. Since the underlying meta-training distribution of MDPs encode a set of fundamental RL problems (e.g. long-term credit assignment and the need for propagating state knowledge), the LPG is capable of substantial out-of-distribution generalization. Furthermore, adding more types of environments to the meta-train set improves the test performance of LPGs. The papers reviewed in the previous section are concerned with discovering objectives or parameter schedules offline. But can we also let MPGs discover potentially even better pillars to solve the RL problem online and during single optimization run? Xu et al. (2020) propose the FRODO algorithm, which has nothing to do with jewelry or Goblins, but stands for Flexible Reinforcement Objective Discovered Online. What makes FRODO special, is that the interplay between naive outer loss and involved inner loss is trained online within a single task and single agent‚Äôs lifetime. It does not require any task distribution or the capability to reset the environment. The target simply adapts as the agent experiences the world. Somewhat like real lifelong learning! As in the offline case, Œ∑ is no longer low dimensional, i.e. a single hyperparameter, but a neural net parametrizing the entire target return G(ùúè) = g_Œ∑(ùúè). The inner loop target values do not have to encode the crucial Bellman consistency, i.e. that the current state value may be decomposed into reward and discounted next state value. Instead, this may (or may not) be discovered by the outer loop. You might imagine that training an entire network online might be even trickier in terms of non-stationary learning dynamics. And you are right. In order to overcome this (Xu et al., 2020) propose to add a prediction consistency regularizer on the meta-level: The authors validate FRODO on the ATARI benchmark and panel A of figure 8 clearly indicates that the learning progress induced by FRODO improves throughout the learning process (exactly as in the cartoon sketch in the beginning of this post). Furthermore, an ablations study provides evidence for the importance of the consistency loss (panel B of figure 8). Finally, panel C shows how the learned targets vary across different ATARI games as well as the course of training. For some games FRODO learns to provide target returns close to v-trace (e.g. for simple games like Pong and Seaquest), while for other more challenging games learned return targets can differ quite significantly. In this post we reviewed a set algorithms, which tune large parts of the RL pipeline using higher-order gradients. This enabled not only highly non-trivial training schedules but also to let the system decide what is important to learn at different stages of the process. How far can this hierarchical optimization paradigm be pushed? Here is a set of my personal open questions:",,0,18,Towards Data Science,2021-01-02,2021
https://towardsdatascience.com/classification-metrics-confusion-matrix-explained-7c7abe4e9543,Classification Metrics‚Ää‚Äî‚ÄäConfusion Matrix Explained,,1,15,['Classification Metrics ‚Äî Confusion Matrix Explained'],"I am starting a series of blog posts aiming to cover the basics of various data science and machine learning concepts. I‚Äôm mainly doing this to understand better these concepts myself. I hope that during this process, I can help others understand them too. Okay, let‚Äôs do it! In the field of machine learning, a confusion matrix (also known as an error matrix) is a table that allows us to visualize the performance of an algorithm. It is used for classification tasks only. The name comes from the fact that it makes it easy to see if an algorithm is confusing two or more classes (i.e. doesn‚Äôt make correct predictions) Let‚Äôs start with the simplest example. Imagine that we trained a machine learning model to detect if there is a dog in a photo or not. This is a binary classification task meaning that there are only two classes (‚Äúdog‚Äù or ‚Äúnot a dog‚Äù in the photo). The labels used for the training process are 1 if there is a dog in the photo and 0 otherwise. In the binary classification task, we often call these classes Positive and Negative. When we pass a new photo to our model it predicts whether there is a dog in our photo. Now, imagine that we want to classify 10 new photos. We could use our classifier to do the categorization of the photos. Each photo receives a prediction containing the label (0 or 1) which represents the two classes (dog or not a dog). Therefore, for each photo, we‚Äôll have the predicted class and the actual class. Given this information, we could generate a confusion matrix for these 10 photos. Later, I‚Äôll give you a link to an awesome example for plotting confusion matrices. For now, let‚Äôs say that the following confusion matrix is returned after we have passed the predicted classes and actual classes: Each column of the matrix represents the instances in the actual class, while each row represents the instances of the predicted class (or vice versa). We trained a model to detect between two classes, so we end up having only 4 cells that represent different information: We could easily see that our model predict 7 of 10 photos correctly and misclassified (confused) 3 photos. From the observations above we could decide if our classifier is good enough or not and proceed with additional analysis on the misclassified photos. This was a very simple example. In some cases, we need to train a model that can predict between more than two classes. Let‚Äôs imagine that we want to train a model that predicts if a photo contains a dog, cat, or rabbit. In this case, the number of classes will be 3. Now imagine that we‚Äôre passing 27 photos to be classified (predicted) and we get the following confusion matrix: Again, each column of the matrix represents the instances in the actual class, while each row represents the instances of the predicted class. However, this time we have 9 cells because we have 3 classes. Please note that by True Cats I mean photos that were classified as cats and are actual cats. Also, by False Cats, I mean photos that we classified as cats but actually are not cats. The True/False word tells us if the predictions are correct and the Cats/Dogs/Rabbits words tell us the actual class. Some insights that could be extracted from this confusion matrix are: Now you know how to read a confusion matrix and what it represents. Here is a great example of how you can easily generate beautiful confusion matrices using the sklearn package. Confusion Matrices could be used to analyze the performance of a classifier and to give us insights into which direction we should work to improve our classifier. However, we just looked at two confusion matrices generated from classifiers trained with a low number of classes (2 and 3). Imagine that we have to train a classifier with 100 classes. In this case, maybe we‚Äôll need some metrics that aggregates the information provided by the confusion matrix. Stay tuned for the next articles in which I‚Äôll show you how we could use accuracy, precision, and recall metrics which definitions are just formulas that use the values from the matrix. Alright, that‚Äôs all folks! I hope that you enjoyed the blog post. Please let me know if you have any feedback for me. Here is an image summarizing the confusion matrix definition and the examples. If you want to be notified when I post a new blog post you can subscribe to my newsletter. Here is my LinkedIn profile in case you want to connect with me. I‚Äôll be happy to be connected with you.",130,1,5,Towards Data Science,2021-01-02,2021
https://towardsdatascience.com/converting-machine-learning-models-to-sas-using-m2cgen-python-190d846090dc,Converting Machine Learning Models to SAS using m2cgen¬†(Python),A hack to deploy your trained¬†ML‚Ä¶,5,27,"['Converting Machine Learning Models to SAS using m2cgen (Python)', 'Package', 'Functionality', 'Demonstration', 'Data']","m2cgen is a very friendly package that can convert a lot of different trained models to supported languages like R and VBA. However, SAS is not yet supported by m2cgen. This article is for those who needs to deploy the trained models in SAS environment. The track introduced in this article is to convert the model to VBA codes first and change the VBA codes to SAS scripts. The scripts used in this tutorial are uploaded to my Github repo, feel free to clone the files. m2cgen m2cgen(Model 2 Code Generator) ‚Äî is a lightweight library which provides an easy way to transpile trained statistical models into a native code (Python, C, Java, Go, JavaScript, Visual Basic, C#, PowerShell, R, PHP, Dart, Haskell, Ruby, F#). The Iris Dataset loaded from sklearn Task 1: Convert XGBoost model to VBA First of all, we import the packages and data needed for this task. Then, let‚Äôs train a simple XGBoost model. Next, convert XGBoost model to VBA. Using the function, export_to_visual_basic of m2cgen can get your trained XGBoost model in VBA language. The scripts to convert to other languages are also as simple as the one to VBA. Here comes the core of this tutorial, after converting the model to VBA, there are some steps needed to convert the VBA codes to SAS scripts such as removing many unnecessary lines that are not used in SAS environment such as ‚ÄúModule xxx‚Äù, ‚ÄúFunction yyy‚Äù and ‚ÄúDim var Z As Double‚Äù, and inserting ‚Äú;‚Äù to the end of statements to follow the syntax rules in SAS. Step-by-step Explanation: The first three parts are quite straight-forward. We simply take away the unwanted lines with the use of regex, then change the beginning of the scripts to ‚ÄúDATA pred_result;\nSET dataset_name;‚Äù where pred_result refers to the output table name after running the SAS scripts and dataset_name refers to the input table name that we need to predict. The last part is to change the ending of the script to ‚ÄúRUN;‚Äù. To follow the syntax rules in SAS, ‚Äú;‚Äù is needed to indicate the end of each statement. Making use of dictonary, we can map the ‚ÄúInputVector‚Äù with the variable names in the input dataset and change all the ‚ÄúInputVector‚Äù in one go. The last part of the conversion steps is to change the prediction labels. Lastly, we can save the output with suffix, ‚Äú.sas‚Äù That‚Äôs the end of the first task, and now, you should be able to convert your trained models to SAS scripts. To double check if there are any issues with the SAS scripts created, you can use the below scripts for checking the difference of python prediction and SAS prediction. Please note that the predicted probabilities (python vs SAS) show a little difference, but the difference should not be very significant. Task 2: Convert XGBoost model to VBA, then to SAS scripts (with missing values) If your data does not have missing values in training data, XGBoost by default puts the ‚Äúmissing values‚Äù on the left node when generating the tree (as illustrated in the tree diagram below) (reference). From the scripts generated from the m2cgen, you can find the conditions tested are always be if a variable is greater than or equal to a given number. Thus if there is missing value in the testing or prediction dataset, the script will leave the ‚Äúmissing value‚Äù to the else part. For example, in our SAS script generated from task 1, the first test condition is ‚Äú If (petal_length) >= (2.45) Then var0 = -0.21827246; Else var0 = 0.42043796;‚Äù, so if petal_length is missing, it is not greater than or equal to 2.45, the var0 will be assigned as 0.42043796. Another example is shown below. What if your training data contains missing values? XGBoost will put the ‚Äúmissing values‚Äù to the left or right node based on the training results. (Thus, you can see the conditions shown in the SAS scripts are sometimes ‚Äú<‚Äù and sometimes ‚Äú>=‚Äù) You can create the dataset with missing values using the below scripts and repeat the steps in task 1 to see and compare the prediction output of SAS and python. I did the testing and found that some rows have the same prediction output while some rows show big differences (see the below picture). I compare the var generated in the intermediate steps in the SAS scripts. Let‚Äôs take the second row shown below as an example. If the condition tested is ‚ÄúIf (petal_length) >= (2.45) Then var0 = -0.217358515; Else ‚Ä¶‚Äù and the petal_length is missing, so it does not fulfill the condition and go to the else statement, then the second condition tested is ‚ÄúIf (petal_width) >= (0.84999996) Then var0 = -0.155172437; Else ‚Ä¶‚Äù and the petal_width is 0.2, again, it does not fulfill the condition and go to the else statement. Next, we go to the third condition, ‚ÄúIf (sepal_length) < (11.600001) Then var0 = 0.411428601; Else ‚Ä¶‚Äù and we see that sepal_length is missing, it should not fulfill the condition but SAS somehow accept it as True and the var0 is 0.411428601. Therefore, to cater for this scenario, I added some scripts to force the script to check if the value is missing or not first. Therefore, the manual scripts to convert VBA to SAS will change to the below scripts. You can find the full version in my GitHub repo. In this tutorial, I made use of the m2cgen package to convert a XGBoost model to VBA codes and then SAS scripts, but it is not necessary to convert to VBA codes first, you can pick other languages like C or JAVA if you prefer. This tutorial just demonstrated a hack to convert the scripts from one language to another language. To know more about using m2cgen, please go to the official Github repository.",77,1,8,Towards Data Science,2021-01-02,2021
https://towardsdatascience.com/balancing-is-unbalancing-5f517936f626,Balancing is Unbalancing,The theory behind imbalanced‚Ä¶,5,18,"['Balancing is Unbalancing', 'How Discriminative Models Work?', 'What Is Special About Imbalanced Data?', 'How Balancing Helps?', 'Notes']","Imbalanced classification is a supervised ML problem where the class distribution is too far from uniform (e.g. 5% positive and 95% negative) and usually, the decisions on data with minority class label are significant to be correct. In this case, training is more challenging, because using ordinary methods, the model is getting biassed to estimate the class with majority labels (majority class), while most of the time, we are concerned about correctly estimating minority class. Different techniques are developed to overcome this challenge such as resampling or weighting; all trying to somehow create a balanced dataset and then using common methods. While these techniques are beneficial in applications, all of them seemed heuristics to me, and I couldn‚Äôt find an exact mathematical definition of the problem. So I decided to think of what exactly happens, and this document is my understanding of imbalanced data challenge. The main concern of supervised probabilistic machine learning is estimating p(y|x) with a parametric distribution q(y|x,Œ∏) using conditional likelihood maximization that can be done by employing cross-entropy loss function (aka log-loss). Furthermore, minimizing the cross-entropy loss function is equivalent to minimizing the KL divergence between the true conditional distribution and the estimated one: In this equation, H(Y|X) is constant, so in order to minimize the expected KL divergence, we should minimize the second term, which is equal to the log-loss. To minimize log-loss, first, we need to have samples from p(x,y) to estimate the expectation, and then, we can solve an optimization problem and find q(y|x,Œ∏) as an estimate of p(y|x). But what if we just have samples from p‚Äô(x,y) = p(x|y) p‚Äô(y) instead of p(x,y) where p‚Äô(y) is the empirical marginal distribution which is different from p(y)? We can rewrite the log-loss as the following and use sample weights: It shows that to learn p(y|x) we need to set sample weights to p(y)/p‚Äô(y), which is equal to 1 if the empirical marginal distribution is the same as the true marginal distribution. Therefore, if p(y+)=5%, but the dataset is balanced and p‚Äô(y+)=50%, we need to set sample weights to 0.1. This is exactly the opposite of what we are doing for imbalanced datasets where we consider equal weights after balancing. Hence, the main question is what is wrong with using the true marginal distribution for imbalanced datasets? A difficulty lies here and balancing is a way out of that. When I was started thinking of this problem, first I remembered the bias towards the majority class and predicting all samples with that, and so I thought that the problem may be due to the hardness of optimization problem and getting stuck in local optima. In fact for imbalanced datasets, marginal and conditional distributions are almost the same with high probability, unlike balanced datasets. Therefore, by setting q(y|x)=p(y) and predicting p(y|x) with p(y), the cross-entropy is equal to H(Y) which is so small. Hence, I thought that predicting with majority class may be a strong local optima in the parameter space. But then I recalled that the loss function is convex for logistic regression and so it should train without any problem. In the following example, without using any balancing techniques, logistic regression works well on imbalanced data, and we can estimate p(y+|x) with œÉ(w·µÄx) very well: After this simulation, I reminded that in my experience, there is no region of feature space where the probability of minority class is near one (like the above figure), and usually the majority class is observable almost everywhere, but with different probabilities. As another example, consider the following figure, and let‚Äôs see how logistic regression works on it: You can observe that the logistic regression model is not learned well, and we can not estimate p(y|x) with œÉ(w·µÄx). By these observations, we can ask an important question: what is the underlying assumptions about the data that should be met to enable us estimating p(y|x) with œÉ(w·µÄx)? The answer can be found in [1]: the class posterior distribution p(y|x) can be written as œÉ(w·µÄT(x)) if and only if the class conditional distributions (p(x|y+) and p(x|y-)) are from the same exponential family with sufficient statistic T(x). For example, a common case is when class conditional distributions are both Normal and we want to discriminate them; In this case, the sufficient statistic is [x, x¬≤] which leads to a quadratic discriminant. If we want to have a linear discriminant, the covariance matrices of the normal distributions should be the same that leads to the cancellation of the quadratic terms (see 4.84 from [2]). Hence, assuming we have a feature extractor that brings us Normal class conditional distributions with the same covariance matrices, œÉ(w·µÄx) is a good estimator of p(y|x). Now let‚Äôs assume that the input ‚Äî or the extracted feature from input e.g. by using a neural network (see [3]) ‚Äî have normal class conditional distributions and we are dealing with an imbalanced dataset where the majority class has a high probability almost everywhere, even in the region of minority class (exactly like Fig. 2.). In this case, the covariance matrix of the class conditionals can not be the same, and inherently, the scale of the majority class is larger. Also, we can not overcome this issue by transforming into a kernel space, because whatever the transformation is, the majority class is presented in the region of the minority class, in addition to some other regions where the minority class is absent. Therefore, logistic sigmoid of a linear function is not a good estimator of the posterior distribution. Suppose we are very radical in choosing the logistic regression model, and although it is a weak learner of the posterior distribution for imbalanced data, we want to use it. In this situation, balancing enables us to build a kernel space in which we have the same scale for class conditional distributions, and make logistic regression as a reasonable candidate. As a result, balancing is a kind of dodge: instead of estimating p(y|x) (which is not possible due to non-uniformity of p(y)) first, estimate p‚Äô(y|x) in a balanced setting, and then reconstruct p(y|x). Hence, when we balance a dataset, we ruin the true marginal distribution and, more precisely, we unbalance it‚Äôs nature. Assume we have predicted p‚Äô(y|x) correctly, after resampling and building a balanced dataset by turning p(y) to p‚Äô(y), where p‚Äô(y+) and p‚Äô(y-) are both 0.5. Now its time to reconstruct p(y|x) which can be done through the following: where we used p‚Äô(x|y)=p(x|y) in the second equation (because we have just changed the marginal distribution of y and p(x|y) is the same as p‚Äô(x|y)). Therefore having p‚Äô(y|x) results in having a scaled version of p(y|x) which is sufficient to make decisions. Now let me discuss two other interesting intuitions, where in the first one I have focused on the importance of minority class, and in the second one, I have addressed how we can model the problem with boosting. Considering Fig. 2, if the dataset is imbalanced, the logistic model can not predict the posterior distribution very well, and there are many bad local optima, among them, just a few overestimate the posterior distribution of the minority class, and most, underestimate it. On contrary, usually, the importance of the minority class is higher and we prefer its probability to be overestimated than underestimated. By increasing the weights of the samples from the minority class, we are changing our loss function to the following and extending cross-entropy: where ECE is the extended cross-entropy, and Œ± shows the sample weights between 0 and 1 which is large for minority class and small for majority class (e.g. Œ±‚Çä=0.8 and Œ±‚Çã=0.2 where the sum is 1). The following two figures compare ECE for Œ±‚Çä=0.5 and Œ±‚Çä=0.8. It is obvious that in Fig. 4, we penalize underestimation of p(y+|x) much more than its overestimation. Also setting the derivative of ECE with respect to q to zero, we can find the minimizer of ECE as: which is equal to p(y+|x) if Œ±‚Çä=0.5. This equation also gives us a way to reconstruct p(y|x) from q(y|x) where q is estimated on a balanced dataset. Logistic regression is a weak classifier on imbalanced data, but as we know, ensembling some weak classifiers can make a strong one. Boosting is one of the methods that sequentially ensemble weak learners by increasing the weights of misclassified samples in each step. Balancing dataset is like considering a two-stage AdaBoost, where the first weak classifier learns nothing but majority class, and the second one trains on a 50‚Äì50 balanced dataset, and then it is used as the main classifier.",,0,8,Towards Data Science,2021-01-02,2021
https://towardsdatascience.com/creativity-and-dreams-with-ai-the-year-2020-6125b7900e9d,Creativity and Dreams with AI: The Year¬†2020,My very personal review with¬†examples,7,61,"['Creativity and Dreams with AI: The Year 2020', 'Visual Art', 'Film', 'Literature', 'Music', 'AI/ML Art Galleries', 'The journey continues.']","The year 2020 was a weird one. Our grandchildren will ask us, ‚ÄúHow did you get through it all?‚Äù. But if you ask me right now, my year 2020 was under a star of continuous discovery of Artificial Intelligence as a Creative Instance. I‚Äôve been studying it very closely, and here are two introductory insights I‚Äôve come to understand over this year: Usually, my year in review consists of various highlights of works other people did. But this time, I collected the rest of the last year‚Äôs vanity (sorry, I just need it right now) and would like to show what is possible with AI: All works are done by me. No, wait, actually not: the AI created them, I was just some kind of sidekick, curator, facilitator, button-pusher, director‚Ä¶ So let‚Äôs say: What you see further on are fruits of an inspiring co-authorship of a human and a machine. Forget DeepFakes; this is just a dark side aspect of the AI power. After all, artificial intelligence can be used to create entire worlds. The illustrations for my short crime story ‚ÄúK√∂stner‚Äôsche Intelligenz‚Äù were created with the help of StyleGAN2 (implemented in Artbreeder). This AI model is trained on a massive number of different photographs (faces, objects, etc.) and can generate completely new worlds. This dreamlike quality of ‚Äúmachine hallucination‚Äù enabled appropriate illustrations to describe my dream in ‚ÄúThe Border Wall‚Äù. You undoubtedly know this feeling of awakening after an intense dream: as soon as you open your eyes, the visuals and other data of your dream vanish, disappear, and are overlapped by information your brain receives from the world of wakefulness. Using Generative Adversarial Networks you can not only reconstruct and capture your dreams as you still remember them but also share them with the world (whether it wants it or not). In another video art installation for Holly Grimm‚Äôs Art Project ‚ÄúAikphrasis‚Äù I used transformations of various StyleGAN2-seeds, again with the creative power of Artbreeder. Holly Grimm pursued with this collaborative project a fascinating target. GPT-3 generated descriptions of non-existing artworks ‚Äî and multimedia artists had the request to visualize this artwork, bringing machine dream into reality. Here is my contribution (voices and music are AI-generated as well): Read more about it here. 3D Ken Burns Effect creates a unique spatial effect: using a single photo as input the model can detect occlusion and generate a parallax-alike camera movement. You can bring alive the images of human history. Wondering how AI might imagine the Outside of a limited image, I tried around various parameters, exaggerated the coordinates of the virtual camera trajectory, and ended up with the following transgression of boundaries: This brought me to an idea to reuse my photographs in these lucid ways ‚Äî as a short movie, even as series. I called them ‚ÄúdreAIms‚Äù ‚Äî and here is the first episode of the journey over the Outside of our reality. The fringes of our realm in AI imagination are fascinating ‚Äî woven with white threads, like a texture. Text. In the beginning, there was a word. You can continue ontological games with this. Last year, OpenAI presented to the world an NLP model called GPT-2. It could be used to create new texts without any human intervention (i.e. you still needed to input a prompt; AI wrote the completion). You could even train GPT-2 on specific sources so that the NLP model would dream up new ‚Äúhallucinations‚Äù of the original. I have used GPT-2 as an author and here are two short films with scripts created by AI. As if David Lynch secretly sneaked into GPT-2 and wrote this completely absurd, yet not illogical, and mysterious story. For this short film, I used AI-generated music by JukeBox (see this article below) and voices by Replica Studios. In my article ‚ÄúAI as a Movie Maker‚Äù I described how to create movies using various ML/DL approaches. Every year on the 16th of June James Joyce‚Äôs fans celebrate Bloomsday ‚Äî and undertake a journey across streets and places in Dublin, mentioned in ‚ÄúUlysses‚Äù. I took part in Bloomsday, but rather virtually. Fine-tuned on ‚ÄúUlysses‚Äù, GPT-2 delivered me a screenplay: dozens of text pages in the style of Joyce. I took some scenes and ‚Äî like in ‚ÄúEmpty Room‚Äù, transformed them into an AI-driven short film: ‚ÄúBloomsday MMXX‚Äù. Pareidolia is a survival mechanism of human perception: our brain misinterprets patterns with things we know. Back in the old times, it was better to mistake a bush for a tiger instead of being unaware of a lurking hungry predator. That‚Äôs why we see faces everywhere. AI works on the same principle ‚Äî it can recognize only what it‚Äôs trained on. That‚Äôs why it generated so many dogs faces in times of Google Deep Dream. Using the upload function of Artbreeder, I could incept the real images into the StyleGAN2 trained on faces (FFHQ). Interestingly, whatever you give this model, it will look for face patterns (since it‚Äôs trained on faces). Here is an example of how it works. I uploaded a photo of a coffee cup. This phenomenon brought me to the idea of making a series ‚ÄúparAIdolia‚Äù. Here is one of the episodes (with AI-created music): Expanding the StyleGAN2 model with user-created contents makes Artbreeder an incredible collaborative tool. GPT-2 was 2019, but GPT-3 became the AI milestone of 2020 par excellence. OpenAI developed an incredible model. Trained on 570 GB of text (and a huge chunk of human knowledge in almost any language), this NLP model can create text in any genre and on any topic. And also in nearly every language. Logical, coherent, original, unique. Still not an AGI it provides some hidden layer of a text understanding. I gave GPT-3 a task to write a Love Letter by a Toaster, and it fulfilled the request perfectly: GPT-3 understands what‚Äôs a toaster. It also knows the features of a love letter. But combining them in a typical exercise for human creative writing into a convincing work like above is astonishing. Check out the Love Letter versions written by things and characters here. With the help of GPT-3, I created short crime stories like ‚ÄúK√∂stner‚Äôs Intelligence‚Äù, took part in NaNoWriMo with multimedia stories, or generated new poetry: medium.com I also had ontological discussions about our reality: Applying the famous Nabokov Questionnaire on AI was an insightful experience (published in Harpers‚Äô Magazine in October). Remember my ‚ÄúEmpty Room‚Äù storyboard, written by GPT-2? I used GPT-3 and got a very different story ‚Äî a psychological dead-end of a relationship: medium.com Apropos, relationships. In an experiment, I allowed an unsupervised conversation between two AI agents ‚Äî and it escalated pretty quickly‚Ä¶ I did a short movie based on this text: After intensive work with GPT-3 I see how much potential, knowledge, and creativity brings this NLP model. The good news is: that since it‚Äôs trained on a vast data set, it is less biased than other models with smaller datasets. OpenAI released another treasure trove in 2020 that strangely got little attention: JukeBox. Trained on a million and a half songs, this AI model creates music. In many styles and genres. As a finished audio file, complete with audience responses, guitar scratches, and unearthly soundscapes. For the audiophiles among us, I would like to offer my ever-expanding music collection: Each of these pieces of music is unique for itself and will never be produced twice. You can feed JukeBox with particular texts. So I did it with the famous Hamlet‚Äôs monolog. JukeBox created an entirely new song: JukeBox also produces songs from scratch in unknown languages with an eerie beauty: Or you even can write a new song for an existing singer. Here I let Nat King Cole sing ‚ÄúJabberwocky‚Äù by Lewis Carroll (because why not): The animations btw. are made using TokkingHeads by Rosebud, a comfortable app using both: wav2lip and First Order Motion Model. You have AI-music, you have AI-voices and also AI-text. Time for an AI-driven podcast. For my first issue, I still wrote the moderation between two moderators ‚Äî Alchemists. Next episode will feature unsupervised dialogues between them. Contents? You have music, some radio recordings from another dimension, a radio play, and also a poem. Here btw. I wrote more about the production of a podcast with entirely AI-generated contents: towardsdatascience.com In case you are looking for online galleries with AI art, I would like to present to you these great sources: Presented by Emil Wallner, a resident at the Google Arts & Culture Lab. mlart.co This gallery is curated by the artist and AI-galerist Luba Elliott. www.aiartonline.com We are just at the beginning. After my experiments, I understood: AI is far from being a standalone creator. But it is also much more than just a tool. It‚Äôs an endless collection of ideas, unexpected twists, and thought fodder. It‚Äôs a Muse that can help with writer‚Äôs block. AI is a co-author full of weirdness and the knowledge of the world ‚Äî whether about historical persons, fairy tales, or astrophysics. AI will never replace us ‚Äî but inspire and complement. It‚Äôs time for humans and machines to work together in the cultural field. We have arrived at the future.",143,0,8,Towards Data Science,2021-01-02,2021
https://towardsdatascience.com/january-edition-a-better-world-awaits-d98e7d2964c7,January Edition: A Better World¬†Awaits,"<strong class=""markup--strong markup--h4-strong"">Happy New Year and welcome¬†to</strong>",3,24,"['January Edition: A Better World Awaits', 'New podcasts', 'New videos']","For our first monthly edition of the new year, we want to share some of our favorite picks from the newest column on Towards Data Science: Data for Change. Amidst the turmoil and heartbreak of 2020, the narrative around big data was redefined. Our focus shifted from personal gain to how we can use data science for the good of people everywhere. This year, we saw more articles than ever before about using data science and machine learning to model pandemics, predict wildfires, uncover racial discrimination in our institutions and more. Our team of editors has truly enjoyed exploring and learning from these pieces. By dedicating a column to this topic and casting a greater spotlight onto these stories, we hope to further one of our core beliefs at TDS: that data science can provide insight into urgent social, environmental, and political challenges. Hopefully, the selection below will enrich and expand your understanding of how we can apply data science tools for social good. How will you use your skills to make this world a better place in 2021? We can‚Äôt wait to add your contribution to this column! Elliot Gunn and Linda Chen, Editors at Towards Data Science. By Ioana Spanache, PhD ‚Äî 6 min read Going beyond what type of movies we want to see to what type of world we want to live in. Resources, examples, and opportunities for doing data science for social good. By Disha Kewalramani ‚Äî 9 min read Statistical analysis of police stops and searches over the years to identify if the police force is biased against minority groups. By Lauren Low ‚Äî 7 min read Comparing tree species and biomass data from UC Santa Cruz‚Äôs ForestGEO site to wildfires ‚Äî like the one just northwest of Santa Cruz proper. Here‚Äôs what we found. By Rakesh Chintha ‚Äî 20 min read How do scientists model an epidemic? How does the government come up with lockdown plans? How do we know if we flattened the curve already? By Adarsh Menon ‚Äî 6 min read The exponential impact a single person can have on flattening the curve visualised using pandas and matplotlib in python. By Barnett Yang ‚Äî 21 min read With the COVID-19 pandemic placing the U.S. economy in turmoil, how could decreasing economic prospects along racial lines impact the academic achievement gap? By Taha Bouhoun ‚Äî 7 min read A Monte Carlo Simulation to evaluate the change in infrastructure on ambulance response time (Case Study of the London Tower Bridge). By Eunjoo Byeon ‚Äî 7 min read Identifying some of the key factors that attribute to the progression of depressive symptoms using multiple linear regression. By Chan Min Yi ‚Äî 7 min read How do time spending on different aspects of our lives influence our happiness levels? An analysis of survey data using Jupyter Notebook. We also thank all the great new writers who joined us recently Marie Coolsaet, Joe Kennedy, Osasona Ifeoluwa, Jonah Kanner, Leah Pope, Alina Zhang, Linda Wehrstein, Shubham Pathania, Guillermina Sutter Schneider, Farsim Hossain, Barnett Yang, Rahul D. Ghosal, Marco Susilo, Marc Nehme, Nathan Pratt, Dr. Amanda J. Cheney, Darci Taylor, German Osin, Daniel Saunders, Eram Munawwar, Arnab Borah, Julia Qiao, Sam Starkman, Amichaud Romain, Naser Tamimi, Efe Buyuk and many others. We invite you to take a look at their profiles and check out their work.",,0,3,Towards Data Science,2021-01-02,2021
https://towardsdatascience.com/intro-to-software-engineering-in-the-julia-programming-language-3c32411c5683,Intro To Software Engineering In The Julia Programming Language,,5,21,"['Intro To Software Engineering In The Julia Programming Language', 'Introduction', 'File structure', 'Creating A Module', 'Conclusion']","Video for this article Github github.com While the Julia programming language is certainly most well-known for its scientific use, there are still packages that need to be developed for that purpose. That being said, software engineering in Julia might not be the most vital skill to have for the language, it is important to have packages to do science with in the first place. Packages an important part of the ecosystem, and I personally think that regardless of scientific purposes, packages are always a great way to keep your algorithm skill in tip-top shape! For this tutorial, we are going to be working on a very exciting project that actually might come in handy in the future! Our project is called SuperFrame.jl, which is going to be an extension to an existing package, DataFrames.jl. So along with learning more about working with packages such as DataFrames.jl, we are also going to be getting familiar with creating our own awesome project. With that in mind, I decided we might as well make it official! Let‚Äôs create a project on Github! If the project is actually useful, we could work together to push it to the Julia package registry and it might become something people use ‚Äî which is exciting because we are working together on it! Here is the repository I created!: github.com As a convention, commits should improve a brief description of what is going to be included in that commit. That being said, the repository is open to any commits, but you should probably wait till we are actually working with it to understand the exact methodology and standardization we are going to use ‚Äî which of course is going to be DataFrames.jl-based by convention. Fortunately for Julia-based developers, developing a package with a full environment included in it is actually incredibly easy. We can create the full file-structure with the appropriate Project.toml, which will contain our virtual environment, from the Julia Pkg REPL. So now let‚Äôs get to an appropriate directory and run Julia. I personally use ~/Projects. Now we will enter the Pkg REPL by using the ] key: Finally, we can use the activate command to create our new virtual environment with a Project.toml file and a Manifest.toml file: In order to actually create the appropriate files and environment we will need to add a dependency with the add command. Of course, since we are creating a DataFrames.jl extension, we are going to be adding DataFrames.jl: Now we can backspace out of the PKG REPL and then press ctrl + D to exit the Julia REPL. Pro tip: The pwd() function in Julia will push the working directory of your REPL. This comes in handy a lot, actually. I think this is a great function to take advantage of, since often in Julia it can be hard to tell where you are in relation to your file-system. The last thing we are going to work on today before wrapping up this quick introduction is creating a basic module in Julia. Looking back at the module Compose.jl‚Äôs page, we see that we need a src directory, so let‚Äôs cd into the folder with our new environment and create a new src directory: Now for today I just used nano to edit up a quit bit of text, but in the future I will be using the Atom text editor with the Juno extension. Of course, we will also be reviewing how to set up this environment, which should be relatively straightforward ‚Äî so don‚Äôt worry. This is my preference, however, so if there is a preferred text editor that you would like to use then feel free to use that! Finally, the last thing we are going to do is create a module object in Julia. The key-word for a module in Julia is module which I would describe as direct. We follow the module word with a name to import, for this example of course it is going to be SuperFrame. As with the other things we will write in Julia, we also need to write end to complete our statement. I also went ahead and included a ‚Äú Using DataFrames‚Äù, this is just temporary code, as we will likely be using direct imports in the future in order to be more efficient. That being said, writing that to our hard drive will complete the initial setup of our new package! I am incredibly excited about creating this package along with my readers and watchers. This is really fun for me and I really hope that contributions are going to be made, because this is our project together and I think it is going to turn out great with a little direction and provide some very useful interactive experience and learning. SuperFrame.jl is going to be a package that supports several different data formats by making types that work better than traditional DataFrames.jl-based types. While DataFrames.jl is a great package, it would be great to work more mathematically or in a certain way with some data. While we are at it, we can also create some cool interactive visualizations and take the package pretty far into working with things like image data-frames. Are you excited about the project yet, because I sure am! I am also very excited to be engaged in your learning atmosphere for these programming concepts ‚Äî because programming is awesome and fun! Thank you very much for reading, I appreciate it, and happy new year and 2021!",145,0,5,Towards Data Science,2021-01-02,2021
https://towardsdatascience.com/using-machine-learning-to-classify-server-incidents-36f837c107a,Using Machine Learning to Classify Server Incidents,A step-by-step on how Machine Learning can be¬†used‚Ä¶,5,15,"['Using Machine Learning to Classify Server Incidents', 'Introduction', 'Data Preparation', 'Modeling', 'Evaluation']","When we talk about incident management, automation of the incident resolution is a must-have for every company, but to have automation that works in the right incident and with the right information it is necessary to retrieve information from the server incident automatically, where most of its content is based on text, i.e., the incident description, and then classify (we call it ‚Äúmatch‚Äù) which already available automation will probably solve the incident. The common way used to retrieve information from the incidents that do not require deep programming skills is using a regular expression, but if you already worked with it you know how messed it can be, and the minimum modification in the content of the incident can cause the regex to fail. In this post, we describe how we are using machine learning techniques to improve that match. The data used to train the model is based on incidents that were resolved by one of the available automation in the last six months, which is around 100k incidents from more than 180 clients in 9 different countries. We restrict the data to only 6 months because we continuously deploy new automation and fix some regular expression issues, so to prevent wrong/old data to be used, the smaller interval should be enough. The raw data set that we retrieve from our data lake has 14 columns, but we will focus only on the necessary ones to create the model. In this particular case, as component and sub component are additional to summary information, we decided to concatenate these three fields creating a ‚Äúfeature‚Äù field, this process will make the next steps easier, besides that, we set all content to lowercase. In some countries we have incidents with more than one language, English and Spanish for example, to solve that we used two libs, the langdetect to check the language of the incident, we adopted English as our main language, and googletrans to translate the incident to English if necessary. This process can take a few hours depending on your data set size, we tried to filter only the necessary text to translate to English. To finish this step, we separate the data into feature and class, the automation_name is the class we want to predict and the feature is the result of the processing we did above. Having the two fields ready, we can finally start the machine learning phase itself. To make it simple to run our model, we create a pipeline, the pipeline has three main components: One important step in this phase is to remove the stop words and punctuations, this two components can create a complexity not necessary for our case, so using the nltk library and the function below attached to CountVectorizer, we get rid of them. After the pipeline, it is business as usual, we separated the data into training and test and will evaluate the results of each classifier. In this particular case, we want a balanced metric of Precision and Recall, as at this point, none of them can cause more trouble than the other. Having this in mind, we chose to evaluate the F1-Score and Macro Average, we tried a few classifiers and you can check the results below: The model that best fit our data was XGBoost, but there a lot of more things you can do to improve these numbers, like hyperparameter optimization, upsampling the minority classes, and a few others, but I will let you try this by your own. :) To use this in production, we were required to have some guarantees that the classifier predicts the automation with a probability above a certain threshold, to do this we used the predict_proba from the sklearn lib, this function returns the probability that the classifier returned for each class, having this and the threshold, which we set as 97%, we were able to implement and monitor the classifier in production without worries. Here is the snippet that we used for that: Hope this article will help you to understand how to work with text data using Machine Learning and how we found an opportunity and applied this in our day to day job willing to improve our client's satisfaction.",18,1,5,Towards Data Science,2021-01-02,2021
https://towardsdatascience.com/how-a-simple-graph-saved-someones-job-neo4j-7262c813937f,How a simple graph saved someone‚Äôs job‚Ää‚Äî‚ÄäNeo4j,"Drinks, an informal conversation, a few graphs¬†helped‚Ä¶",1,49,['How a simple graph saved someone‚Äôs job ‚Äî Neo4j'],"The photograph isn‚Äôt pretty when compared to what people post in their blogs but this old piece of paper has an interesting story associated with it. A few days ago, I was cleaning my bag which has too many compartments ‚Äî typical of Swiss gear and I found a pocket that was not touched for years. A small hidden pocket acting as home for business cards, crumpled papers, and receipts from the past. That‚Äôs when I discovered the old, stained paper with this crude and rough sketch of what I scribbled down around 4 years ago. Spatial info: An Irish bar in Denver, Colorado Thermodynamic info: An exceptionally hot evening, perfect for citrus drinks Temporal info: Around 6:30 pm I had travelled from east coast to Denver on a sales pitch meeting with a potential client and it didn‚Äôt go as well as planned. There were budget cuts at their end and my proposal got pushed to the next quarter. My return flight was the next morning and I had a dinner scheduled with that client in the evening. Well, consulting makes you a kind of food and drink connoisseur. Dan and I decided to meet at a local bar for a drink before heading to dinner. As it was an especially hot day, we decided to prolong our stay in the bar and have a couple of more beers(also because Denver is quite high on the brewery scene, so one could sample many craft beers). Dan was telling me about the business restructuring that is going on and that will lead to budget cuts across the organisation for the time being. Their idea was to make the organisation lean and flexible and hire more folks in tech and analytics sector. The conversation somehow got diverted to the creation of new departments and hunt for departmental heads within the organisation. I had done primary research on agent-based models and Thanks to Thomas Schelling I liked to fit the statement in conversations and this was the perfect opportunity. What you see at the macroscopic level might not be true at microscopic level I must have drawn the graph for him on the piece of paper above then to show him how people from various departments connect to each other and how sometimes those who hold the highest titles in the department might not be the ‚Äòfulcrums‚Äô or the most important ones in running the day to day business. So, before reaching a hard conclusion, learn what various people do at their job and don‚Äôt rely only on data given by their supervisors or the title of the chair they are holding. I gave him a few ideas on how to do it and what tools he could use. Three or four months forward, I met Dan again at the initiation of a new project and that‚Äôs when he told me that he created graphs for small parts of the organisation and quantified the importance of the people in the business setting. I will simulate a case that he mentioned and interested him. Employee.csv contains three major columns. Employee_1 and Employee_2 is the pair of 2 people who would have interacted with each other. Weight is the measure of interaction between the two. It is a simple count of how many times Employee_1 and Employee_2 exchanged emails or were present in the same meeting. The observations were harvested for the past 3 months. The data was collected from reading the metadata of the email and SMTP servers and no one's privacy was breached in the process. Click on Add Database, provide a username and password and you are all set. Once you add the database, click the three horizontal dots and from Manage section you will add a few plugins that you will make your job a little easier. In the Plugins tab, go ahead and install APOC and Graph Data Science Library. Go ahead and start your DB and open with the Neo4j browser. (Please note that when you create your DB for the first time, the number of nodes and relationships will be 0) If you are using macOS then /Users/<your user folder>/Library/Application Support/com.Neo4j.Relate/Data/dbmss/<folder related to the DB>/import/ Place your .csv files in the import folder. <folder related to the DB you created above> ‚Äî If it is your first project then you will have only one folder under /dbmss, so place your .csv in the /import there nonchalantly and audaciously. (Only for mac users: The above folder is much easier to find on Windows or Linux as in macOS the /Users/<your user folder>/Library is hidden, so you can type /Users/<your user folder>/Library in spotlight search and get to the folders) Load the CSV in the DB Let‚Äôs see how it looks Well, it is a simple graph which doesn‚Äôt do much for me except telling me that people are connected to each other. Let‚Äôs find the degree and weighted degree for each of the employees. Degree ‚Äî Number of employees to which an employee is connected to. Weighted degree ‚Äî Sum of weights of the relationship for each employee. The numbers for degree and weighted degree are quite different, we want a better metric that is more robust. Let‚Äôs try to find the PageRank for each of the employees. It is more robust and captures the status of the employees at the entire graph level than only at their node level. Pagerank seems to be more controlled and not scattered like weighted degree and degree. It can be a much better metric to measure the power of each node. I want to do some clustering as well on the employees i.e. which of them are highly connected subgraphs within the graph. In graph terms, what I want to accomplish is to detect the communities. I had no idea how to accomplish it but a quick search helped. All the queries are uploaded on Github here. Now that I have everything I needed, I can use Neo4j‚Äôs Graph Data Science Library to visualise the data. It will launch a UI which is called NEuler(Noy-ler) in Neo4j‚Äôs vernacular. Once you connect, you will have to select which label, what weight property you want to run your graph on. I selected Employee and PageRank respectively. Select the options from the drop-down to make your graph colourful and interpretable. And here are the results, The graph shows the connections and the various communities that exist in the small employee pool. The thicker the edge between the employees, the higher the interaction is based on the page rank score. Now, let‚Äôs focus on the interesting observation Dan had. Sue, Amanda, Chris, Mark, and Yan all worked for the Accounts department with Sue assuming the head of the department's role. You should see that the edge between her and Yan is quite bleak. Yan has a thick edge between her and Alex(a major stakeholder and an important person in the ecosystem of the organisation). Not only that, Yan is the only bridge that connects the Accounts department to the rest of the organisation. Before Dan accomplished the exercise(which must be a similar version of what I have concocted here), Yan‚Äôs name featured in the list of those who were being axed from their department. She isn‚Äôt a heavy lifter within her accounts department but she lifts a lot of weight nevertheless and that would never have been realised had we taken only departmental feedback into consideration. A simple visual showed that Yan is an influential person in her organisation even though her intra-department score might be low, her inter-department dependency makes her indispensable. She was not only saved from being fired but in the next cycle, she was given a position by Alex‚Äôs department. Graph DB avoided a mistake from being made if one had only looked at the organisational structure from a qualitative perspective. Thankfully that wasn‚Äôt the case here for Yan. This is the power of looking at data in a graphical manner, you see patterns that you could have missed otherwise. Other than that, many people are visual learners, present them a table and a graph, more often than not they will reach for the graph. After this, I can say that the visit to the bar that evening was a fruitful one! Please let me know your comments and you can use the Github repo here.",17,0,8,Towards Data Science,2021-01-02,2021
https://towardsdatascience.com/how-to-make-a-state-of-the-art-model-with-fastai-bd11e168b214,How to Make a State of the Art Model with¬†Fastai,Image classification with Learning Rate Finder¬†and‚Ä¶,1,27,['How to Make a State of the Art Model with Fastai'],"When I first started my journey with fastai, I was very excited to build and train a deep learning model that could give amazing results in a short span of time. Now that we‚Äôve seen some under the hood training jargon that fastai uses along with some Pytorch essential functions, it is now time to see what results a little more effort into building the model yields. I‚Äôll be linking my previous articles in which I document my learning with fastai at the end of this article. :) We will need this data to get started. It is from the Rock, Paper, Scissors dataset from Kaggle. This task becomes a multi-class image classification problem with three classes (each with train, valid, test folders) containing RGB colour images of size 300x300. Specify the import function for all things fastai vision and set the path variable. Now we shall define a datablock for getting the data from the folders. We specify these to make sure our data is available to the model while writing minimal code: This will return a dataloader that will give us a batch size of bs and an image size of size. As Jeremy Howard says in his book: start training using small images, and end training using large images. Spending most of the epochs training with small images, helps training complete much faster. Completing training using large images makes the final accuracy much higher. This is an experimental technique that has been proving to be extremely useful in getting much higher accuracies than when otherwise done with the same sizes of images throughout. Let‚Äôs now see how can be train in multiple sizes, shall we? We‚Äôll get the batch size as 64 and image size as smaller 128x128. Now let‚Äôs go ahead and calculate what learning rate we should use for this part of the training. First, we make a model by utilising transfer learning with the following line. Then, we plot a graph to see about finding the learning rate. The output looks like this, with a clear visualisation of what our losses will look like if we take a specific value of the learning rate. Looks like taking a learning rate of around 1e-3 will be enough to make sure our losses decrease with training. We‚Äôll choose that. We see quite remarkable results in the first few epochs itself. Note: I trained the model on the GPU which is why it only takes mere seconds with each epoch. If you were to train on the CPU only, it‚Äôll take much longer, sometimes even ~10 minutes approx for each epoch. Now that we have the model trained on the smaller image sizes, we can proceed to the second part of the training. We use the batch size as 128 and our image sizes as 224 for our next fine tuning of the model. This, as you can infer, yields us an accuracy of almost 95% in our training and it only takes about three minutes to train on the GPU! Fastai enables us with the ability for rapid development of any deep learning task, and as I‚Äôve experimented with it in the previous weeks, I‚Äôve found myself being more and more in love with its super simple approach. If you‚Äôre keen to follow along this journey with me, make sure to follow me for continued updates as I explore more deep learning tasks with this amazing library. As I promised earlier, here are the other articles I‚Äôve written for fastai. Happy coding! üòÅ Also, here is the GitHub repo link with all of the code: github.com Do you want to get one free, clean email from me every week or two weeks containing the best of my curated articles and tutorials that I publish? Join my Codecast! Connect with me on Twitter and LinkedIn!",26,0,5,Towards Data Science,2021-01-02,2021
https://towardsdatascience.com/using-databricks-clusters-in-ml-pipelines-adc2bde3c8ed,Using Databricks Clusters in ML Pipelines,Run production Spark jobs and train ML models on Databricks‚Ä¶,4,17,"['Using Databricks Clusters in ML Pipelines', 'Running production jobs on Databricks clusters', 'Integral Benefits', 'Conclusion']","In this post I would like to describe my experience executing production Spark and ML jobs on Databricks job clusters.So far, I‚Äôm a big fan of Databricks solutions since I find them so much better than the alternatives that I‚Äôve used , and no ‚Äî I‚Äôm not a Databricks employee. In my daily job, I develop an automated AI-based predictive analytics platform that simplifies and speeds the process of building and deploying predictive models.I‚Äôve spent my last two years building data and ML pipelines that includes data cleaning , structuring, feature engineering, training, evaluation, predictions and monitoring jobs. Two reasons made me start using Databricks: Since I‚Äôve started using Databrikcs, I‚Äôve found so many more powerful capabilities that I like, which I‚Äôm going to discuss in this post. We developed a custom Databricks Airflow Operator for our needs, which we use to execute production jobs. This Airflow Operator creates one of two types of clusters for each job by its type and workload: Each one of the jobs gets a dedicated cluster that turns off right after the job finishes. It‚Äôs possible running a lot of clusters in parallel in order to execute many independent jobs. The operator knows to execute each one of the jobs with the right parameters, on the right type of cluster, manage failures, retries and more.It‚Äôs simple to create a cluster using Databricks REST API, this is an example request to create a Single Node Cluster: You can find additional information about the different parameters at the end of the post [1]. Managed SparkSpark is already installed and configured, enables to create clusters on-demand quickly, manage them with ease and turn them off when the task is complete. Databricks RuntimeDatabricks offers a couple of available runtime configurations, for example ‚ÄúDatabricks Runtime ML‚Äù which automates the creation of a cluster optimized for machine learning.This configuration includes the most popular machine learning libraries, such as TensorFlow, PyTorch, Keras, XGBOOST, Scikit-Learn, Pandas and a lot more.It speeds up the cluster creation time, and I can ‚Äústamp‚Äù my jobs with a specific runtime configuration so that I can use it again in inference pipelines. It promises the same versions of libraries between the train pipeline and the predict pipeline, making them compatible with each other. Debugging is easyIt‚Äôs possible to debug jobs using ‚ÄúDatabricks-Connect‚Äù [2]. It just need to be installed in the local virtual environment, and configured with the Databricks account details.It enables debugging jobs remotely on a cluster and become very helpful especially when we want to debug jobs on a lot of data that doesn‚Äôt fit our local machine memory. Easy to use your own Python PackagesIt‚Äôs possible to install Python packages on clusters and access them from our jobs. It makes it super easy to use internal package and common objects and code.It‚Äôs possible to install packages from a couple of sources, and using the UI or API. Built in AWS and Azure integrationThere is an integration between Databricks and AWS and Databricks and Azure. We can use a single API to execute jobs on both of the clouds instead of building these integrations on our side. Notebooks and Data CatalogJupyter Notebooks are available, with pre-configured spark session and out-of-the-box visualization capabilities.It‚Äôs also possible to execute heavy workloads easily and it‚Äôs possible to save processed DataFrames to the data catalog and let other teams and colleagues access them using their notebooks. In this post, I briefly described why I like using Databricks Clusters as an infrastructure to run Spark jobs and train models.There are so many details I didn‚Äôt describe since I wanted to give an overview of the subject without going too deep.If there is something specific that interests you, I would appreciate your comment, and I‚Äôll write about it in my next post. Links:1. Databricks Jobs API:https://docs.databricks.com/dev-tools/api/latest/jobs.html 2. Databricks Connect:https://docs.databricks.com/dev-tools/databricks-connect.html",264,1,4,Towards Data Science,2021-01-02,2021
https://towardsdatascience.com/on-the-ethics-of-telemedicine-and-predictive-modeling-for-service-demand-d52243db6ac2,On the Ethics of Telemedicine and Predictive Modeling for Service¬†Demand,,10,72,"['On the Ethics of Telemedicine and Predictive Modeling for Service Demand', 'Teleneurology', 'Ethics in medicine', 'Computational modeling', 'Previous modeling efforts', 'Data and methods', 'Results', 'Computational conclusions', 'Ethical considerations', 'References']","Telemedicine is widespread. Services range from real-time consultations, remote monitoring, to electronic medical record data transfers. Telemedicine has the potential to transform healthcare, but ensuring it is ethically acceptable requires anticipating and addressing potential pitfalls. These include effects of attempting to utilize one-size-fits-all implementations, patient privacy issues, erosion of patient-doctor relationships, and the temptation to assume that new telehealth service technologies must be effective. Emergency teleneurology care has grown in magnitude, impact, and validation. Stroke is a leading cause of death in the US and the timely treatment of stroke results in better outcomes for patients. Teleneurology provides evidence-based care to patients even when a board-certified neurologist is not physically on site. Determining staffing demand for telemedicine consultation for a specific period of time is an integral part of the decision-making activities of providers of acute care telemedicine services. Here, we describe the building of a forecasting model to predict consultation demand to optimize telemedicine provider staffing [0]. Ethics have a strong position within medicine, reflecting deontological principles of seeing an action as right or wrong in itself depending on its conformity with a moral rule and not on its consequences. This is then overlaid with a utilitarian ethic of maximizing good and bringing about the greatest happiness to the greatest number. The medical tradition is based on patients‚Äô rights, doctors‚Äô duties, and attempts to balance different interests. To a degree, it also reflects an Aristotelian account of virtues and the values of a profession; widely known statements of the medical commitment to serve humanity include the Hippocratic Oath and the World Medical Association‚Äôs International Code of Medical Ethics [6]. However, despite being based on such seemingly stable systems, ethics in medicine is influenced by wider trends in science, society, and politics. The development and availability of diverse, cheap, and reliable technologies, financial pressures on the healthcare sector, and a growing demand from the public for universal access to high-quality medical care has led to a rapid growth of telemedicine services. Telemedicine is a hundred-year-old concept that describes the process of using information technologies and telecommunications platforms to deliver healthcare services, which range from consultations, education, exchanging patient records data, research, and more [1]. Recently, telemedicine market growth was projected to be $41.8 billion with greater than 19% growth from 2018‚Äì2022, yet currently only addresses 0.1% of global demand [5,6]. The decentralization of healthcare delivery and health budgets has also encouraged a number of independent telehealth projects. These projects are often small-scale, bottom-up, and driven by small teams of health professionals from diverse disciplines, e.g. using a video link between a local doctor‚Äôs surgery and a hospital consultant. It has been observed that the medical tradition is dominant in most of such projects, and the popularity of the term ‚Äòtelemedicine‚Äô (as opposed to ‚Äòtelehealth‚Äô) reflects the preoccupation with medical aspects of healthcare delivery. In constructing a computational model to predict the demand for telemedicine services over time, with the goal of helping both hospitals and telemedicine service providers better understand staffing needs in terms of the distribution of on-site and via-telemedicine doctors, we argue that this is too narrow a focus. We hypothesize that demand prediction as executed below is not as optimal as it could be, as compared to if we had somehow encoded into the model viewpoints on particular issues relating to the growth of telehealth activity. However, we take care to identify and understand which objective, quantifiable features contribute most to forecasting telemedicine service demand, and discuss the inclusion of particular ethical dilemmas for involved parties into the prediction logic afterwards. In this analysis, we refer to telemedicine as the use of technology to deliver care remotely between patients and providers. We specifically analyze teleneurology, an application of telemedicine to treat patients who require a neurology specialist, such as for acute ischemic stroke, migraines, or epilepsy. In this context, consultation demand is the quantity and frequency of consult requests from hospitals. Hospitals across the nation are interested in predicting the number of future visits within a specific time (i.e. within one week or one month). This information is valuable because it impacts hospital cost assessment in terms of overcrowding, hospital understaffing, and insufficient bed availability [2]. Telestroke and teleneurology are telecommunication-based services specific to treating patients who have stroke and neurological disorders in need of care from a neurologist. In a randomized, blind, and prospective trial, telestroke was more patient-specific, sensitive, and higher predictive values than traditional telephone-based consultations [19]. These are typically deployed in underserved facilities that lack 24/7 stroke expertise [2,13,15]. Previous studies have shown that telestroke services performed as reliably as bedside neurologists by comparing their assigned NIHSS (National Institute of Health Stroke) scores, from both in-person neurologists and those treating via telemedicine [4]. The study concludes that doctors practicing through telemedicine can be used to help emergency physicians administer tissue plasminogen activator (tPA), a drug that dissolves blood clots used to treat ischemic stroke [2,4,6]. Telestroke has also been shown to be cost-effective, with and incremental cost-effectiveness ratio of $108,363/QALY (quality-adjusted life year) in the 90-day horizon and $2,449/QALY in the lifetime horizon [14]. The QALY is a generic measure of disease burden, incorporating impact on both the quantity and quality of life, used in economic evaluations; one QALY equates to one year in perfect health. Similar studies found that the cost-effectiveness of hub-and-spoke telestroke can increase the number of patient discharges [16]. Demand prediction could improve these services even further, to predict how many clinicians should be physically present at the hospital versus how many should be available remotely via telemedicine initiatives. Many hospital departments are interested in forecasting demand for care for a variety of reasons. For example, emergency departments (ED) in the U.S. experienced widespread overcrowding issues, which negatively impacts patient quality of care and hospital costs [7]. Predictive studies using Poisson models on EDs have shown with up to 90% confidence that the highest number visits during the week is Monday and increases from 7:00AM until it peaks at noon [2,18]. Another study forecasted hospital bed demand by analyzing prediction techniques: hourly historical average, seasonal autoregressive integrated moving average (ARIMA), and sinusoidal with an autoregression (AR)-structured error term. ARIMA performed the best and was able to estimate demand for bed count 4 to 12 hours in advance [7]. However, to current knowledge, there is no study that evaluates the cost-effectiveness of teleneurology implementation by forecasting demand for consultations across different clients. This is true because telemedicine services vary nationwide based on the demands of individual hospitals, which can be on available bed count, hospital type, value-based care or fee-for-service care, hospital staffing, etc. The study we present here can help shed light on what factors affect return on investment for telemedicine services for both hospitals and providers. The dataset has 411 hospitals and 97,593 consultations since July 2015. The number of consultations were predominantly focused on stroke, followed by Transient Ischemic Attack (TIA), encephalopathy, seizures, and stroke that requires tPA administration (tissue plasminogen activator, tPA stroke). Although less frequent, tPA Stroke is the highest clinical priority. The hospital clients‚Äô bed count sizes ranged from 10 to 1432. Geographically, the data shows a heavier representation of hospitals in the Southeast, which occupy around 35% of the total number of clients. They are in Figure 1, which shows the fields used to run the models. Within teleneurology, around 53,000 females received consultations compared to 43,000 males, which is summarized by the pool of females dominating at 55% overall. As the number of consultations was dependent on Provider Diagnosis, an overwhelming majority of consultations were based on stroke, followed by TIA, then tPA stroke, which is more serious and life-threatening. The relationship between tPA administration and a variety of factors, including gender and age, is highly correlational. Although there was no correlational relationship between gender, there is a normal-like distribution that models the relationship between age and tPA (Figure 2). The data shows that tPA Administration accumulates mostly in New Jersey at 278 cases, followed by Pennsylvania at 165 cases (Figure 4). Interestingly, New York, which had less than half the number of tPA administrations than North Caroline or New Jersey despite experiencing similar demand for teleneurology. Using polynomial regression fitting, tPA administration and the months of the year follow a cubic relationship with correlation coefficient (r¬≤ value) of 0.76. OLS linear regression and polynomial regression models are used to find strong correlations between features and demand. Ridge regression and the lasso were also tested on the dataset to find the best fit model. Ridge regression is used for fine-tuning the complexity of the model with a regularization term, while lasso regression suits sparse data by forcing the sum of the absolute value of the regression coefficients to be less than a fixed value, causing some certain coefficients to be set to zero and reducing the problem to a simpler model that does not include those coefficients [12]. Given below are plots depicting the weekly and monthly telemedicine service demand aggregated across all partnering hospitals, over a period of approximately a year. The weekly demand (Figure 7) appears to be centered around 850‚Äì950 patients utilizing telemedicine services. These counts oscillate around this center on an approximately 1.5-week basis, and demand is noted to spike slightly around the holidays and new year periods. The monthly demand plot (Figure 8) below confirms the above observation, where dips and peaks are observed either once or twice a month (the sudden downward-sloping lines at the immediate left and right of the plots are due to the provided data beginning and ending in the middle of a month). An OLS linear regression and polynomial regression were conducted on the dataset (Figure 9), where the generated trends are depicted with a dotted green line and solid red line respectively, overlaid on a scatterplot of demand aggregated weekly. The following results were outputted for the OLS regression, with the slope of the line being 0.21, the y-intercept as 845.71, and the r¬≤ value as 0.04. The low correlation coefficient outputted prompted running a model with additional input variables. After experimenting with various orders of fitted polynomials, training, testing, and cross-validation were executed to generate accuracy scores, which were no longer highly variable from run to run due to an augmented independent variables set in the system. The following features were used to train the new regression models: ‚Äòtime zone‚Äô, ‚Äòvisit initiated‚Äô, ‚Äòstate‚Äô, ‚Äòsex‚Äô, ‚Äòage‚Äô, ‚Äòservice line‚Äô, ‚Äòreason for consult‚Äô, ‚Äòprovider diagnosis‚Äô, ‚Äòhospital type‚Äô, ‚Äòbed count‚Äô, ‚Äòstroke center‚Äô, advanced comprehensive stroke center, and total ER visits (Figure 10). Running linear regression on these features, most of the data points matched the predicted value where r¬≤ = 0.78 (Table 1). Applying lasso regression on the same features, the r¬≤ = 0.77, which was less optimal than OLS linear regression. Ridge regression on all inputs yielded slightly better results, with r¬≤ = 0.78 (Table 2), and performed the best overall. Running the weekly prediction linear regression model with just ‚Äòprovider diagnosis‚Äô, the model yielded r¬≤ = 0.74. Using the same model with Service Line in place of ‚Äòprovider diagnosis‚Äô, the accuracy increased to r¬≤ = 0.77. Weekly prediction with only ‚Äòreason for consult‚Äô performed the best even compared to training all the features together with r¬≤ = 0.79 (Table 1). Interestingly, weekly forecasting with ‚Äòreason for consult‚Äô and ‚Äòprovider diagnosis‚Äô bumped the accuracy to r¬≤ = 0.81. It was expected that ‚Äòreason for consult‚Äô and ‚Äòprovider diagnosis‚Äô would yield strong results but surprising that ‚Äòservice line‚Äô was also an equally good predicting factor. The sparse nature of this dataset enabled a slight change in the target variable to cause huge variances in the calculated weights. The plots below, one for each of ridge regression (Figure 11) and the lasso (Figure 12), set a certain regularization (Œ±) to reduce this variation. When Œ± is very large, the regularization effect dominates the squared loss function, causing the coefficients tend to zero. At the end of the path, as Œ± tends toward zero and the solution toward the ordinary least squares, coefficients exhibit large oscillations. In outputting accuracy statistics for the models built for this study, we set an Œ± enabling a maximal score. This analysis aims to reasonably predict future consult demand to optimize hospital staffing and analyze return on investment for both parties, and demonstrates that there is a strong correlation between week or month of a telemedicine consultation request and the predicted number of consultations. We have found that certain combinations of input features, including hospital characteristics and certain consult characteristics (namely ‚Äòprovider diagnosis‚Äô, ‚Äòservice line‚Äô, and ‚Äòreason for consult‚Äô), yield forecasted telemedicine consult demand with around 78% accuracy overall. Ongoing work includes improving the accuracy of the current predictive model by adding a classification by service line and the reason for consult and provider diagnosis component to it, in order to further break down the provided forecasted demand outputs. We will continue to work around dataset limitations, which include the dataset consisting entirely of data from one telemedicine provider and its clients, for a single practice, only over a one-year time period. Ideally, we would like more information on the degrees to which telemedicine was utilized in consults over various hospitals, patient demographics data (to analyze which hospitals get greater telemedicine service demand and if this is correlated with patient demographics), and notes of which partnering hospitals specialize in which service lines. Noting that the spikes and dips in demand over the year are not dramatic, we look to build in an anomaly detection component into the model, to search specifically for which points and microtrends most affect telemedicine service demand at certain points over time. We also seek to augment the dataset, to broaden the size of the training and test sets, to further address optimizations to the predictive model. As the field of telemedicine further develops, we also acknowledge that the treatment of ethical issues in telehealth needs to be seen as going beyond individual projects or clinical situations, and be considered in relation to society as a whole and the varied interest groups within it. We note that telemedicine applications pose a number of practical ethical dilemmas for those involved. Examples include telemedicine services‚Äô effects on the doctor-patient relationship, balance of benefits between doctor and patient, questions of accessibility, data security and patients‚Äô confidentiality, the use of innovative but unproved technologies, allocation of scarce resources and opportunity costs, and medical practice, employment, and job satisfaction. Perhaps these problems arise because telemedicine applications involve ethical and evaluation traditions from multiple disciplines coming together, leading to conflicting values and unworkable or partial evaluation frameworks. This is precisely why telemedicine services should increasingly be evaluated in terms of empowering patients and communities, contributing to social cohesion, and democratizing service delivery, instead of intensifying social exclusion (information-rich and information-poor) and depersonalization of health services (providing treatment, not care) [4,6]. We see telemedicine as needing to be understood not only as a medical technology or treatment, but also as service to genuinely improve health. Many proponents of telehealth maintain that it offers a way to follow a duty to do the best for an individual person and the utilitarian aim of allocating limited resources in the best way possible to the benefit of the whole population, and such trends could be computationally modeled relatively easily. However, for future work on forecasting telemedicine service demand, we argue for the incorporation of a more Aristotelian approach when evaluating the changing landscape that is telehealth, with an emphasis on the achievement of healthy people leading fulfilled lives. Notably, telehealth developments have revealed several problems: privacy issues involving patient data, the presence of inaccurate and obsolete data within databases, security breaches, information overload, lack of user-friendliness, and weak standards for linking patient‚Äôs medical and personal information to achieve interoperability for individual records [6]. How new applications fit with changes in healthcare delivery and health information infrastructure, as well as the quality and accuracy of online information, are also of deep concern. Telemedicine developments seek to ‚Äúimprove health care locally, regionally, and worldwide by using information and communication technology‚Äù [8]. However, in many instances of telehealth use, patients have not been the ones to decide whether conventional medicine or telemedicine fits their needs better, or understand how their concerns are incorporated into the technologies. Consequently, ethical problems may arise related to the intentions of those who develop these systems (e.g. SOC Telemed, the health consulting company for which we analyzed service demand data in this report) and the goals they seek. That the design of telemedicine services may benefit service providers and clinicians more than the ill or elderly calls for more attention to setting ethical design and evaluation principles. We consider several additional avenues for research: understanding how provider-centric versus patient-centric telemedicine services are, whether the shift to remote services promotes rationality and efficiency at the expense of values traditionally at the heart of caregiving, how the replacement of human contact with new technologies could best be handled, and to what extent telemedicine services are aimed not toward the improvement of health or well-being but to create market needs. Additionally, situations often arise in which telemedicine care inadvertently leads to poorer quality care. For example, in cases where doctors participated in initiatives to provide accessibility to remote communities and solutions to overcrowded or understaffed hospitals, being distant from patients and overwhelmed by a flood of reports and medical images from several sources led to misdiagnoses [15]. Furthermore, remote clinicians often lack information when they interpret information in the absence of direct knowledge of the patient, possibly leading to imperfect prescriptions. Informed consent also becomes an ethical challenge in such scenarios. While being informed (in the medical sense, being aware of benefits and burdens) is commonly understood in situations like drug trials, new telehealth technologies involve new kinds of risks, often times impossible to anticipate. Indeed, designing telemedicine services is difficult, since different individuals attribute different meanings to the same technology. (For example, a patient may consider a phone-based diet and exercise counseling program as a way for the service company to reduce costs and contact with practitioners while other users may view the app as an empowering alternative to depending on a human counselor.) Thus, standards for evaluating telehealth services are necessary; evaluation should include not only technological and economic, but also ethical and social aspects of the new technologies addressing the long-term effects on personal well-being. All in all, technology should complement clinical and care decisions based on values fundamental to the practice of healthcare ‚Äî compassion, care, humaneness ‚Äî and not be pursued as an end in itself. [0] Kumar, A., Hung, N., Wu, Y., Baek, R., & Gupta, A. (2020). Predictive Modeling for Telemedicine Service Demand. Telehealth and Medicine Today, 5(2). https://doi.org/10.30953/tmt.v5.186 [1] Moore, M. (1999). The evolution of telemedicine. Future generation computer systems, 15(2), 245‚Äì254. [2] Schwamm, L. H., Rosenthal, E. S., Hirshberg, A., Schaefer, P. W., Little, E. A., Kvedar, J. C., Levine, S. R. (2004). Virtual TeleStroke support for the emergency department evaluation of acute stroke. Academic Emergency Medicine, 11(11), 1193‚Äì1197. [3] American Stroke Association. (n.d.). About Stroke. Retrieved from http://www.strokeassociation.org/STROKEORG/AboutStroke/Treatment/Stroke Treatment\_UCM\_492017\_SubHomePage.jsp [4] Hess, D. C., Wang, S., Hamilton, W., Lee, S., Pardue, C., Waller, J. L., Adams, R. J. (2005). REACH: clinical feasibility of a rural telestroke network. Stroke, 36(9), 2018‚Äì2020. [5] Wootton, R. Telemedicine support for the developing world. Journal of telemedicine and telecare, 14(3), 109‚Äì114, 2008. [6] American Hospital Association. Telehealth: Helping Hospitals Deliver Cost-Effective Care. Issue Brief, 1‚Äì7, 2016. [7] Schweigler, L. M., Desmond, J. S., McCarthy, M. L., Bukowski, K. J., Ionides, E. L., Younger, J. G. (2009). Forecasting models of emergency department crowding. Academic Emergency Medicine, 16(4), 301‚Äì308. [8] Schouten, P. Better patient forecasts and schedule optimization improve patient care and curb staffing costs, 2014. [9] Mayo Clinic. (2016, April 20). Chronic traumatic encephalopathy. Retrieved May 15, 2018, from https://www.mayoclinic.org/diseases-conditions/chronic-traumatic- encephalopathy/symptoms-causes/syc-20370921 [10] Mayo Clinic. (2018, March 03). Seizures. Retrieved May 15, 2018, from https://www.mayoclinic.org/diseases-conditions/seizure/symptoms-causes/syc-20365711 [11] Mayo Clinic. (2016, April 20). Chronic traumatic encephalopathy. Retrieved May 15, 2018, from https://www.mayoclinic.org/diseases-conditions/chronic-traumatic-encephalopathy/symptoms-causes/syc-20370921 [12] VanderPlas, J. (n.d.). In Depth: Linear Regression. Retrieved from https://jakevdp.github.io/PythonDataScienceHandbook/05.06-linear-regression.html [13] Adams, H. P., Del Zoppo, G., Alberts, M. J., Bhatt, D. L., Brass, L., Furlan, A., Lyden, P. D. (2007). Guidelines for the early management of adults with ischemic stroke. Circulation, 115(20), e478-e534. [14] Nelson, R. E., Saltzman, G. M., Skalabrin, E. J., Demaerschalk, B. M., Majersik, J. J. (2011). The cost-effectiveness of telestroke in the treatment of acute ischemic stroke. Neurology, 77(17), 1590‚Äì1598. [15] Singh, R., Mathiassen, L., Mishra, A. (2015). Organizational Path Constitution in Technological Innovation: Evidence from Rural Telehealth. Mis Quarterly, 39(3). [16] Switzer, J. A., Demaerschalk, B. M., Xie, J., Fan, L., Villa, K. F., Wu, E. Q. (2012). Cost-effectiveness of hub-and-spoke telestroke networks for the management of acute ischemic stroke from the hospitals‚Äô perspectives. Circulation: Cardiovascular Quality and Outcomes, CIRCOUTCOMES-112. [17] Ali, S. F., Viswanathan, A., Singhal, A. B., Rost, N. S., Forducey, P. G., Davis, L. W., Schwamm, L. H. (2014). The TeleStroke mimic (TM)-score: a prediction rule for identifying stroke mimics evaluated in a Telestroke Network. Journal of the American Heart Association, 3(3), e000838. [18] McCarthy, M. L., Zeger, S. L., Ding, R., Aronsky, D., Hoot, N. R., Kelen, G. D. (2008). The challenge of predicting demand for emergency department services. Academic Emergency Medicine, 15(4), 337‚Äì346. [19] Capampangan, D. J., Wellik, K. E., Bobrow, B. J., Aguilar, M. I., Ingall, T. J., Kiernan, T. E., Demaerschalk, B. M. (2009). Telemedicine versus telephone for remote emergency stroke consultations: a critically appraised topic. The neurologist, 15(3), 163‚Äì166. [20] Ali, S. F., Hubert, G. J., Switzer, J. A., Majersik, J. J., Backhaus, R., Shepard, L. W., Schwamm, L. H. (2018). Validating the TeleStroke Mimic Score: A Prediction Rule for Identifying Stroke Mimics Evaluated Over Telestroke Networks. Stroke, 49(3), 688‚Äì692.",,0,16,Towards Data Science,2021-01-02,2021
https://towardsdatascience.com/lesser-known-skills-required-for-you-being-a-good-data-analyst-f1dd5a072260,Less-known skills required for you to be a good data¬†analyst,"All are non-technical skills, not¬†SQL‚Ä¶",5,29,"['Less-known skills required for you to be a good data analyst', '1. The capability of guessing the result before running an analysis', '2. Spotting an anomaly', '3.Story telling skill', 'Conclusion']","What does a data analyst do? A data analyst is someone who scrutinises information using data analysis tools. The meaningful results they pull from the raw data help their employers or clients make important decisions by identifying various facts and trends. From targetjobs (Link) Over the past five years working on data analytical roles across different industries, I have worked with some data analysts and I started to identify what a good data analyst should equip. Apart from some standard technical skills like SQL and visualization tools, there are some other skills necessary but not considered equally as technical skills. When one is asked about soft skills required for a data analyst, common answers are logical thinking, problem-solving skill, or attention to detail. I am not saying they are not important, however, my answer covers some skills that seem no one is mentioning. Below I will list out some uncommon answers based on my five-year experience in this role. You may question how could be possible to know the result without performing any analysis. This is true. I am not a psychic. Here I am talking about ‚Äúguessing‚Äù, not telling. To achieve this, you need to know the data and business very well. For example, I worked in a commercial mobile department of a telecom company. Every week I needed to collect all sales records to calculate the weekly performance. As you may expect, the average revenue per each sales record was a steady number. Therefore before performing any analysis I already knew how the result should be. To be capable of guessing correctly, you have to know the domain range of attributes, like the age of customers, spending per sales record. This also represents that you need to deeply understand the business behind the data. You should not expect the average age of customers to be 40+ if you work in a social media company, but this can be the case if you work in a bank. When 1 + 1 = 3 This is somehow related to attention to detail. But in my point of view, this is a byproduct of the capability that I mentioned above. The reason for spotting an anomaly accurately is that you realize the result from your analysis is not even remotely closed to your expectation. If you don't have any guess before the analysis, you don't have a reference to compare and you thus have no idea if your analysis is accurate or not. Spotting an anomaly can be also demonstrated on viewing charts. As visualization is more important in a business, we all spend numerous of time building different dashboards for presentation. And since people inevitably make mistakes, spotting an anomaly is required before showing any result to the public. It is meaningless to only show one chart on a dashboard. As a result, you can check if all charts are synchronized with others, like ranges of distribution and changes over time/categories. You should not get any conflicting messages from different charts. If so, this is already a red flag on your analysis. While presenting conflicting messages is a red flag, showing the same pictures across multiple charts is also a red flag. This can happen when you wrongly group your categories, or wrongly setting parameters in your chart. How to check is based on the type of chart, for example, in a scatter plot you can check the distribution of scatters, any repeating outliners on the same position; in a bar chart you can check the magnitude of each category. How to explain your analysis? Tell a story It is more than a presentation skill. Storytelling is not only about how to present your analysis, it‚Äôs also about how you understand your analysis. What makes a good story? Stories are strange things. They are difficult to define, yet always contain the same elements: characters, a plot, and a journey. In a good story those three parts need to all be written well. From Thanet Writers (Link) Without characters, there is no way for a reader to access the story From Thanet Writers (Link) There should be a main theme throughout your analysis, no matter it‚Äôs the sales of a company or the profile of customers. Your story should only focus on this character and this character alone. Otherwise, the audience will not follow your analysis and your work will be worthless. A story needs to go somewhere, it needs a reason to be a story. Plots can be simple or complex, wide-reaching or a brief snippet; as long as there is a plot. From Thanet Writers (Link) It is meaningless and a waste of time if there is no objective in your analysis. As I was told by my former manager, ‚ÄúDon‚Äôt do it for show.‚Äù You need to know the reason for doing this analysis before any action. This does not only prevent you from derailing towards the goal but also helps you explain the meaning of your analysis. The characters progress through the plot, making decisions, and their choices affect what happens next. The story moves forward. From Thanet Writers (Link) The audience should receive a message from your analysis so that people can take action (or understand no action is needed). If there is no conclusion, there are two possible reasons: one is that your analysis is not deep enough; two is your analysis does not match with the objective. Either way, your work is not completed. Combining all three components, when you present your analysis, you can allow the audience to focus on a particular theme with a current situation that is needed to be solved. People often weigh more on technical skills and less on soft skills in data analytical roles. The problem is the number does not speak itself and there is someone to explain. This is what a data analyst also does, not just sitting behind a computer writing codes and playing with Excel or other visualization tools. I hope my sharing helps you equip a more comprehensive background for being a data analyst in 2021. See you next time.",38,0,5,Towards Data Science,2021-01-02,2021
https://towardsdatascience.com/7-predictions-for-data-in-2021-3b7fbc92b71f,7 predictions for data in¬†2021,"Including DataOps, Data Strategy and¬†xAI.",1,3,['7 predictions for data in 2021'],"Humans are notoriously poor at predicting the future (especially in the longer term). However, it still can be a useful exercise. Let‚Äôs look at what I think 2021 will bring to data science, engineering, and strategy. This list is in no particular order and is mostly focused on my observations in Germany: With this list in mind, I want to make two wishes for 2021 in data. First, I hope the whole field, but especially the ML part of it, becomes more ‚Äúboring‚Äù, but useful. Second, we start to use this fantastic technology to solve the pressing issues we face and move to a more optimistic and ambitious future. Note: This article originally appeared on https://boyanangelov.com/blog/data-predictions-2021/",227,0,2,Towards Data Science,2021-01-02,2021
https://towardsdatascience.com/large-scale-distributed-randomforest-with-kubernetes-operator-797c68c065c,Large Scale Distributed RandomForest with Kubernetes Operator,Distributed Training in Kubernetes‚Ä¶,6,13,"['Large Scale Distributed RandomForest with Kubernetes Operator', '1. What is Kubernetes Operator', '2. The DistributedTraining Operator', '3. Set up the Operator in Your Cluster', '4. My Example Demo', '5. Conclusion']","A few days ago I was exploring the RandomForest model with a huge dataset. To my disappointment, the training data couldn‚Äôt fit into the memory of the single working node. I had to manually split the training data to train a few RandomForest models, before combining the models into a single one. This prompted me to an idea: can I come up with a framework to automate and distribute the trainings for ensemble-able models with huge datasets? I could just prepare and submit a simple configuration file and the framework will orchestrate, monitor and distribute the trainings and produce the model artifact automatically. After a few days of research and implementation, I have come up with the first version of such a framework, which is a Kubernetes Operator. I have made use of operator-sdk (0.19.4) and the Kubernetes cluster (1.16.15) from GCP. The operator fulfills the basic requirements for the automation of the distributed training, although there is still much room for improvement. The github repository is here. In the following sections, I will explain what Kubernetes Operator is, followed by what the DistributedTraining Operator looks like. And finally, I will also show you how to set up the Operator in your own cluster. In a nutshell, a Kubernetes operator is an application-specific controller that extends the functionality of the Kubernetes API to create, configure, and manage instances of complex applications on behalf of a Kubernetes user. You might be familiar with the basic Kubernetes concepts like Pod, Deploy, Service etc. These are the built-in Kubernetes API Resources. However, automating and managing complex stateful applications have become and hassle in the early days of Kubernetes. Since version 1.7, Kubernetes introduced an API plugin mechanism: Custom Resource Definition or CRD. CRD allows user to specify their own API Resource (Customer Resource). Kubernetes Operator makes use of custom Kubernetes controller and Custom Resource (CR) to manage applications and their components. In my example, the Custom Resource is called DistributedTraining, which consist of 1 master Deployment & Service with 1 Pod, and 1 worker Deployment with user-specified number of Pods. The Operator automatically orchestrate, monitor and manage all the components and the working relationships among them. Below is the working flow of whole process. What user needs to do is just to specify and submit the configuration file. The Operator will take care of the rest of the steps. As you can see that the YAML configuration file is fairly straight -forward. After preparing the training data in the GCS bucket, you just need to specify the model-related values, the bucket name as well as the number of workers, depending on the Kubernetes capacity and your requirements. Then you could just submit the file to the cluster (kubectl apply -f <file_name>) and wait for the model artifact to be available. Using a simulated dataset of 40,000 data points, I have validated the models from the 3 sources: Please refer to this notebook for further reference In this article, I have introduced the Kubernetes Operator that I developed for Distributed Training of ML models. Kubernetes is a very popular and powerful platform for automatically orchestrating and managing distributed workloads. The declarative nature of workloads in Kubernetes, plus its exceptional capability for defining and managing complex relationship within applications, make Kubernetes one of the most suitable tools for handling complicated ML workloads. I hope this small little tool could make your life easier if you have similar use cases!",19,1,6,Towards Data Science,2021-01-02,2021
https://towardsdatascience.com/why-learn-to-forget-in-recurrent-neural-networks-ddf89df3f0ab,"<strong class=""markup--strong markup--h3-strong"">Why ‚ÄòLearn To Forget‚Äô in Recurrent Neural¬†Networks</strong>",Illustrated with a simple¬†example,1,39,['Why ‚ÄòLearn To Forget‚Äô in Recurrent Neural Networks'],"Consider the following binary classification problem. The input is a binary sequence of arbitrary length. We want the output to be 1 if and only if a 1 occurred in the input but not too recently. Specifically, the last n bits must be 0. We can also write this problem as one on language recognition. For n = 4, the language, described as a regular expression, is (0 or 1)*10000*. Below are some labeled instances for the case n = 3. Why this seemingly strange problem? It requires remembering that (i) a 1 occurred in the input and (ii) not too recently. As we will see soon, this example helps explain why simple recurrent neural networks are inadequate and how injecting a mechanism that learns to forget helps. Simple Recurrent Neural Network Let‚Äôs start with a basic recurrent neural network (RNN). x(t) is the bit, 0 or 1, that arrives at time t in the input. This RNN maintains a state h(t) that tries to remember whether it saw a 1 sometime in the past. The output is just read out from this state after a suitable transformation. More formally, we have Next, let‚Äôs consider the following (input sequence, output sequence) pair and assume n = 3. To discuss the behavior and learning of the RNN on this pair, it will help to unroll the network in time as is commonly done. Think of this as a pipeline with stages. The state travels from left to right and gets modified during the process by the input at a stage. Let‚Äôs walk through what happens inside a stage in a bit more detail. Consider the third stage. It inputs the state h2 and the next input symbol x3. h2 may be thought of as a feature derived from x1 and x2 towards predicting y3. The box first computes the next state h3 from these two inputs. h3 is then carried forward to the next stage. h3 also determines the stage‚Äôs output y3. Consider what happens when the input 1000 is seen. y4 is 1 and since y^4 is less than 1 (which is always the case) there is some error. Following the backpropagation-through-time learning strategy, we will ripple the error back through time to the extent needed to update the various parameters. Consider the parameter b. There are 4 instances of it, attached to x1 through x4 respectively. The instances attached to x2 through x4 don‚Äôt change since x2 through x4 are all 0. So none of these b instances have any impact on y^4. The instance of b attached to x1 increases as making this change gets y^4 closer to 1. As we continue seeing x5, x6, x7, x8, and their corresponding targets y5, y6, y7, and y8, the same learning behavior will happen. b will keep increasing. (Albeit less so as we need to backpropagate the errors further back in time to get to x1.) Now imagine x9 is 1. y9 must be 0. y^9 is however large. This is because the parameter b has learned that xi = 1 predicts yj = 1 for j >= i. b has no way of enforcing that xi = 1 must be followed only by 0s, numbering at least 3. In short, this RNN is unable to capture the joint interaction of xi = 1 and all the bits that follow it are 0s, numbering at least 3, towards predicting yj. Also note that this is not a long-range influence. n is only 3. So the weakness of the RNN on this example cannot be explained in terms of vanishing error gradients when doing backpropagation-through-time [2]. There is something else going on here. An RNN that learns to forget Now consider this version We didn‚Äôt just pull it out of a hat. It is a key one in a popular gated recurrent neural network called GRU. We took this equation from it‚Äôs description in [1]. This RNN has an explicit mechanism to forget! It is z(t), a value between 0 and 1, denoting the degree of forgetfulness. When z(t) approaches 1, the state h(t-1) is completely forgotten. When h(t-1) is completely forgotten, what should h(t) be? We encapsulate this is in an explicit function hnew(t) denoting ‚Äúnew state‚Äù. hnew(t) is derived solely from the present input. This makes sense because if h(t-1) is to be forgotten, all we have in front of us is the new input x(t). More generally, the next state h(t) is a mixture of the previous state h(t-1) and a new state hnew(t), modulated by z(t). Does this RNN have the capability to do better on this problem? We will answer this question in the affirmative by prescribing a solution that works. The accompanying explanation will reveal what roles the various neurons play in making this solution work. Consider x(t) is 1. y(t) must be 0. So we want to drive y^(t) towards 0. We can make this happen by setting e to a sufficiently negative number (say -1) and forcing h(t) to be close to 1. One way to get the desired h(t) is to force z(t) to be close to 1 and set c to a sufficiently positive number and d such that c+d is sufficiently positive. We can force z(t) to be close to 1 by setting a to be a sufficiently positive number and b such that a+b is sufficiently positive. This prescription operates as if The case x(t) is 0 is more involved as y(t) depends on the recent past values of x. Let‚Äôs explain it in the following setting: There is a lot in here! So let‚Äôs walk through it row by row. We are looking at the situation when processing the last 4 bits of the input x = ‚Ä¶1000 in sequence. The corresponding target is y = ‚Ä¶0001. We assume that the parameters of the RNN have been somehow chosen just right (or learned) as surfaced below. (These have to be consistent with the settings we used when x(t) was 1, of course.) In short, we are describing the behavior of a fixed network in this situation. Now look at hnew. When x(t) is 1, we have already discussed that hnew(t) should approach 1. When x(t) is 0, hnew(t) equals tanh(cx(t)+d)=tanh(d). We are calling this D. Next look at z. When x(t) is 1, we already discussed that z(t) should approach 1. When x(t) is 0, since we want to remember the past, let‚Äôs set z(t) to approximately ¬Ω. For this, we just need to set b to 0. This can be achieved without unlearning the z(t) that works when x(t) is 1. For the remaining rows, let‚Äôs start from the last row and work our way in. In the y^ row, we describe what we want, given the y targets. Given that we have fixed e to a sufficiently negative number, this gives us what we want from our states. We call them h^. So now all that remains is to show that h can be made to match up with h^. First let‚Äôs zoom into these two rows and while at it also transform h to a more convenient form It can be seen that choosing D such that -‚Öì < D < -1/7 will meet the desiderata. It's easy to find d such that tanh(d) is in this range. The prescription for the case x(t) = 0 may be summarized as So as 0s that follow a 1 are seen, h(t) keeps dropping. If enough 0s are seen, h(t) becomes negative. Summary In this post, we discussed recurrent neural networks with and without an explicit ‚Äòforget‚Äô mechanism. We discussed it in the context of a simply-described prediction problem which the simpler RNN is incapable of solving. The RNN with the ‚Äòforget‚Äô mechanism is able to solve this problem. This post will be useful to readers who‚Äôd like to understand how simple RNNs work, how an enhanced version with a forgetting mechanism works (GRU in particular), and how the latter improves upon the former. Further Reading",7,0,7,Towards Data Science,2021-01-02,2021
https://towardsdatascience.com/10-statistical-concepts-you-should-know-for-data-science-interviews-373f417e7d11,10 Statistical Concepts You Should Know For Data Science Interviews,"Study smart, not¬†hard.",13,56,"['10 Statistical Concepts You Should Know For Data Science Interviews', 'Introduction', '1) P-values', '2) Confidence Intervals and Hypothesis Testing', '3) Z-tests vs T-tests', '4) Linear regression and its assumptions', '5) Logistic regression', '6) Sampling techniques', '7) Central Limit Theorem', '8) Combinations and Permutations', '9) Bayes Theorem/Conditional Probability', '10) Probability Distributions', 'Thanks for Reading!']","Statistics can feel really overwhelming at times, and it‚Äôs no surprise as the practice of statistics has been around for thousands of years! When it comes to data science interviews, however, there are only so many concepts that interviewers test. After going through hundreds and hundreds of data science interview questions, I compiled the 10 statistical concepts that came up the most often. In this article, I‚Äôm going to go over these 10 concepts, what they‚Äôre all about, and why they‚Äôre so important. With that said, here we go! The most technical and precise definition of a p-value is that it is the probability of achieving a result that‚Äôs just as extreme or more extreme than the result if the null hypothesis is too. If you think about it, this makes sense. In practice, if the p-value is less than the alpha, say of 0.05, then we‚Äôre saying that there‚Äôs a probability of less than 5% that the result could have happened by chance. Similarly, a p-value of 0.05 is the same as saying ‚Äú5% of the time, we would see this by chance.‚Äù So if the initial definition doesn‚Äôt stick with you, remember the example I just gave above! Confidence intervals and hypothesis testing share a very close relationship. The confidence interval suggests a range of values for an unknown parameter and is then associated with a confidence level that the true parameter is within the suggested range of. Confidence intervals are often very important in medical research to provide researchers with a stronger basis for their estimations. A confidence interval can be shown as ‚Äú10 +/- 0.5‚Äù or [9.5, 10.5] to give an example. Hypothesis testing is the basis of any research question and often comes down to trying to prove something did not happen by chance. For example, you could try to prove when rolling a dye, one number was more likely to come up than the rest. Understanding the differences between z-tests and t-tests as well as how and when you should choose to use each of them is invaluable in statistics. A Z-test is a hypothesis test with a normal distribution that uses a z-statistic. A z-test is used when you know the population variance or if you don‚Äôt know the population variance but have a large sample size. A T-test is a hypothesis test with a t-distribution that uses a t-statistic. You would use a t-test when you don‚Äôt know the population variance and have a small sample size. You can see the image below as a reference to guide which test you should use: Linear Regression is one of the most fundamental algorithms used to model relationships between a dependent variable and one or more independent variables. In simpler terms, it involves finding the ‚Äòline of best fit‚Äô that represents two or more variables. The line of best fit is found by minimizing the squared distances between the points and the line of best fit ‚Äî this is known as minimizing the sum of squared residuals. A residual is simply equal to the predicted value minus the actual value. In case it doesn‚Äôt make sense yet, consider the image above. Comparing the green line of best fit to the red line, notice how the vertical lines (the residuals) are much bigger for the green line than the red line. This makes sense because the green line is so far away from the points that it isn‚Äôt a good representation of the data at all! There are four assumptions associated with a linear regression model: Logistic regression is similar to linear regression but is used to model the probability of a discrete number of outcomes, typically two. For example, you might want to predict whether a person is alive or dead given their age. At a glance, logistic regression sounds much more complicated than linear regression, but really only has one extra step. First, you calculate a score using an equation similar to the equation for the line of best fit for linear regression. The extra step is feeding the score that you previously calculated in the sigmoid function below so that you get a probability in return. This probability can then be converted to a binary output, either 1 or 0. To find the weights of the initial equation to calculate the score, methods like gradient descent or maximum likelihood are used. Since it‚Äôs beyond the scope of this article, I won‚Äôt go into much more detail, but now you know how it works! There are 5 main ways that you can sample data: Simple Random, Systematic, Convenience, Cluster, and Stratified sampling: Simple random sampling requires using randomly generated numbers to choose a sample. More specifically, it initially requires a sampling frame, a list or database of all members of a population. You can then randomly generate a number for each element, using Excel for example, and take the first n samples that you require. Systematic sampling can be even easier to do, you simply take one element from your sample, skip a predefined amount (n) and then take your next element. Going back to our example, you could take every fourth name on the list. Convenience sampling takes a sample from a group that is easy to contact, for example asking people outside a shopping center. You just sample the first people you run into. This technique is often considered poor practice to use as your data could be viewed as bias. Cluster sampling starts by dividing a population into groups, or clusters. What makes this different that stratified sampling is that each cluster must be representative of the population. Then, you randomly selecting entire clusters to sample. For example, if an elementary school had five different grade eight classes, cluster random sampling might be used and only one class would be chosen as a sample, for example. Stratified random sampling starts off by dividing a population into groups with similar attributes. Then a random sample is taken from each group. This method is used to ensure that different segments in a population are equally represented. To give an example, imagine a survey is conducted at a school to determine overall satisfaction. It might make sense here to use stratified random sampling to equally represent the opinions of students in each department. The central limit theorem is very powerful ‚Äî it states that the distribution of sample means approximates a normal distribution. To give an example, you would take a sample from a data set and calculate the mean of that sample. Once repeated multiple times, you would plot all your means and their frequencies onto a graph and see that a bell curve, also known as a normal distribution, has been created. The mean of this distribution will closely resemble that of the original data. You can improve the accuracy of the mean and reduce the standard deviation by taking larger samples of data and more samples overall. Combinations and permutations are two slightly different ways that you can select objects from a set to form a subset. Permutations take into consideration the order of the subset whereas combinations do not. Combinations and permutations are extremely important if you‚Äôre working on network security, pattern analysis, operations research, and more. Let‚Äôs review what each of the two are in further detail: Definition: A permutation of n elements is any arrangement of those n elements in a definite order. There are n factorial (n!) ways to arrange n elements. Note the bold: order matters! The number of permutations of n things taken r-at-a-time is defined as the number of r-tuples that can be taken from n different elements and is equal to the following equation: Example Question: How many permutations does a license plate have with 6 digits? Definition: The number of ways to choose r out of n objects where order doesn‚Äôt matter. The number of combinations of n things taken r-at-a-time is defined as the number of subsets with r elements of a set with n elements and is equal to the following equation: Example Question: How many ways can you draw 6 cards from a deck of 52 cards? Note that these are very very simple questions and that it can get much more complicated than this, but you should have a good idea of how it works with the examples above! Bayes theorem is a conditional probability statement, essentially it looks at the probability of one event (B) happening given that another event (A) has already happened. One of the most popular machine learning algorithms, Na√Øve Bayes, is built on these two concepts. Additionally, if you enter the realm of online machine learning, you‚Äôll most likely be using Bayesian methods. A probability distribution is an easy way to find your probabilities of different possible outcomes in an experiment. There are many different distribution types you should learn about but a few I would recommend are Normal, Uniform, and Poisson. The normal distribution, also known as the Gaussian distribution, is a bell-shaped curve that is quite prominent in many distributions, including the height of people and IQ scores. The mean of the normal distribution is equal to Œº and the variance is equal to œÉ. The Poisson distribution is a discrete distribution that gives the probability of the number of independent events occurring in a fixed time. An example of when you would use this is if you want to determine the likelihood of X patients coming into a hospital in a given hour. The mean and variance are both equal to Œª. A uniform distribution is used when all outcomes are equally likely. For example, a coin has a uniform distribution as well as a dye. And that‚Äôs all! I hope that this helps you in your interview prep and I wish you the best of luck in your future endeavors. Having a strong understanding of these ten concepts will serve as a strong base for further learning in the realm of data science and machine learning. As always, I wish you the best in your endeavors! Not sure what to read next? I‚Äôve picked another article for you: towardsdatascience.com and another! towardsdatascience.com",1600,6,9,Towards Data Science,2021-01-03,2021
https://towardsdatascience.com/7-popular-activation-functions-you-should-know-in-deep-learning-and-how-to-use-them-with-keras-and-27b4d838dfe6,7 popular activation functions you should know in Deep Learning and how to use them with Keras¬†and,,11,54,"['7 popular activation functions you should know in Deep Learning and how to use them with Keras and TensorFlow 2', '1. Sigmoid (Logistic)', '2. Hyperbolic Tangent (Tanh)', '3. Rectified Linear Unit (ReLU)', '4. Leaky ReLU', '5. Parametric leaky ReLU (PReLU)', '6. Exponential Linear Unit (ELU)', '7. Scaled Exponential Linear Unit (SELU)', 'How to choose an activation function?', 'Conclusion', 'References']","In artificial neural networks (ANNs), the activation function is a mathematical ‚Äúgate‚Äù in between the input feeding the current neuron and its output going to the next layer [1]. The activation functions are at the very core of Deep Learning. They determine the output of a model, its accuracy, and computational efficiency. In some cases, activation functions have a major effect on the model‚Äôs ability to converge and the convergence speed. In this article, you‚Äôll learn the following most popular activation functions in Deep Learning and how to use them with Keras and TensorFlow 2. Please check out Notebook for the source code. The Sigmoid function (also known as the Logistic function) is one of the most widely used activation function. The function is defined as: The plot of the function and its derivative. As we can see in the plot above, The Sigmoid function was introduced to Artificial Neural Networks (ANN) in the 1990s to replace the Step function [2]. It was a key change to ANN architecture because the Step function doesn‚Äôt have any gradient to work with Gradient Descent, while the Sigmoid function has a well-defined nonzero derivative everywhere, allowing Gradient Descent to make some progress at every step during training. The main problems with the Sigmoid function are: To use the Sigmoid activation function with Keras and TensorFlow 2, we can simply pass 'sigmoid' to the argument activation : To apply the function for some constant inputs: Another very popular and widely used activation function is the Hyperbolic Tangent, also known as Tanh. It is defined as: The plot of the function and its derivative: We can see that the function is very similar to the Sigmoid function. Tanh has characteristics similar to Sigmoid that can work with Gradient Descent. One important point to mention is that Tanh tends to make each layer‚Äôs output more or less centered around 0 and this often helps speed up convergence [2]. Since Tanh has characteristics similar to Sigmoid, it also faces the following two problems: To use the Tanh, we can simply pass 'tanh' to the argument activation: To apply the function for some constant inputs: The Rectified Linear Unit (ReLU) is the most commonly used activation function in deep learning. The function returns 0 if the input is negative, but for any positive input, it returns that value back. The function is defined as: The plot of the function and its derivative: As we can see that: It‚Äôs surprising that such a simple function works very well in deep neural networks. ReLU works great in most applications, but it is not perfect. It suffers from a problem known as the dying ReLU. Dying ReLU During training, some neurons effectively die, meaning they stop outputting anything other than 0. In some cases, you may find that half of your network‚Äôs neurons are dead, especially if you used a large learning rate. A neuron dies when its weights get tweaked in such a way that the weighted sum of its inputs are negative for all instances in the training set. When this happens, it just keeps outputting 0s, and gradient descent does not affect it anymore since the gradient of the ReLU function is 0 when its input is negative. Hands-on Machine Learning [2], page 329 To use ReLU with Keras and TensorFlow 2, just set activation='relu' To apply the function for some constant inputs: Leaky ReLU is an improvement over the ReLU activation function. It has all properties of ReLU, plus it will never have dying ReLU problem. Leaky ReLU is defined as: The hyperparameter Œ± defines how much the function leaks. It is the slope of the function for x < 0 and is typically set to 0.01. The small slope ensures that Leaky ReLU never dies. To use the Leaky ReLU activation function, you must create a LeakyReLU instance like below: Parametric leaky ReLU (PReLU) is a variation of Leaky ReLU, where Œ± is authorized to be learned during training (instead of being a hyperparameter, it becomes a parameter that can be modified by backpropagation like any other parameters). This was reported to strongly outperform ReLU on large image datasets, but on smaller datasets it runs the risk of overfitting the training set [2]. To use Parametric leaky ReLU, you must create a PReLU instance like below: Exponential Linear Unit (ELU) is a variation of ReLU with a better output for z < 0. The function is defined as: The hyperparameter Œ± controls the value to which an ELU saturates for negative net inputs. The plot of the function and its derivative: We can see in the plot above, According to the authors, ELU outperformed all the ReLU variants in their experiments [3]. According to [2, 3], the main drawback of the ELU activation is that it is slower to compute than the ReLU and its variants (due to the use of the exponential function), but during training this is compensated by the faster convergence rate. However, at test time, an ELU network will be slower than a ReLU network. Implementing ELU in TensorFlow 2 is trivial, just specify the activation function when building each layer: To apply the function for some constant inputs: Exponential Linear Unit (SELU) activation function is another variation of ReLU proposed by GuÃànter Klambauer et al. [4] in 2017. The authors showed that if you build a neural network composed exclusively of a stack of dense layers, and if all hidden layers use the SELU activation function, then the network will self-normalize (the output of each layer will tend to preserve mean 0 and standard deviation 1 during training, which resolves the vanishing/exploding gradients problem). This activation function often outperforms other activation functions very significantly. SELU is defined as: where Œ± and scale are pre-defined constants (Œ±=1.67326324 and scale=1.05070098). The plot of SELU and its derivative: The main problem with SELU is that there are a few conditions for SELU to work: To use SELU with Keras and TensorFlow 2, just set activation='selu' and kernel_initializer='lecun_normal': We have gone through 7 different activation functions in deep learning. When building a model, the selection of activation functions is critical. So which activation function should you use? Here is a general suggestion from the book Hands-on ML Although your mileage will vary, in general SELU > ELU > leaky ReLU (and its variants) > ReLU > tanh > logistic. If the network‚Äôs architecture prevents it from self-normalizing, then ELU may perform better than SELU (since SELU is not smooth at z = 0). If you care a lot about runtime latency, then you may prefer leaky ReLU. If you don‚Äôt want to tweak yet another hyperparameter, you may just use the default Œ± values used by Keras (e.g., 0.3 for the leaky ReLU). If you have spare time and computing power, you can use cross-validation to evaluate other activation functions, in particular, RReLU if your network is over‚Äêfitting, or PReLU if you have a huge training set. Hands-on ML, page 332 In this article, we have gone through 7 different activation functions in Deep Learning and how to use them with Keras and TensorFlow. I hope this article will help you to save time in building and tuning your own Deep Learning model. I recommend you to check out the Keras documentation for the activation functions and to know about other things you can do. Thanks for reading. Please check out the notebook for the source code and stay tuned if you are interested in the practical aspect of machine learning. More can be found from my Github",260,0,10,Towards Data Science,2021-01-03,2021
https://towardsdatascience.com/advanced-numpy-master-stride-tricks-with-25-illustrated-exercises-923a9393ab20,Advanced NumPy: Master stride tricks with 25 illustrated exercises,"Includes code, explanations and‚Ä¶",7,182,"['Advanced NumPy: Master stride tricks with 25 illustrated exercises', 'Exercises', '1D exercises', '2D exercises', '3D exercises', '4D exercises', 'Resources']","Best viewed with a Chrome browser on desktop Changelog:31 Dec 2022: Use Medium‚Äôs new code block for syntax highlighting6 Jan 2022: Updated Problem 19 based on comments from Sirouan Nouriddine5 Jan 2022: Fix typos and improve clarity30 Dec 2021: As of NumPy 1.20.0, there is a sliding_window_view API which can also serve as a higher-level API on top of the as_strided API. See here for more (Jump to the exercises here) The stride tricks API can be seen as an extension of the usual way of accessing and manipulating NumPyndarray‚Äôs, giving users more flexibility to control the resulting NumPy view. While it is an esoteric feature, one particularly practical usage is when sliding windows or rolling statistics are concerned. In this article, we will go through 25 different exercises that use this API (and compare to how we would‚Äôve done it normally). For this article, it is recommended that the reader have mid-level knowledge of Python, NumPy, numpy.dtype, numpy.ndarray.strides, and numpy.ndarray.itemsize. For a quick introduction to NumPy arrays and strides, see the section on üí° A bit of background below. The exercises, which are arranged with increasing difficulty, are formatted in the following order: üí° A bit of background How would you access a specific item from a block of fixed-size elements which have been (i) placed contiguously, and (ii) organised into nested sub-groups? Answer: strides. What I just briefly described is a NumPy N-dimensional array (ndarray) data structure, and we use an algorithm called the strided indexing scheme together with strides to traverse it. Here are 4 quick bites you should know about NumPy arrays. 1) Elements in NumPy arrays occupy memory. Every element in a NumPy array uniformly occupies n bytes. For example, every element in an array with data type np.int64 occupies 8 bytes. To find out the item size, np.ndarray.itemsize. 2) Elements in NumPy arrays are stored contiguously in memory. This means they are stored side-by-side (unlike elements in Python lists). 3) There is a piece of information called shape that you probably already know, that defines how large this array is for every dimension. To access this information, np.ndarray.shape. 4) There is another piece of information called strides that indicate the number of bytes to jump to reach the next value in the dimension. To access this information, np.ndarray.strides. With these 4 pieces of information, the memory location of an element can be found by a linear combination of the dimension with the strides as the coefficients. For a more in-depth explanation, refer to my references below. Answer üí° ExplanationAdjacent elements in the output (i.e. 1 ‚Üí 2, 2 ‚Üí 3) were originally 1 byte apart (=1 element away √ó 1 byte) in the input. The shape of this dimension is 3. Code Similar to Answer üí° ExplanationAdjacent elements in the output (eg. 1 ‚Üí 2, 2 ‚Üí 3, 6 ‚Üí 7) were originally 1 byte apart (=1 element away √ó 1 byte) in the input. The shape of this dimension is 8. Code Similar to Answer üí° ExplanationAdjacent elements in the output (eg. 1 ‚Üí 2, 2 ‚Üí 3, 23 ‚Üí 24, 24 ‚Üí 25) were originally 2 bytes apart (=1 element away √ó 2 bytes) in the input. The shape of this dimension is 25. Code Similar to Answer üí° ExplanationAdjacent elements in the output (i.e. 1 ‚Üí 3, 3 ‚Üí 5) were originally 2 bytes apart (=2 elements away √ó 1 byte) in the input. The shape of this dimension is 3. Code Similar to Answer üí° ExplanationAdjacent elements in the output (i.e. 1 ‚Üí 6, 6 ‚Üí 11, 11 ‚Üí 16) were originally 40 bytes apart (=5 elements away √ó 8 bytes) in the input. The shape of this dimension is 4. Code Similar to Answer üí° ExplanationAdjacent elements in the output (i.e. 1 ‚Üí 7, 7 ‚Üí 13, 13 ‚Üí 19, 19 ‚Üí 25) were originally 48 bytes apart (=6 elements away √ó 8 bytes) in the input. The shape of this dimension is 5. Code Similar to Answer üí° ExplanationAdjacent elements in the output (i.e. 1 ‚Üí 1) were originally 0 bytes apart (=0 elements away √ó 8 bytes) in the input. The shape of this dimension is 5. Code Similar to Answer üí° Explanation Output‚Äôs left-to-right dimension (axis=-1):Adjacent elements in the output (eg. 1 ‚Üí 2, 2 ‚Üí 3, 8 ‚Üí 9) were originally 8 bytes apart (=1 element away √ó 8 bytes) in the input. The shape of this dimension is 4. Output‚Äôs left-to-right dimension (axis=-2):Adjacent elements in the output (eg. 1 ‚Üí 6, 2 ‚Üí 7, 9 ‚Üí 14) were originally 40 bytes apart (=5 elements away √ó 8 bytes) in the input. The shape of this dimension is 3. Code Similar to Answer üí° Explanation Output‚Äôs left-to-right dimension (axis=-1):Adjacent elements in the output (eg. 1 ‚Üí 2, 7 ‚Üí 8, 13 ‚Üí 14, 19 ‚Üí 20) were originally 8 bytes apart (=1 element away √ó 8 bytes) in the input. The shape of this dimension is 2. Output‚Äôs left-to-right dimension (axis=-2):Adjacent elements in the output (eg. 1 ‚Üí 7, 2 ‚Üí 8, 13 ‚Üí 19) were originally 48 bytes apart (=6 elements away √ó 8 bytes) in the input. The shape of this dimension is 4. Code Similar to Answer üí° Explanation Output‚Äôs left-to-right dimension (axis=-1):Adjacent elements in the output (eg. 1 ‚Üí 3, 21 ‚Üí 23, 13 ‚Üí 15) were originally 16 bytes apart (=2 elements away √ó 8 bytes) in the input. The shape of this dimension is 3. Output‚Äôs left-to-right dimension (axis=-2):Adjacent elements in the output (eg. 1 ‚Üí 11, 13 ‚Üí 23, 15 ‚Üí 25) were originally 80 bytes apart (=10 elements away √ó 8 bytes) in the input. The shape of this dimension is 3. Code Similar to Answer üí° Explanation Output‚Äôs left-to-right dimension (axis=-1):Adjacent elements in the output (eg. 1 ‚Üí 6, 7 ‚Üí 12, 3 ‚Üí 8) were originally 5 bytes apart (=5 elements away √ó 1 byte) in the input. The shape of this dimension is 3. Output‚Äôs top-to-right dimension (axis=-2):Adjacent elements in the output (eg. 1 ‚Üí 2, 2 ‚Üí 3, 11 ‚Üí 12) were originally 1 byte apart (=1 element away √ó 1 byte) in the input. The shape of this dimension is 3. Code Similar to Answer üí° Explanation Output‚Äôs left-to-right dimension (axis=-1):Adjacent elements in the output (eg. 1 ‚Üí 1, 6 ‚Üí 6, 16 ‚Üí 16) were originally 0 bytes apart (=0 elements away √ó 4 bytes) in the input. The shape of this dimension is 4. Output‚Äôs top-to-bottom dimension (axis=-2):Adjacent elements in the output (eg. 1 ‚Üí 6, 6 ‚Üí 11, 11 ‚Üí 16) were originally 20 bytes apart (=5 elements away √ó 4 bytes) in the input. The shape of this dimension is 5. Code Similar to Answer üí° Explanation Output‚Äôs left-to-right dimension (axis=-1):Adjacent elements in the output (eg. 1 ‚Üí 2, 2 ‚Üí 3, 7 ‚Üí 8, 11 ‚Üí 12) were originally 8 bytes apart (=1 element away √ó 8 bytes) in the input. The shape of this dimension is 3. Output‚Äôs top-to-bottom dimension (axis=-2):Adjacent elements in the output (eg. 1 ‚Üí 4, 4 ‚Üí 7, 7 ‚Üí 10) were originally 24 bytes apart (=3 elements away √ó 8 bytes) in the input. The shape of this dimension is 4. Code Similar to Adapted from a StackOverflow post [1]. Similar to [2] and [3]. Answer üí° Explanation Output‚Äôs left-to-right dimension (axis=-1):Adjacent elements in the output (eg. 1 ‚Üí 2, 3 ‚Üí 4, 4 ‚Üí 5) were originally 1 byte apart (=1 element away √ó 1 byte) in the input. The shape of this dimension is 3. Output‚Äôs top-to-bottom dimension (axis=-2):Adjacent elements in the output (i.e. 1 ‚Üí 2, 2 ‚Üí 3, 4 ‚Üí 5, ‚Ä¶, 7 ‚Üí 8) were originally 1 byte apart (=1 element away √ó 1 byte) in the input. The shape of this dimension is 8. Code Similar to Question taken from a StackOverflow post [4]. Answer üí° Explanation Output‚Äôs left-to-right dimension (axis=-1):Adjacent elements in the output (eg. 0 ‚Üí 1, 1 ‚Üí 10, 31 ‚Üí 40) were originally 1 byte apart (=1 element away √ó 1 byte) in the input. The shape of this dimension is 6. Output‚Äôs top-to-bottom dimension (axis=-2):Adjacent elements in the output (eg. 0 ‚Üí 10, 10 ‚Üí 20, 41 ‚Üí 51) were originally 2 bytes apart (=2 elements away √ó 1 byte) in the input. The shape of this dimension is 4. Code Similar to Answer üí° Explanation Output‚Äôs left-to-right dimension (axis=-1):Adjacent elements in the output (eg. 1 ‚Üí 2, 3 ‚Üí 4, 4 ‚Üí 5) were originally 1 byte apart (=1 element away √ó 1 byte) in the input. The shape of this dimension is 4. Output‚Äôs top-to-bottom dimension (axis=-2):Adjacent elements in the output (i.e. 1 ‚Üí 5, 5 ‚Üí 9, 2 ‚Üí 6, 6 ‚Üí 10) were originally 4 bytes apart (=4 elements away √ó 1 byte) in the input. The shape of this dimension is 3. Code Similar to Answer üí° Explanation Output‚Äôs left-to-right dimension (axis=-1):Adjacent elements in the output (i.e. 1 ‚Üí 2, 6 ‚Üí 7, 16 ‚Üí 17, 21 ‚Üí 22) were originally 2 bytes apart (=1 element away √ó 2 bytes) in the input. The shape of this dimension is 2. Output‚Äôs top-to-bottom dimension (axis=-2):Adjacent elements in the output (i.e. 1 ‚Üí 6, 2 ‚Üí 7, 16 ‚Üí 21, 17 ‚Üí 22) were originally 10 bytes apart (=5 elements away √ó 2 bytes) in the input. The shape of this dimension is 2. Output‚Äôs box-to-box dimension (axis=-3):Adjacent elements in the output (i.e. 1 ‚Üí 16, 2 ‚Üí 17, 6 ‚Üí 21, 7 ‚Üí 22) were originally 30 bytes apart (=15 elements away √ó 2 bytes) in the input. The shape of this dimension is 2. Code Similar to Answer üí° Explanation Output‚Äôs intrabox left-to-right dimension (axis=-1):Adjacent elements in the output (eg. 1 ‚Üí 2, 2 ‚Üí 3, 17 ‚Üí 18) were originally 1 byte apart (=1 element away √ó 1 byte) in the input. Shape of this dimension is 3. Output‚Äôs intrabox top-to-bottom dimension (axis=-2):Adjacent elements in the output (eg. 1 ‚Üí 7, 11 ‚Üí 17, 12 ‚Üí 18) were originally 6 bytes apart (=6 elements away √ó 1 byte) in the input. The shape of this dimension is 2. Output‚Äôs left box-to-right box dimension (axis=-3):Adjacent elements in the output (eg. 1 ‚Üí 11, 8 ‚Üí 18, 9 ‚Üí 19) were originally 10 bytes apart (=10 elements away √ó 1 byte) in the input. The shape of this dimension is 2. Code Similar to This question is taken from a StackOverflow post here [5]. Answer üí° Explanation Output‚Äôs intrabox left-to-right dimension (axis=-1):Adjacent elements in the output (eg. 1 ‚Üí 2, 2 ‚Üí 3, 6 ‚Üí 7) were originally 2 bytes apart (=1 element away √ó 2 bytes) in the input. Shape of this dimension is 4. Output‚Äôs intrabox top-to-bottom dimension (axis=-2):Adjacent elements in the output (eg. 1 ‚Üí 6, 2 ‚Üí 7, 3 ‚Üí 8) were originally 10 bytes apart (=5 elements away √ó 2 bytes) in the input. The shape of this dimension is 2. Output‚Äôs left box-to-right box dimension (axis=-3):Adjacent elements in the output (eg. 1 ‚Üí 1, 3 ‚Üí 3, 7 ‚Üí 7) were originally 0 bytes apart (=0 elements away √ó 2 bytes) in the input. The shape of this dimension is 3. Code Similar to Answer üí° Explanation Output‚Äôs left-to-right dimension (axis=-1):Adjacent elements in the output (eg. 1 ‚Üí 3, 2 ‚Üí 4, 10 ‚Üí 12) were originally 8 bytes apart (=2 elements away √ó 4 bytes) in the input. The shape of this dimension is 2. Output‚Äôs top-to-bottom dimension (axis=-2):Adjacent elements in the output (eg. 1 ‚Üí 2, 3 ‚Üí 4, 9 ‚Üí 10, 11 ‚Üí 12) were originally 4 bytes apart (=1 element away √ó 4 bytes) in the input. The shape of this dimension is 2. Output‚Äôs left box-to-right box dimension (axis=-3):Adjacent elements in the output (eg. 1 ‚Üí 5, 5 ‚Üí 9) were originally 16 bytes apart (=4 elements away √ó 4 bytes) in the input. The shape of this dimension is 3. Code Similar to Question adapted from SciPy 2008 conference [6]. Answer üí° Explanation Output‚Äôs intrabox left-to-right dimension (axis=-1):Adjacent elements in the output (eg. 1 ‚Üí 2, 12 ‚Üí 13, 16 ‚Üí 17) were originally 8 bytes apart (=1 element away √ó 8 bytes) in the input. Shape of this dimension is 5. Output‚Äôs intrabox top-to-bottom dimension (axis=-2):Adjacent elements in the output (eg. 1 ‚Üí 6, 8 ‚Üí 13, 11 ‚Üí 16) were originally 40 bytes apart (=5 elements away √ó 8 bytes) in the input. The shape of this dimension is 2. Output‚Äôs left box-to-right box dimension (axis=-3):Adjacent elements in the output (eg. 9 ‚Üí 14, 14 ‚Üí 19) were originally 40 bytes apart (=5 elements away √ó 8 bytes) in the input. The shape of this dimension is 3. Code Similar to Answer üí° Explanation Output‚Äôs intrabox left-to-right dimension (axis=-1):Adjacent elements in the output (eg. 1 ‚Üí 2, 5 ‚Üí 6, 7 ‚Üí 8, 10 ‚Üí 11) were originally 1 byte apart (=1 element away √ó 1 byte) in the input. Shape of this dimension is 3. Output‚Äôs intrabox top-to-bottom dimension (axis=-2):Adjacent elements in the output (eg. 1 ‚Üí 4, 2 ‚Üí 5, 8 ‚Üí 11) were originally 3 bytes apart (=3 elements away √ó 1 byte) in the input. The shape of this dimension is 2. Output‚Äôs left box-to-right box dimension (axis=-3):Adjacent elements in the output (eg. 1 ‚Üí 7, 2 ‚Üí 8, 3 ‚Üí 9) were originally 6 bytes apart (=6 elements away √ó 1 byte) in the input. The shape of this dimension is 2. Code Similar to Adapted from a StackOverflow post on 2D convolution here [7]. Answer üí° Explanation Output‚Äôs intrabox left-to-right dimension (axis=-1):Adjacent elements in the output (eg. 1 ‚Üí 2, 17 ‚Üí 18, 24 ‚Üí 25) were originally 1 byte apart (=1 element away √ó 1 byte) in the input. Shape of this dimension is 3. Output‚Äôs intrabox top-to-bottom dimension (axis=-2):Adjacent elements in the output (eg. 1 ‚Üí 6, 2 ‚Üí 7, 3 ‚Üí 8) were originally 5 bytes apart (=5 elements away √ó 1 byte) in the input. The shape of this dimension is 3. Output‚Äôs left box-to-right box dimension (axis=-3):Adjacent elements in the output (eg. 1 ‚Üí 3, 6 ‚Üí 8, 21 ‚Üí 23, 23 ‚Üí 25) were originally 2 bytes apart (=2 elements away √ó 1 byte) in the input. The shape of this dimension is 2. Output‚Äôs top box-to-bottom box dimension (axis=-4):Adjacent elements in the output (eg. 1 ‚Üí 11, 2 ‚Üí 12, 15 ‚Üí 25) were originally 10 bytes apart (=10 elements away √ó 1 byte) in the input. The shape of this dimension is 2. Code Similar to Answer üí° Explanation Output‚Äôs intrabox left-to-right dimension (axis=-1):Adjacent elements in the output (eg. 1 ‚Üí 2, 2 ‚Üí 3, 4 ‚Üí 5) were originally 8 bytes apart (=1 element away √ó 8 bytes) in the input. Shape of this dimension is 3. Output‚Äôs intrabox top-to-bottom dimension (axis=-2):Adjacent elements in the output (eg. 1 ‚Üí 4, 7 ‚Üí 10, 8 ‚Üí 11) were originally 24 bytes apart (=3 elements away √ó 8 bytes) in the input. The shape of this dimension is 2. Output‚Äôs left box-to-right box dimension (axis=-3):Adjacent elements in the output (eg. 1 ‚Üí 1, 10 ‚Üí 10, 12 ‚Üí 12) were originally 0 bytes apart (=0 elements away √ó 8 bytes) in the input. The shape of this dimension is 2. Output‚Äôs top box-to-bottom box dimension (axis=-4):Adjacent elements in the output (eg. 1 ‚Üí 7, 2 ‚Üí 8, 3 ‚Üí 9) were originally 48 bytes apart (=6 elements away √ó 8 bytes) in the input. The shape of this dimension is 2. Code Similar to Answer üí° Explanation Output‚Äôs intrabox left-to-right dimension (axis=-1):Adjacent elements in the output (eg. 1 ‚Üí 2, 2 ‚Üí 3, 4 ‚Üí 5) were originally 8 bytes apart (=1 element away √ó 8 bytes) in the input. Shape of this dimension is 2. Output‚Äôs intrabox top-to-bottom dimension (axis=-2):Adjacent elements in the output (eg. 1 ‚Üí 4, 7 ‚Üí 10, 8 ‚Üí 11) were originally 16 bytes apart (=2 elements away √ó 8 bytes) in the input. The shape of this dimension is 2. Output‚Äôs left box-to-right box dimension (axis=-3):Adjacent elements in the output (eg. 1 ‚Üí 1, 10 ‚Üí 10, 12 ‚Üí 12) were originally 32 bytes apart (=4 elements away √ó 8 bytes) in the input. The shape of this dimension is 2. Output‚Äôs top box-to-bottom box dimension (axis=-4):Adjacent elements in the output (eg. 1 ‚Üí 7, 2 ‚Üí 8, 3 ‚Üí 9) were originally 64 bytes apart (=8 elements away √ó 8 bytes) in the input. The shape of this dimension is 2. Code Similar to ‚ö†Ô∏è While stride tricks give you more control over the resulting NumPy view, the API is not memory-safe ‚Äî things can get pretty nasty if you miscalculate the itemsize (honestly I think this API should not allow client code to have to deal with item size as I haven‚Äôt seen any benefits of exposing this) or the shape or the existing strides, returning data that is actually not the initial array you created, but from a different array altogether which you probably defined few lines back üò±. This is known as buffer overflow and it‚Äôs not hard to encounter this using the stride tricks API. What‚Äôs worse is when you decide to write to this data üò±üò±. It is for this reason that the stride tricks documentation cautions the user to exercise extreme care when using it. Understanding the stride tricks API can be challenging, and I had problems with that. However, the trick (no pun intended) is to start with smaller dimensions and visualise the output of the tensors. Have fun! Found a mistake? Let me know in the comments! :) Shoutout to David Chong for reviewing this article. If you like my content and haven‚Äôt already subscribed to Medium, subscribe via my referral link here! NOTE: A portion of your membership fees will be apportioned to me as referral fees. The N-dimensional array (ndarray) (numpy.org) Advanced NumPy (scipy-lectures.org) An Illustrated Guide to Shape and Strides (ajcr.net) Using stride tricks with NumPy (ipython-books.github.io) [1] https://stackoverflow.com/questions/40084931/taking-subarrays-from-numpy-array-with-given-stride-stepsize [2] https://stackoverflow.com/questions/4923617/efficient-numpy-2d-array-construction-from-1d-array [3] https://stackoverflow.com/questions/47483579/how-to-use-numpy-as-strided-from-np-stride-tricks-correctly [4] https://stackoverflow.com/questions/15722324/sliding-window-of-m-by-n-shape-numpy-ndarray [5] https://stackoverflow.com/questions/23695851/python-repeating-numpy-array-without-replicating-data [6] https://mentat.za.net/numpy/numpy_advanced_slides/ [7] https://stackoverflow.com/questions/43086557/convolve2d-just-by-using-numpy",471,5,21,Towards Data Science,2021-01-03,2021
https://towardsdatascience.com/how-to-become-a-computer-vision-engineer-in-2021-c563545d4c9a,How To Become A Computer Vision Engineer In¬†2021,#1 Massive¬†Open‚Ä¶,12,69,"['How To Become A Computer Vision Engineer In 2021', '1. Massive Open Online Courses (MOOCs)', '2. Machine Learning Libraries And Framework', '3. Read Books', '4. Cloud Services', '5. Certifications', '6. Deep Learning', '7. Mobile and Edge Devices', '8. Programming Languages', 'Conclusion', 'Willing to take things further?', 'Want more from me?']","I only became a professional Computer Vision Engineer in 2020, but the steps and strategies I used to get into the Machine Learning industry have drastically changed within a short amount of time. Artificial Intelligence moves at the speed of Innovation You are probably not surprised by the constant nature of change within the ML industry as you are well aware that Artificial intelligence moves at the speed of Innovation. My point is, what worked for most ML practitioners in 2020 to obtain roles, might not necessarily work in 2021. We all need to adapt. This article will present eight methods you can explore and use today in order to start your path into becoming a Computer Vision Engineer. MOOCs, a modern tool for learning introduced in 2008, is currently the favoured method for Data Scientists and Machine learning practitioners to gain domain expertise that‚Äôs usually accompanied by recognised accreditations and certificates. In most cases, MOOCs are significantly cheaper than the traditional method of learning through academic institutions and Universities. It is not uncommon to come across MOOCs that provide students with financial support through discounts and payment programs. Another advantage MOOCs have over academic institutions is the flexibility to take courses, online classes and exams at a time that suits you as opposed to the constricting structure of a time-table and set exams time allocations. ‚ÄúEducation is not about thinning the herd. Education is about helping every student succeed.‚Äù ‚Äî Andrew Ng There are tons of Computer Vision related MOOCs on the internet; the main hurdle you are likely to face is selecting the right online course that meets your need. I would advise initially observing job requirements in terms of skills and technologies expected from candidates and working backwards to select the appropriate online courses. Here are links to some MOOCs below: There are plenty MOOCs out there, but when searching, consider the following factors: job relevancy, technology demand, time length, course reviews and cost. MOOCs aren‚Äôt just intended for beginners, seasoned ML practitioners and deep learning engineers take intermediate and advanced MOOCs on specific computer vision related topics to upskill or gain domain expertise. In 2021 speed, applicability and practicality are the key focus for ML practitioners. In order to transition from a student of machine learning to a professional taking MOOCs to focus on the practical aspect of ML as opposed to the theoretical content taught in Universities can be a time and cost saving decision. Do note that there are benefits to taking advanced degrees at Universities, you should conduct your own in-depth research before making career defining decisions. Computer Vision as a field encompasses many approaches and techniques to solve common CV problems such as object detection, face recognition, pose estimation etc. You are typically not expected to reinvent the wheel or develop novel algorithms to solve trivial computer vision tasks (except you work within research). Many tools you will use during your studies and career are available through ML libraries and frameworks. ML Libraries and framework provide a suite of tools at the ready for ML practitioners to implement, train, test and deploy computer vision solutions. There are popular tools and libraries such as TensorFlow and PyTorch, and others like FastAI, Caffe2, Keras, Scikit-Learn, MXnet, Darknet etc. The ML industry seems to have settled on TensorFlow(Keras) and PyTorch as the industry standard. In my current role as a Computer Vision Engineer, I make extensive use of the TensorFlow platform for developing ML models for different environments. Here‚Äôs a typical checklist you can use as a guide when learning an ML library: NOTE: The list above doesn‚Äôt encompass all you need to know, in fact I doubt I‚Äôve touched 20% of what you should cover to ensure you are ready for a professional CV Engineer role. Nonetheless, the list above is meant to act as a guide, feel free to add and modify to the list above. To become a Computer Vision Engineer in 2021 and beyond, you‚Äôll have to refer to practical machine learning and computer vision books as a learning resource. A Computer Vision Engineer never stops learning, mainly because the field of Artificial Intelligence progresses every day. It‚Äôs common to meet ML practitioners that study at the same level as students while maintaining a professional career. I know this because I‚Äôm one of those ML practitioners that still have to read books, blogs, research papers and articles, to ensure I don‚Äôt fall behind the ML industry. One of the most highly recommended books for CV Engineers is Hands-On Machine Learning with Scikit-Learn, Keras, & TensorFlow By Aur√©lien G√©ron. This book applies to all ML practitioners, from Data Scientists to NLP Engineers I‚Äôm explicitly trying to get the message across to you that reading practical books should be part of your ongoing personal learning strategy. Suppose you find specific areas of your chosen domain that you might lack expertise in, either maths, statistics, programming or algorithms. In that case, there are tons of practical books that are easy to follow and effective for all ML practitioners of varying levels. Should Computer Vision Engineers be informed on Cloud Services? Yes, but you are not required to be a specialist, there are Cloud and Data Engineers whose sole responsibility is to specialise in Cloud solutions and services. Nonetheless, CV Engineers understanding how to run machine learning models on Cloud Services such as GCP, Microsoft Azure, and AWS is essential. Here are some reasons why CV Engineers should understand and utilise cloud services: Working with cloud services can be daunting, and sometimes unexpectedly costly ‚Äî especially if you forget to turn off an instance. Having cloud services knowledge does put you at the top percentile of CV Engineers in the industry, in terms of skillsets. I‚Äôm not referring to certificates or accreditations received when taking online courses. There are certifications given to ML practitioners that can show expertise within certain libraries, cloud services and frameworks. TensorFlow, AWS, and GCP are all tools and resources that I‚Äôve mentioned in this article. Another piece of information is that the companies and organisations that provide these tools and services also issue recognised certificates. A few years ago having a portfolio accompanied by an advanced degree was sufficient enough to secure ML roles. With the increased influx of ML practitioners, it is plausible to see certifications used to shortlist candidate pools by recruiters and employers. Below are lists of certifications that are relevant to ML practitioners: From this point onwards, the included sections on becoming a computer vision engineer contain obvious information. Still, there‚Äôs content you will find valuable. Deep Learning is a field concerned with utilising deep artificial neural networks to detect patterns within data. Computer Vision Engineers typically leverage deep learning models to solve CV tasks. It‚Äôs an understatement to say that CV Engineers must understand the essential concepts and ideas within the DL field. In 2021, Deep Learning will take a slight turn from the dominant utilisation of Convolutional Neural Network as the building blocks of models to the recently introduced Transformer architecture. It‚Äôs most likely not a job requirement that ML practitioners understand how to implement and leverage Transformers for computer vision tasks ‚Äî mainly as this is still an area of research. Some proactive ML practitioners are now exploring Transformers and understanding how they can be implemented and applied. Instagram, TikTok, YouTube, Facebook‚Ä¶these are all companies with mobile apps where you‚Äôll find ML models leveraged in some form or manner. In 2021, Computer Vision Engineers have to be aware of tools and frameworks that enable the development and integration of models in a mobile environment. There are several platforms, tools and frameworks CV Engineers should be informed on, here are a few: A useful learning resource for understanding the application of computer vision and deep learning in a mobile environment is the book Practical Deep Learning For Cloud, Mobile & Edge. Any form of Software Engineering profession requires an understanding of at least one programming language. Typically, Computer Vision Engineers are well versed in Python. You will most likely find that Python is the most requested programming language in most machine learning related job roles. In 2021, CV Engineers will need to at least have expertise in one language and be at an intermediate level in two or three other languages. The reason for this is because Deep Learning models are utilised in different platforms and environments. Each of these environments leverages other core programming languages to perform functionalities and operations. I currently integrate deep learning models within mobile environments, which requires that I‚Äôm proficient in Python, JavaScript and Swift. In 2021, I‚Äôll be expanding my programming language skillset to include Kotlin and Java. The trick here is not to cram all the syntax available in a language and move on to the next. Instead, it‚Äôs more important to understand the basic principles of Object-Oriented programming and coding patterns utilised in most programming languages. The demand for Computer Vision / Deep Learning Engineer will most likely increase in correlation to the adoption of more camera orientated AI applications. The content required for ML practitioners to study to become CV Engineers might be overwhelming, but once you get started, complete a few MOOCs, and pick up core ML libraries, you‚Äôll find that learning tends to get easier. In this article, I‚Äôve presented eight methods you can adopt and act upon today to become a CV Engineer in 2021. To recap, here‚Äôs a summaries list below: 1. Take online courses in order to gain domain expertise or improve knowledge. 2.Use machine libraries and frameworks. 3. Read practical ML/DL books 4. Be aware of cloud services such as GCP, AWS etc 5. Consider obtaining certifications for certain tools and libraries 6. Understand Deep Learning fundamentals 7. Pick up tools, libraries and frameworks that enable the integration of deep learning models in mobile environments. 8. Understand programming patterns and principles, such as object-orientated programming. With the number of machine learning models in production, the requirement for developing a reliable, safe and secure infrastructure is increasing. Machine Learning Operations(MLOps) is the principle of adopting software development and deployment practices to the machine learning workflow for easy reproducibility, traceability and deployment. Neptune.ai is a platform that inserts itself as a solution for one of the many processes involved in MLOps and developing a robust machine learning application. More specifically, Neptune.ai helps with the registry of metadata produced from your machine learning pipelines and workflow. More information on how this system applies to computer vision is in the link below: neptune.ai Thanks for reading",653,5,9,Towards Data Science,2021-01-03,2021
https://towardsdatascience.com/land-cover-classification-of-satellite-imagery-using-convolutional-neural-networks-91b5bb7fe808,Land Cover Classification of Satellite Imagery using Convolutional Neural¬†Networks,,11,45,"['Land Cover Classification of Satellite Imagery using Convolutional Neural Networks', 'Table of Contents', 'Introduction', 'Convolutional Neural Network (CNN)', 'Salinas HSI', 'Implementation of CNN', 'Training', 'Results', 'Conclusion', 'More From Author', 'References']","Land cover classification using remote sensing data is the task of classifying pixels or objects whose spectral characteristics are similar and allocating them to the designated classification classes, such as forests, grasslands, wetlands, barren lands, cultivated lands, and built-up areas. Various techniques have been applied to land cover classification, including traditional statistical algorithms and recent machine learning approaches, such as random forest and support vector machines, e.t.c. This article covers a hands-on Python tutorial on the land cover classification of satellite imagery using Convolutional Neural Network (CNN). Let‚Äôs get started ‚ú® Satellite imagery has a wide range of applications which is incorporated in every aspect of human life. Especially remote sensing has evolved over the years to solve a lot of problems in different areas. In Remote Sensing, hyperspectral remote sensors are widely used for monitoring the earth‚Äôs surface with a high spectral resolution. Hyperspectral Imaging is an important technique in remote sensing, which collects the electromagnetic spectrum ranging from the visible to the near-infrared wavelength. Hyperspectral imaging sensors often provide hundreds of narrow spectral bands from the same area on the surface of the earth. In hyperspectral images (HSI), each pixel can be regarded as a high-dimensional vector whose entries correspond to the spectral reflectance in a specific wavelength. With the advantage of distinguishing subtle spectral differences, HSIs have been widely applied in diverse areas such as Crop Analysis, Geological Mapping, Mineral Exploration, Defence Research, Urban Investigation, Military Surveillance, Flood Tracking, etc. Use below Articles and Research Papers for a better understanding of Hyperspectral Images. towardsdatascience.com towardsdatascience.com link.springer.com Deep learning is a subset of machine learning that yields high-level abstractions by compositing multiple non-linear transformations. Among deep learning algorithms, Convolutional Neural Networks (CNNs) have gained popularity in computer vision and remote sensing fields, especially for image classification. Convolutional Neural Networks (CNN) is a type of deep learning method that uses convolutional multiplication based on artificial neural networks. Recently, CNN has been widely used in land cover classification, showing remarkable performance. Typical CNNs are composed of convolutional layers, pooling layers, and fully connected layers. Given a pixel of the HSI (a vector for 1-D CNN), several filters with a specific window size sweep the image (or the vector) to create feature maps at convolutional layers. Filters are trained to extract significant features of the input data. Pooling layers reduce the spatial size of feature maps by extracting a representative value, such as a mean or maximum value, from a given window. This process is widely used to make the CNN model more robust by avoiding overfitting problems while considerably decreasing the computational cost. Fully connected layers produce the final result of classification or regression with the features from previous layers. Besides, Dropout is a widely used regularization method to alleviate the overfitting problem. Dropout randomly drops a few connections between layers by setting the weights of the connections to zero. Dropout can be applied to any of the aforementioned layers. The Salinas HSI was collected by the 224-band AVIRIS sensor over Salinas Valley, California. It is characterized by high spatial resolution (3.7-meter pixels). The area covered comprises 512 lines by 217 samples. As with the Indian Pines scene, we discarded the 20 water absorption bands. This image was available only as at-sensor radiance data. It includes vegetables, bare soils, and vineyard fields. The RGB composite image of the salinas HSI is shown below. The below figure shows 10 bands of the Salinas HSI data. The Salinas HSI contains 16 classes such as Broccoli Green Weeds, Fallow, Stubble, Celery, Grapes, Vine Yard, Corn, Lettuce, e.t.c. The below figure shows the information about the classes along with the number of samples. The ground truth of the salinas HSI data is shown below, the spectral samples of the HSI contain no information which is defined as Zero (0) and they have to be discarded before the analysis and they are depicted in black color. Let‚Äôs build a one dimensional CNN with different layers such as Convolution, Maxpooling, Dropout, and Dense Layers. The architecture of the CNN model is shown below: Let‚Äôs start by loading the data, the salinas data is available in the .mat format. The below coded serves the purpose of loading the data and converting the data into a pandas data frame for further processing. The below code normalizes the data and splits the data into train and test in the ratio of 70:30. A brief explanation of optimizer, loss, and callbacks used for training the DNN. Adam is an optimization algorithm that can be used instead of the classical stochastic gradient descent procedure to update network weights iterative based on training data. The advantages of using adam optimizer are: Cross-entropy is the default loss function to use for multi-class classification problems. In this case, it is intended for use with multi-class classification where the target values are in the set {0, 1, 3, ‚Ä¶, n}, where each class is assigned a unique integer value. Mathematically, it is the preferred loss function under the inference framework of maximum likelihood. It is the loss function to be evaluated first and only changed if you have a good reason. Cross-entropy will calculate a score that summarizes the average difference between the actual and predicted probability distributions for all classes in the problem. The score is minimized and a perfect cross-entropy value is 0. Cross-entropy can be specified as the loss function in Keras by specifying ‚Äòcategorical_crossentropy‚Äò when compiling the model. Different callbacks are such as Model Check Point, Early Stopping, and TensorBoard. EarlyStopping: One technique to reduce overfitting in neural networks is to use early stopping. Early stopping prevents overtraining of your model by terminating the training process if it‚Äôs not really learning anything. This is pretty flexible ‚Äî you can control what metric to monitor, how much it needs to change to be considered ‚Äústill learning‚Äù, and how many epochs in a row it can falter before the model stops training. ModelCheckpoint: This callback will save your model as a checkpoint file (in hdf5 or h5format) to disk after each successful epoch. You can actually set the output file to be dynamically named based on the epoch. You can also write either the loss value or accuracy value as part of the log‚Äôs file name. The below code is used to train the CNN model. The code for the Accuracy and Loss graph during the training is shown below and the X-Axis represents epochs and Y-Axis represents the Percentage. The CNN model after training has 92.40% accuracy, Let‚Äôs see the confusion matrix ‚Äî A tabular representation often used to describe the performance of a classification model (or ‚Äúclassifier‚Äù) on a set of test data for which the true values are known. The output is shown below: The classification report is shown below: This article formally introduces hyperspectral images and their applications, implementation of Convolutional Neural Networks (CNN) for land cover classification of Salinas HSI, also interprets the results in the form of classification report, confusion matrix. The detailed hands-on tutorial can be accessed using the below GitHub link. github.com towardsdatascience.com towardsdatascience.com towardsdatascience.com towardsdatascience.com www.ehu.eus plotly.com keras.io",329,3,7,Towards Data Science,2021-01-03,2021
https://towardsdatascience.com/advance-nlp-model-via-transferring-knowledge-from-cross-encoders-to-bi-encoders-3e0fc564f554,"<strong class=""markup--strong markup--h3-strong"">Advance BERT model via transferring knowledge from Cross-Encoders to Bi-Encoders</strong>",Data Augmentation‚Ä¶,6,39,"['Advance BERT model via transferring knowledge from Cross-Encoders to Bi-Encoders', 'üìö Background and challenges', 'üìñ Technique highlight', 'üìù Experimental evaluation', 'üìï Final Thoughts', 'References']","Currently, the state-of-the-art architecture models for NLP usually reuse the BERT model which was pre-trained on large text corpora such as Wikipedia and the Toronto Books Corpus as the baseline [1]. By fine-tuning deep pre-trained BERT, a lot of alternative architectures were invented like DeBERT, RetriBERT, RoBERTa,‚Ä¶ that achieved substantial improvements to the benchmarks on a variety of language understanding tasks. Among common tasks in NLP, pairwise sentence scoring has a wide number of applications in information retrieval, question answering, duplicate question detection, or clustering,... Generally, there are two typical approaches proposed: Bi-encoders and Cross-encoders. On the other hand, no methodology is perfect in all aspects and Bi-encoders is not an exception. The Bi-encoders method usually achieves lower performance compared with the Cross-encoders method and requires a large amount of training data. The reason is Cross-encoders can compare both inputs simultaneously, while the Bi-encoders have to independently map inputs to a meaningful vector space which requires a sufficient amount of training examples for fine-tuning. To solve this problem, Poly-encoders was invented [5]. Poly-encoders utilizes two separate transformers (similar to cross-encoders), but attention was applied between two inputs only at the top layer, resulting in better performance gains over Bi-encoders and large speed gains over Cross-encoders. However, Poly-encoders still have some drawbacks: they cannot be applied for tasks with symmetric similarity relations because of an asymmetrical score function and Poly-encoders representations cannot be efficiently indexed, causing issues for retrieval tasks with large corpora sizes. In this article, I want to introduce a new approach that can use both Cross-encoders and Bi-encoders in an effective way - data augmentation. This strategy is known as Augmented SBERT (AugSBERT) [6], which uses BERT cross-encoders to label a larger set of input pairs to augment the training data for SBERT bi-encoders. Then, SBERT bi-encoders is fine-tuned on this larger augmented training set, which yields a significant performance increase. The idea is very similar to Self-Supervised Learning by Relational Reasoning in Computer Vision. Therefore, in a simple way, we can think that it is Self-Supervised Learning in Natural Language Processing. For more details, it will be presented in the next section. There are three major scenarios for the Augmented SBERT approach for either pairwise-sentence regression or classification task. In this scenario, the straight forward data augmentation strategy is applied to prepare and extend the labeled dataset. There are three most common levels: Character, Word, Sentence. However, the word level is the most suitable one for the sentence pair task. Based on the performance of training Bi-Encoders, there are few suggested methodologies: Insert/substitute word by contextual word embeddings (BERT, DistilBERT, RoBERTA or XLNet) or substitute word by synonym (WordNet, PPDB). After creating the augmented text data, it is then combined with the original one and fit into Bi-Encoders. However, in the case of few labeled datasets or special cases, simple word replacement or increment strategies as shown are not helpful for data augmentation in sentence-pair tasks, even leading to worse performance compared to models without augmentation. In short, the straight forward data augmentation strategy involves three steps: In this case, because of the limited labeled datasets (gold dataset), the pre-trained Cross-encoders are used to weakly label the unlabeled data (same domain). However, randomly selecting two sentences usually leads to a dissimilar (negative) pair; while positive pairs are extremely rare. This skews the label distribution of the silver dataset heavily towards negative pairs. Therefore, the two appropriate sampling approaches are suggested: After that, the sampled sentence pairs will be weakly labeled by pre-trained Cross-encoders and be merged with the gold dataset. Then, Bi-encoders are trained on this extended training dataset. This model is called Augmented SBERT (AugSBERT). AugSBERT might improve the performance of existing Bi-encoders and reduce the difference with Cross-encoders. In summary, AugSBERT for a limited dataset involves three steps: This scenario happens when we want SBERT to attain high performance in different domain data (without annotation). Basically, SBERT fails to map sentences with unseen terminology to a sensible vector space. Hence, the relevant data augmentation strategy domain adaptation was proposed: Generally, AugSBERT benefits a lot when the source domain is rather generic and the target domain is rather specific. Vice-versa, when it goes from a specific domain to a generic target domain, only a slight performance increase is noted. In this experiment, I will introduce a demo on how to apply AugSBERT with different scenarios. First, we need to import some packages The main purpose of this scenario is extending the labeled dataset by the straight forward data augmentation strategies, therefore, we will prepare train, dev, test dataset on the Semantic Text Similarity dataset (link) and define batch size, epoch, and model name (You can specify any Huggingface/transformers pre-trained model) Then, we will insert words by our BERT model (you can apply another argumentation technique as I mentioned in the Technique highlight section) to create a silver dataset. Next, we define our Bi-encoders with mean pooling with both(gold + silver) STS benchmark dataset. Finally, we will evaluate our model in the test STS benchmark dataset. In this scenario, we will use Cross-encoders that were trained on the limited labeled dataset (gold dataset) to soft label the in-domain unlabeled dataset (silver dataset) and train Bi-encoders in both datasets (silver + gold). In this simulation, I also use again STS benchmark dataset and create new pairs of sentences by pre-trained SBERT model. First, we will define Cross-encoders and Bi-encoders. Step 1, we will prepare train, dev, test like before and fine-tune our Cross-encoders Step 2, we use our fine-tuned Cross-encoders to label unlabeled datasets. Step 3, we train our Bi-encoders in both gold and silver datasets Finally, we will evaluate our model in the test STS benchmark dataset. In this scenario, all the steps are very similar to scenario 2 but in a different domain. Because of the capability of our Cross-encoders, we will use a generic source dataset (STS benchmark dataset) and transfer the knowledge to a specific target dataset (Quora Question Pairs) And train our Cross-encoders. Labeling Quora Question Pairs dataset (silver dataset). In this case, the task is classification so we have to convert our score to binary scores. Then, training our Bi-encoders Finally, evaluating on test Quora Question Pairs dataset AugSBERT is a simple and effective data augmentation to improve Bi-encoders for pairwise sentence scoring tasks. The idea is based on labeling new sentence pairs by using pre-trained Cross-encoders and combining them into the training set. Selecting the right sentence pairs for soft-labeling is crucial and necessary to improve the performance. The AugSBERT approach can also be used for domain adaptation, by soft-labeling data on the target domain. You can contact me if you want further discussion. Here is my Linkedin Enjoy!!! üë¶üèª [1] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. [2] Thomas Wolf, Victor Sanh, Julien Chaumond, and Clement Delangue. Transfertransfo: A transfer learning approach for neural network-based conversational agents. [3] Pierre-Emmanuel Mazare, Samuel Humeau, Martin Raison, and Antoine Bordes. Training millions of personalized dialogue agents. [4] Nils Reimers and Iryna Gurevych. SentenceBERT: Sentence Embeddings using Siamese BERTNetworks. [5] Samuel Humeau, Kurt Shuster, Marie-Anne Lachaux, and Jason Weston. Poly-encoders: Architectures and pre-training strategies for fast and accurate multi-sentence scoring. [6] Nandan Thakur, Nils Reimers, Johannes Daxenberge, and Iryna Gurevych. Augmented SBERT: Data Augmentation Method for Improving Bi-Encoders for Pairwise Sentence Scoring Tasks. [7] Giambattista Amati. BM25, Springer US, Boston, MA.",311,1,9,Towards Data Science,2021-01-03,2021
https://towardsdatascience.com/five-obstacles-faced-in-linear-regression-80fb5c599fbc,Five Obstacles faced in Linear Regression,These five obstacles may occur when you train a¬†linear‚Ä¶,8,44,"['Five Obstacles faced in Linear Regression', 'Non-Linearity of the response-predictor relationships', 'Correlation of error terms', 'A non-constant variance of the error term [Heteroscedasticity]', 'Collinearity', 'Outliers And High Leverage Points', 'Summary', 'Happy Learning!']","Linear Regression is one of the most trivial machine algorithms. Interpretability and easy-to-train traits make this algorithm the first steps in Machine Learning. Being a little less complicated, Linear Regression acts as one of the fundamental concepts in understanding higher and complex algorithms. To know what linear regression is? How we train it? How we obtain the best fit line? How we interpret it? And how we access the accuracy of fit, you may visit the following article. towardsdatascience.com... After understanding the basic intuition of Linear regression, certain concepts make it more fascinating and more fun. These also provide a deep understanding of flaws in the algorithm, its impact, and remedies. And, we will explore these concepts in the article. We all know, Linear regression involves a few assumptions. And, these assumptions make the structure of this algorithm straightforward. However, this is the reason why it has lots of flaws and why we need to study and understand these flaws. Five problems that lie in the scope of this article are: The reason for this problem is one of the assumptions involved in linear regression. It is the assumption for linearity, which states that the relation between the predictor and response is linear. If the actual relation between response and the predictor is not linear, then all the conclusion we draw becomes null and void. Also, the accuracy of the model may drop significantly. So, how can we deal with this problem? The solution to the problem mentioned above is to plot Residual Plots. Residual plots are the plot between the residual, the difference between the actual value and predicted value, and the predictor. Once we have plotted the residual plot, we will search for a pattern. If some patterns are visible, then there is a non-linear relationship between response and predictor. And, if the plot shows randomness then we are on the right path! After analyzing the type of pattern, we can use non-linear transformations such as square root, cube root, or log function. Which removes the non-linearity to some extent, and our linear model performs well. Let try to fit a straight line to a quadratic function. We will generate some random points using NumPy and take their squares as the response. Let us see the scatter plot between x and y (Fig.1). Now, let us try to fit a linear model to this data and see the plot between residual and predictor. We can see a quadratic trend in the residual plots. This trend helps us to identify the non-linearity in data. Further, we can apply the square root transformation to make data more suitable for the linear model. If the data is linear, then you would get random points. The nature of the residual would be randomized. In that case, we can move forward with the model. A principal assumption of the linear model is that the error terms are uncorrelated. The ‚Äúuncorrelated‚Äù terms indicated that the sign of error for one observation is independent of others. The correlation among error terms may occur due to several factors. For instance, if we are observing the weight and height of people. The correlation in error may occur due to the diet they consume, the exercise they do, environmental factors, or they are members of the same family. What happens to the model when errors are correlated? If the error terms are correlated then the standard error in the model coefficients gets underestimated. As a result, confidence and prediction intervals will be narrower than they should be. For more insights, please refer to the example below. The solution is the same as described in the above problem, Residual Plots. If some trends are visible in residual plots, these trends can be expressed as some functions. Hence, they are correlated! To understand the impact of correlation on the confidence interval, we should note two trivial points. Now, suppose we have n data points. We calculate the standard error (SE) and confidence interval. Now, we doubled our data. Hence, then we would have observations and error terms in pair. If we now recalculate the SE, then we will calculate it corresponding to 2n observations. As a result, the standard error will be lower by a factor of root ‚àö2 (SE is inversely proportional to the number of observations). And, we will obtain a narrower confidence interval. The source of this problem is also an assumption. The assumption is that the error term has a constant variance, also referred to as Homcedacity. Generally, that is not the case. We can often identify a non-constant variance in errors, or heteroscedasticity, from the presence of funnel shape in residual plots. In Fig.2, the funnel represents that the error terms have non-constant variance. One possible solution is to transform the response using a concave function such as log and square root. Such a transformation results in shrinkage of the response variable, consequently reducing heteroscedasticity. Let us try to apply log transformation to points generated in problem 1. We can observe a linear trend after transformation. Hence we may remove non-linearity by applying concave functions. Collinearity refers to a situation in which two or more predictor variables are correlated to one another. For example, we can find some relation between height and weight, Area of house and number of rooms, experience, and income, and many more. In linear regression, we assume that all the predictors are independent. But often the case is the opposite. The predictors are correlated with each other. Hence, it is essential to look at this problem and find a feasible solution. When the assumption of independence is neglected, the following concerns arise: There are two possible solutions to the problem. Linear Regression is greatly affected by the presence of Outliers and Leverage points. They may occur for a variety of reasons. And their presence hugely affects to model performance. It is also one of the limitations of linear regression. Outlier: An outlier is an unusual observation of response y, for some given predictor x. High Leverage Points: Contrast to an outlier, a high leverage point is defined as an unusual observation of predictor x. There are several techniques available for identifying an outlier. This includes interquartile range, scatter plots, residual plots, quartile-quartile plots, box plots, etc. As this is a limitation of linear regression, it is vital to take the necessary steps. One method is to drop the outlier. However, this may lead to some loss of information. We can also use feature engineering to deal with outliers. In this article, we have seen five problems while we are working with linear regression. We have seen the sources, impacts, and solutions for each of the problems. Though Linear regression is the most basic machine learning algorithm, it has a vast scope for learning new things. For me, these problems provide are different point of view for Linear regression. I hope understanding these problems will provide you with novel insights when you solve any problem. You may also check the complete playlist for Linear regression.",27,0,7,Towards Data Science,2021-01-03,2021
https://towardsdatascience.com/bird-by-bird-using-deep-learning-4c0fa81365d7,Bird by Bird using Deep¬†Learning,Advancing CNN model for fine-grained classification using transfer‚Ä¶,7,52,"['Bird by Bird using Deep Learning', 'Introducing the related work', 'Classification of bird species using ResNet', 'Advancing the deep learning model', 'Conclusions', 'More coming soon!', 'References']","This article demonstrates how deep learning models used for image-related tasks can be advanced in order to address the fine-grained classification problem. For this objective, we will walk through the following two parts. First, you will get familiar with some basic concepts of computer vision and convolutional neural networks, while the second part demonstrates how to apply this knowledge to a real-world problem of bird species classification using PyTorch. Specifically, you will learn how to build your own CNN model ‚Äì ResNet-50, ‚Äì to further improve its performance using transfer learning, auxiliary task and attention-enhanced architecture, and even a little more. Computers perform extremely well when it comes to crunching numbers. Solving tons of equations to get a human to the Moon? No problem. Determine whether a cat or a dog appears in an image? Oops‚Ä¶ The task that is inherently easy for any human being seemed to be impossible for first computers. During the years, algorithms evolved as well as the hardware did (remember the Moor‚Äôs law? R.I.P.). The field of computer vision appeared as a trial to solve the task of classifying images using computers. After the long period of development, many sophisticated methods were created. However, all of them suffered from the lack of generalizability: a model built to classify cats vs. dogs couldn‚Äôt distinguish, for example, birds. In 1989, Yann LeCun and his colleagues had proposed [1], and further developed [2] the concept of convolutional neural network (CNN). The model itself was inspired by a human visual cortex, where a visual neuron is responsible for a small piece of a picture that is visible to an eye ‚Äì the neuron‚Äôs receptive field. Structurally, it was expressed in the way that a single convolutional neuron (filter) scanned an input image step-by-step, being applied to different parts of the image many times, which refers to a concept of weight sharing (Figure 1). Of course, since LeCun‚Äôs LeNet-5, the state-of-the-art of CNN models has been developed greatly. The first successful large-scale architecture came out with AlexNet [3] that won the ILSVRC 2012 challenge achieving the top-5 error rate of 15.3%. Later advancements gave many powerful models that were mainly improved throughout the usage of larger and more complex architectures. The thing is, as the network goes deeper (depth is increasing), its performance gets saturated and starts degrading. To address this problem, the residual neural network (ResNet) was developed [4] to effectively direct the input over some layers (also known as skip- or residual connections). The core idea of the ResNet architecture is to pass a part of a signal to the end of a convolutional block unprocessed (by just copying values) in order to enlarge gradient flow through the deep layers (Figure 2). Thus, the skip connection guarantees that performance of the model does not decrease but it could increase slightly. The next part explains how the discussed theory can be actually applied for solving the real-world problem. Bird species recognition is a difficult task challenging the visual abilities for both human experts and computers. One of the interesting datasets related to the fine-grained classification problem is Caltech-UCSD Birds-200-2011 (CUB-200-2011) [5] consisting of 11788 images of birds belonging to 200 species. To address this problem, the goals of the current tutorial will be: (a) to build a CNN model to classify bird images w.r.t. their species and (b) to determine how the prediction accuracy of a baseline model can be boosted using CNNs of different architectures. For that, we will use PyTorch, one of the most popular open-source frameworks for deep learning. By the end of this tutorial, you will be able to: First, you need to download an archive containing the dataset and store it into the data directory. It can be done manually from the following link, or using the Python code provided in the following GitHub repository: github.com Now, let‚Äôs import packages that we will use in this tutorial: In this tutorial, we plan to pre-train a baseline model using the ImageNet dataset. As pre-trained models usually expect input images to be normalized in the same way, heights and widths should be at least of size 224 x 224 pixels. There might many ways for the image transformation be used to fullfill above specifications, but what might be the optimal one? Exploratory data analysis is an essential starting point of any data science project, which lays the foundation for the further analysis. Since we are interested to define the optimal data transformation strategy, we are going to explore bird images to see what useful we can grasp on. Let‚Äôs have a look at some bird examples of the sparrow family (Figure 3). Seems like there can be a high similarity among birds related to different species which is really hard to spot. Is that a White-throated or a Lincoln Sparrow? Well, even experts can be confused‚Ä¶ Just out of interest, we‚Äôll sum up all classes of the Sparrow family to understand how many of them are there in our dataset: The code above gives us the value of 21, implying that there are dozen different species can be represented only by a single family. And now we see why CUB-200-2011 is perfectly designed for fine-grained classification. What do we have is the many similar birds potentially related to different classes, and we, actually, plan to deal with that problem here. But before getting in a real deep learning, we want to determine an appropriate strategy for data pre-processing. For that, we will analyse the marginal distributions of width sand heights by visualizing box plots for the corresponding observations: Indeed, the size of images varies considerably. We also see that heights and widths of the majority images are equal to 375 and 500 pixels, respectively. So, what might be the appropriate transformation strategy for this kind of data? CUB-200-2011 dataset contains thousands of images, so it might affect the computational time. To overcome that we first create class DatasetBirds to make data loading and pre-processing easy: All pre-trained models expect input images to be normalized in the same way, such as the height and width are at least 224 pixels. As you might noticed from our previous analysis, the size of the data varies considerably, and many images have landscape layout rather than portrait one, and width is commonly close to the maximum value along both dimensions. In order to improve the ability of the model to learn bird representation, we‚Äôll use data augmentation. We want to transform images in a such way, so we maintain the aspect ratio. One solution is to scale images uniformly, so that both dimensions are equal to the larger side using the maximum padding strategy. For that, we‚Äôll create a pad function to pad images to 500 pixels: Assuming birds to appear at any image part, we make the model able to capture them everywhere by randomly-cropping and flipping images along both axes during the model training. While the images of the test split will be center-cropped before feeding into ResNet-50, as we expect the majority birds to be located at this image part referring to the previous data exploration. For that, we are going to crop images by 375 x 375 pixels along both dimensions, as that is the average size of the majority images. We‚Äôll also normalize images by mean [0.485, 0.456, 0.406] and standard deviation [0.229, 0.224, 0.225] to make distribution of pixel values closer to the Gaussian one. Then, we‚Äôll organize images of the CUB-200-2011 dataset into three subsets to insure the proper model training and evaluation. As authors of the dataset suggest the way to assemble the training and test subsets, we split our data accordingly. Additionally, the validation split will be defined to further fine-tune the parameters of the model during the model evaluation process. For that, the training subset will be split using stratified sampling technique that ensures that each subset have equally balanced classes of different species. We‚Äôll set up parameters for data loading and model training. To leverage computations and be able to proceed large dataset in parallel, we will collate input samples in several mini-batches and also denote how many sub-processes to use to generate them in order to leverage the training process. After we‚Äôll create a DataLoader object to yield samples of an each data split: We are going to use ResNet-50 model for classification of bird species. ResNet (or Residual Network) is a variant of convolutional neural networks that was proposed as a solution to the vanishing gradient problem of large networks. PyTorch provides the ResNet-50 model on torchvision.models, so we will instantiate the respective class and set the argument num_classes to 200 given the dataset of that number of bird species: More specifically, the chosen architecture is 50 layers deep and composed of 5 stages, 4 of which with residual blocks and 1 comprise a convolution, batch normalization and ReLU operations. Next point is to set the learning rate of our model as well as a schedule to adjust it during the training for the sake of the better performance. Training of the ResNet-50 model will be done using the Adam optimizer with an initial learning rate of 1e-3 and an exponentially decreasing learning rate schedule such as it drops by a factor of gamma at each epoch. Finally, we are ready to train and validate our model to recognize and learn the difference between bird species. The cross-entropy loss and accuracy metric will be accumulated per epoch in order to inspect the model performance dynamics. Following all of the training experiments, we test the model using the subset of previously unseen data to assess the overall goodness in bird classification using the accuracy metric. Figure 5 depicts the model performance metrics for ResNet-50: As we see, the baseline model performs really poor as it overfits. The one of main reasons is the lack of diverse training samples. Just a quick note: CUB-200-2011 dataset has ~30 images per specie. Seems like we are stuck‚Ä¶isn‚Äôt it? Actually, there are some ways we can address to overcome these issues. Well, we ran into a number of challenges in our previous analysis, so we may start thinking about how we can address these follow-up questions: Let‚Äôs figure out how we can advance our baseline model in more detail. As it was said before, deep neural networks require a lot of training samples. Practitioners have noticed that, in order to train a deep neural network from scratch, the amount of data should grow exponentially with the number of trainable parameters. Luckily, generalization ability of a model that was trained on a larger dataset can be transferred to another, usually, simpler task. In order to improve the performance of thebaseline model for bird classification, we will use weight initialization obtained from the general-purpose model pre-trained on the ImageNet dataset, and further fine-tune its parameters using the CUB-200-2011 one. The training process remains the same, while the model will rather focus on the fine-tuning of hyper-parameters. PyTorch provides pre-trained models in torch.utils.model_zoo. Construction of a pre-trained ResNet-50 can be done by passing pretrained=True into constructor. This simple trick provides us with the model that already has well initialized filters, so there is no need to learn them from scratch. We will also set a lower learning rate of 1e-4 in the optimizer, as we are going to train a network that was yet pre-trained on a large-scale image-classification task. And here are results: As we see, the use of the pre-trained model allows to solve the overfitting problem giving 80.77% test accuracy. Let‚Äôs continue experimenting on that! Now we can extend this approach even more. Why do we have to increase the complexity of a single task if we can add another one? No reason at all. It was noticed that introduction of an additional ‚Äì auxiliary ‚Äì task improves the network‚Äôs performance forcing it to learn more general representation of the training data. As Caltech-UCSD Birds-200‚Äì2011 dataset includes bounding boxes in addition to class labels, we will use this auxiliary target to make the network to train in a multi-task fashion. Now, we will predict 4 coordinates of bird‚Äôs bounding box in addition to its specie by setting num_classes to 204: Now we need to slightly modify our training and validation blocks, as we want to make predictions and calculate the loss for two targets corresponding to a correct bird specie and its bounding box coordinates. Here‚Äôs an example execution: Results are even better ‚Äì integration of the auxiliary task provides the stable increase of accuracy points giving 81.2% on the test split ‚Äì as shown in Figure 7. In the last few paragraphs we were focused on the data-driven advancement of our model. However, at some point the complexity of the task can exceed the model‚Äôs capacity resulting in a lower performance. In order to adjust the model‚Äôs power to the difficulty of the problem, we can equip the network with additional attention blocks that will help it to focus on important parts of the input and ignore irrelevant ones. Attention module allows to highlight relevant regions of feature maps and returns values varying in range [0.0, 2.0], where the lower value implies the lower priority of a given pixel for the following layers. So we‚Äôll create and instantiate the class ResNet50Attention corresponding to the attention-enhanced ResNet-50 model: After that, we are ready to train and evaluate the performance of the attention-enhanced model pre-trained on the ImageNet dataset and advanced with the multi-task learning for bird classification using the same code we utilized before. Final accuracy score has been increased to 82.4%! Figure 8 shows summary results generated during the analysis: Results clearly indicate that the final variant of the ResNet-50 model advanced with transfer and multi-task learning, as well as with the attention module, greatly contributes to the more accurate bird predictions. Here, we used different approaches to improve the performance of a baseline ResNet-50 for the classification of bird species from CUB-200‚Äì2011 dataset. What could we learn from that? Here are some take-home messages from our analysis: In summary, there is a space for improvements of the model performance. Additional advancements can be achieved by further optimization of model hyper-parameters, the use of a stronger data augmentation, regularization, meta-learning techniques. The focus of the next tutorial will be on the interpretability of deep learning models. Interested to keep it on? Subscribe and stay updated on more deep learning materials at ‚Äì https://medium.com/@slipnitskaya.",235,2,17,Towards Data Science,2021-01-03,2021
https://towardsdatascience.com/text-mining-and-sentiment-analysis-for-yelp-reviews-of-a-burger-chain-6d3bcfcab17b,Text Mining and Sentiment Analysis for Yelp Reviews of A Burger¬†Chain,,6,34,"['Text Mining and Sentiment Analysis for Yelp Reviews of A Burger Chain', '1 Case Background', '2 Data Understanding', '3 Text Mining', '4 Sentiment Analysis', '5 Conclusion']","Text, such as social media posts and customer reviews, is a gold mine waiting to be discovered. We can turn this unstructured data into useful insights, which can help companies better understand how customers like their products or services and more importantly, why, and then make business improvements as quickly as possible. Super Duper Burgers is one of my favourite burger restaurants. Every time I went there, I would always see customers queueing up for the burgers. One day I was thinking, why people are so obsessed with this burger chain? I know there are lots of reviews on Yelp and maybe this is a good start to figure out the secrets behind. I used the Yelp API and got related information of 17 Super Duper Burgers restaurants in the Bay Area, such as urls, review counts, ratings, locations, etc. Then I used Beautiful Soup to do web scraping and get reviews for each restaurant. I not only got the content of the reviews, but also the date and the rating from that specific customer. Date is useful when we do time series analysis and ratings can be the target variable if we apply any supervised learning algorithm to do prediction. I got 10,661 pieces of reviews in total. Visualization is a good way to do exploratory data analysis. The reviews were from 2010‚Äì04‚Äì09 to 2020‚Äì12‚Äì21 and it was more than ten years. The number of reviews per month were increasing and it could mean that the burger chain is becoming more and more popular in the past decade. After COVID, the number dropped significantly and only around 30 customers would write a review every month. Most customers were satisfied with the restaurants and over 70% of them gave ratings of 4 or 5. Over time, there is no so much variation in the rating and it was quite stable at 4. In the review, some character references such as ‚Äú&amp;amp;‚Äù are not useful in our text content and I removed them. Next, I wanted to make sure all the reviews are in English and I did the language detection with a library called langdetect and the specific function detect_langs. 8 out of 10,661 reviews were detected as other languages. Most of them are very short and have some sort of emphasis on the word: waaaaaay for way and guuuud for good. In this case, the detection is not that accurate. If we look closer to each of these 8 reviews, all of them are actually in English and I will keep them. In every language, there are words that occur too frequently and are not informative, such as ‚Äúa‚Äù, ‚Äúan‚Äù, ‚Äúthe‚Äù, ‚Äúand‚Äù in English. It is useful to build a list containing all the stopwords and get rid of them before we do any text mining. Depending on the specific context, you may also want to add more to the list. In our case, words like ‚Äúsuper‚Äù, ‚Äúduper‚Äù are not very meaningful. Word cloud is a very popular way to highlight the words with high-frequency in textual data. The more a specific word appears in the text, the bigger and bolder it will be in the word cloud. We can see that ‚Äúburger‚Äù, ‚Äúgarlic fries‚Äù, ‚Äúcheese‚Äù and some other words were mentioned by lots of customers. In addition to the word cloud, we may be also interested in how much exactly a word appeared across all the reviews. Here, we are actually trying to transform the text data into a numeric form and Bag-of-Words is the simplest form of text representation in numbers. It basically builds a list of words occurring within a collection of documents (corpus) and keeps track of their frequencies. Obviously, ‚Äúburger‚Äù and ‚Äúburgers‚Äù are saying the same thing and we can do better than that by using stemming. Stemming is the process of transforming words into root forms, even if the stemmed word is not a valid word in the language. In general, stemming will tend to chop off suffixes such as ‚Äú-ed‚Äù and ‚Äúing‚Äù as well as plural forms. Under the Bag-of-Words approach, the word order is discarded. However, in many cases, the sequence of words is very important. For example, compare these two sentences: 1) I am happy, not sad. 2) I am sad, not happy. The meaning of them are totally different but they will get the same numeric representation with single-token BoW. In order to better capture the context, we can consider pairs or triples of words that appear next to each other and they can also give us more useful information. Garlic fries seem to be the most popular menu item for this burger chain, even over the burgers! Other top-selling dishes include mini burger, ice cream, veggie burger and chicken sandwich. Pairs of tokens give us more insights than single ones. Even though the bigrams give us more information, it only answers the question of WHAT. If I were the business owner, I would be definitely interested in WHY: Why people love the fries? Is it because of the special flavor or the sauce? To answer this question, I will use the Word2Vec model and see what the words are most likely around our target words such as fries, burgers, service, etc. Word2Vec uses a neural network model to learn word associations from the corpus. Compared to BOW and n-grams, Word2Vec leverages the context and better captures the meaning and relationship of the word. There are two model architectures behind Word2Vec: continuous Bag-of-Words (CBOW) and skip-gram. I am not going to give too many details about the algorithms here and you can find more in other articles and papers. In general, CBOW is faster while skip-gram is slower but does a better job in representing infrequent words. We can easily do the job with Gensim in Python. First, I got the good reviews with rating of 4 or 5 and did some basic preprocessing. Now we can build the model and see what the customers love most about the service of the burger chains. Obviously, people really appreciate their friendly customer service as well as the fast and quick response. We can do the same for other target words that we are interested in. These surrounding words are very informative and they can better explain why people love or explain about certain things. Sentiment analysis is the process of understanding the opinions of people about a subject. There are two types of methods: lexicon/rule based and automated. This method has a predefined list of words with sentiment scores and it matches words from the lexicon with words from the text. I will use the VADER analyzer in the NLTK package. For each piece of text, the analyzer provides four scores: negative, neutral, positive and compound. The first three are easy to understand and for the compound score, it is a combination of positive and negative scores and ranges from -1 to 1: below 0 is negative and above 0 is positive. I am going to use the compound score to measure the sentiment. Generally, the sentiment for this burger chain is positive and we can notice there is a decreasing trend in the past decade, especially after the pandemic. We can also use historical data with known sentiment to predict the sentiment of a new piece of text. Here I will use two supervised learning classifiers: logistic regression and naive bayes. First, I labeled the positive reviews as ‚Äú1‚Äù (with rating of four or five) and negative reviews as ‚Äú0‚Äù (with rating of one or two). 85% out of 9271 reviews are positive. Then, I vectorized the reviews using BoW and split them into training set and test set. Now we can build the models. The first one is logistic regression. The second model is Naive Bayes. Finally, we can compare the generalization performance of these two models. It turned out that both models worked really well, with accuracy over 90%. Of course we can still improve the models, by using n-grams, Tf-idf, etc. Text mining not only allows us to know what people are talking about, but how they talk about it. It is very important and beneficial for brand monitoring, product analysis and customer service. With Python, it is convenient for us to leverage all kinds of library to dive deeper into the text and get valuable insights.",19,2,9,Towards Data Science,2021-01-03,2021
https://towardsdatascience.com/6-questions-i-was-asked-at-data-scientist-interviews-39a095d87c6c,6 Questions I was Asked at Data Scientist Interviews,A guide that will help you prepare for your¬†next‚Ä¶,1,35,['6 Questions I was Asked at Data Scientist Interviews'],"Data science has experienced a monumental growth in recent years. Thus, the demand for data scientists has increased tremendously which drove many people to make a career change to work in this field. There is one particular action at the core of this series of events: interviews. The ambition and aspiration to become a data scientist are not enough to get you a job. A comprehensive set of skills is expected from the candidates. Data science is an interdisciplinary field so the required skills do not focus on a certain topic. In this article, I will share the 6 questions that was asked to me at data scientist interviews. I have picked the questions in a way that covers the different subjects so you get an overview of what to typically expect at a data scientist interview. The questions are related to Python, machine learning, SQL, and databases. I will not only provide the answers but also explain the topic in a broader context. In machine learning, overfitting arises when a model tries to fit the training data so well that it cannot generalize to new observations. An overfit model captures the details and noise in training data rather than the general trend. Thus, overfit models seem to be outstanding on training data but performs poor on new, previously unseen observations. The main reason of overfitting is model complexity. Regularization controls the model complexity by penalizing higher terms in the model. If a regularization terms is added, the model tries to minimize both loss and complexity of model. The two main reasons that cause a model to be complex are: L1 regularization, also called regularization for sparsity, is used to handle sparse vectors which consist of mostly zeroes. L1 regularization forces the weights of uninformative features to be zero by subtracting a small amount from the weight at each iteration and thus making the weight zero, eventually. L2 regularization, also called regularization for simplicity, forces weights toward zero but it does not make them exactly zero. L2 regularization acts like a force that removes a small percentage of weights at each iteration. Therefore, weights will never be equal to zero. If we take the model complexity as a function of weights, the complexity of a feature is proportional to the absolute value of its weight. L1 regularization penalizes |weight| whereas L2 regularization penalizes (weight)¬≤. Classification and clustering are two types of machine learning tasks. Classification is a supervised learning tasks. Samples in a classification task have labels. Each data point is classified according to some measurements. Classification algorithms try to model the relationship between measurements (features) on samples and their assigned class. Then the model predicts the class of new samples. Clustering is an unsupervised learning task. Samples in clustering do not have labels. We expect the model to find structures in the data set so that similar samples can be grouped into clusters. We basically ask the model to label samples. This is a coding question. The choice of programming language is usually Python. We have the following list of tuples which needs to be sorted based on the second items in tuples. We have two options. The first option is to return a sorted version of the original list so the original one is not modified. The second option is to sort in place which means the original list is modified. In Python, an object is an iterable if we can iterate over its elements using a loop or comprehension (e.g. list, dictionary). Generators are iterators which are a specific kind of iterable. Generators do not store the values in memory so we can iterate over them only once. The values are generated as we iterate. The yield keyword can be used as the returned keyword in functions. The difference is that function returns a generator if the yield keyword is used instead of return. It is very useful and efficient when we have a function that returns a large set of values which will only be used once. When a function contains the yield keyword, it becomes a generator function. In other words, the yield converts a function to a generator so it returns values one by one. Both are techniques that are used when designing a database schema. The goal of normalization is to reduce data redundancy and inconsistency. The number of tables is increased with normalization. The goal of denormalization is to execute queries faster. It is achieved by adding redundancy. The number of tables is lower than the normalization technique. Consider we are designing a database for a retail business. The data to be stored contains customer data (name, email address, phone number) and purchase data (purchase date and amount). Normalization suggests to have separate tables to store customer data and purchase data. The tables can be related by using a foreign key such as customer id. In that case, when there is an update on customer data (e.g. email address), we only update one row in the customer table. Denormalization suggests to have all data in table. When we need to update the email address of a customer, we need to update all the rows that contains a purchase of that customer. The advantage of denormalization over normalization is to run queries quicker. It is highly likely that you will have a question about SQL queries. I was asked to write the select statement to retrieve data from a table based on the given query. Consider we have the following item tables. Find the average price of items at each store and sort the results by average price. We can solve it by applying the avg function to the price column and grouping the values by store id. The sorting can be achieved by adding the order by clause at the end. These are the questions I have actually been asked at an interview. You may not encounter the exact same question but the topics are usually the same. It is important to note that the questions are likely to come from different areas. This is an indication of what is expected from data scientist. Having a broad range of skills will take you one step ahead in the competitive job market. I‚Äôm planning to write a more comprehensive articles that include more interview questions. Stay tuned for upcoming articles! Thank you for reading. Please let me know if you have any feedback.",207,2,6,Towards Data Science,2021-01-03,2021
https://towardsdatascience.com/beautifying-the-messy-plots-in-python-solving-common-issues-in-seaborn-7372e6479fb,Beautifying the Messy Plots in Python & Solving Common Issues in¬†Seaborn,Let me make your life¬†easier‚Ä¶,6,26,"['Beautifying the Messy Plots in Python & Solving Common Issues in Seaborn', 'Basic Set-up', 'Add Style', 'Plotting Frequency/Count Data', 'Plotting Categorical x Quantitative', 'Happy New Year!']","Creating presentable plots in Python can be a bit daunting. It‚Äôs especially so if you are used to making your visualizations using other BI software or even R, where most plots come already prettified for you. Another problem is that there are many ways things can go wrong and ways to resolve the issue will depend on the choices you made for the plot. Here, I will demonstrate a few ways to easily create plots in Python for the various scenarios, and show you how to resolve some of the issues that may arise in each case. In this post, I will focus on efficiency and share some of the tidbits that will make creating visually appealing plots fast. Pyplot in Matplotlib is a must-have to plot in Python. Other libraries are likely all using Matplotlib as its backend. Seaborn is one example. Seaborn adds some nice functionalities, but these functionalities do create confusion sometimes. Let‚Äôs import all our packages first. Use %matplotlib inline to display plots if you are using an iPython platform that allows you to display your plots in the front-end, such as Jupyter Notebook. If you just want something presentable up and running quickly, I highly recommend assigning a plot style. Plot styles instantly apply multiple stylistic elements to your plots and save some troubles. Another reason to assign a style ahead of the time is to keep the overall look consistent throughout. If you use different plot methods (sns, plt, pd) in your document, you could end up with inconsistent plots. I especially love fivethirtyeight for its visibility and simplicity. Audiences from academia are likely to be more familiar with ggplot style as it is a popular library in R. See the below image to compare some of the styles. seaborn is great, but it does come with too many options, which might be more than what we want at this point. If you want to look at a full list of available style, run plt.style.available. Going forward, I will use fivethirtyeight to stylize all my plots. If you are using Pandas, it also comes with some plotting capabilities (but its backend is Matplotlib). It‚Äôs handy if you want to just quickly look at the distribution (histogram or density plot) or one-to-one direct relationship between two columns (line or scatter plot). Note that Pandas plotting does not automatically find the best plot type. Default is always a line plot. Now that we have the basic set up, let‚Äôs look at different scenarios. Seaborn‚Äôs Countplot offers a quick way to display the frequency of each value. But things can go very wrong sometimes‚Ä¶ We see a number of problems here, our tick labels on the x-axis are overlapping, and the legend box is in a not so ideal location. Let‚Äôs see how to resolve these issues. We can override the settings for x-ticks by using Matplotlib. rotation indicates a degree to rotate the text and ha (horizontal alignment) shifts the labels so it aligns on the right side. To move the position of the legend, we need to assign the legend location. We can override the legend setting using Matplotlib. bbox_to_anchor allows you to set the location of the legend manually. If you just want to put this on the upper right corner of the plot, we can also add location info loc = ‚Äòupper right'. The plot looks better, but it‚Äôs a bit hard to read. It‚Äôll be more clear if the bars were stacked per method. countplot has a parameter called dodge that‚Äôs set to True by default. If we set this to False, it will stack the bar plots. Our plot looks much better but the overall order seems very random. We can manually set the order of plots using countplot as well. This function can also work as a filter. (While we are at it, let‚Äôs remove the x-label and set it as a title too.) Great! Now our frequency plot looks much better. You can easily try many different options to plot values of categories using Seaborn‚Äôs catplot. By default, Catplot will be a strip plot, but you can change the option by assigning a kind parameter to a different plot type, such as box or violin. Just to confuse everyone a bit more, you can also plot these categorical plots by directly calling them (e.g. sns.boxplot or sns.violinplot) and the available parameters will be different. Let‚Äôs try to fix a messy catplot. Oh, no! This time it did put the legend outside, but the x-ticks are again overlapping. The lines also seem to be too thick for the boxplot, and the outlier markers are very big. Lastly, the plot is a bit too narrow. We know how to fix the x-ticks, now let‚Äôs fix the other issues. To optimize the linewidth, we can manually set the linewidth of the plot. Now that the outliers seem to be way out of proportion compared to our nice new lines. Let‚Äôs also make them smaller. If you want to remove them altogether, you can instead use showfliers = False. Lastly, the overall plot is looking too narrow. So let‚Äôs try to widen the plot area by changing the aspect ratio. The aspect value changes the width holding the height constant. If you use sns.boxplot directly, it won‚Äôt have the aspect parameter and we will need to change the aspect ratio by setting the figure size using Matplotlib. fig = plt.figure(figsize = (w, h)) Lastly, sometimes plots may have their title or legend cropped when saved locally. To prevent this issue, call plt.tight_layout() before saving the plot. Today, we briefly looked at a few tips in setting up a plot in Python using Matplotlib and Seaborn, and how to solve a few of some common issues when using Seaborn. If you are constantly running into any other issues, please leave a comment and I can add it to the post!",254,0,7,Towards Data Science,2021-01-03,2021
https://towardsdatascience.com/making-publication-quality-figures-in-python-part-ii-line-plot-legends-colors-4430a5891706,"Making publication-quality figures in python (Part II): Line plot, Legends,¬†Colors",,5,48,"['Making publication-quality figures in python (Part II): Line plot, Legends, Colors', 'Drawing line plot', 'Colors and ‚Äúzorder‚Äù', 'Legends', 'Continuing reading']","This is the second article of my python visualization tutorial: making publication-quality figures. Here is a list of articles I have posted so far and will post soon: If you haven‚Äôt checked the tutorial I, I recommend doing that first before proceeding with reading this article. However, if you‚Äôve already had a good understanding of what Figure and Axes objects are, feel free to skip the first part. In this article, I will use the line plot as an example to show you: Again, all the codes will be available at my GitHub page:https://github.com/frankligy/python_visualization_tutorial Without further ado, let‚Äôs get started! I will thread all the content into making a simple line plot. To start with, we are going to load the matplotlib packages and change the global setting a bit to make sure the font type and font family is what we want, check out the first tutorial if this is something I want to know more about. Also, I will make a bit of preparation for making the line plot, just create some random data x,y1,y2 which I will draw the figures. A line plot is arguably the simplest figure compared to others, it will help us to demonstrate the dynamic trend of your data and aid readers in comparing intrinsic differences among several categories. Above, I created an array x and two responsive arrays y1 and y2 . Now let‚Äôs first plot the y1 against variable x by using ax.plot()basic function. You will have a very basic line plot: Then let‚Äôs again start to play with some parameters, and I will explain to you what each parameter means and how to understand them. But first, let us see the effect: Now let‚Äôs think about, what is a line plot made of? It is a series of markers, and a line, right? So when we adjust the appearance of a line plot, we will first adjust the appearance of markers, including the marker style, marker size, marker face color, marker edge color, marker edge width. Then let‚Äôs move on to the line and we can adjust the line style and line width. To help you understand this process, we ask the question of why the basic plot we first plotted by default setting looks like that? To answer this question, we need to inspect rcParams global setting again. We will see, by default, there won‚Äôt be any marker style, so you didn‚Äôt see any marker but a blue line. Then the marker size is 6.0 by default and we now amplify them to 8.0. The line width by default is 1.5, now it is 3.0. I hope now we will feel more familiar with the line plot and understand how to control it since we‚Äôve already understood all its constitutions. There will be a shortcut argument called fmt standing for formatted string, fmt='ro-' which is equivalent to marker='o',markerfacecolor='red',linestype='-' . But again, I recommend fully understand what each argument represents by explicitly specifying them. Are we done? Of course not, but this is the first step for my following materials. Next, let‚Äôs draw the second line on top of the first line we just drew out. Let me ask you two questions: 2. We didn‚Äôt specify the color of the second plot, why is that rendered as orange color? Let‚Äôs first address question 1. To achieve that, we need to use zorder arguments. It determines which line (Artist object) will be drawn first, the higher the zorder is, the later the line would be drawn, hence it would stay more forward to us. Now let‚Äôs increase the zorder of the blue line to 3. For the second question, we will touch a bit upon python colors. Basically, matplotlib determine the second line‚Äôs color to be orange based on a global setting rcParams['axes.prop_cycle'] , it is a list like this: As you can see, there are ten colors, and if you draw 10 lines sequentially, each line will be assigned to a color-based index in this color list. By default, the first line will adopt the first hexadecimal color ‚Äú#1f77b4‚Äù, which is blue, the second line will naturally adopt the second one ‚Äú#ff7f0e‚Äù, which is orange. Make sense now, right? But let me show you a bit more about the color systems. I don‚Äôt intend to explain to your the whole color theory, for example, the complementary color schema, etc. Because I am not an expert on that and I don‚Äôt want to pretend to be. But here are some tips I followed when I make figures, the rule of thumb is, In categorical variables, you need to pick colors that have obvious contrast, i.e. blue versus orange. In continuous variables, like heatmap, you‚Äôd better choose a continuous color map (cmap) to reflect their relatedness. So in a line plot, we would like to reflect the differences among different lines, I would suggest you stick with python default color system for most cases since the color they use indeed have a very contrastive effect, like the blue and orange ones I showed above. You can represent every color via using the hexadecimal symbol, and you can convert each hexadecimal symbol to (Red, Green, Blue) tuple using like below: In Maplotlib, It has some shortcuts for representing colors too, there are 8 built-in one letter color (b-blue, g-green, r-red, c-cyan, m-magenta, y-yellow, k-black, w-white) and a tableau palette which is the same as the setting rcParams['axes.prop_cycle'] we just printed out. In addition, there are a bunch of colors you can access through a specific name, like ‚Äúorange‚Äù, the full list is shown here https://matplotlib.org/3.1.0/gallery/color/named_colors.html Please check them out and gain a better sense of the color system in maplotlib. In summary, whenever you want to tell maplotlib to draw anything in a certain color, you can choose from: For continuous and discrete colormap, I will cover that when we draw a scatter plot or heatmap, because I believe it will make more sense when you actually use it and see its effect. But for now, I hope you have a better understanding of how to choose the color and understand different color representation when it comes to categorical/qualitative variables. We will still use this line plot example to illustrate how to add legends to the python plot. The easiest way is to add a label argument to your blue line and orange line. Then we call function ax.legend() which will automatically detect these two lines and the corresponding label information. However, this method doesn't help us understand how it actually works under the hood. Hence, I will show you how to explicitly add a legend to a figure. ax.legend() the function accepts two arguments, handles and labels , hanleswould be a list storing matplotlib Artist object. The blue line we just drew would be an Artist object, so is the orange line. labels will be a list storing string objects corresponding to the labels you want to assign to each Artist object in thehandles list. We first need to extract the Artist object from the plot we just drew. I use two variable p1 and p2 to store the returned data from ax.plot() function. Let‚Äôs inspect these two variables: You see, p1 will be a list with only one element, we access this element by p1[0], and p1[0] will be a Line2D object, which is an Artist object we can pass to the ax.legend() function. Same for p2[0]. Now let‚Äôs add the legend explicitly: I will always use the latter approach because it is how ax.legend() function actually work. Again, the shortcut is convenient, but understanding the mechanisms can help you achieve what you can not achieve with a simple shortcut. Now I want to challenge myself a bit and show you legend is actually a separate element in a python plot. This time, we will not extract Artist objects from the line plot we just drew. Instead, I will create handles from the scratch. Here, I actually didn‚Äôt plot the line but just use ax.plot() function to get two handles, h1 and h2 , which will be used for legend adding. Now you know, actually legend is something you can flexibly change as you‚Äôd like to. Finally, I will share with you one more tip for getting full control of legend, which is adjusting its position and its appearance. I added two additional arguments, it means, using the legend‚Äôs upper left corner as an anchor point, and put this anchor point to the ax coordinates (1,1), remember here the whole ax object would be in range (0,1). Now let‚Äôs see the effect and everything will make sense then: If you want to get rid of the frame of the legend box and add a title to the legend, we can do: Just add two arguments, title and frameon . Done! Alright, this brings us to the end of this tutorial, I hope it can help you in some way. My goal is really helping you to understand the underlying detail of maplotlib plotting process. With that understood, you can use these skills to design your own figures, sometimes aesthetic preference varies from people to people, but I intend to share with you the technical mechanisms from which you can build off your own plot using your favorite aesthetic flavor. If you like these tutorials, follow me on medium and I will teach you how to make a violin plot and dendrogram in matplotlib, thank you so much for your support. Connect me on my Twitter or LinkedIn, also please ask me questions about which kind of figure you‚Äôd like to learn how to draw in a succinct fashion, I will respond! All the codes are available at https://github.com/frankligy/python_visualization_tutorial Tutorial III: box plot, bar plot, scatter plot, histogram, heatmap, colormap Tutorial IV: violin plot, dendrogram Tutorial V: Plots in Seaborn (cluster heatmap, pair plot, dist plot, etc)",74,0,9,Towards Data Science,2021-01-03,2021
https://towardsdatascience.com/understanding-accuracy-recall-precision-f1-scores-and-confusion-matrices-561e0f5e328c,"Understanding Accuracy, Recall, Precision, F1 Scores, and Confusion Matrices",This article¬†also‚Ä¶,11,40,"['Understanding Accuracy, Recall, Precision, F1 Scores, and Confusion Matrices', 'Introduction', 'Data üìà', 'Key Terms üîë', 'Accuracy üéØ', 'Recall üì≤', 'Precision üêæ', 'F1 Score üöó', 'Confusion Matrix ‚ùì', 'Using Sklearn to generate Classification Report üëî', 'Conclusion']","Accuracy, Recall, Precision, and F1 Scores are metrics that are used to evaluate the performance of a model. Although the terms might sound complex, their underlying concepts are pretty straightforward. They are based on simple formulae and can be easily calculated. This article will go over the following wrt to each term At the end of the tutorial, we will go over confusion matrices and how to present them. I have provided the link to the google colab at the end of the article. Let‚Äôs assume we are classifying whether an email is spam or not We will have two arrays, the first array will store the actual value while the second array will store the predicted values. These predicted values are obtained from a classifier model. The type of the model is not important, we are interested in the predictions our model made. 0- email is NOT spam (negative) 1- email IS spam (positive) This case occurs when the label is positive and our predicted value is positive as well. In our scenario, when the email is spam and our model classified it as spam as well. This case occurs when the label is negative but our model‚Äôs prediction is positive. In our scenario, when the email is not spam but our model classifies it as spam. This is similar to True Positive, the only difference being the label and predicted value are both negative. In our scenario, when the email is not spam and our model classifies it as not spam as well. This case occurs when the label is positive but the predicted value is negative. In a way, opposite of False Positive. In our scenario, when the email is spam but our model classifies it as not spam. The only condition for this case is that label and the prediction value are the same. In our case, when the model classifies a spam email as spam and a non-spam email as non-spam. The correct prediction can also be calculated as the sum of True Positives and True Negatives The condition for this case is that the label and the prediction value must not be equal. In our scenario, an incorrect prediction is when our model classifies a spam email as not spam and a non-spam email as spam. Incorrect Prediction can also be calculated as the sum of the False positives and False Negatives Accuracy is the ratio of correct predictions to the total number of predictions. It is one of the simplest measures of a model. We must aim for high accuracy for our model. If a model has high accuracy, we can infer that the model makes correct predictions most of the time. High accuracy can sometimes be misleading. Consider the below scenario A spam email is rare compared to a non-spam email. As a result, the number of occurrences with label = 0 is higher than that of label = 1. In the above, code, our labels array has 8 non-spam emails and 2 spam emails. If our model is built in a way that it always classifies an email as non-spam, it will achieve an accuracy of 80%. This is highly misleading since our model is basically unable to detect spam emails. Recall calculates the ratio of predicted positives to the total number of positive labels. In our above case, our model will have a recall of 0 since it had 0 True Positives. This tells us that our model is not performing well on spam emails and we need to improve it. A high recall can also be highly misleading. Consider the case when our model is tuned to always return a prediction of positive value. It essentially classifies all the emails as spam Although the above case would have low accuracy (20%), it would have a high recall score (100%). Precision is the ratio of the correct positive predictions to the total number of positive predictions In the above case, the precision would be low (20%) since the model predicted a total of 10 positives, out of which only 2 were correct. This tells us that, although our recall is high and our model performs well on positive cases, i.e spam emails, it performs badly on non-spam emails. The reason our accuracy and precision are equal is since the model is predicting all positives. In the real world, a model would correctly predict some of the negative cases leading to higher accuracy. However, the precision would still remain unchanged since it only depends on the correct positive predictions and total positive predictions F1 score depends on both the Recall and Precision, it is the harmonic mean of both the values. We consider the harmonic mean over the arithmetic mean since we want a low Recall or Precision to produce a low F1 Score. In our previous case, where we had a recall of 100% and a precision of 20%, the arithmetic mean would be 60% while the Harmonic mean would be 33.33%. The Harmonic mean is lower and makes more sense since we know the model is pretty bad. A confusion matrix is a matrix to represent the number of True Positives, False Positives, True Negatives, and False Negatives Assume we are working with the following data You can also pass a parameter normalize to normalize the calculated data. Below is the output Accuracy alone can not determine if a model is good or bad but accuracy combined with precision, recall, and F1 Score can give a good idea about the performance of the model. colab.research.google.com I recently created a blog using WordPress, I would love it if you could check it out üòÉ realpythonproject.com Check out my tutorial on Bias, Variance and How they are related to underfitting, overfitting towardsdatascience.com Connect with me on LinkedIn www.linkedin.com Originally published at https://realpythonproject.com on January 2, 2021.",151,0,8,Towards Data Science,2021-01-03,2021
https://towardsdatascience.com/handling-geospatial-data-in-aws-a82ae364f80c,Handling Geospatial Data in¬†AWS,A short introduction on how to¬†store‚Ä¶,1,15,['Handling Geospatial Data in AWS'],"With databases, warehouses, analysis, and visualization tools extending their capability to handle geospatial data, we are in a position to capture and use GIS data more than ever. While individual products supported GIS, cloud providers didn‚Äôt offer complete GIS solutions early on. With time, that has changed a lot. Cloud providers like AWS have extended support wherever it was required. For a software company like AWS, we need to understand that the choice of products they build is directly impacted by the requirements of usually their high-paying clients. Now that a lot of companies see the need for geospatial data and that those companies are able to acquire and store it at a reasonably low cost (as location data is omnipresent and storage has gotten cheaper), we have seen wider adoption of geospatial databases, analysis, and visualization tools. Although AWS doesn‚Äôt have a full-fledged solution for Geospatial data, it does provide some great features in different services. In this post, we are going to discuss a bit about that. AWS has two managed relational database services ‚Äî RDS and Aurora. You can obviously install your own relational database using EC2 but that isn‚Äôt a managed solution so we won‚Äôt count that here. Efficient storage of geospatial data is very important for anything we want to do with that data down the line. AWS RDS simply supports whatever version of the relational database service it is running so the support for GIS is dependent upon the version of the database. For instance, if you‚Äôre running RDS for MySQL, you‚Äôll have the PostGIS features supported for that MySQL version. If you‚Äôre running PostgreSQL, you‚Äôll need to install the PostGIS extensions on PostgreSQL running on RDS. On the other hand, using Aurora, which is an engine by AWS written for different relational databases, you can get some additional features, improvements, and optimizations on top of the usual features supported by the original versions of the relational databases. For instance, read about how Aurora has optimized geospatial indexing by using Z-order curves. Redshift, although based on Redshift, didn‚Äôt support geospatial data for the longest time, finally launched support for it in 2019. Currently, in Redshift, we can use find and use native GIS data types. Here‚Äôs the full list of functions supported by Redshift for GIS data. aws.amazon.com This is for Redshift but what happens when you have geolocation data stored in S3. How do you query that? Given that you have stored your geolocation data in the GeoJSON format, you should be able to create an external table in Athena using nature GIS types and query the data using Athena. See the following example for more details. A quick note about visualization too ‚Äî with AWS, you can use Quicksight to create geographical/geospatial charts using data from Athena and elsewhere. Although not as rich and powerful as full-fledged visualization tools like Looker and Tableau, Quicksight does do the trick when simple visualizations are required. Services like SNS, SQS don‚Äôt care about what kind of data is being received by them as long as it is in the prescribed format. This means that if there is a producer of geolocation data, then it can be consumed by events/streaming services in AWS. Having said that, AWS IoT Events makes it really easy to tackle geolocation data (IoT events, in general). I don‚Äôt know if that phrase fits right with Amazon Location Service (ALS) which is set to compete with the likes of Google Maps in providing point-of-interest data to companies without compromising on data privacy and security, as there will be provisions of the data not leaving the AWS network ever, although this may come at an additional cost. techcrunch.com We are yet to see how mature this service is. Is it going to provide address translation, refinement, and normalization too? A couple of days ago I came across an amazing service called Placekey which tries to solve the problem of address normalization with great accuracy. We‚Äôll wait and watch if Amazon has some tricks up its sleeve. Currently, Amazon Location Service is in preview. aws.amazon.com With Amazon Location Service already in preview, is AWS going to concentrate more on getting more geolocation data specific services, maybe a specialized geolocation database or a visualization engine? In the last couple of years, AWS has definitely taken up a share in the time-series, graph, document database market. More geolocation services might actually be on the cards.",38,0,4,Towards Data Science,2021-01-03,2021
https://towardsdatascience.com/logistic-regression-in-python-a-helpful-guide-to-how-it-works-6de1ef0a2d2,Logistic Regression in Python‚Äî A Helpful Guide to How It¬†Works,,7,75,"['Logistic Regression in Python‚Äî A Helpful Guide to How It Works', 'Preface', 'Intro', 'This story covers the following topics:', 'What category of algorithms does logistic regression belong to?', 'How does logistic regression work?', 'Logistic regression in Python']","Just so you know what you are getting into, this is a long article that contains a visual and a mathematical explanation of logistic regression with 4 different Python examples. Please take a look at the list of topics below and feel free to jump to the sections that you are most interested in. Machine Learning is making huge leaps forward, with an increasing number of algorithms enabling us to solve complex real-world problems. This story is part of a deep dive series explaining the mechanics of Machine Learning algorithms. In addition to giving you an understanding of how ML algorithms work, it also provides you with Python examples to build your own ML models. Looking at the below chart's supervised learning branch, we can see that we have two main categories of problems: regression and classification. While logistic regression has a ‚Äúregression‚Äù in its name, it actually belongs to the classification algorithms. However, there are some similarities between linear regression and logistic regression, which we will touch upon in the next section. The below graph is interactive, so make sure to click on different categories to enlarge and reveal moreüëá. If you enjoy Data Science and Machine Learning, please subscribe to get an email whenever I publish a new story. Let‚Äôs begin the explanation by looking at the following example. Assume we have a class of 10 pupils where each of them had to take an exam. Their preparation time, final score, and outcome (pass/fail) are displayed below. Note, the passing score is 40. Now, let‚Äôs see how we would approach this problem using linear regression vs. logistic regression. If we were to build a simple linear regression model, we could use ‚Äòhours of study‚Äô as our independent variable and ‚Äòfinal score‚Äô as the dependent (target) variable. This is because ‚Äòfinal score‚Äô is a continuous variable as required by regression. This would lead us to a result summarized by a best-fit line taking the following form: Note, adding more independent variables would result in having more elements in your equation: Let‚Äôs now assume that we do not have a ‚Äòfinal score.‚Äô All we have is an outcome( pass/fail flag). We want to build a logistic regression model where we use ‚Äòhours of study‚Äô to predict a student's likelihood of passing the exam. As you can see from the table above, there is a strong correlation between ‚Äòhours of study‚Äô and ‚Äòexam outcome,‚Äô although we cannot perfectly separate the two classes. Hence, we want to have a model that gives us a probability of passing the exam given the study hours. This is done by using a logistic function, also known as a sigmoid function: If we were to plot a logistic function on a chart, it would look like this: To understand how the data is mapped to the logistic function, we first need to learn about the relationship between probability, odds, and log-odds. Obviously, we could simplify it further, which would lead us back to the original equation of probability expressed through odds. However, we are happy with this form because now we can go one step further to find the log-odds equation. Let‚Äôs use another example to plot the data onto a graph to understand how the log-odds equation is created. We can plot this data onto a chart with ‚Äòstudy hours‚Äô on the x-axis and log-odds on the y-axis: Now, this looks familiar. The relationship between our independent variable x (hours of study) and log-odds is linear! This means that we can draw the best fit-line through the points using the same type of line equation: This makes our Logistic function: A general form with multiple independent variables becomes: When you build logistic regression models, the algorithm's goal is to find the coefficients Œ≤(0), Œ≤(1), etc. Unlike linear regression, though, it is not done by minimizing squared residuals but finding the maximum likelihood instead. Maximum likelihood is most often expressed through a log-likelihood formula: There are multiple methods available to maximize the log-likelihood. Some of the most commonly used ones would be gradient descent and Newton‚ÄìRaphson. In general, methods used to find the coefficients for the logistic function go through an iterative process of selecting a candidate line and calculating the log-likelihood. This is continued until the convergence is achieved and the maximum likelihood is found. Note, I will not go into the mechanics of these algorithms. Instead, let‚Äôs build some logistic regression models in Python. Now is the time to build some models using the knowledge that we acquired. We will use the following libraries and data: Let‚Äôs import all the libraries: We will use data on chess games from Kaggle, which you can download following this link: https://www.kaggle.com/datasnaek/chess. Once you have saved the data on your machine, we ingest it with the following code: As we will want to use the ‚Äòwinner‚Äô field for our dependent (target) variable, let‚Äôs check the distribution of it: It is good to see that the wins between white and black are quite balanced. However, a small minority of matches ended up in a draw. Having an underrepresented class will make it harder to predict it, which we will see the multinomial examples later. For the binary outcome model, we will try to predict whether the white pieces will win using the player rating difference. Meanwhile, for the multinomial case, we will attempt to predict all three classes (white win, draw, black win). First, let‚Äôs derive a few new fields for usage in model predictions. Let‚Äôs start building! We will use the difference between white and black ratings as the independent variable and the ‚Äòwhite_win‚Äô flag as the target. After splitting the data into train and test samples, we fit the model. We chose sag (stochastic average gradient) solver for finding beta parameters of the log-odds equation this time. As listed in the comments below, there are other solvers, which we will try in the next few examples. This gives us the following log-odds and logistic equations: Let‚Äôs check our model performance metrics on the test sample: A quick recap on the performance metrics: We can see that while the model is not great, it still helps us to identify the white win in 64% of the cases, which is better than a random guess (a 50% chance of getting it right). Next, let‚Äôs plot a Logistic function with each class mapped onto it. We will do some data preparation first: We will use masking in the graph to create two separate traces, one with events (white won) and the other with non-events (white did not win). As you can see, it is simply a boolean array contain True for 1 and False for 0. Let‚Äôs take a look at what is displayed here. Quick note, I had to offset green and red dots by a small amount (0.01) to avoid overlapping for easier reading. In summary, while the model can correctly predict a white win in 64% of the cases {p(white win)>0.5}, there are also lots of cases (36%) where it did not predict the outcome successfully. This suggests that having a higher rating in chess does not guarantee success in a match. Let‚Äôs add an additional independent variable to the next model. We will use a field called ‚Äòturns,‚Äô which tells us the total number of moves made in a match. Note that we are somewhat cheating here as the number of total moves would only be known after the match. Hence, this data point would not be available to us if we were to make a prediction before the match starts. Nevertheless, this is for illustration purposes only, so we will go ahead and use it anyway. Note that we have two slope parameters this time, one for each independent variable. Œ≤(2) is slightly negative, suggesting that a higher number of ‚Äòturns‚Äô indicates a lower chance of white winning. This makes sense as the white not winning also includes ‚Äòdraws,‚Äô and they are more likely to occur after a long match (after many moves). Let‚Äôs take a look at model performance metrics on a test sample: We can see that all classification metrics have improved for this model with 66% correct predictions. Not a surprise, given we used the ‚Äòturns‚Äô field, which gives us information about how the match has evolved. Let‚Äôs now do some data prep and plot a logistic function again, although this time, it will be a surface on a 3D graph instead of a line. It is because we used 2 independent variables in our model. Plot the graph: This graph shows how the black dots at the top (class=1) and the bottom (class=0) have been mapped onto the logistic function prediction surface. In this case, green dots show probabilities for class=1 and blue ones for class=0. Let‚Äôs now build a model that has 3 class labels: Note that for a multinomial case, we have three intercepts and 3 pairs of slopes. This is because the model creates a separate equation for predicting each class. Let‚Äôs look at the model performance: As expected, the model had some difficulty predicting class=0 (draw) due to the unbalanced data. You can see a lot fewer draw outcomes (175 in the test sample) than wins by either white or black. Based on precision, we can see that the model got 43% of its ‚Äòdraw‚Äô predictions right. However, the recall is only 0.02, meaning that there were very few cases where the model predicted a ‚Äòdraw‚Äô with most of the ‚Äòdraw‚Äô outcomes being unidentified. There are multiple ways of dealing with unbalanced data, with one approach being to oversample the minority class (in this case, class=0). We will use the ‚Äúrandom oversampler‚Äù from the imbalanced-learn package to help with our quest. These are the final results. We can see that the model accuracy has gone down due to a reduction in precision for class=0. This is expected with oversampling as the model expects the class to be much more common than it actually is, leading to more frequent predictions of a ‚Äòdraw.‚Äô While this harmed precision, it has helped with recall as the model was able to identify more of the ‚Äòdraw‚Äô outcomes. Clearly, this model is far from ideal and more work is needed to improve it. This can be done by adding more independent variables and employing additional techniques such as undersampling majority classes. However, the purpose of these examples was to show you how you can build different types of logistic regression models rather than finding the best model for this specific set of data. I believe I have given you plenty of examples to work with. Hence, I will stop the story here. This has been one of the longer stories I have written. If you managed to get all the way to the end, then kudos to you! üëè I hope you now have a good understanding of what logistic regression is and that I have inspired you to open your notebook and to start building logistic regression models yourself. Cheers! üëèSaul Dobilas If you have already spent your learning budget for this month, please remember me next time. My personalized link to join Medium is: solclover.com Related stories you may like: towardsdatascience.com towardsdatascience.com",90,1,13,Towards Data Science,2021-01-03,2021
https://towardsdatascience.com/something-every-data-scientist-should-know-but-probably-doesnt-the-bias-variance-trade-off-25d97a17329d,Something Every Data Scientist Should Know But Probably Doesn‚Äôt: The Bias-Variance Trade-off,,11,48,"['Every Data Scientist Should Know: The Bias-Variance Trade-off Generalization is Wrong', 'Introduction', 'Background', 'Late 2018: The Double Descent Phenomenon', 'How Does It Work?', 'Conclusion', 'About the Author', 'More Trending Articles:', 'References', 'Further Reading', 'Code']","A groundbreaking and relatively new discovery upends classical statistics with relevant implications for data science practitioners and statistical consultants Data science is a fascinating field. C-level executives are enamored by its promised impact on top line revenue and practitioners are intrigued by the rapid pace of innovation. There‚Äôs already so much to know and it seems like every year, a few more things to learn. This article draws attention to a relatively novel idea that is probably controversial to most data scientists and maybe a handful of statisticians: the bias-variance tradeoff generalization does not generalize and only applies to very specific scenarios. In fact, at the time of this writing, the bias-variance tradeoff has been empirically disproven for specific, realistic scenarios of almost every model known, including linear regression! Obviously, this is not surprising to experienced deep-learning practitioners or those few thousand people avidly tracking the expanding body of relevant literature. This article summarizes a dozen of the most prominent and ‚Äúrecent‚Äù (from 2018 through Dec. 2020) research papers on double descent out of ~70+ [2]. By virtue of what a summary is and given the hundreds of pages of research being summarized, this article glosses over many details and explains enough about the main ideas to have a high-level understanding. The term for this *groundbreaking* discovery is the ‚Äúdouble descent‚Äù phenomenon and this idea goes by several names: double descent, deep double descent, double descent phenomenon/curve/risk curve. There are at least two types of double descent: model-wise and sample-wise. This article discusses the former in greater depth than the latter, though the latter is covered briefly in my discussion of linear model double descent. I have written this article to be accessible to a broader audience. To that end, I will restate the same idea with different terminology to help familiarize my audience with the jargon. For readers with relevant background, skip the Background section :) Statisticians, data scientists and deep learning practitioners are aware of the classical statistics concept of bias-variance tradeoff: The bias-variance trade-off implies that a model (i.e. an equation) ‚Äúshould balance under-fitting and over-fitting: rich enough to express underlying structure in data, simple enough to avoid fitting spurious patterns‚Äù [1]. Figure 2.11 shown at left is a classic bias-variance diagram illustrating the different impact of increasing model complexity on prediction error for training vs. test data. A more complex model predicts values closer to the actual value or demonstrates fewer cases of misclassification. In other words, the model exhibits less bias. Figure 2.4 shown at left is a well-known visualization illustrating a linear model that predicts Income based on two variables: Years of Education and Seniority. A model‚Äôs fit, or ability to predict a a dependent variable for a dataset, increases with increasing model complexity. Examples of increasing a model‚Äôs complexity include adding more terms, non-linear/polynomial terms (e.g. x¬≤, x¬≥, etc.) or step-wise/piecewise-constants for multiple linear regression, increasing the number of training epochs/training time for a neural network or increasing the number of decision nodes (‚Äúdepth‚Äù) and increasing the number of leaves at each decision node of a decision tree and increasing the number of trees in the case of a random forest. Variance is a measure of how much change is observed in the approximation of the function, i.e. the model or equation. When the function approximation is done with varying training datasets, more flexible models generally see greater variability because they change more easily to fit different datasets. In other words, variance refers to the amount by which the function, an equation relating a set of inputs to an output, changes, when built using one or more different training data sets [3, 5]. The bias-variance tradeoff is a statement on the relationship between interpolation and generalization. Conventional statistical wisdom states that increasing model complexity beyond the point of interpolation, or vanishing training error, is a recipe for overfitting and a model with poor generalization; meaning it will perform poorly on a different, unseen data set [3, 4]. However, subsets of the machine learning community regularly train models to perfectly fit training datasets, such that there is zero training error and these models go on to perform well on unseen, test data. This empirical contradiction has inspired feverish excitement about the ‚Äúmystical‚Äù properties of neural networks and motivated the need for explanation [1a]. Starting in February 2018, Berklin, Mikhail et al. wrote a series of articles seeking to do just that [1a, 1b, 1c, 1d]. In December 2018 [1a], Belkin et. al. formalized an empirical observation: the bias-variance tradeoff only holds true for distinct scenarios and coined the term ‚Äúdouble descent‚Äù to describe the phenomenon where bias-variance tradeoff doesn‚Äôt hold true. Figure 1 from the first paper in their series of papers on the topic helps build intuition: The work of these Belkin et. al. spurred dozens of subsequent publications that formalized corroboration of the double descent phenomenon in multiple data sets and multiple model types. The foundational text ‚ÄúESL‚Äù (Elements of Statistical Learning) is potentially most responsible for teaching the bias-variance concept to the majority of practitioners today and one of its authors, Trevor Hastie corroborated the double descent phenomenon with his and his collaborators results in March 2019 [4], with numerous revisions as recent as just a few weeks ago (7 Dec 2020). Point is, this is exciting stuff with a lot of eyes on new developments. No one knows. ‚ÄúFully understanding the mechanisms behind model-wise double descent in deep neural networks remains an important open question. However, an analog of model-wise double descent occurs even for linear models. A recent stream of theoretical works analyzes this setting (Bartlett et al. (2019); Muthukumar et al. (2019); Belkin et al. (2019); Mei & Montanari (2019); Hastie et al. (2019)). We believe similar mechanisms may be at work in deep neural networks‚Äù (Nakkiran, Preetum, et al., arxiv, December 2019, [8a]). Nakkiran et. al. [8a] posits potential explanations for double descent. The second model is over-parametrized. In this toy example, the over-parameterized model can have a number of values for Œ∏1 and Œ∏2 that can result in multiple interpolating models that accurately predict Y. Stochastic gradient descent is able to find the model that best ‚Äúmemorizes‚Äù or ‚Äúabsorbs‚Äù the noise in the data, making it robust to new datasets. In other words, having more model parameters than observations means that there are multiple subsets of features that will allow fitting of the over-parameterized model to the training data and stochastic gradient descent. In statistical parlance, an over-parameterized model, p>n does not have a unique least square objective does not have a unique minimizer So why does double descent happen? There are hypotheses and I described two above, but no one knows‚Ä¶yet. Double descent is a robust phenomenon demonstrated for a breadth of neural net architectures, random forests, ensemble methods and even linear regression for both popular and synthetic datasets. Double descent can be described simply as a test error curve that observes two descents. Model-wise double descent is a test error curve that observes two descents with increasing model capacity/complexity/flexibility. Sample-wise double descent is a test error curve that observes two descents with increasing the number of observations in a training dataset. 4. Random forests [1a]. Figure 11 from Belkin et. al. demonstrates double descent with increasing ‚Äúmodel capacity,‚Äù which in this case means increasing maximum number of trees in a single forest and the maximum number of leaves for each tree in the forest. 5. Ensemble models consisting of multiple random forest models [1a]. Figure 12 below from Belkin et. al. [1a] shows increasing ‚Äúmodel capacity,‚Äù i.e. increasing the number of forests in the ensemble and the number of trees in each forest. The above 4 points are examples of ‚Äúmodel-wise double descent,‚Äù basically, increasing model capacity / complexity / flexibility can cause demonstrate the tradition bias-variance tradeoff and then a second descent in test error. It turns out, and here‚Äôs another revelation, more data can potentially hurt model performance! Which leads to the following case of sample-wise double descent: 6. Linear models. Nakkiran et. al. showed [8b] that sample-wise double descent can occur for over-parameterized models. Figure 1 below from Nakkiran et. al. [8b], shows the test mean-squared error (MSE) of a multiple linear model with 1,000 features trained on [0, 2000] samples. The model is over-parameterized for ‚ÄúNum. Samples‚Äù less than 1000. As the size of the training dataset increases from 0 to 900 observations and we see test MSE decrease, as expected since ‚Äúmore data is always better.‚Äù However, when the number of samples in the training set increases from around 900 to 1,000 observations, the test MSE increases. What? More data results in worse out-of-sample, test error?! Then, test error drastically decreases again, when the training dataset increases from 1,000 to ~1,100 observations before resuming the previous velocity of improvement in test MSE. Two takeaways: 1. more data can hurt in the over-parameterized model, 2. double descent in test error is observed when changing training dataset size. This is a great result because it is very simple experiment with a model familiar to most and because it should be easy to reproduce (I will try to and share my code if this gets 1K claps :). Yes, my head hurts too. But pretty exciting and mind-blowing stuff, right? Andrew Young is an R&D Data Scientist Manager at Neustar. For context, Neustar is an information services company that ingests structured and unstructured text and picture data from hundreds of companies in the domains of aviation, banking, government, marketing, social media and telecommunications to name several. Neustar combines these data ingredients then sells a finished ‚Äúdish‚Äù with added value to enterprise clients for purposes like consulting, cyber security, fraud detection and marketing. In this context, Mr. Young is a hands-on lead architect on a small R&D data science team responsible for building, optimizing and maintaining a system feeding all products and services responsible for $1+ billion in annual revenue for Neustar (Follow him on LinkedIn to stay up-to-date on the latest trends in data science!) [1a] Belkin, Mikhail, et al. ‚ÄúReconciling modern machine learning practice and the bias-variance trade-off.‚Äù arXiv preprint arXiv:1812.11118 (2018). https://arxiv.org/abs/1812.11118 [1b] Belkin, Mikhail, Siyuan Ma, and Soumik Mandal. ‚ÄúTo understand deep learning we need to understand kernel learning.‚Äù arXiv preprint arXiv:1802.01396 (2018). https://arxiv.org/abs/1802.01396 [1c] Belkin, Mikhail, Alexander Rakhlin, and Alexandre B. Tsybakov. ‚ÄúDoes data interpolation contradict statistical optimality?.‚Äù The 22nd International Conference on Artificial Intelligence and Statistics. PMLR, 2019. https://arxiv.org/abs/1806.09471 [1d] Belkin, Mikhail, Daniel Hsu, and Ji Xu. ‚ÄúTwo models of double descent for weak features.‚Äù SIAM Journal on Mathematics of Data Science 2.4 (2020): 1167‚Äì1180. https://arxiv.org/abs/1903.07571 [2] Deng, Zeyu, Abla Kammoun, and Christos Thrampoulidis. ‚ÄúA model of double descent for high-dimensional binary linear classification.‚Äù arXiv preprint arXiv:1911.05822 (2019). https://arxiv.org/abs/1911.05822 [3] Hastie, Trevor, Robert Tibshirani, and J. H Friedman. The Elements of Statistical Learning: Data Mining, Inference, and Prediction. 2nd ed. New York: Springer, 2009. Print. https://web.stanford.edu/~hastie/ElemStatLearn/ [4] Hastie, Trevor, et al. ‚ÄúSurprises in high-dimensional ridgeless least squares interpolation.‚Äù arXiv preprint arXiv:1903.08560 (2019). https://arxiv.org/abs/1903.08560 [5] James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. An Introduction to Statistical Learning: With Applications in R. , 2017. Print. https://statlearning.com/ [6] Mitchell, Tom. Carnegie Mellon University. Machine Learning lecture notes. https://www.cs.cmu.edu/~guestrin/Class/10701-S05/slides/NNet-CrossValidation-2-2-2005.pdf [7] Muthukumar, Vidya, et al. ‚ÄúHarmless interpolation of noisy data in regression.‚Äù IEEE Journal on Selected Areas in Information Theory (2020). https://arxiv.org/abs/1903.09139 [8a] Nakkiran, Preetum, et al. ‚ÄúDeep double descent: Where bigger models and more data hurt.‚Äù arXiv preprint arXiv:1912.02292 (2019). https://arxiv.org/abs/1912.02292 [8b] Nakkiran, Preetum. ‚ÄúMore data can hurt for linear regression: Sample-wise double descent.‚Äù arXiv preprint arXiv:1912.07242 (2019). https://arxiv.org/abs/1912.07242 [9] Xu, Bing, et al. ‚ÄúEmpirical evaluation of rectified activations in convolutional network.‚Äù arXiv preprint arXiv:1505.00853 (2015). https://arxiv.org/pdf/1505.00853.pdf Other people astounded by this phenomenon (I just skimmed their posts but could be informative to read other perspectives) [10] https://news.ycombinator.com/item?id=21730020 [11] https://www.lesswrong.com/posts/FRv7ryoqtvSuqBxuT/understanding-deep-double-descent [12] https://medium.com/@LightOnIO/beyond-overfitting-and-beyond-silicon-the-double-descent-curve-18b6d9810e1b [13] https://medium.com/@LightOnIO/recover-the-double-descent-curve-with-an-opu-21df319142aa [14] https://github.com/lightonai/double-descent-curve",427,0,11,Towards Data Science,2021-01-03,2021
https://towardsdatascience.com/benchmark-m1-part-2-vs-20-cores-xeon-vs-amd-epyc-16-and-32-cores-8e394d56003d,"Benchmark  M1 (part 2) vs 20 cores Xeon vs AMD EPYC, 16 and 32¬†cores",Benchmark M1 (part 2) on¬†MLP‚Ä¶,1,28,['M1 competes with 20 cores Xeon¬Æon TensorFlow training'],"In the first part of M1 Benchmark article I was comparing a MacBook Air M1 with an iMac 27"" core i5, a 8 cores Xeon(R) Platinum, a K80 GPU instance and a T4 GPU instance on three TensorFlow models. While the GPU was not as efficient as expected, maybe because of the very early version of TensorFlow not yet entirely optimized for M1, it was clearly showing very impressive CPU performances beating by far the iMac and 8 cores Xeon(R) Platinum instance. While it‚Äôs usually unfair to compare an entry price laptop with such high-end configurations, the M1 has an advantage; it has 8 physical cores. The iMac 27"" Core i5 has 4 cores and the 8 cores Xeon(R) Platinum is like every instance counting the vCPU, so only has 4 physical cores. I‚Äôve doubled checked core id, siblings and physical id to make sure by following this paper. In this article, I compare the M1 with more powerful configurations having 8 to 20 physical cores (16 to 40 hyper-threaded cores). The AMD EPYC configurations are instances while the Xeon(R) Silver is a BareMetal, meaning a true physical dedicated server. The Xeon(R) server has two CPUs with 10 cores (20 threads) each, so totalize 20 physical cores (40 threads). It uses TensorFlow 2.3.1 to benefit from some compilation options. At startup it displays the following: I also compared with the Intel(R) MKL delivered with Anaconda. Both are showing similar performances. The MacBook Air is using Apple Silicon native version of TensorFlow capable to benefit from the full potential of the M1 (even if part 1 article shows that GPU does look yet optimized). As a reminder (as shown in this previous article) here are the M1 specs. You can find the models and the dataset used in the previous article. The following plots shows the results for trainings on CPU. In CPU training, the MacBook Air M1 exceeds by far the performances of the two AMD EPYC and of the 20/40 cores Intel(R) Xeon(R) Silver on MLP and LSTM. Only the convnet gives a very small advantage to the Xeon(R) for 128 and 512 samples batch size and the AMD EPYC 16/32 is only slightly better for 512 samples batch size. The following plot shows how many times other devices are slower than M1 CPU. For MLP and LSTM M1 is about 5 to 13 times faster than 8/16 cores AMD EPYC, 3 to 12 times faster than 16/32 cores AMD EPYC and 2 to 10 times faster than 20/40 cores Xeon(R) Silver BareMetal. For CNN, M1 is roughly 2.5 times faster than the others for 32 samples batch size, 2 times faster than AMD EPYC on the other batch size and only 10% to 20% slower than 20/40 cores Xeon(R) Silver on 128 and 512 samples batch size. Let‚Äôs check the CPU consumption for the Xeon(R) during training. Half of the cores are loaded at about 70% meaning the hyper-threading is useless in this case, only the physical cores are used. Now let‚Äôs check the M1 CPU history during the whole benchmark. It‚Äôs surprisingly showing that only 4 cores are really used but only at 50% during MLP and CNN training. The maximum load happened during the LSTM training; this is also the only case where the 4 other cores are loaded up to 50%. We can suppose the most loaded are the ‚Äúhigh performance‚Äù cores at 3.2 GHz. So how does the M1 can, by only partially using half of its cores, achieve such superior performances, beating by far a 20/40 cores Xeon(R) Silver using a TensorFlow version compiled with AVX-512 instructions? The M1 ‚Äúperformance‚Äù cores have a frequency 50% higher than the Xeon(R) but this one has 20 physical cores, making this frequency difference not sufficient to explain such performance gap. As Apple is never disclosing details of its processor‚Äôs designs, it‚Äôs difficult to know how the different parts of M1 are really used. Anyway, we can try to formulate some hypothesis. Again, these are hypothesis and only Apple can explain how it really works. From these tests, it appears that Of course, these metrics can only be considered for similar neural network types and depths as used in this test. For big trainings and intensive computing lasting for more than 20 minutes, I will still go for cloud-based solutions as they provide cards built for such long heavy load and enable sending several jobs simultaneously. But this scenario is only for some specific research representing only 10% of my work, mostly for professional usage in some specific business area. As a machine learning engineer, for my day-to-day personal research, M1 Mac is clearly the best and the most cost-efficient option today. Thank you for reading.",123,3,6,Towards Data Science,2021-01-03,2021
https://towardsdatascience.com/linear-regression-in-r-a-case-study-d22fdc0ede54,Linear Regression in R: A Case¬†Study,How to evaluate the extent of a linear relationship between¬†two‚Ä¶,1,47,['Linear Regression in R: A Case Study'],"Borrowed from the domain of statistics, linear regression is a handy model with emerging popularity in machine learning algorithms. Particularly useful for predictive analytics, the goal is to make the most accurate predictions possible based on historical data. Linear regression models the relationship between independent and dependent variables. When one dependent variable is being evaluated, the process is termed simple linear regression; when more than one is considered, the process is called multiple linear regression. Thankfully, the process to run both scenarios is possible on datasets imported in R. In this particular case study, I wanted to see if there was a significant linear relationship between the number of fish meals consumed per week and the total mercury levels found amongst fishermen. The dataset used in this analysis is attached as an appendix item at the end of the article. Since we have data between two variables only, I looked at applying a simple linear regression model to the dataset in question. This article focuses on practical steps for conducting linear regression in R, so there is an assumption that you will have prior knowledge related to linear regression, hypothesis testing, ANOVA tables and confidence intervals. In case you require additional background on these topics, I recommend checking out the tutorials listed at the end of this article on the prior-mentioned topics. Step 1: Save the data to a file (excel or CSV file) and read it into R memory for analysis This step is completed by following the steps below. 1. Save the CSV file locally on desktop 2. In RStudio, navigate to ‚ÄúSession‚Äù -> ‚ÄúSet Working Directory‚Äù ->‚ÄúChoose Directory‚Äù -> Select folder where the file was saved in Step 1 3. In RStudio, run the commands: data <- read.csv(‚Äúfisherman_mercury_levels.csv‚Äù) attach(data) Step 2: To get a sense of the data, generate a scatterplot. Consciously decide which variable should be on the x -axis and which should be on the y-axis. Using the scatterplot, evaluate the form, direction, and strength of the association between the variables. Looking at the plot, there is a noticeable positive, linear association between the number of fish meals consumed per week and the total mercury levels found amongst the fishermen. As the number of fish meals increase per week, the total mercury levels similarly increase. The association is strong since most points are compacted towards each other (if a line was drawn to depict linear regression, most of these data points will be close in proximity instead of scattered away from this line). Step 3: Calculate the correlation coefficient. What does the correlation tell us? Correlation coefficient is a statistical measure that evaluates the strength and direction (positive or negative) of the relationship between two variables. By running the cor() function in R between the number of fish meals per week and the mercury levels for the 100 fishermen, the value was calculated to be 0.78. Since the correlation coefficient values range from -1 to 1, with negative values showing a negative correlation and positive values showing a positive correlation, it can be concluded that these values are both positively correlated. In terms of strength of correlation, higher values indicate stronger correlations between the two variables compared to lower calculated values for correlation coefficient. In this case, 0.78 indicates a strong correlation, especially when considering that the closer the correlation coefficient is to 1, the stronger the correlation. Step 4: Find the equation of the least-squares regression equation and write out the equation. Add the regression line to the scatterplot you generated above. Equation of the least-squares regression line is evaluated using the formula below. Step 5: What is the estimate for Œ≤1 beta_1? How can we interpret this value? What is the estimate for Œ≤0 beta_0? What is the interpretation of this value? The estimate for Œ≤1 is 0.4841, which is the value for the slope of the least-square regression line. This value indicates a positive, linear increase in the responsive variable when the explanatory variable increases. There is approximately 1 unit increase in mercury level amongst the fishermen when the number of meals per week that include fish increases by 2. The estimate for Œ≤0 is 1.3339, which reflects the value of the y-intercept of the least-square regression line. It is of particular interest because this value reflects the expected mercury level when no meals containing fish are being consumed by the fishermen per week. The beta_0 value shows that there is a baseline level for mercury level that is not null (or zero), and that not consuming fish in the weekly meals does not guarantee a zero-level of mercury in the fishermen. Step 6: Calculate the ANOVA table and the table which gives the standard error of Œ≤^ 1 (hat beta 1) . Formally test the hypothesis that beta_1 = 0 using either the t-test at the alpha level a=0.10. Formal Test for Linear Association 2. Specify the Alternative Hypothesis 3. Set the Significance Level i. Determine the appropriate value from the t-distribution with n-2 = 100‚Äì2 = 98 degrees of freedom and associated with a right-hand tail probability of Œ± = 0.10. ii. Using R, the t-value associated with Œ± = 0.10 and df = 98 is 1.2903. iii. Reject H0 if t ‚â• 1.2903 Otherwise, do not reject H0 4. Compute the t-value 5. Conclusion Reject H0 since 12.26 > 1.29. There is a linear association between the number of meals including fish per week and the levels of mercury in the fishermen. As a result, it can be concluded that eating more meals with fish per week increases the levels of mercury amongst fishermen. The ANOVA table can be used to find the R^2 value for this linear association. The R^2 value above indicates that 61% of the variability in mercury levels amongst the fishermen can be explained by the number of fish meals consumed by the fishermen per week. Additionally, the 90% confidence interval for Œ≤1 is calculated below. The 90% confidence interval for Œ≤1 is 0.41 to 0.55. This value indicates that there is a 90% probability of the slope of the regression line lying within this range. Appendix https://gist.github.com/vsipra/a895d0310c279cd1120349445dc00878#file-fisherman_mercury_levels-csv 2. R Code https://github.com/vsipra/medium/blob/main/fisherman_mercury_levels_r_code 3. Statistical Tutorials Five-Step Hypothesis Testing: www.scribbr.com ANOVA Procedure: sphweb.bumc.bu.edu Hope this article helps you on your data analytics journey!",32,0,6,Towards Data Science,2021-01-03,2021
https://towardsdatascience.com/how-to-visualize-hypergraphs-with-python-and-networkx-the-easy-way-4fe7babdf9ae,How to visualize hypergraphs with Python and networkx‚Ää‚Äî‚ÄäThe Easy¬†Way,,5,24,"['How to visualize hypergraphs with Python and networkx ‚Äî The Easy Way', 'First ideas and attempts', 'Decomposing a hypergraph into many graphs', 'Other examples with random hypergraphs', 'Conclusion']","Graphs are awesome, hypergraphs are hyperawesome! Hypergraphs are a generalization of graphs where one relaxes the requirement for edges to connect just two nodes and allows instead edges to connect multiple nodes. They are a very natural framework in which to formulate and solve problems in a wide variety of fields, ranging from genetics to social sciences, physics, and more! But whereas for manipulating and visualizing graphs there are many mature Python libraries, the same cannot be said for hypergraphs. I recently needed to visualize some hypergraphs and could not find any library which satisfied me; moreover, as far as I could see, all of them were representing hypergraphs via Euler diagrams (i.e., like the hand-drawn hypergraph above). Unfortunately, this method was not producing good plots for my use case; what I ended up doing was to represent a hypergraph as a collection of graphs, and plotting them via the awesome networkx library. I decided to write about my approach to plotting hypergraphs, as it produces quite interesting plots for the kind of data I am looking at lately (essentially Ising models, but containing many body interactions too), and, with some adaptations, it could be of some help to others struggling with hypergraph visualization tasks. We will start by representing a hypergraph in Python with the following code: Notice that this is just a very basic way to do so, as edges should really be Python frozensets, so that a collection of them can also be a set, and the node set should also be a frozenset or a set. A better schema would help us greatly in case we were developing a library for hypergraph algorithms, but in this post I decided to keep the simple schema above for the sake of simplicity. We can start by noticing that any hypergraph can be transformed into a graph via the so-called star expansion, i.e., we create a new graph having as node set both the nodes and the edges of the original hypergraph, and the edges are given by the incidence relations in the original hypergraph (if a node n was part of an edge e in the hypergraph, there should be an edge between n and e in the new graph). This is way less complicated than it sounds, as shown in the picture below: In many applications plotting the star expansion of a hypergraph might be enough. For example, the star expansion of test_hypergraph (defined above) looks as follows By making some edits, such as expanding only the edges connecting three or more nodes and choosing a different layout for the extra nodes, one can also plot test_hypergraph like this: In the plot above we can read, for instance, the edge (a,b,d) from the three red lines connecting a, b, and d. While the fewer extra nodes caused by a partial star expansion make the second plot slightly more readable, both of the plots above are still deeply unsatisfying. My main issues with these visualizations (and similar ones I ended up producing while experimenting on this topic) are the following: Solving #2 is very challenging, and most likely heavily dependant on the kind of hypergraph one wants to visualize. I found a strategy which works quite well for solving #1 in case it makes sense to split the edges of a hypergraph by their cardinality, and will explain my approach below. The key idea is that we will decompose the edges of a hypergraph by how many nodes they contain, in a way completely analogous to how physicists speak of 2-body interactions, 3-body interactions, and so on, and plot these different ‚Äúcomponents‚Äù of the hypergraph separately. To decompose the edge set of a hypergraph (assuming a schema like the one we used for test_hypergraph above) by the edges' cardinality, we can write For instance, the decomposed_edges dictionary for test_hypergraph looks as follows So then, we can try to plot, for each edge order i, the star expansion of the hypergraph given by the node set of the original graph, together with the edge set decomposed_edges[i] (except for i = 2, where we will just plot the graph without star expanding it), and see what it looks like. The code to achieve this is the following: Applying the plot_hypergraph_components function to test_hypergraph produces the following plot: This script should be pretty straightforward, but let me know if you want anything clarified further. One weird choice I had to make is to let g be a networkx.DiGraph, as I was not able to draw curvy lines via the connectionstyle argument if g was instead a networkx.Graph. The fact that the red nodes (which are the extra nodes obtained by star expansion) have always the same order in each subplot makes this plot very easy to read. It is still not perfect, but arguably much more readable than our earlier attempts. For instance, it is obvious from the rightmost subplot that there is only one edge of order 4, and it is (a,b,c,d), and from the subplot in the middle it is easy to see that the only edges of order 3 are(a,b,e),(a,b,d), and (a,c,d). We can also generate some random hypergraphs by starting with a set of nodes and adding edges at random between them, until we reach a specified number of them, i.e., A sample of them ( order=5 and size=10) is shown below; empirically, I found them to be readable enough, even when a planar layout was not achievable. The procedure illustrated above can be quite helpful in plotting hypergraphs via networkx. Especially in contexts such as quantum physics, statistical mechanics, and so on, where the order of a hyperedge matters, this could be a helpful visualization. A drawback of the proposed scheme could be that in each of the subplots the position of the nodes can vary in general; this could be trivially solved by arbitrarily fixing the pos argument in the code for plot_hypergraph_components, but maybe problem-specific properties can help in making a meaningful choice for it. If you made it this far, thanks for reading :-)",,0,6,Towards Data Science,2021-01-03,2021
https://towardsdatascience.com/practical-guide-for-virtual-environments-in-python-b59bd5fe8f1,Practical Guide for Virtual Environments in¬†Python,Using virtualenv and pipenv¬†tools,1,30,['Practical Guide for Virtual Environments in Python'],"The projects we work on are very much likely to have many dependencies that need to be installed. These dependencies facilitate many tasks in projects. However, we need to be careful about them especially when working on multiple projects. Just like any other technology, the software packages or programming languages are constantly being improved. Thus, new versions are being introduced. Different projects might require different versions of a package or software. For instance, we might have one project that requires Python 2.7 and another one with Python 3.6. As the number of projects and dependencies increase, it becomes hard to follow up and handle such differences. One way to overcome this issue is to use virtual environments. They can be considered as bounding boxes for software packages. We can develop a project in a virtual environment and install all the dependencies specific to that project. What we have in the virtual environment is not affected by the changes in the global scope of our machine. There are many virtual environment tools for Python such as pipenv, virtualenv, venv, and so on. In this article, we will go over some examples using virtualenv and pipenv to get familiar with the idea of virtual environments and how they work. Let‚Äôs start with the virtualenv. We first install it from the terminal using python package installer (pip). We create a sample project file as our working directory. We are now inside the demoproject directory. We will create a virtual environment using the following command. It‚Äôs been created. We can run the ls command to see the files in the current working directory. The next step is to activate the virtual environment. Once the virtual environment is activated, its name is displayed in the terminal as below: We can now install packages. We now have pandas installed in our virtual environment. The freeze command shows the list of installed packages. NumPy has also been installed because it is a dependency for Pandas. The installed version of Pandas is 1.1.5. We can specify the version we need while installing a package. If you just want to check the installed version of a particular package, the freeze command is used with grep: We can also install several packages saved in a text file. It is better than installing dependencies one-by-one especially when there are several of them. In order to exit the virtual environment, we use the deactivate command. The next tool we will discover is pipenv which can be installed using the pip: Let‚Äôs create a new virtual environment using pipenv. Pipenv allows for installing a dependency while creating the virtual environment. For instance, I could have added pandas at the end of the command above so that the virtual environment is created with pandas installed. We run the shell command to activate the virtual environment. We are now in the virtual environment. Let‚Äôs also install pandas to this one. The graph command shows a detailed overview of the installed packages. We can uninstall a specific package or all the packages in the virtual environment using the uninstall command. The following command uninstalls all the packages. We type the ‚Äúexit‚Äù command to exit the virtual environment. Virtual environments are great tools to manage multiple projects at the same time. There are numerous packages and libraries which are updated in no time. Thus, it is exhaustive and not efficient to try to manually follow up. What we have covered in this article can be considered as a practical introduction to Python virtual environments. There is, of course, much more to learn both theoretically and practically. The official documentation of virtualenv and pipenv provide more detailed overview about these tools. Thank you for reading. Please let me know if you have any feedback.",103,2,4,Towards Data Science,2021-01-03,2021
https://towardsdatascience.com/the-magic-of-synthetic-data-using-artificial-intelligence-to-train-artificial-intelligence-with-cebe0d50a411,The Magic of Synthetic Data,Using Artificial Intelligence to Train Artificial Intelligence with¬†GPT-2,15,24,"['The Magic of Synthetic Data', 'Synthetic Data Background', 'The Business Value of Synthetic Data', 'GPT-2', 'Yelp Open Dataset', 'Generating Synthetic Review Data', 'Finetuning GPT-2', 'The Art of Generating GPT-2 Prompts', 'Combining Synthetic Data with Genuine Data', 'Performance Testing', 'Performance Testing Results', 'Lessons Learned', 'Concluding Remarks', 'Resources:', 'Sources:']","Recently while working on a NLP project I ran into a problem. I didn‚Äôt have enough data. Classification Models use input data to predict the likelihood that the subsequent input data will fall into predetermined categories. To perform effective classifications, these models require large datasets for training. So I created a method of using Artificial Intelligence to generate relevant synthetic data that would improve performance of my Classification Models. This method resulted in my Baseline Model‚Äôs Accuracy being increased by 9.49% and Precision being increased by 7.63%. It is becoming common practice to utilize synthetic data to boost the performance of Machine Learning Models. It is reported that Shell is using synthetic data to build models to detect problems that rarely occur; for example Shell created synthetic data to help models to identify deteriorating oil lines.(Higginbotham, 2020) It is common practice for Machine Learning Practitioners to generate synthetic data by rotating, flipping, and cropping images to increase the volume of image data to train Convolutional Neural Networks. In 2018, U.S. companies spent nearly $19.2 Billion on data acquisition and solutions to manage, process, and analyze this data.(Sweeney, 2019) The technique discussed in this paper could reduce the cost of data acquisition by reducing the amount of data needed to train high performance NLP classification models. This technique could also be used to improve current models by expanding training datasets. These benefits could lead to companies and organizations achieving their goals more effectively while minimizing costs. I decided to use OpenAI‚Äôs GPT-2 Model to generate my synthetic data. The GPT-2 Model is a large-scale transformer-based language model that is pre-trained on a large corpus of text: 8 million high-quality webpages. The objective of GPT-2 is to predict the next word given all of the previous words within some text.(Radford, 2020) I decided to use the Yelp Open Dataset to acquire my data. The Yelp Open Dataset contains anonymized reviews on various businesses and services. I created two subsets of data of pizza restaurant reviews. Within this subset of data I divided the ratings into a‚ÄúPositive‚Äù subset of data and a ‚ÄúNegative‚Äù subset of data. Ratings that were 4 or 5 stars were categorized as ‚ÄúPositive‚Äù. Ratings that were 1 or 2 stars were categorized as ‚ÄúNegative‚Äù. My Negative dataset contained 225 observations and the Positive dataset also contained 225 observations. My first task was to create two GPT-2 models and train one of them on genuine negative Yelp pizza review data and the other model on genuine positive Yelp pizza review data. I then had these models generate synthetic negative and positive review data that was combined into one a single dataset. I chose to generate synthetic reviews with the 355 million parameter GPT-2 model. I used the GPT-2 Generate Method to customize the types of synthetic data that will be generated. I tweaked the following parameters with the GPT-2 Generate Method to create my responses: length= This set the length of the synthetic review. temperature= This sets how creative the synthetic review would be. The higher the temperature the more creative the synthetic review. prefix= The prefix is the prompt that GPT-2 model will use to generate the review nsamples= sets the number of reviews that will be generated on a particular run. When generating synthetic reviews I wanted to ensure that the responses expanded on the genuine data and produced responses that were a strong representation of the genuine data. So when I wrote the prefix prompts I used words that were heavily represented in the genuine datasets. I wrote a Python Function that organized the genuine dataset corpus into trigrams (3 word consecutive combinations), bigrams (2 word combinations) and words. This function also provides a count and numerically sorts the occurrences of these words and combinations. After I created the synthetic Positive and Negative datasets I used Python Pandas to concatenate genuine Negative and Positive Datasets. To ensure there was a fair and equal analysis of the performance metrics, I used the scikit-learn train_test_split method to establish a single ground truth test set consisting of 198 observations derived from a totally separate dataset from the Yelp Open Dataset. I then built two baseline models on two datasets using the Multinomial Naive Bayes Classifier Algorithm. The two datasets were: The genuine Yelp Pizza Reviews Dataset (450 observations) and the combined Genuine and Synthetic Yelp Reviews Dataset(11,380 observations). I chose Precision, Accuracy, Recall, and F1 as performance metrics for the three baseline models. Overall, the Combined (Synthetic and Genuine) Model outperformed the Genuine Model on all performance metrics. For example, the Precision in the Combined (Synthetic and Genuine) Model went up to .8913 from the Precision score of.8281 in the Genuine Model. I also performed a Confusion Matrix Analysis. The Genuine Model had more True Positives but less True Negatives when compared to the Synthetic and Genuine Model. The Synthetic and Genuine Model had more False Positives but less False Negatives when compared to the Genuine Model. As you can see from my testing, I added almost 11,000 synthetic data observations to achieve my best performing model. When I went above 11,000 synthetic observations the performance actually went down. So be sure that you find the sweet spot ratio for synthetic to genuine data to ensure you have the best performing model. When I was initially created my test dataset with sklearn train_test_split, I used the Genuine Dataset. My combined (Synthetic and Genuine) models were achieving scores of .97 and above. I then discovered that the models had inadvertently been trained on the test data through the GPT-2 model. This is because the GPT-2 model had access to the entire dataset before the train_test_split method was applied. To fix this issue, I went back to the Yelp Pizza Review dataset and build a dataset on observations that were not involved at all with my training dataset. The scores adjusted accordingly and this problem was resolved. The GPT-2 generate method allows you to tweak the generated synthetic reviews. In some of my previous model building efforts, I noticed that the synthetic reviews would mirror the genuine data too closely if I had the temperature parameter set to less than .3. But it could sometime be too creative if I had it at .9 or above, which would require more monitoring. I also found that 150 words was the best length for my synthetic reviews. In conclusion, the combined (Synthetic and Genuine) model outperformed the Genuine Model in all performance metrics. This technique has the possibility of allowing organizations and businesses to build high performing NLP classification models without the high cost associated with large scale data acquisition. There are opportunities in exploring this technique on datasets with a larger observation count. There are also opportunities in exploring GPT-2 prompt design to better guide the GPT-2 model in generating relevant text. This is an exciting Machine Learning Technique that I feel deserves further exploration. Yelp Open Dataset GPT-2 Prompt Aid Tool Github",16,0,7,Towards Data Science,2021-01-03,2021
https://towardsdatascience.com/7-tips-for-a-successful-data-science-project-in-the-industry-1aa4166708,7 Tips for a Successful Data Science Project in the¬†Industry,,10,34,"['7 Tips for a Successful Data Science Project in the Industry', 'Introduction', '1. Define the problem and the goal', '2. Fully understand the data', '3. What will be the situation in production?', '4. Keep it simple', '5. Consult others', '6. Error Analysis', '7. Documentation', 'Conclusion']","I decided to write up this article since it includes topics that aren‚Äôt usually covered in courses, or any other Data Science-related material. Most of the material out there includes tools you should learn, explanations for algorithms you should know, the life-cycle of a Data Science project, and such. This article is different. It includes tips that I gathered based on my experience, that are rather generic. It‚Äôs relevant regardless of the domain you are working in. Perhaps it‚Äôs more related to those working in a start-up environment, but not only. And also, frankly, I see this article as a reference for myself to go back to from time-to-time, to make sure I‚Äôm on track :) What is considered a successful project in the industry? Simply put, if your project brings value to your company, it‚Äôs successful! But aside from the accuracy (or any other metric used) of your solution, a key factor is the speed at which the solution was tailored. DS projects often take time, but ideally, we‚Äôd prefer them to be as short as possible. There are situations that midway into a project a mistake is discovered, forcing us to start (almost) over again. This is a situation we‚Äôd like to avoid. The intent of these tips is both to create solutions that are better and to integrate them in minimal time possible. So how can we maximize the value we bring to our company? Manager: ‚ÄúI need you to build a model that will identify bad data flowing in the system.‚Äù Me: ‚ÄúHmm, sure, no problem. Just let me know in which DB the data is located, and I‚Äôll get right to it.‚Äù But what exactly is ‚Äúbad data‚Äù? Why do we want to filter it out? What is the business value in this project? It‚Äôs important to define what the problem is exactly, and what is our goal. What is the cost of a False-Positive in this case? Is it equivalent to a False-Negative? Ask all questions needed in order to reach a clear target, and create a clear metric for your project, preferably one which is a single value. It‚Äôs a shame if you perform a whole cycle of building a model, only to find out that you didn‚Äôt fully comprehend what the problem actually was. You might not exactly be back to square one, but that was a lot of effort spent in the wrong direction. There are some cases where you‚Äôre dependant on other personnel in your company to fully understand the project. Don‚Äôt procrastinate, move forward toward a clear definition of what is required in your project. Since we‚Äôre discussing the beginning of the project, I‚Äôll add another note related to this stage of a project. Your manager doesn‚Äôt necessarily understand the difficulty of the problem at hand. It‚Äôs important to reflect an initial outcome of the project (what accuracy you will reach, etc.). You'll probably be off a bit, but when you come back with results, you prefer not to get an ‚Äúoh‚Ä¶‚Äù reaction. Be aligned from the beginning. Manager: ‚ÄúThere are missing values in the label column. Those indicate that the annotater didn‚Äôt find anything in the image.‚Äù That sounds logical. But turns out that missing values also indicate cases that the annotator simply couldn‚Äôt decide which of the 6 possible labels is the most accurate one, so he left it empty. These two scenarios are completely different, yet if both types of missing values are treated the same, this will harm your trained model. It‚Äôs crucial to make sure you understand your data well. You might engineer some feature, and later find out that it was engineered wrongly (e.g. you took a ‚Äúzip code‚Äù column and bucketed it to different regions in a certain country, but turns out that not all the zip codes belong to that same specific country). Whether it‚Äôs the label column, or a column you‚Äôll use as a feature, make sure that your understanding of the data is as accurate as you want your model to be. You can even consult the annotators themselves or their managers. Whatever it takes. Manager: ‚ÄúThe relevant data is in table1, table2, table3, and table4. A simple join will get you all the data.‚Äù Cool, you got everything you need. You move forward with your model, get great results, and you‚Äôre ready to deploy it to production. You just need to make final adjustments. Me: ‚ÄúSay, which source provides field A?‚Äù Manager: ‚ÄúField A? That‚Äôs some enrichment we do to the data once it finished the full flow in the system.‚Äù Oops, that‚Äôs not good. It turns out this is even a feature that doesn‚Äôt add too much to the model‚Äôs accuracy. But your model was already trained using it. Make sure you understand what will be the situation when your model will be in production, so your model will have all the data it needs, and it will be suitable for that specific setting in production. Me: ‚ÄúI think I‚Äôll try the super-duper-really-cool-model first. What do you think?‚Äù Manager: ‚ÄúYou‚Äôre the one with the knowledge. Go for it!‚Äù As said, generally your goal is to bring results relatively quickly. Your manager doesn‚Äôt always have a sense of what the possible solutions might be. Usually, simple solutions are more valuable when the time invested is taken into account. And very often they‚Äôll even perform better than the super complex solution you think will be cool here, regardless of the time invested. It‚Äôs, of course, not always the case, but at least first try the more ‚Äúsimpler‚Äù solutions before moving on to the ‚Äúcool‚Äù ones. In addition, very often it‚Äôs possible to solve the problem at least partially with a simple solution. When doing that, you‚Äôre already creating value. Once done, you can move forward to the more complex solutions, if still needed. Me: ‚ÄúYeah, I tried algorithm A. It looks like a good solution here.‚Äù Colleague: ‚ÄúThat‚Äôs interesting, it might work well here. Though I think I would try algorithm B here.‚Äù Me: ‚ÄúHuh‚Ä¶ Didn‚Äôt think of that. Good idea!‚Äù Hopefully you‚Äôre not a single Data Scientist in your company. If so, it‚Äôs a great opportunity to learn from others and hear how they would tackle your problem. Don‚Äôt feel uncomfortable. There‚Äôs no one who can come up with all possible solutions, there‚Äôs no one that knows everything. Besides, very often, as a result of brainstorming together, you‚Äôll come up with an idea yourself that you wouldn‚Äôt think about without this consultation. With regards to this topic, I would also add the importance of planning your project and breaking your work into tasks. The outcome of a group brainstorm should be just that. This will enable you to stay focused and not get caught up too much researching a specific algorithm that is a potential solution. Manager: ‚ÄúI wonder why the model performs well for scenarios A and B, but not for scenario C.‚Äù Me: ‚ÄúI think the other algorithm will work better. And also training on more data will probably do the trick.‚Äù Yeah, this is probably not what you were dreaming about when you landed this job. But it‚Äôs extremely important. Looking into several (or many) mistakes manually can give you a good intuition about what‚Äôs wrong. Don‚Äôt feel as if this ‚Äúisn‚Äôt something I should be doing‚Äù. This will probably be more effective than just throwing more data at your model. Me: ‚ÄúThere was a reason I filtered out those rows from the data. I can‚Äôt remember why I did that‚Ä¶‚Äù This is sometimes a step that‚Äôs neglected, or done only partially. The current project you‚Äôre working on has elements that you‚Äôll forget about next month when you move on to your next project. Document what steps you took throughout the project, what data was selected and why, the queries you ran on your DB (and your code, of course!). The more, the better. This will be extremely useful when you might have to revise the project and refine your solution, or perhaps when you will be working on a different project that is related to this one. You can even benefit from documentation while still working on the project (‚Äúwhy did I decide to take route A instead of route B again..?‚Äù). If I‚Äôd have to sum up in a few words of advice what one would need to do in order to deploy a successful project in the industry, I would say it‚Äôs important to be pragmatic and communicate well with your colleagues and managers. Don‚Äôt be passive, be initiative, be sure that you have all the info you need. And keep your focus on your goal. I hope you found this insightful :) Best of luck!",,0,6,Towards Data Science,2021-01-03,2021
https://towardsdatascience.com/a-story-of-frequentist-statistical-inference-12d38a7bcd77,A Story of Frequentist Statistical Inference,"The concept of statistical inference, hypothesis testing‚Ä¶",6,74,"['A Story of Frequentist Statistical Inference', 'A Statistical Inference Story', 'Hypothesis Testing', 'Significance Level', 'The p-Value', 'Type I Error and Type II Error']","Learning statistics should be fun and intuitive, at least that‚Äôs what I think. However, when we try to learn statistics either in the graduate school or online, all of the technical jargon related to statistics are being thrown at us all of a sudden. Take a look at the definition of statistical hypothesis by Wikipedia below: A statistical hypothesis is a hypothesis that is testable on the basis of observed data modeled as the realised values taken by a collection of random variables. ‚Äî Wikipedia What? For the beginners who have just started learning statistics, the definition of statistical hypothesis above is hardly going to help. So in this post, we are going to cover the fundamentals of frequentist statistical inference in a rather fun way. Hopefully, this post will help you to build intuition about inferential statistics. Without further ado, let me introduce you to a story between a below-average student named Jimmy and his professor. On a summer afternoon, just before the holiday season starts, Jimmy gets an unusual call from his professor, asking him to come by to his office. ‚Äò I want to discuss something with you, please come by to my office now‚Äô, his professor said to him. In his professor‚Äôs office, Jimmy just sits there behind the desk, wondering what kind of discussion that his professor has in mind. Basically, both parties are shocked due to different reasons. The professor is shocked to find out that Jimmy, a below-average student, got the highest score in the final exam. On the other hand, Jimmy is shocked due to the explicit accusation of cheating from his professor, although he did study hard for the final exam. The scene between Jimmy and his professor above is the thing that you need to know about the good and the ugly side of statistical inference. Statistical inference always comes with uncertainty. However, it is a powerful tool to make sense of everything. It tells us what is probable and what is improbable. With statistical inference, you have the power to observe some patterns and then using probability to determine the most likely explanation of those patterns. If Jimmy was an A+ student, his professor wouldn‚Äôt be surprised that he got the highest score in the exam. However, since Jimmy was a below-average student, the fact that he got the highest score in the exam this semester is totally unexpected. The cheating accusation from the professor is understandable, although he can‚Äôt really prove that Jimmy was cheating. From the professor‚Äôs perspective, it is possible that Jimmy got the highest score because he studied properly, but it‚Äôs highly unlikely. It‚Äôs an anomaly. So, how come the professor knows what is likely and what is unlikely? From the historical data. The records of Jimmy‚Äôs not-so-good exam scores in the past several semesters. Statistical inference relies heavily on data to make sense of everything. That‚Äôs the beauty of statistical inference. It combines the data and probability to enable us to draw meaningful insights of a phenomenon. Hypothesis testing is just the nerdy term to describe the scene between Jimmy and his professor above. You can think of a hypothesis as an assumption or a belief. So when you try to make sense of your assumption or belief with sample data, then you‚Äôre basically doing hypothesis testing. Hypothesis testing is the way of trying to make sense of assumptions by looking at the sample data. In the world of hypothesis testing, there are two things that you should know: the null hypothesis and the alternative hypothesis. Null hypothesis assumes everything is alright, nothing exceptional happened, Jimmy is innocent. Meanwhile, alternative hypothesis assumes something fishy or exceptional is going on. Jimmy got a good grade because he was cheating. Based on the story above, the professor could frame his null and alternative hypothesis to be something like this: Next, our main job is to reject one of these two conflicting hypotheses. If we reject the null hypothesis, it means that we accept the alternative hypothesis and vice versa. But, how do we accept one and reject another? Again, by looking at our data. Remember, statistical inference is a combination of data and probability. Let‚Äôs say that the professor has the historical data of 1000 students‚Äô exam scores in each subject from the past several semesters. In each subject, Jimmy‚Äôs exam score was consistently residing in the bottom 3. However, in this semester‚Äôs exam, Jimmy got the highest score out of those 1000 students. The professor then thinks: ‚ÄòBased on the evidence that I‚Äôve seen in the data, how ridiculous does it sound that Jimmy got the highest score out of 1000 students due to a random chance? Nah, it‚Äôs very ridiculous. It can‚Äôt be due to a random chance. Something fishy was happening‚Äô. Hence he decides to reject the null hypothesis in favor of alternative hypothesis. Jimmy must have been cheating! The professor‚Äôs decision to reject the null hypothesis, i.e that Jimmy got the highest grade due to a random chance, is understandable. How come a student who is consistently residing in the bottom 3 out of 1000 students in terms of exam scores suddenly gets the highest grade? That‚Äôs possible, but highly unlikely. However, now we need to expand the concept a little bit more by addressing the fundamental question: how ridiculous or improbable does the null hypothesis have to be for us to reject it in favor of alternative hypothesis? The answer is: It depends on you and your specific field of research. Everyone has their own subjective judgment to judge whether the null hypothesis is ridiculous enough. However, as a rule of thumb, people have a common threshold value to reject the null hypothesis: 5% or 0.05 in decimal form. This 0.05 is the significance level, which represents the upper bound for the likelihood of observing an outcome in our data given that our null hypothesis were true. Significance level represents our tolerance level of an outcome before we decide to reject the null hypothesis. Let‚Äôs say that the professor has a large historical data of Jimmy‚Äôs exam scores from the past several semesters. Based on the data, Jimmy‚Äôs exam scores can be represented as a distribution as follows. From the curve, we can see that overall, Jimmy has a mean grade of 40 and standard deviation of 10. The professor is then able to form his null hypothesis to be something like: ‚ÄúJimmy is just a bang below-average student. I mean, look at his grades. The mean of his grades is 40! I wouldn‚Äôt be surprised if in this semester‚Äôs exams he‚Äôll get an average score around 40‚Äù. Now this semester, the professor would like to conduct 10 exams. Based on the distribution above and the fact that he would like to conduct 10 exams, the professor is able to make an assumption of what Jimmy‚Äôs average exam score this semester is going to be. First, he would calculate the standard error, which is the standard deviation (10) divided by the square root of number of samples (he would conduct 10 exams this semester). Based on the standard error of 3.16, the professor then can construct a distribution curve as follows: Based on a distribution like above, the professor is able to form an assumption based on the so-called Empirical Rule: ‚ÄúIf I would conduct several experiments and each experiment consists of 10 exams, I‚Äôd expect that‚Ä¶ For the significance level, let‚Äôs say that the professor follows the rule of thumb, which means that he has a significance level of 5% before he convinces himself to reject his null hypothesis. This means that if he would conduct several experiments and each experiment consists 10 exams, he‚Äôd expect that in 95% of the experiments, Jimmy‚Äôs average exam score would lie roughly within two standard errors from the mean, or around 33.81 to 46.19. Conversely, only in 5% of the experiments Jimmy‚Äôs average exam scores would be below than 33.81 or higher than 46.19. Now what does it mean if the professor has a 1% or 0.01 significance level? This means that if he would conduct several experiments and each experiment consists of 10 exams, he‚Äôd expect that in 99% of the experiments Jimmy‚Äôs average exam score would lie within roughly 2.5 standard errors from the mean, or around 31.88 to 48.12. Conversely, only in 1% of the experiments Jimmy‚Äôs average exam score would be below than 31.88 or higher than 48.12. The lower the significance level, the more cautious you are to declare that something exceptional has happened. It means that you need very strong evidence or anomaly before you decide to reject the null hypothesis. Now let‚Äôs get back to Jimmy‚Äôs professor and his 5% significance level. After correcting all of the exam sheets, the professor finds out that Jimmy got an average score of 50 for his 10 exams this semester. Based on his significance level, the professor has the right to reject the null hypothesis, i.e the fact that Jimmy got 50 due to a random chance, because: The magnitude of how surprised the professor is when he found out that Jimmy got a good grade in his exam can be quantified in a single, scalar value. And this value is called the p-value. What p-value does is that it provides you with a summary about how unlikely is the outcome that you‚Äôve just seen given that our null hypothesis were true. The lower the p-value, the more surprising the evidence that we‚Äôve just seen given that our null hypothesis were true. In statistics, p-value stands for probability value and you can compute this value from the distribution of your sample data. Comparing the p-value with your significance level will give you the power to assess whether you should reject your null hypothesis or not. If Jimmy got a 60 on his average exam score this semester, the p-value is so tiny in the eyes of his professor (probably around 0.00001) that he can‚Äôt help but accusing Jimmy of cheating, i.e he is rejecting the null hypothesis in favor of alternative hypothesis. Meanwhile if Jimmy got 50 on average instead of 60, the p-value will still be small but not as small as before. However, the p-value is small enough such that it‚Äôs still below the professor‚Äôs significance level (0.05), which means that the professor will most likely still reject the null hypothesis. Now if Jimmy got an average of 42, which is within the range of two standard errors from the mean, the p-value is large in the eyes of his professor (above 0.05). This means that nothing surprising has happened and everything is normal. Jimmy is just being Jimmy, a consistent below-average student. He‚Äôs getting 42 due to a random chance. When you have a p-value in the palm of your hand and then comparing it with your significance level, you‚Äôll have the power whether to reject the null hypothesis or not. However, you need to keep in mind what we‚Äôve discussed in the very first section: statistical inference always comes with uncertainty. It‚Äôs not a magic tool. Not even the best mathematician in the world will save you from not having a chance to make a mistake when doing statistical inference. The point of statistical inference is to make sense of your problem with the help of data and probability. Based on the evidence that we‚Äôve seen from the data and by looking at the p-value and our significance level, we have two choices: The thing is, the decision to reject or accept the null hypothesis is completely subjective. As mentioned earlier, everybody can fine tune their own significance level. This means that we are prone of making error. In statistics, there are two types of error: Type I error or Type II error. Type I error is falsely rejecting the null hypothesis. Type II error is failing to reject the null hypothesis. Type I error means that you‚Äôre falsely rejecting the null hypothesis. You‚Äôre saying that something exceptional has happened when in reality, nothing has happened and it‚Äôs all just a fugazi, fairy dust (just imagine the scene with Matthew McConaughey in the Wolf of Wall Street). Now let‚Äôs get back to Jimmy and his professor. Let‚Äôs say that Jimmy got an average of 47 in this semester‚Äôs exams because he studied really hard. Harder than he ever did. Jimmy got his scores fair and square. However, his professor doesn‚Äôt believe it as he expected that 95% of the time Jimmy‚Äôs average exam score would lie in between 33.81 to 46.19. Because of his 5% significance level, the p-value is small enough in his eyes. Hence, the professor is accusing Jimmy of cheating when he actually wasn‚Äôt. His professor made a Type I error. He is wrongly accusing an innocent student. Meanwhile, Type II error means that you fail to reject the null hypothesis. It means that you‚Äôre saying nothing has happened when in reality, something exceptional has happened. Same as before, let‚Äôs say that Jimmy got an average of 47 in this semester‚Äôs exams. Spoiler alert: he was cheating the entire exams, hence why he got an average of 47. However, this time his professor has a very low significance level (0.01). It means that the professor will only have the confidence to accuse Jimmy of cheating if Jimmy‚Äôs average exam score is roughly above 48. When the professor found out that Jimmy got an average of 47, his significance level is smaller than the p-value such that he failed to reject the null hypothesis. He concluded that Jimmy is innocent and Jimmy got a 47 because he studied hard when he actually didn‚Äôt. His professor made a Type II error. He is wrongly letting a guilty student free without punishment. So now you know that there are two types of errors. But which one is worse? Again, the answer is depending on the field of research that you‚Äôre working on. If you‚Äôre working as a doctor who constantly tries to diagnose whether a patient has cancer or not, you might want to increase your significance level to suppress the chance of you making a Type II error. You don‚Äôt want to tell a patient that they‚Äôre alright and healthy when in fact, they have colon cancer. On the other side, if you‚Äôre building an email spam classifier, you might want to build an algorithm with a lower significance level to suppress the chance of your classifier making a Type I error. You don‚Äôt want an important, non-spam email to be classified as spam by your classifier. You‚Äôll get a complaint from your colleague and customer if that happens. And that‚Äôs it for the introduction of statistical inference with a frequentist perspective. Hopefully, this article helps you in your journey of learning statistics.",75,0,12,Towards Data Science,2021-01-03,2021
https://towardsdatascience.com/how-to-run-better-and-more-intuitive-a-b-tests-using-bayesian-statistics-480acf4b8679,How to run better and more intuitive A/B tests using Bayesian statistics,A guide to Bayesian A/B testing with examples and¬†code,1,34,['How to run better and more intuitive A/B tests using Bayesian statistics'],"A/B testing is one of the most useful statistical techniques used in technology, marketing and research today. Its value is that A/B testing allows you to determine causation, while most analysis only uncovers correlation (i.e. the old adage ‚Äúcorrelation not causation‚Äù). Despite the power and prevalence of A/B testing, the vast majority follow a single methodology based on t-tests that draws from the frequentist school of statistics. This note will walk through an alternative approach using the Bayesian school of statistics. This approach returns more intuitive results than the traditional, frequentist approach as well as some useful, additional insights. The traditional, frequentist approach uses hypotheses as the framework for an A/B test. The null hypothesis is often the status quo, e.g. mean of A is equal to mean of B, and the alternative hypothesis tests whether there is a difference, e.g. mean of A is larger than mean of B. A confidence level, e.g. 5%, is selected and the experiment can have one of two conclusions: This sort of language isn‚Äôt how we tend to speak in business and can be difficult to understand for people less familiar with A/B testing. In particular, the second conclusion doesn‚Äôt provide much insight; having spent time and money to run a test you are only able to conclude that no conclusion is possible. (For more information about this approach check-out my previous post about implementing A/B tests in Python). The Bayesian approach instead focuses on probabilities. If testing the same example above, the null hypothesis being mean of A is equal to mean of B, the Bayesian method calculates the estimated difference in means as well as the probability one is larger than the other ‚Äî rather than just whether the difference in means is 0. In my opinion, the Bayesian approach is superior to the frequentist approach because it can effectively accept and reject the null hypothesis with a specific probability. This approach makes for more useful recommendations. Two example conclusions (analogous to the frequentist conclusions) are: This sort of language gives a sense of how probable the conclusions are so that decision makers are empowered to choose their own risk tolerance and it avoids situations where the null hypothesis cannot be rejected and no conclusion drawn. Even more useful is that it calculates the estimated difference between means. Together this means a possible conclusion from a Bayesian test is ‚ÄúMean A is estimated to be 0.8 units larger than Mean B and there is an 83% probability Mean A is larger than Mean B‚Äù. As a bonus, the Bayesian approach also enables comparisons between variances of A and B and inherently manages for outliers. The drawback to the Bayesian approach is that the mathematics underpinning it can be more challenging. A good understanding of Bayesian statistics and Markov chain Monte Carlo sampling is helpful but not completely critical. The following sections walk through an example of how to use the Bayesian approach for A/B testing and code examples in R. To demonstrate the Bayesian approach I‚Äôll use data from a set of surveys I performed in early 2020. The surveys comprised 13 questions around 3 themes regarding respondents‚Äô opinions on the measures implemented to combat Coronavirus (4 questions), respondents‚Äô approval of the government response to Coronavirus (3 questions) and general household activity questions (5 questions). A full list of questions is included here. For this example we‚Äôll focus on questions that had a numerical answer such as ‚ÄúHow many hours a day are you spending with your family members or roommates?‚Äù The surveying was designed to comprise 6 similar but distinct survey versions. The purpose of running these slightly different surveys was to A/B test whether the differences between them caused statistically different results. The difference between each survey was either the ordering of questions or the way questions were phrased in either a positive or negative way. An example of a positively worded vs negatively is: The table below shows a summary of the different survey versions. 291 survey responses were recorded in total comprising 45‚Äì47 responses for each of the survey versions. This means the results of Survey 1 can be compared with Survey 3 and Survey 5 for differences in ordering and with Survey 2 for differences in wording. The following analysis is largely based on the 2012 research paper ‚ÄúBayesian Estimation Supersedes the t Test‚Äù by Kruschke and the R package BEST. Code is available in my Github. This Bayesian technique, as with any Bayesian estimation, draws on a set of prior beliefs which are updated with evidence from data to return a set of posterior distributions. The following analysis uses a t distribution and Markov Chain Monte Carlo algorithm per Kruschke - 2012 and a noncommittal prior that has limited impact on the posterior distribution. The noncommittal prior has a minimal impact on the posterior distributions which is useful for this study as there was no baseline or set of prior beliefs this study could easily compare against. This methodology is also effective for managing outliers and required adjusting for only one data point that was an error. If the previous paragraph is a bit complex, don‚Äôt worry. You can still go through the steps below and get an easy to interpret output. To learn more, read the paper by Kruschke. Step 1: Load packages and data First step is to install the required packages. We will be using the BEST package that uses the JAGS package. Download JAGS first prior to running BEST. Next install BEST. Once this is all completed, load the packages. Also load in the data and set it up for analysis. We‚Äôre using survey_data_v2.csv which can be found here. Step 2: Create the function for Bayesian analysis Next we want to create a function that will allow us to choose which survey versions are to be compared and which survey question the test will compare. The function runs a Markov chain Monte Carlo sampling method that constructs a posterior distribution of our test, i.e. the probability one mean is larger than the other and the estimated difference in means. Step 3: Run test Finally, select the two sets of data to compare. In this example we will use survey version 1 and 2, and compare question 2. Change the function variables to test different surveys and questions. Step 4: Interpret output After running the above code a pop-up will show the following output. It mainly shows histograms of 100,000 credible parameter-value combinations that are representative of the posterior distribution. The most important output for A/B testing is the mid-right distribution that shows the difference of means. For our example, it shows that on average Mean A is 0.214 units larger than Mean B and that Mean A has an 82.9% probability of being larger than Mean B. This result is the main conclusion for the A/B test. Note, a traditional t-test would have simply returned the result that we can‚Äôt reject the null hypothesis at the 95% level of confidence. The other output shows other useful information for interpreting the data. The two top-right graphs with y as the axis show the actual distribution of test data. The other figures show the posterior distributions. The five histograms on left hand side figures show the individual posteriors corresponding to the five prior histograms. The bottom-right graphs show comparisons between the groups A and B. The Bayesian approach to A/B testing has three key benefits over the traditional, frequentist approach: These benefits combine for more useful and intuitive recommendations that empower decision makings to better understand test results and select their own level of risk. https://github.com/bondicrypto/bayesian_abtesting [1] Kruschke, John K. ‚ÄúBayesian Estimation Supersedes the t Test.‚Äù Journal of Experimental Psychology . Vol. 142, no. 3, 2012, pg. 573‚Äì603, accessed 03 January 2021, <https://cran.r-project.org/web/packages/BEST/vignettes/BEST.pdf> [2] Gallo, Amy 2017. A Refresher on A/B Testing , Harvard Business Review, accessed 03 January 2021, <https://hbr.org/2017/06/a-refresher-on-ab-testing> [3] Hussain, Noor Zainab and Sangameswaran, S. 2018, Global advertising expenditure to grow 4.5 percent in 2018: Zenith , Reuters, accessed 03 January 2021, <https://www.reuters.com/article/us-advertising-forecast/global-advertising-expenditure-to-grow-4‚Äì5-percent-in-2018-zenith-idUSKCN1M30XT> [4] Lavorini, Vincenzo, Bayesian A/B Testing with Python: the easy guide, Towards Data Science, accessed 03 January 2021,<https://towardsdatascience.com/bayesian-a-b-testing-with-python-the-easy-guide-d638f89e0b8a> [5] Mazareanu, E. 2019, Market research in U.S. ‚Äî Statistics & Facts , Statista, accessed 03 January 2021, <https://www.statista.com/topics/4974/market-research-in-us/>. [6] NSS 2016. Bayesian Statistics explained to Beginners in Simple English , Analytics Vidhya, accessed 03 January 2021, <https://www.analyticsvidhya.com/blog/2016/06/bayesian-statistics-beginners-simple-english/>",15,0,7,Towards Data Science,2021-01-03,2021
https://towardsdatascience.com/mistborn-the-final-eyebrow-54466c815285,Mistborn: The Final¬†Eyebrow,An analysis of Allomancer social‚Ä¶,7,33,"['Mistborn: The Final Eyebrow', 'Eyebrow data collection', 'For whom the brows raise', 'Temporal arcs of eyebrow snark', 'One Brow to raise them all, and in the Deepness bind them', 'Low brows and no brows', 'Eyebrows rise, Empires fall']","TLDR: Enjoy some interactive visualizations summarizing eyebrow interaction data in Mistborn. If you‚Äôve found yourself raising your eyebrows at the Mistborn series by Brandon Sanderson, you are not alone. Yes, Mistborn is a fun fantasy read: the first era features a lovable bunch of thieves who navigate social disparity, political intrigue, and revolution amidst the steely rule of an oppressive Empire. And while Mistborn‚Äôs magic system of Allomancy ‚Äî the ability to wield cleverly versatile powers from ingesting small shavings of various metals ‚Äî is perhaps the coolest magic system I‚Äôve ever read, there‚Äôs a certain feature of the story that really rises above the rest: eyebrows. Yes, eyebrows. The number of times the characters of this series ‚Äúraise an eyebrow‚Äù at each other is astounding. Perhaps it was the intonation of the audiobook narrator that made the phrase jump out so distinctly, but it became such a joke with my partner as we were listening that whenever a character ‚Äúraised an eyebrow,‚Äù we‚Äôd immediately turn to each other with our wildest eyebrow-contorting expression. But then‚Ä¶ I got curious. How many times did Kelsier ‚Äúraise an eyebrow‚Äù? Was there a pattern to it? Did he raise his eyebrows equally at all his crewmates or did he find certain crewmates particularly perplexing? As an aspiring data scientist‚Ä¶ I decided to dig in a bit further. Below is a pilot analysis of the social dynamics present in the first book of the Mistborn series, The Final Empire, as conveyed through the raising of eyebrows. Books 2 and 3 will be covered in a future post. This article contains spoilers for The Final Empire, so if you haven‚Äôt read it yet and want to, stop here and save this for later. But if you‚Äôre caught up and intrigued, read on for data and interpretation covering three main analyses: Just to provide a brief summary for how the data were collected: In total, we recorded 53 eyebrow interactions in The Final Empire, 48 in The Well of Ascension, and 42 in The Hero of Ages. My first analysis was to simply count the total number of eyebrow source (‚Äúraiser‚Äù) and target (‚Äúraisee‚Äù) instances for each character (Figure 1). Unsurprisingly, Kelsier is far and away the most frequent eyebrow source, doling out 19 distinct raises. This makes sense: his position as crew leader combined with his roguish charisma naturally lends itself to witty banter and friendly condescension towards his oddball crew. Additionally, if ‚Äúraising an eyebrow‚Äù is one of Sanderson‚Äôs author quirks, Kelsier‚Äôs role as one of the two primary point-of-view (POV) characters simply gives him more page time during which to waggle those brows. The next most frequent eyebrow source is Kelsier‚Äôs apprentice and co-primary-POV character, Vin, clocking in with 8 raises. Again, Vin gets a ton of page time so her second place ‚Äúraiser‚Äù status is not too surprising, though sarcastic Soother Breeze is not far behind her with 6. But what really stands out about Vin is her first place eyebrow target status: Vin is the recipient of a whopping 22 eyebrow raises! Quiet and strange, the crew don‚Äôt always know what to make of Vin and often regard her with concern or skepticism. However, this veritable mountain of eyebrow targeting can‚Äôt be purely explained by page time as Kelsier receives only 6 raises and says plenty of ridiculous things. So far, these results aren‚Äôt all that exciting: the two POV characters are involved the bulk of the eyebrow interactions, with the main support cast ‚Äî Breeze, Ham, Dockson, Sazed, and Elend ‚Äî racking up a handful each. Several minor characters fire off a brow or two in passing. Next, I investigated the temporal dynamics of eyebrow interactions to see if characters‚Äô brow behaviors changed throughout the story. To get a better sense of the distribution of eyebrow raises across the series, for each character, I plotted each of their eyebrow interactions ‚Äî both as a source or a target ‚Äî over time. The x-axis is the page number (time) and y-axis includes the main characters involved in at least 3 interactions (Figure 2). From this view, we can still clearly see that Kelsier and Vin are involved in the most eyebrow interactions as they have the most dots, but there is an intriguing difference in when these interactions are occurring! Kelsier has a relatively regular pattern of eyebrow raises throughout the beginning of the story, interspersed with a sprinkling of eyebrows targeted at him. But at about ‚Öî of the way through the book, he suddenly stops raising his brows at others‚Ä¶ It‚Äôs not his death ‚Äî that happens towards the very end of the book and in the figure we can see that he is still alive and targetable (orange dots) in the final few hundred pages. I believe this sudden brow ceasefire approximately coincides with a major setback to his plans (the destruction of Yeden‚Äôs hidden forces). Though aware that his plan to overthrow the Lord Ruler is extremely risky and the odds of success are highly improbable, perhaps Kelsier realizes that the time for playful banter has come to an end and he needs to get serious. Sure, he still spiels the crew about the importance of smiling and keeping a positive outlook, but his highly animated eyebrows are no longer part of his leadership style as he adjusts his plan for the final act‚Ä¶ Conversely, Vin spends the entire first half of the book as a target of eyebrow raises. As discussed earlier, her skittish disposition is something that takes a while for the others to get used to and she remains wary as she tries to find her place among this goofy crew. But then! Something shifts: right at the halfway point through the story, Vin suddenly becomes an eyebrow raiser 5 times in quick succession! While she still receives a handful of raises after her sudden cluster as an eyebrow source, this moment seems to have opened her up and she becomes comfortable raising her eyebrows in the future‚Ä¶ So what happened? What triggered this shift? To dig further into this question, I reformatted the figure design to additionally visualize the connection between the source and target of each eyebrow encounter (Figure 3). This figure is a bit wild, and I apologize in advance for the visual complexity! While I enjoy the simplicity of Figure 2, I designed this alternate version to allow for investigation of the specific eyebrow interchanges over time. I think the figure is slightly more effective as an interactive visualization, available here. Just to orient you, it‚Äôs set up the same way as Figure 2: the x-axis is page number (time) and the y-axis has a section for each character to mark their eyebrow exchanges. Now instead of blue and orange, a point is a diamond if the character was the eyebrow source and a circle if the character was the eyebrow target. Vertical lines now directly connect the source and target of each interaction, with the color tied to the source character. Getting back to the analysis of Vin‚Äôs sudden venture into the league of eyebrow raisers, let‚Äôs focus on her pattern of interactions, emphasized in bold below: Sliding along Vin‚Äôs track in the middle of the plot, we still see the open circles marking her as the target of many eyebrow raises ‚Äî they primarily come from Kelsier but Ham, Breeze, Marsh, and Sazed all join in. But Vin‚Äôs eyebrow source cluster mentioned above is entirely directed at Elend! Elend sneaks in a raise at Vin a few pages earlier upon their initial encounter at the ball at Keep Venture, but by Vin‚Äôs second and third balls, she has built up the nerve to raise her eyebrows at a prominent nobleman. This marks a key moment in Vin‚Äôs transformation: we meet her as a street urchin ‚Äî skirting in the shadows, trying to avoid scrutiny ‚Äî and watch her become a real player in the game of courtly intrigue and grow into a self-confident crew member as she comes into her power as a Mistborn. Brow raises are clearly an important marker of this arc: after flexing her eyebrow muscles openly and often at Elend, she then proceeds to fire shots at her closest mentors, Sazed and Kelsier. In fact, Kelsier‚Äôs final eyebrow interaction of the series is as a recipient from Vin. Oh, how the Apprentice indeed becomes the Master‚Ä¶ In my final analysis, I was curious about the overall social network of characters as defined by their eyebrow exchanges. Here I dropped the time component and summarized the data in a directed graph (Figure 4). Again, this network shows that Vin and Kelsier are the hubs of eyebrow activity, but there is a clear distinction between raises from Kelsier to Vin (12) versus from Vin to Kelsier (1). This directionality feels important to capture as it emphasizes the Master of Snark vs Perplexing Apprentice power dynamic. We can also see more crew dynamics: Breeze and Ham create a tightly connected component with Kelsier and Vin. Perhaps we can perceive Breeze‚Äôs heightened exasperation with Ham‚Äôs philosophizing given the thicker flow of brow raises from Breeze‚Äôs node to Ham‚Äôs. Additionally, Vin and Kelsier exchange eyebrows at about equal rates with Sazed while stalwart Dockson only has outgoing eyebrows: towards Kelsier and an unnamed soldier. Ever logical, no one finds cause to raise an eyebrow at Dox. Notably, Vin is the only crew member to exchange eyebrows with the members of the nobility. She is also a bit of a mystery to the nobles and earns both friendly and not-so-friendly raises from Elend and Shan. But the thickest arrow again marks that solid outflow of brow-raises towards Elend, demonstrating her growing confidence. While her noblewoman improvisation is a bit rough around the edges, she eventually embraces her role and evolves from political pawn to political player. (An interactive version of the network is available below. While interactivity is not essential to understanding the network, I‚Äôm learning/practicing D3 and thought it was fun to drag the character bubbles around.) While ‚Äúraising an eyebrow‚Äù is a mannerism shared by many characters in the Mistborn universe, there are a few notable exceptions. Among the crew, Clubs is involved in zero eyebrow exchanges. The grizzled general isn‚Äôt outwardly phased by any of the crew‚Äôs shenanigans, nor does he say anything particularly perplexing. Among the villainy, two Obligators raise several brows, but we get none from the Lord Ruler or any Steel Inquisitors (do they even have eyebrows?). When these baddies arrive on scene, intense action swiftly follows, leaving no extra time for brow-based banter. (üéµWe have seen the Crew do it all!üéµ) By their brows combined, this goofy crew pulled off the impossible. While their metallic mastery takes center stage as they Steel Push, Iron Pull, and Pewter‚Ä¶ Pummel their way to a shattering victory over the Lord Ruler, the brows in the background really knit them all together. Overall, the exchange of eyebrows seems to play an important role in shaping social dynamics between characters, as well as underscore key moments of character transformation as they reckon with their changing worlds. And that‚Äôs The Final Eyebrow of The Final Empire! It‚Äôs always wonderful when series nail character development trajectories across books, and thus examining changes in characters‚Äô eyebrow behaviors between this book and the rest of the series may reveal some interesting patterns. Especially with the kingpin brow-raiser, Kelsier, sidelined for the remainder of the series, who‚Äôs brows will rise to the occasion? Keep an eye out for a follow up analysis delving further into the eyebrow antics in The Well of Ascension and The Hero of Ages, books 2 and 3 of the Mistborn series. Just for fun: A big thanks to Matt Johnson, Claire Johnson, and Kylie Fournier for help with data collection/visualization and early feedback!",,0,10,Towards Data Science,2021-01-03,2021
https://towardsdatascience.com/tensorflow-2-how-to-use-autoencoder-for-interpolation-91fefd4516c9,TensorFlow 2: How to use AutoEncoder for Interpolation,,5,28,"['TensorFlow 2: How to use AutoEncoder for Interpolation', 'Autoencoder', 'Interpolation Methods', 'Autoencoder as Interpolator', 'Concluding Remarks']","Autoencoders are another Neural Network used to reproduce the inputs in a compressed fashion. Autoencoder has a special property in which the number of input neurons is the same as the number of output neurons. Look at the above image. The goal of Autoencoder is to create a representation of the input at the output layer such that both output and input are similar but the actual use of the Autoencoder is for determining a compressed version of the input data with the lowest amount of loss in data. This is very similar to what Principal Component Analysis does, in a black-box manner. Encoder part of Autoencoder compresses the data at the same time ensuring that the important data is not lost but the size of the data is reduced. The downside of using Autoencoder for interpolation is that the compressed data is a black box representation‚Äî we do not know the structure of the data in the compressed version. Suppose we have a dataset with 10 parameters and we train an Autoencoder over this data. The encoder does not omit some of the parameters for better representation but it fuses the parameters to create a compressed version but with fewer parameters (brings the number of parameters down to, say, 5 from 10). Autoencoder has two parts, encoder, and decoder. The encoder compresses the input data and the decoder does the opposite to produce the uncompressed version of the data to produce a reconstructed input as close to the original one as possible. Interpolation is a process of guessing the value of a function between two data points. For example, you are given x = [1, 3, 5, 7, 9], and y = [230.02, 321.01, 305.00, 245.75, 345.62], and based on the given data you want to know the value of y given x = 4. There are plenty of interpolation methods available in the literature ‚Äî some model-based and some are model-free, i.e. data-driven. The most common way of achieving interpolation is through data-fitting. As an example, you use linear regression analysis to fit a linear model to the given data. In linear regression, given the explanatory/predictor variable, X, and the response variable, Y, the data is fitted using the formula Y = Œ≤0 + Œ≤1X where Œ≤0 and Œ≤1 are determined using least square fit. As the name suggests, linear regression is linear, i.e., it fits a straight line even though the relationship between predictor and response variable might be non-linear. However, the most general form of interpolation is polynomial fitting. Given k sample points, it is straightforward to fit a polynomial of degree k -1. Given the data set {xi, yi}, the polynomial fitting is obtained by determining polynomial coefficients ai of function by solving matrix inversion from the following expression: Once we have coefficients ai, we can find the value of function f for any x. There are some specific cases of polynomial fitting where a piecewise cubic polynomial is fitted to data. A few other non-parametric methods include cubic spline, smoothing splines, regression splines, kernel regression, and density estimation. However, the point of this article is not polynomial fitting, but rather interpolation. Polynomial fitting just happens to facilitate interpolation. However, there is an issue with polynomial fitting methods ‚Äî whether it is parametric or non-parametric, they behave the way they are taught. What it means is that if data is clean, the fitting will be clean and smooth, but if data is noisy, the fitting will be noisy. This issue is more prevalent in sensor data, for example, hear-beat data captured from your heart-rate sensor, distance data from LiDAR, CAN Bus speed data from your car, GPS data, etc. medium.com Further, because of the noise, they are harder to deal with, especially if your algorithm requires performing double, or second derivative on such data. In general, those sensor data are timeseries data, i.e. they are collected over time, thus the response variable might be some physical quantity such as speed, the distance of objects from LiDAR mounted on the top of a self-driving car, heart-rate, and predictor variable is time. While operating on such data, there can be a few objectives: I want to have data interpolated to some time-stamp over which my sensor couldn‚Äôt record any response, but since sensors operate in the real-time world and because of the underlying physics, those data stay noisy, I also want a reliable interpolation that is not impacted by sensor noise. Further, my requirement may also include derivatives of such timeseries data. Derivatives tend to amplify the noise present in the underlying timeseries data. What if there is a way by which I can get an underlying representation of the data, discarding the noise at the same time? Autoencoder comes to the rescue to achieve my objective in such a case. To demonstrate the denoising + interpolation objective using Autoencoder, I use an example of distance data collected from a vehicle by my lab, where the response variable is the distance of the vehicle ahead of my vehicle, and the predictor is time. I have made a small subset of the data available on my GitHub repo as a part of the demonstration that you are free to use. However, it is really small and serves no purpose beyond the tutorial described in this article. github.com Okay, it is time to code now. Note: Before you use data, I should point out that the time (predictor) and message (response) must be re-scaled. In my case, the original time starts from 1594247088.289515 (in POSIX format, in seconds) and ends at 1594247110.290019. I normalized my time value using the formula (time - start_time)/(end_time - start_time). Similarly, the response variable was normalized using (message - message_min)/(message_max -message_min). The sample data provided in my GitHub is already normalized and you can reuse it out of the box. As you can see, I have not performed any regularization as I deliberately want to do overfitting so that I can use the underlying nature of data to the full extent. Now it‚Äôs time to make a prediction. You will see that I rescaled back the time axis to original values before making predictions. For this example, I hadtime_original[0] = 1594247088.289515 ,time_original[-1] = 1594247110.290019 , msg_min = 33, msg_max = 112 Note that I am creating much denser time-points in variable newtimepoints_scaled which allows me to interpolate data on unseen time-points. Finally, here is the curve: While I trained for only 1000 epochs, your training might not be that short, if your data is big. The biggest advantage of this method is taking derivatives, as from the following plot, it is clear that the derivative performed on the original data is poor ‚Äî may not even represent the true derivative! The only downside of this method is time-complexity. Depending on the number of data points, it may take hours before your training is complete. However, if you have access to High-Performance Computing clusters, Amazon EC2, or likewise, it may not take too much time to train your Autoencoder. The notebook to reproduce this tutorial can be found on my GitHub at https://github.com/rahulbhadani/medium.com/blob/master/01_02_2021/AutoEncoder_Interpolation_TF2.ipynb. A longer version of the article is posted on ArXiv.org. If this article benefits you, please use the following citations for referencing my work: or If you like this article, you will want to learn more about how to use TensorFlow 2. Check some of my other articles on TensorFlow 2: medium.com medium.com",,0,8,Towards Data Science,2021-01-03,2021
https://towardsdatascience.com/how-to-break-in-machine-learning-jobs-based-on-6-years-experience-in-ml-f798ac4a0cfa,How to break in Machine Learning jobs?‚Ää‚Äî‚ÄäBased on 6 years experience in¬†ML,Disclaimer: 2 yrs¬†of‚Ä¶,5,12,"['How to break into Machine Learning domain', 'My Journey', 'Breaking into Machine Learning ‚Äî What I hear', 'Breaking into Machine Learning ‚Äî What I suggest', 'Final Words']","Disclaimer: 2 yrs of academic + 4 yrs of professional experience. I don‚Äôt claim to be a mentor/coach nor do I claim myself to having an extraordinary track record. Although, whatever I am putting down in this blogpost is a result of practical experience that I have over interviewing 100+ profiles in the ML domain in last 2‚Äì3 years. What we are witnessing today is a flurry of courses in Machine Learning and enormous ‚Äòinterest‚Äô in undergraduate students in the pursuing a career in ML. I personally have been approached by numerous undergrads and even some experienced person asking for guidance on how to start with a job in Machine Learning. In this blog, I am consolidating the thoughts and surfacing some myths that a general audience has while starting the journey. Trusting the rationale behind the reason that you would want to venture into ML without being swayed by the hype the field has, here are some of the pointers for you to think for: Continuing with pointers in the above section, here are some guidelines for developing a profile in the machine learning domain. In my opinion, courses are must to understand the rationale and concepts behind ML algorithms. Again, gentle projects or projects (assignments) in the online courses are a good way to start with practical implementation, they are no where sufficient if done only in the scope of course completion. What you do in addition to specified projects counts. Here are some ways to extend the project: Again always prefer quality over quantity! 2‚Äì3 great projects standout from 10‚Äì15 generic projects If you are a noob in statistical machine learning with much information about deep learning, there is a ‚Äòhigh‚Äô chance that you would NOT be preferred about the candidate who is strong at statistical concepts. Again always prefer quality over quantity! Great understanding of a couple of DL architecture and a couple of statistical ML models is much much better than having just touched on everything. There are a lot of accepted constructs that I have heard from candidates which are true in most cases however they don‚Äôt have explanation of why those constructs work. Answers to these questions can only observed/understood if you dive deeper into the concepts and readings. This is where formal reading and genuine interest helps. PS: You are not expected to be aware of everything but a small start is still a start. About Me:",8,0,6,Towards Data Science,2021-01-03,2021
https://towardsdatascience.com/animated-time-series-plots-in-r-3777fa738456,Animated time series plots in¬†R,A quick guide to animating your time¬†series,1,7,['Animated time series plots in R'],"A few weeks back I published an article analyzing the mobility trends in South Africa during 2020. Included in that article was a collection of animated time series plots that traced out the mobility trends over time to paint the picture of how mobility looked in the year marked by covid-19 lockdowns. In this article I will provide a walkthrough of the code for my analysis, focusing on creating animated time series plots like the one below. The data we will use can be found on Google‚Äôs covid-19 community mobility reports page and has been included in my GitHub repository as well. Once the .csv file has been loaded into R, we need to split out the national level data. In addition to the plots, the analysis included a comparison table for the average change in mobility for each period of lockdown. This was achieved by first removing the data points that correspond to South African public holidays, which are likely outliers. Next, the remaining data was split into each alert level, and finally, the averages were computed for each alert level. There are a few packages that are needed to plot the graphs: The other element of setup for our plots is setting up the markers for each alert level, which we will plot as vertical lines with text labels: The code for the plot should look familiar to those who have used ggplot2, apart from the very last time. We choose our national dataset, map our aesthetic to have the date on the x-axis and the percentage change in mobility on the y-axis, add another time series on the same axis, add axis labels, set the colours for our lines and include our vertical lines to segment the alert levels. The last line (line 15 in the snippet below) is all that we need to animate the plot. The parameter we supply to the transition_reveal function is the same as the x-axis aesthetic in our graphs because we want to create a gradually revealing time series. Finally, we render the plot to a GIF using the animate function and use anim_save (which by default saves the last rendered graphic) to save our animated time series plot as a GIF. The file will be saved into the same directory as the R file we are working with. The remainder of the plots are generated in the same fashion and the complete code is available in the repository here.",21,0,3,Towards Data Science,2021-01-03,2021
https://towardsdatascience.com/teaching-ai-the-fibonacci-sequence-849430397963,Teaching AI the Fibonacci Sequence,Using regression models to build¬†the‚Ä¶,2,10,"['Teaching AI the Fibonacci Sequence', 'Writing the Code']","In this article, I will train a Machine Learning model on just a few samples of the Fibonacci sequence, then use the model to estimate the missing Fibonacci numbers. In reality, the problem is very simple. Every number in the Fibonacci sequence is obtained by summing the previous two numbers. The first two numbers in the sequence are 0 and 1. To train my model I will only need a few samples. If I had to use 5 or 5000 samples, the result would be identical. Let me explain: What I did is isolating every iteration of the Fibonacci sequence, resulting in two variables (features) and one variable (label) that has to be predicted by using the features. By graphing the dataset, I can see the points in a 3D space. Even if they do not seem to follow a straight line (just look at A, B, and C) they are all situated onto the same hyperplane: f(x, y)=x+y. The data is clear. I can simply use a linear regression model applied in 3 dimensions. What the Machine Learning model will learn, essentially, is just to sum numbers. Because every single number on the training set could be found in this hyperplane, it means that the model will use the same hyperplane to predict future values. For example, by inputting f(8, 13), the output of the hyperplane will be 21, which is a sum of the two features, and so on with every following number. Given the samples, I will use a simple regression model. The library I will be using is sklearn. By using the .fit() method I will input both features (columns [0, 1]) and labels (column [2]). Now that the model is complete, it is ready to make predictions. Given the simplicity of the model and the mathematical certainty, I skipped the validation steps. As you will see, the model will be able to predict the next numbers in the Fibonacci sequence with no errors. I will begin with the first two numbers of the sequence, then estimating the 3rd. I will repeat the iteration by using the next estimated numbers as predictors infinitely. With the code above I asked the model to output the next 25 numbers in the Fibonacci sequence. Perfect execution!!!",29,0,3,Towards Data Science,2021-01-03,2021
https://towardsdatascience.com/impressions-from-a-kaggle-noob-dd923e8024bf,Impressions from a Kaggle¬†Noob,What I learned from my very first Kaggle competition: The Titanic¬†Dataset,4,47,"['Impressions from a Kaggle Noob', 'A Little Background about Myself', 'The Process: Entering the Titanic Competition', 'Learnings, Take-aways, and the Future']","This time ranging from around Christmas to the first or second week of the new year is usually reserved for learning and improving a skill, or for trying out something new. Last year I read up on deep learning and got a basic understanding of how it works. Unfortunately, it is still too far away from my current job reality, so I left it at that. I was still happy about the new knowledge though. Starting last summer, I focussed more and more on the journey of becoming a better Data Scientist. I had recently changed into the Business Intelligence department and working with data has become my daily bread. After work and on weekends I caught up on basic ML algorithms and wrote some analytics scripts using Scikit-Learn. This turned out to be very fruitful since I was then able to directly apply what I have learned to my daily job. Machine Learning is something I still lack behind and it is the area I will focus most on in 2021. After Christmas, I got my hands on some great books about the topic. Furthermore, I finally started to follow Andrew Ng‚Äôs famous courses on Coursera, although I find myself to be easily bored by passive sit-and-listen classes (which Ng‚Äôs course isn‚Äôt really, to be fair). Working on a real problem with a real dataset, however, is what usually brings me into a flow state. I still had a whole weekend for myself before work starts again, and I was curious about trying out something new. Something next to books and courses and work where I could improve my data science skills and learn practically. I‚Äôm not telling anyone anything new here by saying that Kaggle seems to be a great place for exactly that. I never tried it myself though. And since my impressions are completely fresh, I thought about sharing these with you. Don‚Äôt expect this article to be a guide for how to become a Kaggle grandmaster! I recently just graduated from Novice to Competition Contributor (and that‚Äôs not much to be proud of yet). However, this might still be an interesting read if you‚Äôre about to get started yourself and don‚Äôt want to repeat my mistakes. I will describe my general approach and summarize my five learnings at the end. As briefly mentioned I work as a Junior Data Analyst for an FMCG giant. In my job I create business reports, dig into CRM and digital media data, and talk to colleagues about ways to make their work life more productive. I am not a ML engineer nor a computer scientist. I studied Philosophy (B.A.) and Management (M.Sc) To create more knowledge from data I constantly work on improving my data science skills but I am only at the beginning of my journey. I can code in R and Python, am familiar with reporting tool like PowerBI, and use query languages like SQL and BigQuery. I know a few ML algorithms and work on regression problems, however, my experience is limited (hence the Junior). In short: While I create business value from data in my job I don‚Äôt expect to have the tools necessary to perform well in a Kaggle competition yet. But that‚Äôs not the point for me anyway. The goal is gaining more experience through application of machine learning algorithms on datasets apart from work. Jump out of the plane and assemble the parachute while falling down, right? There are some datasets no one can escape when starting to read about data science. What the MNIST dataset is for image classification is the Titanic dataset for Kaggle starters. The task of the Kaggle Titanic competition is to predict who will survive the Titanic crash. On April 15, 1912, during her maiden voyage, the widely considered ‚Äúunsinkable‚Äù RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren‚Äôt enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew. While there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others. In this challenge, we ask you to build a predictive model that answers the question: ‚Äúwhat sorts of people were more likely to survive?‚Äù using passenger data (ie name, age, gender, socio-economic class, etc). ‚Äî Kaggle Website You start with around 900 instances and 10 features, build you model, predict the survival state for another 400 instances, and upload your prediction to Kaggle for inspecting the accuracy. So everybody starts off with identical data but the models in the competition perform very differently. As you know, it‚Äôs not just the model that is important. It‚Äôs probably equally or even more important to find new features (feature engineering) and to decide which features to keep and which to ignore in the training phase. Still, since this is a rather small dataset and the competition has been around for a while, there are a number of common strategies. However, since my goal was to learn as much as possible by trying out, I decided to not read anything before my first upload. In the initial feature engineering phase I only created the feature ‚ÄúDeck‚Äù, which is based on the character in the cabin number (e.g. ‚ÄúC‚Äù in cabin ‚ÄúC103‚Äù). A little research revealed that cabins of the upper decks (A, B, and C) were the most luxurious and expensive ones, nicely decorated and as far away as possible from the rumbling machines at the bottom of the ship. This must play a role, right? Rich survive? Turns out, having a cabin at all was already a sign of wealth. After that I started taking a look at the data in the EDA phase to see whether I find a relationship between the features visually. I don‚Äôt want to focus on it here and I plan on writing a Kaggle notebook on my process and findings soon (link will be added here, but feel free to follow my Kaggle profile). I created a summary graph which should you provide with a clear picture: primarily upper-class and females survived the tragedy. Next, I preprocessed the data to prepare it for ML algorithms. I did pretty standard stuff like replacing missing values (e.g Age and Fare) with the variable‚Äôs median value, standardizing the numeric values using sklearn‚Äôs StandardScaler and turning categories into ‚Äúnumeric values‚Äù using sklearn‚Äôs OneHotEncoder, all put into a ColumnTransformer. If you‚Äôre interested in the code, check out my GitHub repo for it. The train_df and test_df (which I need to predict) were ready for some models! Now comes the fun part, training the ML models! I was already quite familiar with basic Random Forest from my work experience, so naturally that is what I started with. I did not pay much attention to cross validation or accuracy scores by choice since I just wanted to get a baseline. I‚Äôve uploaded my prediction and got ‚Ä¶ a score of 0.46‚Ä¶ For a binary classification problem I probably would have gotten a better result using something like a coin flip (np.random.binomial(1, 0.5)) than for my ‚Äúsophisticated‚Äù ML model. This is the point where I started to research how other people dealt with the dataset and how their feature engineering and models work. My goal was learning more about the general process and I already spent a few hours completely on my own, so I was fine with getting some inspirations. I particularly liked Ken Jee‚Äôs video Beginner Kaggle Data Science Project Walk-Through (Titanic). While sticking with my preprocessed dataset and features, I created a few more models like Logistic Regression, Naive Bayes, and a VotingClassifier based on these models after watching his video and reading his notebook on Kaggle. Kaggle notebooks in general as well as the discussion section of the Titanic competition turned out to be great places to learn from others. While you could basically ‚Äústeal‚Äù the complete code from others to score quite well in the competition, this was not a strategy that fit my goal. Nonetheless, it helped me to come up with a few models that reached on average a cross validation score of 0.82 (which screamed overfitting, but I decided to ignore it for now). I was ready for uploading some more predictions. In general, while great at the training phase I found the Random Forest models to not work well on the actual to-be-predicted-on dataset (or so I thought). The voting classifiers, however, worked significantly better with a score of 0.62200 for both the hard and soft voting version. I was quite happy but also sleepy at that point. So I decided to call it a day and go to bed. The next day I went back to the feature engineering phased and incorporated a few techniques I read about in the discussion section. My score became worse and I changed back to where I left off the night before. A few models were now reaching the 0.62200 score and I decided to compare the prediction results. Turns out, the models all predicted the class identically: nobody survived, all 0. Of course, I must be right quite often when the baseline survival rate for the Titanic is roughly 32%. To put it in terms I learned later when reading about classification performance measures: My Recall rate was perfect (I identified all the Non-survivors) but my Precision (considering false-positives) was as bad as it can get. Turns out this issue is known as the Precision-Recall Tradeoff. Only the random forest classifier predicted a few survivors but not the right one, hence the worse score. I eventually found the problem in my pre-processing section of the code, solved it, rerun all the models. This time I checked the prediction tables before uploading them to Kaggle and it worked. Since the models now were trained on the right training data, my scores significantly improved. As of the time of writing this article, the Random Forest model tuned with RandomizedSearchCV has been my best model with a score of 0.77511 which I am quite happy with. It still lags behind Jee‚Äôs model with 0.79425 but it brought me up to the top 55% and gave me a better idea of how Kaggle works and what‚Äôs important when approaching such a classification problem. Let‚Äôs summarize what I learned the past two days. Well, the most obvious take-away and Learning 1 has been to check the outputs before uploading the predictions to Kaggle and to think about whether they make sense apart from the model. It‚Äôs easy to get carried away when you‚Äôre in the flow state and excited (especially when lacking sleep) but this is a low-hanging fruit! The reason why I only found out about my mistake quite late was my obsession with the Kaggle score. The score of 0.62200 was higher than for my other models, so it must be better, right? Well, the model predicted all to be dead which turns out to be right most of the time when the survival rate is 0.31. Learning 2: Don‚Äôt be obsessed with the accuracy score too much or it will blind you. Score values in the training phase turned out to be misleading too. Even though I used cross validation, having <1000k instances and 30+ features resulted in overfitting. The Kaggle score was found to be 3% to 8% less than the training score using cross validation. Learning 3: Get your feet wet from time to time and upload a few predictions to see which models work well on the real data instead of overfitting on training data. Kaggle turned out to be better than I expected for learning. The community is really active and people share their approaches and code in the Discussion and Notebook sections as well as outside of Kaggle in YouTube videos, on GitHub or through Medium posts. I heard that this was true for real competitions and that incorporating other‚Äôs approaches is even necessary to score high, which reminded me a bit of the Market Efficiency debate in stock valuations, but that‚Äôs another story. However, working more within the Kaggle universe (i.e. by reading and writing notebooks and participating in the Discussion section) as a great way to improve one‚Äôs skills has been my Learning 4. Talking about Kaggle notebooks: I tend to work in Spyder and split my scripts into sections that I run subsequently. The new feature of Spyder 4 where I can observe all plots in the new plot section was really a great extension. I knew before experimenting with Kaggle that Data Scientists love Jupyter Notebooks primarily because of combination of coding and documenting in a publish-ready format. I cannot live without Spyder‚Äôs Variable Explorer and df.head(5) just does not do the trick for me. However, Spyder alone is really not that great for EDA. Documenting the findings in #comments and ‚Äô‚Äô‚Äôdocstrings‚Äô‚Äô‚Äô is not as visually appealing as a great notebook. This leads me to my final Learning 5: Data Scientists love Jupyter/Kaggle/Google Colab notebooks for a reason and I should start using them more for that kind of work (EDA, documenting the approach, summarizing the findings, sharing with others, etc.). Update January 4, 2021: Tinkering around with the features while working on the Jupyter notebook just got me into the top 7%. On my third day on Kaggle. Contact Me: Kaggle: https://www.kaggle.com/jonasschroeder LinkedIn: https://www.linkedin.com/in/jonas-schr%C3%B6der-914a338a/ ResearchGate: https://www.researchgate.net/profile/Jonas_Schroeder Resources: GitHub Titanic Repo: https://github.com/JonasSchroeder/kaggle_titanic Popular Medium Articles by Jonas Schr√∂der: Can you crawl data from LinkedIn? (Towards Data Science) Research on Influence in Offline and Online Social Networks (Towards Data Science) Social Network Analysis of Related Hashtags on Instagram (using InstaCrawlR, code on GitHub)",31,0,11,Towards Data Science,2021-01-03,2021
https://towardsdatascience.com/how-to-transition-from-academia-to-data-science-5821ebe598a,How to transition from Academia to Data¬†Science,A practical guide,5,45,"['From Academia to Data Science', 'What is Data Science anyway?', 'How has Academia already prepared me for this role?', 'Practical (and very opinionated) advice', 'Wrap up']","Oh boy, another one of these blog posts about transitioning to Data Science from Academia. Well, in this post I‚Äôll try to add a slightly different take on the usual advice along with some more traditional advice. This is not a step-by-step guide because I don‚Äôt think such a thing exists in this case. Instead, this post will be my opinionated views on what you can do to make a smooth transition from Academia to Data Science. Also, this post is not about ‚Äúhow to get a Data Science job‚Äù. I won‚Äôt talk about your resume, or prepping for interviews or any of that stuff. This post is about how to become a Data Scientist. First of all, I should note that while a lot of the things I cover in this post could be generic and apply in many different situations, I will lay down a few assumptions first. And with that out of the way, let‚Äôs get into it. If you have stumbled across the blog, you probably have some idea of what Data Science is or maybe you think you know what it is. Data science could mean that you manage data bases and write SQL code all day. It could mean that you work with ‚Äúbig data‚Äù and do things with Hadoop (is that still a thing, probably‚Ä¶), Spark and other big data tools. God forbid, it could mean that you do most of your work in Excel making linear regression models. But in my experience when most people say Data Science, what they really mean is ‚ÄúMachine Learning Engineer‚Äù, i.e. someone who develops and applies ML models to real data and deploys them for use in real-world scenarios. Of course, there is more to it than that, but that is the gist and this the kind of Data Scientist that I will have in mind for the rest of this post. I know when I was a graduate student I thought that I was only prepared to do one thing with my life ‚Äî be a (astro)physics professor/researcher (after an extended post-doc career). In my time (2009‚Äì2014) in physics grad school, there was almost never any mention of any other career trajectory. It was assumed that this was the path that everyone would take, which basically meant there was one mode of operation: Write Papers. However, there are still a lot of things that you will learn in your academic career (both undergraduate and graduate) that will be very useful for your Data Science (ahem‚Ä¶ Machine Learning Engineer) career. There are some skills that can not be taught. They can only be learned through struggle. If you have done any kind of original research in your academic field then you know the struggle. You have to learn to figure things out for yourself because there is no ‚Äúright answer‚Äù that you can look up in the back of the book. If you have ever written any significant code base then you also know the struggle. You have to figure out how to put everything together and debug the inevitable bugs that will appear. In Data Science, most of your existence is struggle and you need to be able to feel comfortable not knowing what to do in a given moment. You need to be able to experiment and possibly fail. In many cases a stakeholder (fancy language for the person/entity that wants some ML product/functionality) does not know much about ML, all they know is what they want an end product to look like. Actually, in some cases they don‚Äôt even know what they want the end product to look like. It is more common that they have a lot of data (perfectly clean of course) and they want ‚Äúinsights‚Äù from that data. You, as the Machine Learning Engineer need to be prepared to struggle through finding a solution to their problem. Even though the struggles you will go through in a ML based problem may be different than those that you have had in academia, being comfortable with being uncertain will prepare you well for a career as a machine learning engineer. In the next section, I will offer some advice to get more familiar with the ML-based struggle. Very related to ‚Äúthe struggle‚Äù is learning how to learn. Everyone learns in different ways and throughout your academic career it is a good bet that you have discovered your own method. In many cases, definitely in mine, you realize that you don‚Äôt really learn much from your classes. Sure you may do well on tests, but you forget it all in a week. You learn by doing. All the theory is approximately worthless without the practice. Would you trust a doctor that has spent years reading up on the latest surgical techniques but never actually performed a surgical procedure? Would you recruit a ‚Äúguitarist‚Äù who knows all the musical theory but has never actually played a guitar? Later I will offer some advice in this area, but knowing how to learn on your own is one of the most important skills in Data Science, or life for that matter. Overall, soft skills are a combination of people skills and communication skills. This one strongly depends on whether or now you have had to present your work to both technical and non-technical audiences. It also depends if you have been part of a large collaboration. With that being said, being able to present technical information to non-technical audiences in a clear and concise way is a skill that will help you land a good Data Science job. As I mentioned above, I am focusing on an applied Data Science job, not a research data scientist position. This means that you will need to be able to communicate with the stakeholders as well as your teammates. If you have been lucky enough to be part of a large collaboration with many different groups that need to work together, then you will have gained invaluable skills that will really help you in Data Science. Being able to translate between different groups is a very useful skill. For example, when I was a grad student I was part of a data analysis group and we used mostly Bayesian techniques. There was another group that used mostly frequentist techniques. On the surface these things may be very different, but there are some useful translations between the two and I was able to speak both languages, which made things move much more smoothly. Again, in the next section I will give some advice on how to practice your communication skills. As the title says, this advice is very opinionated, but these methods have worked well for me. Before I get into the actual advice let me explain what I look for in a Data Scientist candidate: With that, lets get to the advice Python is the language of Machine Learning. It is very easy to use and it mostly wraps libraries that are written in more performant languages like C. It is growing across all areas, not just ML but for this post I will focus on ML. Python is very powerful in ML because of several purpose-build ML libraries such as pandas, tensorflow, pytorch, scikit-learn, spacy and transformers just to name a few. If you already know Python, that is great. If you don‚Äôt, there are a plethora of resources out there to learn. I would not recommend using a book to learn Python when there are so many great resources out there already. I learned Python after switching from Matlab and have honed my skills over the years by looking at well-designed code bases like those mentioned above, and by watching various YouTube tutorials and other online sources. In fact I have never read a book on Python programming (of machine learning or deep learning or statistics for that matter, but I‚Äôll get into that a bit more later). Ok, so you‚Äôve learned Python. Good to go, right? Well not really, I knew how to use Python for a pretty long time before I actually learned how to design software. I always use the analogy of building a house. You could be really good at using all of the tools, but if you don‚Äôt have a blueprint, know the various codes and standards, or know where and how to get all of the materials, then you are out of luck. While knowing the Python language is very important, actually knowing software design principles (mostly language agnostic) will really let you stand out. This section in itself could serve as several blog posts but I‚Äôll just mention the main highlights here: The list above only scratches the surface in terms of details. I provided a basic tutorial link for each, but there are a lot of other really good resources to learn more about proper coding practices and tools. Here are a few: I‚Äôll end this section by noting that, in my experience, most data scientists (especially of the machine learning engineer variety) only have limited knowledge of these things. So, this is not something that you will be lacking in terms of your competition. However, if you do know these things and take the time to learn them, then it will put you in a position to stand out above your competition. More importantly, it will make you a better data scientist. Ok, here is where the standard advice comes in, but it is standard for a reason. If you are coming from an academic background and not a data science background, you have to do some projects to fill in for your lack of on-the-job experience. In some ways this is a bonus, especially if you are competing against people who have gone to school for data science or a similar field. Remember what I said about being self-motivated. For those in a data science or analytics program, most of their experience is through classes of which they ‚Äúhad‚Äù to do. For you, the self-taught data scientist, you show that you are self-motivated right away by presenting data science projects you have done. Now there are lots of different kinds of projects you can do. I‚Äôll list three here in rough order of importance: Once you have done some projects and gained some ML and data wrangling skills with a large dose of struggle, now you can sharpen up on your communication skills. First off, definitely put whatever you do on GitHub which will make your work public and teach you about version control if you didn‚Äôt already know about it. Make sure you document your project with a project README. This is just the lowest level of sharing your work. Next, you may want to share this work on a personal blog or on Medium. This is another great way to hone your communication skills and to ensure that you actually understood your project. The best way to test your knowledge is to try to explain your project to someone else. However, writing a blog post is still somewhat passive and these skills are not the main communication skills you will need to be a successful Data Scientist. Lastly, you can present your work by giving a talk. There are probably a lot of forums for this kind of thing. In my case I looked on Meetup for any local Data Science related groups. It‚Äôs probably a good idea to attend a few meetups first and introduce yourself to the organizers. They are usually on the lookout for anyone who wants to give a talk. If you get a chance, offer to give a presentation on one of your projects. So, this sounds like a lot and your future company may not even look at all of this work that you have done, but even if they don‚Äôt look at it, the fact that you have done all of this and developed and honed all of these skills will help you immensely in your overall career as a Data Scientist. Back when I was an undergraduate, a physics professor of mine gave me some great advice. He told us that one of the most important things in terms of your career is that you ‚Äúlearn the lingo,‚Äù that is, you learn the language of the field that you are in or want to be in. For example, if someone says to you that they ‚Äúused a transformer model for sentiment analysis,‚Äù what do you need to take away from that statement? You definitely don‚Äôt need to know in any detail what a transformer model is or how to implement it, but you should know that sentiment analysis refers to measuring how ‚Äúpositive‚Äù or ‚Äúnegative‚Äù a segment of text is and that a ‚Äútransformer‚Äù model is a kind of deep learning model that is very popular in natural language processing nowadays. In other words, it is far better to have a wide, but shallow understanding of many concepts instead of a very deep understanding of a few concepts. This is almost the opposite of academic research in many cases. In academia, one pretty much needs to have a very deep understanding of their area at the expense of not knowing much outside of that area. As a data scientist the exact opposite is true, it is far better to have a base understanding of many concepts and their uses rather than knowing the exact algorithmic details. This gets back to my earlier point about learning how to learn. If you have a shallow understanding of many things then that will allow you to assess a given problem quickly and come up with a potential solution; however, to actually implement that solution you will probably need to learn more that you know at the moment. Therefore, you need to be able to learn on your feet. As an academic, it may seem like you always need to come up with something new, but in data science no one really cares if you come up with something new, they only care if you solve the problem. This means that there is no shame is scouring the internet for solutions to a problem similar to yours and adapting it to solve your problem. All of this may seem wrong, but it is some of the best advice that I ever received and I think it has helped me immensely. Now, this all sounds nice, but how do you do it? Well, this is the hard part. I think the best way to develop this skill is to allow yourself to explore many areas, but also control yourself from digging too deep, at least in the beginning when you are still learning. Take introductory online course, follow tutorials, do some simple projects, but don‚Äôt think that you need to know all of the details. Ok, now on to perhaps another controversial piece of advice. Don‚Äôt read books to learn your Data Science/ML skills. At least, don‚Äôt use books as a main source of information. The only thing I would recommend books for is to learn basic programming language fundamentals, (although those are probably better online as well). For some complete anecdotal evidence of this, I have never read a book on programming, machine learning, deep learning, statistics or communication, yet in my academic career I published several papers and led large working groups. Outside of academia, I have advanced fairly quickly and have led projects in several ML areas and I achieved all of that strictly from learning online. Now, learning online does not mean that you passively sit back and watch some YouTube videos. It means that you watch some YouTube videos or online courses or read blog posts or arXiv papers and then try the things out yourself. You struggle. You watch and read some more. Struggle some more. Apply the techniques in your work or in your projects. Struggle some more. By the end of this struggle session, you will realize that you have managed to learn a whole lot, not by some ground up foundational approach but by a more stochastic process of trial and error which eventually leads to a much deeper understanding of the material. This may all sound crazy and it may not work for you, but it has worked for me and it has worked for a lot of other great Data Scientists and Machine Learning Engineers. I have a very shotgun approach to learning in which I dabble in many different things, but I can offer a few good resources here in addition to the resources that I have already sprinkled throughout this post. If you made it this far, congratulations! This post turned out to be much longer than I originally had planned. I have covered a lot of things here and it may seem daunting. When I first started on this journey, I kept thinking that there was no way I could learn all of this stuff and maybe it would just be better to stay in academia. If you are passionate about your academic field then by all means don‚Äôt give up, but if you are just staying in because you think there is no other option then that is a completely wrong assumption. All of the things I have mentioned in this post are things I have learned over the previous 3.5 years as a Data Scientist. I did not do all of these things in the very beginning and have learned a lot since then. Lastly, if you are in this transition and you are feeling stressed, remember once you gain these skills you will be a hot commodity and will be able to get a fulfilling job almost anywhere. Originally published at https://jellis18.github.io on January 3, 2021.",42,1,17,Towards Data Science,2021-01-03,2021
https://towardsdatascience.com/datafication-and-personalization-the-netflix-way-to-extend-value-creation-for-customers-70948ad2239f,Datafication and personalization: the Netflix way to extend value creation for customers!,,4,10,"['Datafication and personalization', 'La Casa de Papel aka Money Heist', 'The future ‚Äî data integration across companies', 'Summary']","Netflix started as a DVD rental company more than 20 years back. With personalization at its core, Netflix recommended and mailed DVDs to its customers. Back then its personalization algorithms had very few data points ‚Äî the past rental history, length of the time a DVD was held, and maybe some additional demographics information. Fast-forwarding to the time when Netflix launched a streaming service, it plunged into datafication of user behavior, capturing the customer browsing history, the points where a customer pressed forward, rewind or pause, the titles added to wish list, and so on. Netflix has divided its customers into a few thousand micro clusters that are, essentially, taste communities, and each individual might be part of multiple taste communities. This datafication in the modern streaming and internet world helped Netflix better understand its customers to provide a more personalized experience - a customized homepage for each customer and a hyper-personalized ‚ÄúRecommended for you‚Äù and ‚ÄúBecause you watched the ABC title‚Äù. La Casa de Papel, a crime thriller, was created and telecasted in Spain back in 2017, placing it for a strong start with a prime time slot. However, the viewership declined consistently over its first two-season run, with the show being summarized as a flop in Spain. In Dec 2017, Netflix bought international streaming rights for La Casa de Papel, dubbed it into English, renamed it to Money Heist, and placed it into its catalog of numerous other foreign-language shows. Money Heist was launched on Netflix without any promotion, simply subject to the mercy of algorithms possibly with the right tags and classification. This is where folks like myself, with an affinity for crime thrillers, had Money Heist added to the Netflix recommendation list. In parallel, in early 2018, the show's actors started observing an unusual phenomenon on their Instagram profiles ‚Äî their followers were just climbing and the actors were not anonymous anymore. Within four months of its global launch, the show became the most-watched foreign-language series on Netflix, and the show was renewed for another two seasons in April 2018 with a significantly higher budget. Needless to say, the next two seasons were received with immense pleasure by fans, with 34M watching it in the first week of its launch. What happened here is pure data science and machine learning. Netflix published this show with tags such as ‚ÄúTV thriller, Suspense, Exciting.‚Äù Some other shows with similar tags include Prison Break, Narcos, Breaking Bad. In its microclusters, Netflix would have identified customers who like TV thrillers and Money Heist started showing up on their recommendation list. With more people watching the show and providing great reviews, the show jumped up the recommendations. With all the datafication, classification, and clustering that Netflix had done on its customers, the show was recommended to the right cluster of customers. I will be writing a detailed article on recommendation systems later. If in a hypothetical situation, Netflix and Instagram were to come together and leverage each other‚Äôs datafication outcomes, both could create extra value for their respective customers. Instagram, with the knowledge of Netflix‚Äôs clusters, could propose new connections, show similar advertisements to the customers in one cluster, and so on. Netflix can identify who are the actors its customers have liked/followed on Instagram and then use the pictures of these popular actors as thumbnails on different titles, a customized picture for each customer, to gain quick attention. If on my Instagram, I have followed √Ålvaro Morte (the Professor in Money Heist) then his picture would show up on Mirage as a thumbnail in my Netflix recommendations. Now if Netflix were to create a new thriller, as it is creating its own content these days, it would identify, through Instagram, the popular thriller genre actors amongst its targeted viewer base. Bingo! Netflix has the cast for its next production. Data is the new oil! More data, better algorithms, and better products leading to more data creation ‚Äî the flywheel continues to support business model innovation and value creation for the customers. Companies have to venture on a datafication journey which is systematically extracting data from activities and transactions that are naturally ongoing in the business, establishing data pipelines that enable large volumes of data with high velocity while capturing enough variety. Companies like Google and Amazon have gotten to their current status, not only through world-class products but through in-depth knowledge of their customers.",66,2,4,Towards Data Science,2021-01-03,2021
https://towardsdatascience.com/how-to-write-better-data-science-articles-in-2021-a34210badb37,How to Write Better Data Science Articles in¬†2021,"New year, same you, better articles.",7,40,"['How to Write Better Data Science Articles in 2021', 'Ignore the imposter syndrome.', 'Write consistently, then only strike when the iron is hot.', 'Use the Feynman Technique.', 'Have a non-tech person read your article.', 'Read. A lot.', 'Final thoughts.']","What do data science and writing have in common? Besides the obvious (that we‚Äôre all looking for more hours in the day to get better at both), data science and writing also hold the title for being the two skills that can turn us into well-rounded individuals in both skills if improved in tandem. Data scientists are commonly referred to as ‚Äústorytellers‚Äù because of the way that they can turn massive data sets into beautiful visualizations that tell stories to the masses. Therefore, it only makes sense that data scientists write about their work to inspire, inform, and teach future data scientists about how to become effective storytellers themselves. A new year is a perfect time for self-improvement (although the self-improvement gurus would argue that any time of year is the perfect time to improve yourself), and that doesn‚Äôt stop at hitting the gym more or reading 52 books in the new year. Instead, in an upcoming year full of unknowns, creating small improvements in your data science and writing skills could be the steady constant you need to see great improvements in both areas by the end of the year. Want to know a secret? I feel like a complete outsider in the data science community. Even though I have an education in software development, data science is a totally new world for me that I don‚Äôt feel like I‚Äôm a part of yet. However, I‚Äôm not letting that stop me from writing about it, and neither should you if you‚Äôre just starting or transitioning into the field. As many others have suggested before me, it‚Äôs important to learn by doing. Not only that, but data science is a field where your ability to learn is much more important than the credentials you have going into it. The data science community is full of individuals from all different backgrounds ‚Äî some who studied engineering or the humanities in university, some who worked in completely unrelated sectors before switching, and others who taught themselves everything they needed to know to break into the field. Besides, when you read a thorough, well-reasoned article, you generally don‚Äôt care about the credentials of the person writing it. That‚Äôs the beauty of the tech industry. You don‚Äôt have to have a Ph.D. to be an authority on a subject or to teach others how to do something. A word of caution: remember to maintain a level of humility and people will be more than willing to help you become a better writer or data scientist. As someone starting, I‚Äôm well aware that my articles won‚Äôt be perfect, and that the code I share won‚Äôt be the perfect solution for a problem. However, as a beginner, if you acknowledge that you‚Äôre still learning, people will be kind in their criticisms and will work to help you refine your articles or your code. Don‚Äôt be that person who ‚Äúknows it all‚Äù. You‚Äôll make more friends and become a well-rounded data scientist by being honest and welcoming of any help that comes your way. The first piece of advice I‚Äôm about to offer is standard and often the most- suggested piece of advice to beginning writers who want to become better. The second piece of advice is perhaps a little more counter-intuitive. To write better data science articles, you need to write consistently. Like working out at the gym, you only see results when you‚Äôre lifting weights in a consistent and regimented manner. Similarly, when you‚Äôre writing, you‚Äôre using a muscle. That writing muscle must be exercised consistently until the point that you can write effective articles regularly. At the point when you feel confident that you can sit down and write out a thorough article on a particular topic in a day (because let‚Äôs face it, forcing yourself to write an entire article on data science in an hour isn‚Äôt kind to yourself, or your editors for that matter), then you know you‚Äôve cemented some good writing habits into your writing muscle. However, at the point where you are consistently and easily producing content, you need to determine if you are producing your best work. At this point, you should be looking to transition to writing in a ‚Äústrike while the iron is hot‚Äù manner. Why? Because Medium prides itself on being a home for high-quality articles and will support and highlight those who do so. Therefore, if you write when inspiration strikes instead of feeling the need to be a hamster on a wheel churning out content, you‚Äôre likely to write data science articles that are especially informative, insightful, and in-depth. Often touted as a method to learn concepts quickly and thoroughly, I further believe that the Feynman Technique is the perfect method to help you write better data science articles. Richard Feynman was a Nobel Prize-winning physicist, best known for his contributions to quantum mechanics, particle physics, and nanotechnology. However, to me, he is most famous for being known as ‚ÄúThe Great Explainer‚Äù. Feynman was known for giving the best lectures on physics, not because he was always sharing complex, hard to understand ideas, but because his lectures on these complex topics were accessible and easily understandable by the masses. Because of this, Feynman was known as ‚Äúthe easiest Nobel Laureate to read in all of history‚Äù. But what made him such an amazing teacher of such a complex subject? It was all thanks to the methodology he used to master a topic. The ‚ÄúFeynman Technique‚Äù, as it became known, is a method to understand anything. ‚ÄúI couldn‚Äôt reduce it to the freshman level. That means we really don‚Äôt understand it.‚Äù ‚Äî Richard Feynman The Feynman Technique involves a four-step process that focuses on the learner achieving true comprehension of a subject through trial and error, discovery, and a free inquiry process. The technique can be broken down into four steps that you follow in order: So how will this help you write better data science articles? Well, if you don‚Äôt understand a topic such that you can explain it in simple terms, how do you expect to be able to teach the topic to others and have them understand it? Not only will using the Feynman Technique make you a more capable writer of data science articles, but it will also give you the ability to learn any data science concept to mastery at the same time. It‚Äôs a win-win, kill-two-birds-with-one-stone kind of situation. It‚Äôs one thing to be able to write good code and to analyze stacks of data ‚Äî it‚Äôs another thing to communicate what you‚Äôre doing and why you‚Äôre doing it to other people. Being a good data scientist means that you‚Äôre also a good communicator, and means that you can describe a concept in varying levels of complexity depending on your audience. Therefore, to write better data science articles, it‚Äôs always a good idea to have a non-tech person read your articles so you can see if anyone can understand what you‚Äôre writing. This is especially important if you‚Äôre writing a series of introductory articles for people just starting in data science. You don‚Äôt want to scare them away before they even begin their journey, do you? Besides, as Mr. Feynman says, if you can‚Äôt explain something simply, you don‚Äôt understand it yourself. I went to university with many people who were very intelligent and could write brilliant code easily. However, when I would ask them for help, they would only be able to explain it at a very complex level that I couldn‚Äôt even remotely understand. So of course, because I didn‚Äôt want to look stupid, I would do the regular ‚ÄúOh great, thank you, that totally makes sense now!‚Äù, and then go find someone else who could explain the concept in words I would actually understand. To relate this to your articles, you don‚Äôt want someone to read your article and then immediately go find someone else‚Äôs article they can better understand. So, to reiterate, to combat this and make your articles more friendly and useful to readers of all different backgrounds, have a non-tech person read your articles and then explain to you what you just wrote. After doing this a few times, you‚Äôll be impressed by how much more accessible your articles become, and your readers will be even more responsive to your work. Schopenhauer famously said that ‚ÄúReading is merely a surrogate for thinking for yourself; it means letting someone else direct your thoughts.‚Äù A long-winded chap, he eventually goes on to say that ‚ÄúThe result is that much reading robs the mind of all elasticity, as the continual pressure of weight does a spring, and that the surest way of never having any thoughts of your own is to pick up a book every time you have a free moment.‚Äù Basically, he‚Äôs saying (from my layman's point of view) that reading won‚Äôt make you smarter, and if anything, will make you dumber and less able to think for yourself. So I get what he‚Äôs trying to say ‚Äî for us to formulate our own understandings of concepts, we need to think for ourselves instead of having the ideas given to us. However, for those of us with lives, we can‚Äôt spend the hours required every day just to reason something out for ourselves. Therefore, my argument is that to write better data science articles, you need to read. A lot. This tip is widely circulated, and for good reason. Unsure of your own writing style? Read the articles of your favorite authors and emulate their writing style. Need to improve your understanding of a specific data science topic? Read multiple articles from different writers about the topic until you find one that gives you that ‚Äúaha‚Äù moment of understanding. Want to learn something new so you can teach it to others? Read about the topic and then create your own project based on the article to solidify your understanding. This tip explains how I‚Äôm able to write an article about how to write better data science articles, without having been a prolific writer on the topic for very long. I‚Äôve read stacks of data science articles on Medium, and because of this, I‚Äôm able to say what makes a good article and what makes a bad article. The good articles all have some things in common: they‚Äôre easy to understand, the writer uses simple terminology and analogies, and you can tell they‚Äôre writing about a topic they‚Äôre passionate about (instead of just pumping out content to make a buck). Because of all this knowledge I‚Äôve consumed through reading, I can now write an article to help others write better data science articles. In other words, consume as many articles about data science and writing as you can, and watch as your skills improve by leaps and bounds. There is no concrete recipe to follow that will guarantee you the most well-received, highest-earning data science article on this platform. However, if you‚Äôre prepared for the marathon-like process of learning to write good blog posts and clean, effective code, and you‚Äôre armed with the tips mentioned in this article, you‚Äôll be well on your way to writing better data science articles in 2021.",295,1,8,Towards Data Science,2021-01-03,2021
https://towardsdatascience.com/7-steps-to-your-own-dockerhub-image-7d1d92d2a3e0,7 Steps to Your Own Dockerhub Image,Share your Python code with others via Dockerhub,9,20,"['7 Steps to Your Own Dockerhub Image', '1. Sign up for a free Dockerhub account', '2. Create a Dockerfile', '3. Build your Docker image', '4. Log in to Dockerhub from your Terminal', '5. Tag your image', '6. Push your image to Dockerhub', '7. Test and confirm a success', 'Conclusion']","The easiest way to package your code for production is by using a container image. DokerHub is like Github for container images‚Äî you can upload and share with others an unlimited amount of publicly available dockerized applications at no cost. In this article, we‚Äôll build a simple image and push it to Dockerhub. The Docker ID you choose is important as you will need to tag all your images with it. For instance, if your Docker ID is ml-practitioner, then all your images will have to be tagged in the form: If you want to create several versions of your container image, you can tag them with different versions, for instance: A Dockerfile contains a set of instructions for your Docker image. You can think of it as a recipe for a cake. Once you build and share your image on Dockerhub, it‚Äôs like sharing your curated recipe with others on your cooking blog. Anyone can then take this well-prepared recipe (your Dockerhub image) and either directly bake a cake based on that (running a container instance from that image as-is), or make some modifications and create even fancier recipes (use it for other Docker images). In fact, in the simple Dockerfile example below, we‚Äôre doing just that: we‚Äôre using an image python:3.8 as a base image (base recipe) and we‚Äôre creating our own image out of it, which has the effect of adding new layers to the base image: The COPY command copies your files to the container directory, and CMD determines the code that is executed when the container is started via docker run image-name. Let‚Äôs say, my image will be called etlexample. In our terminal, we switch to the directory, where we have our Dockerfile, requirements.txt, and src directory with our main code (can contain several modules). Let‚Äôs build a simple ETL example. Here is a project structure that we will use: Your requirements.txt could look as follows (just an example): We then need to build our image: The dot at the end indicates a build context ‚Äî for us, it means that we use Dockerfile from the current working directory for this build. Now your image is ready! When you type docker image ls, you should be able to see your image. myusername indicates the Docker ID that we chose when signing up. You will be then prompted to type your password. Then, you should see: Login succeeded. By default, when we build an image, it assigns the tag latest which simply means the ‚Äúdefault version‚Äù of your image. When tagging, you could assign some specific version to it. For now, we‚Äôll go with the latest tag. When you now type docker image ls again, you will see the image with your Dockerhub username. When you now try to pull this image, you should see: In this short article, we looked at the steps required to build and push a new image to Dockerhub. Make sure to NOT include any sensitive information ‚Äî avoid any hardcoded credentials or any code that you wouldn‚Äôt want to share in the same way within a public Github account. Thank you for reading! If this article was useful, follow me to see my next posts. References & additional resources: [1] https://hub.docker.com/",156,0,3,Towards Data Science,2021-01-03,2021
https://towardsdatascience.com/the-intuition-behind-model-monitoring-bd8b5d8e540b,The Intuition Behind Model Monitoring,Make your machine learning system resilient to changes in¬†the‚Ä¶,4,28,"['The Intuition Behind Model Monitoring', 'Concept Drift Defined', 'How do you detect and address drift?', 'Conclusion']","The world is inherently dynamic and nonstationary ‚Äî constantly changing. It is common for the performance of machine learning models to decline over time. This occurs as data distributions and target labels (‚Äúground truth‚Äù) evolve. This is especially true for models related to people. Thus, an essential component of machine learning systems is monitoring and adapting to such changes. In this article, I will introduce this idea of concept drift or regime change and then discuss three ways to handle it and what you should consider. New tools for model monitoring are emerging, but it is still important to understand how you want to maintain a model Concept drift or regime change is a change in the process that generates your data. We only care about this change if it hurts your model‚Äôs performance. Plotted above, the ‚Äúconcept‚Äù (distinction between red and green data points) has rotated and changed shape. A model that learned the concept during Regime A will perform poorly under Regime B. In model monitoring, data and model results are tracked in order to detect drift. Once detected, drift can be addressed manually or automatically. A general approach to addressing concept drift is to monitor your model to detect drift, retrain the model, and deploy the new model version. There are three common approaches to monitoring for concept drift: This is straight forward: if the model metrics decline below some set level, reevaluate the model. To monitor model metrics, you need to make several decisions that will impact the sensitivity and frequency of your drift detection. 1. How many new predictions do you use in calculating model performance? 2. Which performance metric(s) do you evaluates and which threshold(s) do you apply? 3.How often will you monitor for drift? 4. How error-tolerant is the user of your model? This will help inform your answers to (1)-(3). 5. How do you respond to drift? Manual evaluation and retraining of your model? Automatic updates? Another approach is to monitor the distribution of model prediction or residual values or the confidence in those values. It is far easier to monitor distributional changes the values produced by your model than the potentially high-dimensional input data. The specific formulation of the the statistical monitor depends on the speed and quantity of predictions to monitor. In addition to the questions in the previous section, some relevant decisions to make include: Another option to regularly update your model weights on a periodic basis with new observations. The periodicity of updates could be daily, weekly, or each time you receive new data. If you update your model each time you receive new labeled observations, your model is updated online. This solution is ideal if you anticipate incremental concept drift or an unstable concept. This option is not foolproof because there is still risk that the model drifts away from true target, in spite of the online updates. This could occur for a number of reasons. For these reasons, it is still important to monitor models that are updated online. Understanding and detecting drift is a nontrivial task. Ultimately, selecting the best methods for detecting and responding to drift often requires intimate knowledge of your data, model, and application. The ideal frequency of drift detection and model retraining is subjective and depends on your data and application. Lastly, in any solution, it is important to consider whether changes in model performance metrics are due to sample bias or whether perceived drift is due to randomness or outliers, and not a shift in the data distribution or target concept. For a more in-depth look at concept drift, check out my related article: towardsdatascience.co",79,1,4,Towards Data Science,2021-01-03,2021
https://towardsdatascience.com/donut-plot-for-data-analysis-5aacac591741,Donut plot for data¬†analysis,"In this story, we demonstrate how we draw donut plots from complex¬†excel‚Ä¶",4,16,"['Donut plot for data analysis', 'Explanation of original data', 'Donut plot with Pandas and Matplotlib', 'Story review']","As usual, our work starts with data, just like some people‚Äôs breakfast consists of donuts. There is no relationship between food and data, except that the donut chart has a doughnut-shaped figure. Firstly, we have an excel file that records all sales information of an industry department from 2018 to 2020. The department established in 2018 has just experienced covid-19 year in China. Luckily, it has survived and celebrates the starting of the new year 2021. Now we can look back on what happened to the department in the last three years. The excel composed of 3 sheets, contains the daily sale amount (column E) and the total weight of goods (column F). The receivers of the goods (column C) are actually the companies that pay the money. The product has 3 types (column D) depending on its components, FDY and DTY. generally, There‚Äôre 3 combinations of these raw materials, namely FDY-FDY, DTY-DTY, and hybrid FDY-DTY. FAW (column B) determinates the thickness of the product since the finished goods are grey cloth, a basic product in the textile industry. We write each excel sheet into a data frame. The data frame has the same columns as the initial data. We can regroup the data in our minds. If we would like to explore the relationships between orders and costumers, in other words, the numerical proportion between column C and column E (or F), we can regroup the data frame by a groupby operation, which uses the function pandas.DataFrame.groupby. Code example: From the donut plot, we can clearly see the contribution of each customer to sales. The Customer with ID003 has made the greatest contribution in the year 2018. Similarly, we group the data and compute operations on other groups, such as ‚Äòtype‚Äô, ‚ÄòFAW‚Äô. Therewith we have obtained three donut plots in 2018. This excel records 3 years‚Äô sales in the department. That means we can get 9 donutüç© charts. In the next release, we will explain how to build a beautiful annual report by DIY design with Matplotlib. towardsdatascience.com One page of the report looks like below: All the code and files (png and excel) have been submitted in Github. So far, I have written a series of stories based on the same excel file, for which I have posted other related stories, which could help you understand data and data analysis through the real data and case. medium.com towardsdatascience.com towardsdatascience.com",97,0,4,Towards Data Science,2021-01-03,2021
https://towardsdatascience.com/analyzing-energy-consumption-habits-of-the-texas-gulf-coast-ea46a7dfbb9e,Analyzing Energy Consumption Habits of the Texas Gulf¬†Coast,"Multiple Regression, Bootstrapping, and‚Ä¶",5,31,"['Analyzing Energy Consumption Habits of the Texas Gulf Coast', 'Abstract', 'About the Data', 'Approach', 'Closing Thoughts']","This analysis seeks to synthesize Texas Gulf Coast power consumption data for the three-month summer period of June-August for each year between 2010 and 2016. The data used was sourced directly from ERCOT, which manages ~90% of the state‚Äôs electric load and represents over 26 million Texas customers. All computation was performed using R and RStudio. There are two main questions I‚Äôll attempt to answer through this analysis: 1. During the summer, how much higher or lower is daytime power consumption on a weekday vs. a weekend? 2. How does power consumption increase with temperature, on average? Does this relationship seem to differ between weekends and weekdays? The dataset used for this analysis contains 5,820 individual entries regarding power consumption in the gulf coast region of Texas. This encompasses the area immediately surrounding Houston and extends just south of Matagorda Bay. If you‚Äôd like a visual representation of this area, you can view a map here. The columns in this data set are as follows: Time: The date and time stamp of each data point. Each point covers a one-hour interval between 9 am and 7 pm and begins at the timestamp listed. COAST: Peak demand (measured in megawatts) for the entire coastal region of Texas during the specified hour. See the linked map above for further clarification on this. Temp: Average temperature recorded during the listed one-hour interval as recorded by the weather station at Houston‚Äôs William P. Hobby international airport. All units are in degrees Celsius. Weekday: A binary indicator for whether the day in question was a weekday. In the case of this analysis, 1 = weekday and 0 = weekend. Before tackling any of the questions directly, let‚Äôs begin by constructing a visualization of the data on hand. This is done rather easily by executing the following code block to create a simple scatterplot of daytime energy consumption as a function of temperature, faceted by day classification: This yields the following plot: Rather unsurprisingly, a positive trend can be seen between peak energy demand and the measured temperature. We‚Äôll delve into this relationship further in a moment. For now, though, let‚Äôs tackle the first question. To answer the question of what extent daytime power consumption changes between weekdays and weekends, fitting a simple multiple regression model can be useful. This can be constructed in R like so: As you can see here, I included three main predictor terms for this model: Upon execution, the following results are given: This allows us to express the relationship between these variables through the following linear equation: Energy Demand = -569.919 + 512.214*temp + 962.098*weekday + 0.461*temp:weekday While this model is fine and dandy for simple estimation, it is highly prone to wide error bars. To overcome this, I used a simple bootstrap with resampled load_summer data, and calculated a confidence interval for each parameter based on these results: *Note: R can sometimes be finicky when attempting to bootstrap a multiple regression model. If you receive strange error messages upon running this code, you can simply run the confint() function directly on the original linear model. Your confidence intervals will be a bit wider, but in the same neighborhood. After running this, we can view a 95% confidence interval for each model parameter in the console: Here‚Äôs the output if you run R‚Äôs confint() function directly on the non-bootstrapped linear model. As mentioned earlier, the upper and lower bounds of this simplified method will be slightly off of the estimates provided by the bootstrapped model. For the purposes of this analysis however, either will work: From these results alone, we can say that daytime power consumption is expected to increase anywhere from ~422 MW to 1487 MW on weekdays versus weekends ‚Äî holding all other variables constant. This makes sense given the geography of the region and time of year. While other areas of the country may see higher energy demand over the weekend as individuals spend more time at home, Houston is hot. Both offices and households alike have to spend large amounts of money every month on air conditioning, so it makes sense that energy consumption would be higher during the week when offices across the city are attempting to provide employees some relief from the summer sun. But is there also a correlation between observed temperature and energy consumption? And if so, does this relationship also differ between weekends and weekdays? Let‚Äôs shift focus to the second question and figure that out. If you look at the visualization included at the beginning of this analysis, a positive correlation between these two variables is rather obvious. Again, this makes logical sense ‚Äî energy consumption is bound to increase as temperature does, due to higher use of indoor climate control systems. After once again viewing the output from our bootstrapped confidence intervals, it looks like peak demand can be expected to increase anywhere from ~496.896 MW to 527.219 MW for a one degree increase in temperature, on average. The easiest way to assess whether this relationship differs between weekends and weekdays is to run a simple analysis of variance (ANOVA). This can be done in R by applying the simple_anova() function to our original model: Executing this code block will yield the following results: *Note: ANOVA tables are inherently subject to the ordering of variables within them, and should be read as such. The ANOVA table above is not the ANOVA table for this model, but rather a single version of it. Fortunately, the joint effect we‚Äôre interested here doesn‚Äôt suffer as a result of any ordering variance. However, in the event that you find yourself running an ANOVA on a more complex regression model in the future, definitely keep this in mind. After reviewing these results, the addition of the interaction term doesn‚Äôt appear to have much of an improvement on model performance (there is no measurable R2improvement out to three decimal places at least). This is rather sensible. The effect that temperature has on energy consumption isn‚Äôt going to be altered by day classification, so it stands to reason the confidence interval quoted earlier would apply to both weekends and weekdays. This analysis offers some great insight into the energy consumption patterns of the Texas gulf coast region. However, there are a few shortcomings I‚Äôd like to point out. First and foremost, the confidence intervals for even the bootstrapped linear model I constructed are quite wide. Due to the large natural variance in energy consumption due to surprise weather events and grid outages, this is hardly surprising. As such, the numerical estimates quoted for use in the above model should be treated as just that ‚Äî estimates. Further precision would require breaking down consumption patterns into more specific categories. I personally think it would be interesting to combine the data we have on hand with weather reports from the same time frame. This would allow us to build a more practically useful multiple regression model, as the gulf coast region is more prone to drastic weather changes than other parts of the state. Additionally, the dataset used for this analysis only spans the three-month summer period of June to August. This means any assertions made from these results are only truly applicable to the summer season. It would be unwise to draw any conclusion about energy consumption habits in November or December from the data used here. Similarly, it is not advisable to draw conclusions about the consumption habits of the state as a whole from this analysis. Our data is limited by location and timeframe, therefore our conclusions are too. Nonetheless, this analysis was a quick and fun puzzle to solve using R this past semester. The language‚Äôs built in lm() function makes constructing regression models as simple as writing a few lines of code, and the ggplot2 library allows for quick and easy production of clean visualizations. Modern computing power makes previously complex analysis easy enough for anyone to do in just a few minutes. I‚Äôd encourage you all to check out ERCOT data for yourself and see what you can do with it ‚Äî you might just find something interesting!",16,0,7,Towards Data Science,2021-01-03,2021
https://towardsdatascience.com/how-to-get-started-with-data-analytics-just-in-a-few-minutes-2a06fe5164d4,How to get started with Data Analytics in just a few¬†Minutes,Quick Start with Python and¬†Jupyter‚Ä¶,1,18,['How to get started with Data Analytics in just a few Minutes'],"If you want to learn Python and are interested in topics like data analytics or science you might start right here. All you need is a browser and a few minutes. You can try Jupyter Notebook Online via browser. Just visit the Website and start click Try JupyterLab. For this short Demo, the online version will provide everything one needs. Later, you might download JypterLab or install it via Anaconda distribution. Anaconda is a Python prepackaged distribution of Python, which contains a number of Python modules and packages. For bigger projects I really would recommend working with a stable environment installed on your laptop or via cloud to create, save and share notebooks with your colleagues. Python is used for many applications like websites or game development. Furthermore, it is also very popular for data science and analytic tasks. It was invented from Guido Van Rossum. Some facts about Python are: For data analytics tasks, you can use pandas which is a program library for the Python programming language, that provides tools for managing and analyzing data. Your first project will be to fetch data from an API. Here, I often use these libraries: First Step: Import Libraries and get the Data Step 2: Load the Data into a Pandas Data Frame After requesting the data from API in a JSON format (as seen in the table above) the data was loaded into a pandas data frame: Step 3: Visualize the Data From here, you can easily work with the data and visualize it with the help of many libraries for visualization ‚Äî read more in this article. Et voil√†, we have a visualization of the data: You don‚Äôt need much to learn Python and starting your data analytics projects ‚Äî it can be done with only a browser. Python, Jupyter Notebook and the wide variety of libraries are a great toolset. To dive deeper, you can use the links below. For a brighter understanding, you can of course do a paid online course but I think just starting with some coding is the best way to get into the world of analytics. Here is a great platform to learn it for free. jupyter.org wiki.python.org pandas.pydata.org towardsdatascience.com www.learnpython.org",27,0,3,Towards Data Science,2021-01-03,2021
https://towardsdatascience.com/the-conjuring-universe-my-2-day-data-visualisation-project-556dde6e98c5,The Conjuring Universe: My 2-day Data Visualisation Project,How the project has taught me to¬†start‚Ä¶,7,33,"['The Conjuring Universe: My 2-day Data Visualisation Project', 'Just start with something my sister and I love', 'Set a tight deadline', 'Focus on what‚Äôs truly important', 'Let user stories define the purpose of communication', 'Embrace the good, the bad and the ugly', 'Parting Thoughts']","Show me the full data visualization What I was doing for my personal data projects wasn‚Äôt working. I started something, worked on it for a while, got stuck, then abandoned it when a more exciting idea comes along. During the last week of 2020, after taking a hard look at my half-done projects, it occurred to me that something has to change. So I started reflecting on my past experience, reading through a couple of productivity books and reviewing my favourite project deliverables. Here are 2 things I realised. If I can hit the F5 key to refresh, what can I do differently? Here is what I did. Without leaving myself enough time to ponder (and potentially suffer from analysis paralysis), I decided to start with a topic that my sister and I both love. Why did I think about my sister? Because she has always been my best friend and anchor through thick and thin. If you‚Äôre having a mental block or self-doubt, then thinking about the people you love and creating something fun for them will definitely spark joy and motivation. This is also the most important message that I hope you should take away from this post. A few ideas came up but on a quiet cooling night and being home alone, nothing can beat those spine-chilling horror films. Hmm, there are plenty, but which one? Well, no doubt it has to be The Conjuring Universe, a horror franchise that both my sister and I love the most. What‚Äôs not to love if I can share a few interesting facts from our favourite horror series with my sister? Yay! Now I got a topic, but what can I do with it? Well, let‚Äôs Google and see what data is available. Wikipedia was the first stop and there are a couple of interesting tables showing all movies and characters of the franchise (Annabelle and Valak, anyone still remembers?), their total worldwide gross, ratings and the chronological timeline of events. Those are great starting points for simple analysis for commercial success and public response. I also came across several pages and it seems people are interested in how each film is tied in with the frightening universe of devils and ghosts. This would be great to add some interesting fun facts too. As soon as I browsed through the content, an idea slowly formed. A dark creepy data storyboard introduces facts about The Conjuring Universe. I know that‚Äôs something my sister would love to see. Knowing that I work better when I am rushing for time, I set the goal to gather and analyse data, then design and build a dashboard within 2 days. My instinct told me that a 2-day duration would be just nice to stay motivated while feeling the pressure to be productive. Here is my plan of attack. Well, that sounds like everything I need to get started, or so I thought. But if I am sticking to my old habits, doing the same thing over and over again, how can I expect a different result? This time, I need a new approach. I appreciate the fact that I am self-critical. Not the best trait to have, I know. This has blessed me with the drive to upholding some decent standards, yet it also occasionally triggers worries from countless ‚Äòwhat if‚Äô scenarios. And I know it too well that if I don‚Äôt find a way to cope with my self-criticism, my 2-day project will quickly become a 2-week marathon. But how? Firstly, I acknowledged that having only 2 days means I have to seriously think about the practicality of things. In short, keep it simple, stupid! Secondly, I have to set ground rules to make peace with my self-critical personality. Being inspired by The Agile Manifesto, I have written the below 6 ground rules to ensure my focus on what‚Äôs truly important. Throughout the 2-day duration, these bullet points have been extremely helpful in guiding my decisions and helping me to stay on track. Guess what? I think they are reusable for future projects too. Confession time: I am a fan of user stories. No, I am not talking about long-winded product reviews. What I mean is several short and sweet sentence following a general structure like ‚ÄúAs a [end user role], I want [ability or feature of the product] because [of whatever benefits I can gain or hassle I can avoid].‚Äù You may usually find user stories being used to capture functionalities for software products. But I personally find user stories extremely effective to make sure users come first in whatever I am working on. Here is what I wrote for The Conjuring Universe storyboard. Oh, wait! Why bother writing stories when I didn‚Äôt have enough time, to begin with? During development, these user stories help me to clarify what the audience wants so that I don‚Äôt waste time on gathering irrelevant data or building non-value added features. Just gotta be brutally focused on the main goals since time is ticking, don‚Äôt you agree? What‚Äôs more? During testing and review, the stories help me validate whether my storyboard is completed and working as expected. I reckon 20 minutes for spamming these user stories and wrapping my head around the purpose of my visual communication were truly well spent. If you would like to know more about how people have been writing user stories for software development, check out this awesome article. Lazy to read? Don‚Äôt fret, here are my 3 quick tips for everyone on user stories. So far, things seem to be too good to be true, or so you thought. But let me share some insights about what went wrong and how I managed to pull myself out of the mess. Mistakes shouldn‚Äôt happen more than once, so hopefully, you can avoid making the same mistakes that I was guilty. I started my first sketch by looking at the data and doodling on a blank piece of paper. Nothing wrong with that if you are genuinely creative and good at drawing. But I know I don‚Äôt. Unsurprisingly, 30 minutes passed and all I have was some weird boxes with no concrete idea. Only 30 minutes left until I ate into my data collection time. Red flag! It‚Äôs time for Plan B. How about taking a look at what other people are visualizing for movies? So I went to look at Tableau Public Gallery, focusing on the Film and TV section. A world of possibilities suddenly opened up to me. Since not much time left, I just gotta find a shortcut. So I took screenshots of those designs I like and placed them into a common depository. Below is what I did in Notion, my trusted all-in-one workspace. But you might want to open a PowerPoint blank slide and start pasting some pictures inside. Can‚Äôt find any nice design? Well, then start with simple boxes stating the content you want to present. The sketch is meant to guide what data you need to collect, but the data you can collect will determine how the final charts will look like. So stick to the timing and don‚Äôt get hung up on the small details. When it comes to sketching, I prefer electronic format over paper and pen because I can easily edit, move things around and add new things along the way. But you might think otherwise and that‚Äôs totally fine. The lesson here is to remember to seek help and inspiration from others instead of forcing myself to reinvent the wheel and create some magic from thin air. From the get-go, web scraping has been one of the skills I want to practise in this project. So I happily dived right in, wrote a whole bunch of Python scripts to extract data from several web pages, spent a lot more time to debug and test my scripts to make sure that they work. But wait! Would manual copy and paste work faster? Sadly, yes! With several tables containing simple statistics about 7 movies across different websites, a bunch of characters and production team members, it turned out that I can just copy and paste it into an Excel file, clean up the details a little bit and my dataset is ready for action. Fortunately, being conscious of the tight 2-day duration once again caught me at the right moment. At the rate I was working with, I wouldn‚Äôt be able to complete my data collection within the first day. Remember the 6 ground rules I wrote before? Results over utilising fancy techniques. And that‚Äôs how it had saved me from unnecessarily wasting more time. I have to admit that fiddling with the colour, the font, font size and trying to maintain a consistent format across different pages was annoying. Various options of customisation in Tableau also added to my confusion sometimes. I regretted having to learn the hard way on how to properly format a Tableau dashboard. But hey, it‚Äôs better late than never, right? Here are a couple of time-saving tips that might come in handy for you. Initial setup Create individual charts Consolidate the storyboard Finish the final touches Just in case you are wondering what the end result looks like after 2 days of development (plus a few silly formatting issues resolved after that), feel free to check out the final product here. It‚Äôs difficult to maintain great motivation for personal projects because you simply don‚Äôt get paid for it. But if I can learn so much from a 2-day project, so can you. Personally, I choose to believe that all learning, grit, passion and perseverance is building up to something. Taken individually, each tiny project might not feel super important to me. Yet, I realised I have picked up a ton of skills, which enabled me to take on bigger projects with greater confidence. If you are feeling stuck or think you have lost the motivation to try something new, here are my 4 simple takeaways. Hope they would shed some light and bring back the mojo when the going gets tough. With that, I wish everyone a healthy, happy and amazing 2021. With lots of love! Originally published at http://thedigitalskye.com on January 3, 2021.",46,0,10,Towards Data Science,2021-01-03,2021
https://towardsdatascience.com/the-first-90-days-in-my-first-data-science-role-most-important-lessons-retrospective-821f6baf746,The first 90 days in my first data science role: Most important lessons retrospective,,5,32,"['The First 90 Days in My First Data Science Role', 'Learning the business logic is necessary to even begin contributing', 'Try things and ask lots of questions', 'Translate algorithms to plain English words', 'Conclusion']","I previously wrote about how I first entered the data science field. This is a follow up article on what happens next: how one can make the most of one‚Äôs new role, and hit the ground running. Inspired by some questions from my mentoring sessions on this topic, I outline examples of (sometimes unexpected) things that helped me at the time. Hindsight is 20/20. (Happy 2021, by the way!) When I started my first data scientist role, I was excited yet nervous. I didn‚Äôt find Python coding to be a potential issue, since I had already worked with it in game development, and had passed the interviews which involved Python, after all‚Ä¶ Medium articles: How I entered the data science field ( part 1) ( part 2) The thing is, to begin providing any value as a data scientist, I had to figure out how to access relevant data, navigate the data warehouse, and join different tables to build relevant datasets, in the first place. It‚Äôs not like school or Kaggle, with some pre-built dataset, or my student web-scraping projects where I controlled all the logic. For the most part we had to construct the data we needed, from what was available in the very vast data warehouse. Big thanks to the data engineering team for ensuring that it existed in the first place. In the first few weeks I was pretty much dead weight while I learned these things, bombarding my onboarding buddy with questions such as: Even months (and years) later, I was still learning new bits and pieces of logic behind the data. For example, knowing where to find a internal Wiki page about data outages to explain why there were less records for some month in some table. This process was something I hadn‚Äôt expected at the time as a brand new data scientist. Once again, I was reminded that in the real world, the data isn‚Äôt pre-made like the Iris or Titanic. Neither was it like my small scale web-scraped datasets in my student days. I am grateful for the onboarding process at the time for pairing me up with a buddy for my first modeling project, as well as amazingly patient coworkers. In terms of what this means for teams, is that providing this contextual knowledge is a necessary investment so that the new hire gets up to speed and starts contributing sooner. To my past self, it was eye opening knowing that ‚Äúhey, I can code! I know stats!‚Äù doesn‚Äôt cut it at all as a data scientist. In subsequent roles, I once again had to go through this process of learning business meaning before providing much value in terms of data science. And now, as my perspective starts flipping to being one that provides this contextual knowledge, I keep in mind that it‚Äôs all worth it. This rule applies not only in the first 90 days, but forever in one‚Äôs career ‚Äî but the earlier one realizes this, the better. One behavior that was essential for me to start contributing to projects was asking questions. I know what it‚Äôs like to feel embarrassed for asking a ‚Äúsilly‚Äù question, but in order for the team to deliver, it‚Äôs more productive to simply set aside those thoughts, as it benefits‚Ä¶ no one. With the amount of business logic to learn, no experienced person on the team would expect a new hire to know it all. As it is, it‚Äôs likely an experienced person is still learning as well, as I mention in the previous section. A rule of thumb I use is to at least try some things out before asking. People don‚Äôt like it when someone doesn‚Äôt first spend any effort at all to find a solution, but the point is they will understand if you‚Äôve tried and didn‚Äôt find or recall the answer. It‚Äôs much more productive to ask ‚ÄúHey, what‚Äôs the table name for Ontario [line of business]?‚Äù when you can also say ‚ÄúI‚Äôve tried looking at the Wiki with table names for Quebec, and tried guessing the Ontario table name based on the naming convention. But it didn‚Äôt show up.‚Äù (Purely example: the data warehouse at that role had 1000s of tables, so by selecting all the table names I might not have gotten anywhere. And as a newbie I didn‚Äôt know how to do that.) This isn‚Äôt just lip service. As a new hire, if I at least try to use resources at my disposal, even if it‚Äôs low hanging fruit, it helps me practice getting information on my own in the future. This is quite valuable, and builds independence. The key here isn‚Äôt to spend all day trying to figure something out with the limited knowledge at the top of my head, which probably isn‚Äôt comprehensive anyway, as a new hire. I tend to box the time I can spend on figuring something out, before simply reaching out to ask. The time-box varies: for trivial things like table names, I might try a quick search in the database, and for code I might use a little more time to try a few Stack Overflow solutions. Having attempted a few approaches also saves the time of the person I ask: perhaps they would also suggest the first solution that came to mind, which I would already know doesn‚Äôt work since I had tried it. This helps the coworker being asked to brainstorm other possible solutions apart from the most obvious, which is a better use of problem solving time. All in all, trying things and asking questions to unblock oneself is important to be an effective developer. Even as a senior developer I have no qualms about asking ‚Äúsilly‚Äù questions, after attempting some approaches; it‚Äôs not embarrassing most of the time since asking helps make the project progress faster, in the grand scheme of things. Don‚Äôt be the person walking around the office mumbling things like ‚ÄúI did ALS (Alternating least squares)‚Äù. Let me explain what this means‚Ä¶ We as data scientists enjoy chatting about niche optimizations and academic journals at work, and that‚Äôs fine. In fact, the group I co-run (outside of work), Aggregate Intellect, invites researchers to speak about their work on machine learning at a highly technical level, and has 12k+ YouTube subscribers. But even within these groups that love nerd talk, I know that the best way in a workplace, where there are limited attention resources, to get someone to care about my work, is to describe these ‚Äúcool things‚Äù in plain English. To be frank, in my first 90 days as a data scientist, I was the person walking around mumbling algorithm names. It does take more than 90 days to cultivate the communication skills I mention here, but I note it in hopes that one can start developing those skills earlier! Along with an algorithm and what it does in the mathematical and computational sense, I‚Äôve learned to also mention what it‚Äôs actually doing in the business (e.g. recommending something on a website to increase user engagement). Otherwise, it‚Äôs easy for one‚Äôs project to fall into obscurity. I‚Äôve seen plenty of teams make POCs (proof of concepts) that end up abandoned, not because they might not be useful with some more effort, but because the communication about their impacts didn‚Äôt come across. I think this is one thing a lot of highly technical people make the mistake of doing. It‚Äôs invaluable to describe technical work in ways that help anyone see the value of having a person stationed on the project. Otherwise, it‚Äôs possible for one to be pulled off a project they hold dear, with the repository becoming a code graveyard. Of course, if the project doesn‚Äôt make sense, no amount of communication will help turn it around. The point being, if one has poor communication in terms of technical topics, it often causes unnecessary harm to their own projects, and impedes the ability to work on the better projects in the company, which in turn might impede career growth. It‚Äôs worth spending time on communication ‚Äî I‚Äôve been able to get on large stages internally and externally due to this, despite having relatively low tenure at the time. I‚Äôve written about a technique I use in this post about data science storytelling. As I continue learning in my career as a Principal data scientist, I‚Äôve been reflecting on what I‚Äôve experienced when I began this journey, especially what I (accidentally) did well. Hindsight is 20/20. I do have many more thoughts and observations on how to make the most of one‚Äôs data science learning, and will be gathering more material for a future article on this topic. I hope that some of these thoughts were helpful! As usual, you can find me on LinkedIn or hello@susanshu.com to discuss this post. Originally published at https://www.susanshu.com on January 3, 2021.",9,0,7,Towards Data Science,2021-01-03,2021
https://towardsdatascience.com/universal-approximation-theorem-code-refactoring-for-software-2-0-20d4bdc3cf48,Universal Approximation Theorem: Code Refactoring for Software¬†2.0?,A Thought Experiment,5,8,"['Universal Approximation Theorem: Code Refactoring for Software 2.0?', 'What is UAT?', 'A Lesson Learned', 'Going Deeper', 'Software 1.0 vs. Software 2.0']","What exactly is Universal Approximation Theorem? Well, put in layman‚Äôs terms, UAT just means that giving a one hidden layer neural network with enough neurons. It can approximate(or simulate closely) any continuous function within the given input range. It means that a one hidden layer neural network is an ultimate flexible function approximator. Maybe a little too flexible. Because of the flexibility, Universal Approximation Theorem used to push AI researchers to focus mostly on shallow neural networks, thus in some way hinders the development progress of deep learning. This is interesting. Come to think of it, a ‚Äòshallow and wide‚Äô neural net tends to ‚Äòremember‚Äô all features to approximate the target function. Yet deeper networks tend to be more abstract on feature extraction and finds out patterns that can apply to many parts of the dataset. They obviously generalize better. And achieve better results with less computational power. What does this sounds to you if you‚Äôre a software developer? ‚ÄòCode Refactoring‚Äô! Developers refactor their code to put repetitive code snippets into functions and reuse them as much as possible. Cleaner codes usually are better codes. Deep Neural Networks somewhat does the same thing. By having more layers, it enables the network to ‚Äòrefactor‚Äô themselves better and learn more general patterns, thus more efficient in achieving the same goal. This leads to better (both in performance and efficiency) models. What other software development techniques we can apply to machine learning? More precisely, what we learned from ‚ÄòSoftware 1.0‚Äô can be applied to ‚ÄòSoftware 2.0‚Äô? (If you are not familiar with the concept of ‚ÄòSoftware 2.0, I highly recommend you watch the below video from Andrej Karpathy, not entirely applicable to everything but definitely worth noting and backed by Tesla‚Äôs success!) According to Karpathy, what we currently do in software engineering where talented people write code to complete tasks and solve a problem is ‚ÄòSoftware 1.0‚Äô, where humans contribute to the process by directly telling the computer how to do every single step. In the new paradigm of ‚ÄòSoftware 2.0‚Äô where machine learning and deep learning is widely adopted, human contribute by providing a huge amount of examples of people doing something in the form of the dataset, and the computer along with models will figure out how to do that automatically. In fact, a good amount of Tesla‚Äôs Autopilot system is powered by deep learning models. Some people are still skeptical about whether there will be a bright future for the ‚ÄòSoftware 2.0‚Äô approach. Our path from 1.0 to 2.0 is still up for debate. Re-apply the wisdom from traditional software engineering might still be a good direction to explore for machine learning researchers and practitioners. Fun time! If you want to know more about Universal Approximation Theorem, you can refer to my article below: From Legoland to Neural Networks: Universal Approximation Theorem in Layman‚Äôs Terms | by Michael Li | Dec, 2020 | Towards Data Science",3,0,3,Towards Data Science,2021-01-03,2021
https://towardsdatascience.com/checking-for-dependency-using-binary-expansion-testing-bet-5234a818a18,Checking for dependency using Binary Expansion Testing¬†(BET),Are stars uniformly scattered in¬†the‚Ä¶,1,13,['Checking for dependency using Binary Expansion Testing (BET)'],"Are stars uniformly scattered in the night sky? Are galaxies uniformly distributed in the universe? Do T cells infiltrate all over a tumor for killing cancer cells? How about patches of vegetation on prairies? In many contexts, we want to determine if samples are drawn uniformly from the spaces of interest. Despite the question sounds simple enough, there isn‚Äôt a universal solution that fits all purposes. Recently, Kai Zhang from the University of North Carolina Chapel Hill introduced an elegant framework called Binary Expansion Testing (BET) for this purpose¬π. This post aims to provide an intuitive explanation to BET. Let‚Äôs assume the data points are distributed within a unit square. Then every point is specified by two random variables X and Y. To determine the uniformity, there are two questions one needs to address: 1. Do X and Y follow a uniform distribution? 2. Are X and Y statistically independent? BET is formulated for question 2, therefore let us focus on question 2 first and come back to question 1 afterward. Take a look at the real number x=0.375. Of course, as written in the usual decimal form, But in a binary numerical system, In general, any real number x such that 0‚â§x‚â§1 can be represented as a sequence {b·µ¢} with b·µ¢=0 or 1. It is important to realize that {b·µ¢} can be pictured as a sequence of binary decisions. The first term means the interval [0,1] is partitioned into 2 bins of size 1/2 and being asked if x is located at the left bin (b‚ÇÅ=0) or the right bin (b‚ÇÅ=1). As the answer is ‚Äúleft‚Äù, the second term means the interval [0,1/2] is then further partition into 2 bins of size 1/4 and being asked if x is located at the left bin (b‚ÇÇ=0) or the right bin (b‚ÇÇ=1). This time the answer is ‚Äúright‚Äù, it goes to the third term where the interval [1/4,1/2] is divided into 2 bins of size 1/8 and we have another decision tree b‚ÇÉ. Rather than focusing on how to map a real number to a sequence of binary numbers, binary expansion could be regarded as a way of using binary variables to partition the unit interval. In general, d binary variables can divide the unit interval into 2¬≤·µà bins and each of the binary sequences of length d corresponds to one of the bins. Given a random variable X drawn from [0.1], we can approximate X by d binary random variables, where d determines the bin size and thus the resolution. As it will be clear very soon, it is more convenient to replace b·µ¢ by a·µ¢=2b·µ¢-1. In other words, the binary variables take the values of ¬±1. Now comes the dependency of two random variables X and Y which are drawn from a unit interval. We pick a certain depth d and approximate X and Y by two sequences of binary random variables {X·µ¢} and {X‚±º} with i, j run from 1 to d, and X·µ¢ and X‚±º take the values of ¬±1. Recall d binary variables can partition the unit interval into 2·µà bins, two sets of such variables will partition the unit square into 2¬≤·µà small squares and a data point (X, Y) will fall upon one of the small squares. Nevertheless, the key to BET is not merely assigning individual data points to a grid. The goal of BET is to examine the uniformity of an ensemble of data points distributed on the grid. To achieve this goal, BET bisects the grid and counts the number of data points in each half. The bisection is achieved by a clever trick based on the binary variables. To illustrate this point, let‚Äôs consider the case d=2. The random variables X and Y are mapped to 4 binary variables X‚ÇÅ, X‚ÇÇ, Y‚ÇÅ, Y‚ÇÇ that divide the unit square into a 4-by-4 grid. There are many different ways to bisect the grid, for instance, by dividing it into a left half and a right half, or into an upper half and a lower half, or into a checkerboard. The key observation BET employed: all the possible bisections can be captured by X‚ÇÅ, X‚ÇÇ, Y‚ÇÅ, Y‚ÇÇ, and their combinations. For instance, the variable X‚ÇÅ divides the grid into a left half (X‚ÇÅ=-1, blue) and a right half (X‚ÇÅ=+1, white); the variable Y‚ÇÇ cuts the grid into horizontal bands (Y‚ÇÅ=-1, blue; Y‚ÇÅ=+1, white); and a checkerboard can be obtained by the combination X‚ÇÇY‚ÇÇ=¬±1 (this is actually why we prefer the values ¬±1 to 1 or 0). By enumerating all the possible choices of binary variables, it‚Äôs not hard to see Examples of such bisections are shown below. With all possible bisections enumerated, the inference of uniformity is simple. Essentially, given a bisection, we count the number of data points that fall onto blue and white regions, n=n·µá+n ∑. As the blue and white regions are equal in area, a lack of dependency between X and Y would result in n·µá‚âàn ∑. We can simply use the binomial distribution B(n,p=1/2) as the null and assign a P-value to the observation. As the number of bisections is exponentially large, the P-values should be adjusted for multiple hypotheses testing, say for instance, by multiplying the factor 2¬≤·µà-1. Let‚Äôs look at a concrete example from Kai Zhang‚Äôs manuscript. He and his colleagues have implemented BET in an R package¬≤. It is definitely worthy of a try. In the figure below. the red dots show the distribution of stars on the Celestial sphere, where X and Y are the two coordinate parameters. Using d=2, BET tested for all bisections and found that the bisection shown exhibited an asymmetry with a most significant q-value. BET, therefore, concluded that the stars are not uniformly distributed, which is, of course, the case since stars are densely populated toward the center of the Milky Way. So far in the post, we have outlined the intuition behind BET. A question is not clear: what‚Äôs the value of d? Kai Zhang argued an iterative approach in which one could increase d gradually. Of course, depending on the number of data points, d should not be too large otherwise a lot of bins are empty; also, a large d will cause an explosion in the number of hypotheses, and the asymmetry will not be significant. Finally, one might notice that the goal of BET is to show a lack of dependency between X and Y. If X and Y are regarded as spatial coordinates, the lack of dependency suggests the uniformity of data points within the unit square. Nevertheless, is the opposite always true? Whether or not the two concepts are the same is important because the binning and counting procedures employed by BET show merely uniformity. We will examine the subtle differences and the way BET resolves them in the next post. Reference:",97,1,6,Towards Data Science,2021-01-03,2021
https://towardsdatascience.com/how-to-code-the-value-iteration-algorithm-for-reinforcement-learning-8fb806e117d1,How To Code The Value Iteration Algorithm For Reinforcement Learning,Solving MDPs using reinforcement‚Ä¶,1,34,['How To Code The Value Iteration Algorithm For Reinforcement Learning'],"In this article, I will show you how to implement the value iteration algorithm to solve a Markov Decision Process (MDP). It is one of the first algorithm you should learn when getting into reinforcement learning and artifical intelligence. Reinforcement learning is an area of Machine Learning that focuses on having an agent learn how to behave/act in a specific environment. MDPs are simply meant to be the framework of the problem, the environment itself. MDPs are composed of 5 elements. The goal of the agent in a MDP is to find the optimal policy, which is the set of optimal actions to take at any given state. The easiest way to understand all this is with an example. Let‚Äôs say we have a bear (the agent) that wants to get to the honey (the positive reward) while avoiding the bees around it (negative rewards). At every cell (or state), the bear can take an action. For instance, from his actual position, he could go down or right. Finally, we have transition probabilities. Sometimes, the bear will intend to do something but it will do something else instead (go down instead of left for instance). It makes it more ‚Äúrisky‚Äù to be closer to the bees, as a mistep can mean negative rewards. Now, in order to find the best way to get to the honey, the bear will walk in the environment. He might walk into the bees, get stung, and learn to avoid this state. Eventually, with time, he will figure out how to behave in any given state, to get to the honey without consequences! Smart bear! But how will the bear learn this? The value iteration algorithm is one of the most common algorithm to solve this problem. Let‚Äôs get into it. The algorithm tries to find the value V(st) of being in any given state. It uses the Bellman equation. Don‚Äôt worry, it really isn‚Äôt that complicated. All this means is that the value of being in a state is equal to the maximum of the immediate reward of that state (R) plus the discounted rewards of every adjacent state (St+1), considering the transition probabilities. Therefore, we only look one step ahead in this algorithm. It is an iterative algorithm, where the values for the states keep getting updated until convergence, i.e. when the values stop changing. At the end, the optimal policy is implicit, as it will be the action that gives the best value for every state. Let‚Äôs now get to the fun part! Below is the first part, which represents the initial set up/creation of the MDP (the states, actions, rewards, etc.). The SMALL_ENOUGH variable is there to decide at which point we feel comfortable stopping the algorithm. Noise represents the probability of doing a random action rather than the one intended. In lines 13‚Äì16, we create the states. In lines 19‚Äì28, we create all the rewards for the states. Those will be of +1 for the state with the honey, of -1 for states with bees and of 0 for all other states. Then, we create a dictionary containing all possible actions for any state. Every state has between 2 to 4 possible actions (Down, Up, Left or Right). Finally, we define an initial random policy for every state as well as a value dictionary, containing initially the rewards of every state. This last dictionary is what will be updated during the algorithm. Below is the value iteration algorithm. As can be observed in lines 8 and 14, we loop through every state and through every action in each state. In lines 25‚Äì33, we choose a random action that will be done instead of the intended one 10% of the time. This adds uncertainty to the problem, makes it non-deterministic. Typically, in value iteration, you could have more than one possible random action (to consider all the adjacent states) or even much higher noise. Here however, to keep it simple and ensure the algorithm runs fast, I only chose one possible random action with a relatively small probability of occurence for every state. At line 38, we calculate the value of taking an action in a state. In line 40-41, we save the action associated with the best value, which will give us our optimal policy. Finally, in line 48, the algorithm is stopped if the biggest improvement observed in all the states during the iteration is deemed too small. This super small example converges in 44649 iterations. Here are the values associated with every state. We see that the closer we get to the final reward, the higher the value of being in that state is. We also see that being in state (2,1) has a smaller value (0.259) than being further away, in state (2,0), which has a value of 0.338. That is because when being closer to the negative rewards, the bear could mistep and hit the bees. Therefore, in this scenario, it‚Äôs actually better to be further away from those states even if it means also being further away from the honey! From those values, we can derive the optimal policy. Obviously, the example I used was very simple so it was easy to figure out the optimal policy just by looking at the environment. However, the bigger the environment gets, the harder it is for us humans to figure out the best course of action for every state. For computers, it‚Äôs not. The bear, who previously had no idea what to do, has now learned how to behave in order to get to the honey while avoiding the bees. Good for him! The following repository contains all the code used for this project: https://github.com/francoisstamant/reinforcement-learning-mdp Thanks a lot for reading, I hope you learned something and feel free to reach out if you have any questions! Become a member: https://francoisstamant.medium.com/membership",200,2,5,Towards Data Science,2021-01-04,2021
https://towardsdatascience.com/hierarchical-linear-modeling-a-step-by-step-guide-424b486ac6a3,Hierarchical Linear Modeling: A Step by Step¬†Guide,"In most cases, data tends to be clustered‚Ä¶",4,82,"['Hierarchical Linear Modeling: A Step by Step Guide', 'Case Study', 'Pre-Analysis Steps', 'HLM Analysis Steps']","In most cases, data tends to be clustered. Hierarchical Linear Modeling (HLM) enables you to explore and understand your data and decreases Type I error rates. This tutorial uses R to demonstrate the basic steps of HLM in social science research. Before beginning the analysis, let‚Äôs briefly talk about what is HLM first. HLM (AKA multilevel modeling) analyzes data that is clustered in an organized pattern(s), such as universities in states, non-white males in tech companies, and clinics in hospitals. HLM is an ordinary least square (OLS) that requires all assumptions met (check out my tutorial for OLS assumption and data screening) except the independence of errors assumption. The assumption is likely violated as HLM allows data across clusters to be correlated. Predictors in HLM can be categorized into random and fixed effects. Random effects refer to variables that are not the main focus of a study but may impact the dependent variable and therefore needed to be included in the model. Fixed effects, on the other hand, are key predictors of the study. For example, a psychologist wants to predict the impact of adverse childhood trauma on one‚Äôs tendency to develop borderline personality disorder (BPD) in adulthood. Participants are from collectivist and individualistic cultures, and both cultures likely define parents‚Äô behaviors differently. People in individualistic cultures, such as those in America or the U.K., probably consider parents‚Äô spanking abusive, whereas collectivist individuals, such as Asians and Africans, may consider spanking as a way to enhance a child‚Äôs discipline. Thus, participants from different cultures may be variously impacted by the same behavior from their parents during childhood and may develop BPD symptoms at a different level. According to the example, childhood trauma is treated as the fixed effects based on personality literature and the researcher‚Äôs interest as a psychologist. Cultures could be treated as random effects as the variable potentially impact borderline personality development but not the main focus of the study. It is noteworthy that random effects should be categorical, whereas fixed effects could be dummy variables (a categorical variable with two levels) or continuous variables. Some of you may think, why don‚Äôt we use a single level regression model and control for potential random effects (e.g., cultures according to the mentioned example)? Doing so may introduce wrong standard error estimates as residuals (i.e., observations in the same group) tend to be correlated. For instance, people from the same culture may view a behavior in the same way. A single-level model‚Äôs error term represents clustered data errors across levels, limiting us from knowing how much effects that the key predictor (e.g., childhood trauma) has on one‚Äôs tendency to develop BPD after controlling for cultures in which participants are nested. Still confused? Let‚Äôs look at the equations below: yi = Œ≤0 + Œ≤1xi +ei A single regression model ‚Äî (1) yij = Œ≤0 + uj + eij A variance components model ‚Äî (2) yij = Œ≤0 + Œ≤1xij + uj + eij A mixed model (with random intercepts) ‚Äî (3) i is the number of observation (e.g., participant #1, #2, #3..). j is the category of culture that each observation belongs to (i.e., j = 1 is collectivism, and j = 0 is individualism). Œ≤1 is adverse childhood trauma y is BPD tendency. u is variance in y that is not explained by cultures, controlling for other predictors. e is variance in y that is not explained by childhood trauma controlling for other predictors. According to equation 1, the error term (ei) indicates an unexplained variance of the outcome that is not accounted for by the key independent variable (e.g., childhood trauma). Equation 2 shows two error terms, including the error term of the random effects (uj) (i.e., cultures) and the error term of the fixed effects nested in the random effects (eij) (childhood trauma scores in different cultures). Equation 3 represents a mixed model that integrates equations 1 and 2, accounting for more accurate error estimates relative to the single-level regression model in equation 1. Now that you have some foundation of HLM let‚Äôs see what you need before the analysis. A fictional data set is used for this tutorial. We will look at whether one‚Äôs narcissism predicts their intimate relationship satisfaction, assuming that narcissistic symptoms (e.g., self absorb, lying, a lack of empathy) vary across times in which different life events occur. Thus, fixed effects are narcissistic personality disorder symptoms (NPD). The outcome variable is one‚Äôs intimate relationship satisfaction (Satisfaction). The random effects are Time with three levels coded as 1 (before marriage), 2 (1 year after marriage), and 3 (5 years after marriage). Step 1: Import data Step 2: Data cleaning This tutorial assumes that your data has been cleaned. Check out my data preparation tutorial if you would like to learn more about cleaning your data. For my current data set, all of the assumptions, except the independence of errors, are met, consistent with the HLM requirement. Step 1:An intercept only model. An intercept only model is the simplest form of HLM and recommended as the first step before adding any other predictive terms. This type of model testing allows us to understand whether the outcome variable scores (i.e., relationship satisfaction in this tutorial) are significantly different from zero (i.e., participants have indicated certain relationship satisfaction levels) without considering other predictors. For an OLS model, an intercept is also known as the constant, which in an intercept only model is the mean of the outcome variable, as shown in the below equation: We will use the gls function (i.e., generalized least squares) to fit a linear model. The gls function enables errors to be correlated and to have heterogeneous variances, which are likely the case for clustered data. I will identify my intercept only model as ‚Äòmodel1.‚Äô Here are the results: The p-value is significant, indicating that participants‚Äô relationship satisfaction is significantly different from zero. Step 2: A random intercept model. This step added my random effects (i.e., Time) to see whether the predictor increases a significant variance explained in my dependent variable relative to the previous intercept only model (Model 1). Statistically speaking, if you still remember the earlier equations, the intercept for the overall regression of an intercept only model is still Œ≤0. However, for each group of random effects(i.e., each point of Time after marriage), the intercept is Œ≤0+uj (when uj represents errors of the dependent variable that are not explained by Time). To test the random intercept model, I will use the lme function as an alternative approach in addition to the mentioned gls function. Like gls, the lme function is used to test a linear mixed-effects model, allowing nested random effects and the correlations among within-group errors. Both lme and gls enable the maximum likelihood application. Before including Time as random effects, make sure that the variable is categorical: The output says ‚Äòfalse,‚Äô so I need to convert Time into a categorical variable. Modeling the random intercept: The results: Now, you may wonder how I could know whether my random effects (i.e., Time) are significant. There are a couple of ways to look at this. From my model 1‚Äôs and 2‚Äôs outputs, you will see that model 1‚Äôs AIC = 6543.89, and Model 2‚Äôs AIC = 6533.549. Generally, the two AIC values that differ more than 2 indicate a significant difference in model fitting. The lower the AIC value is, the better fit a model. You can see that including Time as random effects in Model 2 improves my Model 1 (6543.89 -6533.549 > 2). 2. In addition to AIC, we can compare the intercept only model and the random intercept using the ANOVA function. Here are the results: The p-value, 4e-04, is equal to 4 x 10^-4, indicating that the results are highly significant. Adding the random intercept thus significantly improves the intercept only model. In addition to the gls and lme functions from the package nlme, we can use lmer from package lme4. In general, both lme and lmer are effective functions for mixed data analysis with some differences to be considered: To put it simply, I would say for a simple HLM analysis, both lme4 and nlme should provide close parameter values. You may check out this page for comparisons of the packages. If you want to try lme4, you need to install merTools first: Let‚Äôs run our random intercept model using lmer from lme4 Results: You can see that the parameters of model 2 (lme4) and model 2.1 (nlme) are quite close. We can also run an ICC (AKA Intraclass Correlation Coefficient) to see the correlation of observations within groups (i.e., relationship satisfaction within each Time point in my case). The ICC index can range from 0 to 1, with more values indicate higher homogeneity within groups (Gelman & Hill, 2007). You can see that my ICC value is approximately .01, indicating that the relationship satisfaction of participants nested within a point of Time is quite different from each other. Before moving to the next HLM analysis step, I want to make sure that my fixed effects regression coefficient is accurate. To do so, I will request a 95% confidence interval (CI) using confint. If you are not familiar with a CI, the term refers to a range of values that may include the true population parameter with a certain range of percent confidence (mostly 95%). The formula is x bar is the sample mean z is confidence level value n is sample size s is sample SD A CI, let‚Äôs say at 95%, contains two endpoints. We may set a lower 1% limit, meaning that the probability that the true population parameter is below the 1% limit of our data scores is only 1%. We may also set an upper 96% limit, meaning that the probability that the true population parameter is beyond the 96% limit of our data scores is only 4%. The upper and lower limits together indicate that an interval or the probability that we will find the true population parameter out of the range that we set (1% ‚Äî 96%) is 5% (1% + 4%). So we have a 95% confidence interval that the true parameter will be in the upper and lower limit range of our sample. If you want to learn more about CI and its relation to t-distribution, check out this link. Now, confidence levels are different from a confidence interval. If we re-run a study several times and estimate a parameter of interest with a 95% CI, we will get different 95% CI values each Time due to errors in our data that could be caused by several factors, such as participants‚Äô factors, measurement errors, our moods during each analysis. However, 95% of those different CI values will cover the true parameter value, and this concept is confidence levels. If we set a lower limit of our confidence levels at 1%, it means that out of many experiments that we conduct repeatedly, the true parameter value will be lower than this 1% limit in only 1% of those many experiments. If we set an upper 96% limit, the probability that we will find the true parameter value higher than the upper limit is 4% of several experiments that we repeatedly conduct. As humans like symmetrical things, people often set a 95% CI as a lower 2.5% limit and an upper 97.5% limit. The true population parameter value will be below the interval in 2.5% of repeated studies and above it in another 2.5% of those studies. Thus, the confidence levels will cover the true parameter in 95% of all conducted studies. Let‚Äôs get back to our example. If I want to know the confidence levels of model 2.1, I will use the following code. Results: The results indicate that if I re-rerun my study several times, 95% of the times, the intercept coefficient (i.e., the true mean of relationship satisfaction in population considering the random effects of Time) would be somewhere between 4.98‚Äì5.21 approximately. Step 3: Fixed effects in the random intercept model As I am mainly interested in the NPD‚Äôs fixed effects, I will include the predictor in my random intercept model (model 2 or model 2.1). I still let the intercept vary, meaning that each point of Time may have different intercepts of relationship satisfaction scores. To generate fixed effects in the random intercept model, I will use lme() from the nlme package. Results: The fixed effects are significant. Let‚Äôs compare whether the random intercept model with fixed effects (Model 3) is better than the random intercept model (Model 2). Results: The results show a significant difference across the two models, indicating that adding fixed effects significantly improved the random intercept model. An alternative for model fitting in Step 3 is to use the lmer function: Results: You see that the parameter estimates are quite close across the lme and lmer functions. Step 4: Adding a random slope term. In HLM, adding random slopes allow regression lines across groups of random effects to vary in terms of slope coefficients. In my case, the slopes between one‚Äôs NPD and the outcome (relationship satisfaction) across different levels of Time could vary as people‚Äôs NPD symptoms may be weakened or strengthened across Time points, depending on their life events. To test the assumption, I will nest NPD traits in Time and allow the slopes of NPD and relationship satisfaction to vary across different Time levels. Results: The output suggests that the variation in the intercept of Time is fitted with a larger SD of 0.0724. The variation in NPD slopes in predicting relationship satisfaction is fitted with a smaller SD of 0.0024. The results indicate that participants‚Äô relationship satisfaction likely differs across levels of Time more than the severity of NPD symptoms within each point of Time. A weak positive correlation (Corr; r=0.131) between the intercept of Time and the NPD slope means that a more positive value of the intercept is slightly related to a more positive value of the slope. If participants‚Äô intercepts increase by one unit of SD, the slopes will only increase by 0.131 SDs. In other words, the intercept of relationship satisfaction obviously differs across Time, whereas a variation in the slope of the correlation between NPD and relationship satisfaction is subtler. Thus, it is highly likely that Model 4 (adding the random slope term) does not significantly improve Model 3(the random intercept model). Let‚Äôs test the assumption. Results: As expected, adding the random slope term does not significantly improve the random intercept model and increased the AIC value (i.e., worse fit). To exclude the random slope term or not depends on several factors, such as theories that inform your data, whether excluding or including the random slope makes the models converge, and whether you would like to get a parsimonious or maximal model. It all depends on your decision and field of study. This article provides additional detail about random effects that are worth reading. Additional steps: If you have an interaction term, you may test whether adding the term improves your model. I will test whether adding borderline personality disorder traits (BPD), which are highly comorbid with NPD, as a moderator will improve my random intercept model (model 3). I choose to ignore the random slope model (model4) as the term does not improve the model, and studies argue that NPD traits may not change across Time points. Results: The interaction term is significant. We will see whether adding the interaction improves Model 3: As expected, adding the interaction term significantly improves my random intercept only model: I hope by now, you have got a sense of how to conduct simple HLM. Please stay tuned for more complex HLM analysis in the future. For the full codes used in this tutorial, please see below:",56,4,15,Towards Data Science,2021-01-04,2021
https://towardsdatascience.com/read-csv-to-data-frame-in-julia-programming-lang-77f3d0081c14,Read CSV to Data Frame in Julia (programming lang),"Parameters explained. Using CSV.jl, DataFrames.jl‚Ä¶",7,43,"['Read CSV to Data Frame in Julia', 'CSV.jl to parse CSV files', '3 ways how to read CSV to a DataFrame', 'Encoding', 'CSV Reader parameters', 'Installing Julia', 'Conclusion']","The first step in any data analysis is to get the data. There‚Äôs arguably no easier way than to load a CSV file into a data frame. In this tutorial, we will explore how to do that in Julia Programming Language. Unlike python‚Äôs pandas read_csv the functions in Julia are separated into two modules ‚Äî CSV.jl and DataFrames.jl and there is more than one way how to combine them. In this guideline we will see: The full tutorial‚Äôs code is accessible via GitHub ‚Äî Read_CSV.ipynb. As the first step, you have to declare the libraries you will use. In our case CSV and DataFrames. If you haven‚Äôt yet installed them to your Julia environment, run the REPL (command-line interface started usually by typing julia) and run import Pkg; Pkg.add(""CSV""); Pkg.add(""DataFrames""). Let‚Äôs start our journey by reading really simple comma separate values file (CSV) file having 3 columns each containing a different data type (string, int and float). To parse a CSV you just pass it to the CSV.File(path) command. Path argument contains the path to the file. The result is a CSV.File object which can be iterated to get CSV.Rows. In the CSV.Row object you can access the values using the column names. You might have noticed that CSV correctly guessed that our file contains a header on the first row and the ‚Äú,‚Äù (comma) is used as a separator. That is because the header parameter has 1 as the default value. Julia starts indexing with 1 (unlike python‚Äôs 0). Delim parameter has comma as a default value, but it the CSV fails to parse it is using the first 10 rows to guess other suitable delimiters like a semicolon, tab, space, or pipe. You can always specify these parameters manually. Julia convention says that keyword parameters are separated by a semicolon, but comma works as well. In order to turn the CSV.File to a DataFrame you have to pass it to the DataFrames.DataFrame object. There are at least 3 ways how to do that in Julia. You can wrap DataFrame around the CSV.File(path; kwargs). You can see that Julia representation (unlike python pandas) displays the data type of the column, whether it is a string, int or float. The second possibility is to use Julia‚Äôa pipe operator |> to pass the CSV.File to a DataFrame. The result is the same. To make the code similar to other languages, Julia designers decided to add a bit of syntactic sugar and allow the third option. CSV.read() has the path argument to the file as the first parameter and DataFrame object as the second. Other parameters can follow. These methods work in Julia version 1.4.1 and I assume it will be quite stable despite Julia is evolving. Some older tutorials showed other methods, e.g. CSV.read without the DataFrame as argument which no longer works. Julia expects all the strings to be UTF-8 encoded. If it‚Äôs not the case, you must first open the file using its encoding, then pass it to CSV.File and then to the data frame. You can use open(read, path, encoding) or shortcut to read(path, encoding). The encoding comes as an Encoding object, which you can easily create using enc before the string with encoding name ‚Äî e.g. enc""windows-1250"". Julia CSV parser is quite good at guessing basic parameters, though occasionally you will need to specify the parameters manually. Let‚Äôs overview the most basic arguments of the CSV parser. All the parameters are of course explained in the documentation ‚Äî CSV.jl. By default, the separator is set to comma "","". If the CSV is not parsed correctly Julia tries to guess different delimiter using the first 10 rows. You can always hard-code it. The delimiter can be more than 1 character. For example delim=""::"". Sometimes your data doesn‚Äôt start on the first row. Other time you don‚Äôt have any header at all. It‚Äôs also possible that more than 1 line contains the headers. Julia can handle all these cases: In case you choose multiple rows to be headers, then the column names are a concatenation of these rows. You don‚Äôt always need all the columns. You can set only the columns you need using select=[1,2,3] or specify which you want to skip using drop=[""col1"",:col5]. The column can be identified by their order 1,5,25. Remember that Julia starts indexing at number 1, so 1 is the first column. You can also use string ids ""col1"", ""customer_name"" or symbols. Symbols are using colon-prefix (:)[:col1, :customer_name] or you can declare them explicitly [Symbol(""col1""), Symbol(""customer_name"")]. CSV reader will infer the types, but it might guess it wrong. If you know the data types, you can specify them using type or types parameters. You should be aware that if the data cannot be converted to the specific type, the value is turned into missing which is equivalent of NaN in python. Working with dates is another crucial part of the data parsing. Dateformat parameter allows you to set the date format for all columns recognized as date or specifically for each column. If the column was not automatically recognized as date mentioning it in the dateformat parameter will turn it into the date. If the parsing to the defined format fails, the value is turned to missing type which is Nan equivalent. See more about dates in the Julia documentation. These arguments let you set the list of inputs to be considered as true and false boolean values. Some values mean that there is no data available. They are often -1 or 999. You can specify a list of these values which will be interpreted as missing (N/A). Some files contain notes or comments. You can exclude them using comment parameter. When you can to transpose your data and turn columns into rows, you can set transpose=true. Pooling reminds category type in pandas. It allows turning the columns that have only a few values to map this data to ints and dramatically reduce the column size and cost of some operations. I did a few experiments with pool parameter and did not find out how exactly is it calculated. Unlike pandas, you cannot pool a specific column. You can only set the threshold and all columns having the number of unique strings under this threshold will be pooled. Fixed width file is like CSV, only each column always has the same width (number of characters). If the content is shorter, the rest is padded usually with blank spaces. Though some tools, like many database engines, export the data into text using fixed-width file with delimiters for better reading experience. Such a format is not readable using CSV.jl with ignorerepeated=true and you would have to do further operations to turn such a file into a data frame. Are you impressed with Julia features? Would you like to try for yourself? Installing julia on your computer is easy. You go to download Julia page, pick the file for your platform (Win, OS, Linux) or use the docker image. Then start Julia REPL from the command line by typing julia. If you have Jupyter installed (for example for your python projects) you can allow Julia script by running Julia REPL (type julia in command line), then add IJulia package by: I hope you have learned how to load the CSV file into Julia‚Äôs DataFrames module and that you will try it in one of your next projects. You can compare the upload speed and further data processing options in Julia on your own and let me know how do you like Julia in the comments. See all the code in the notebook on github ‚Äî Read_CSV.",59,2,8,Towards Data Science,2021-01-04,2021
https://towardsdatascience.com/serverless-functions-and-using-aws-lambda-with-s3-buckets-8c174fd066a1,Serverless Functions and Using AWS Lambda with S3¬†Buckets,Learn how to build a Lambda function‚Ä¶,1,46,['Serverless Functions and Using AWS Lambda with S3 Buckets'],"In my previous articles you may have seen me going on and on about deploying code on server instances on the cloud, building services to manage those instances, building a reverse-proxy on top of those services and so on. No doubt that some of you may have wished if you could just write code, deploy it somewhere and not bother about the excessive complexities of setting up and managing server instances. Well, depending on your use case, there might be a solution ‚Äî Serverless Functions. Serverless functions allow code to be deployed without you allocating any infrastructure for the code to be hosted on. AWS Lambda is an FaaS (Function as a Service) platform that allows you to build serverless functions. AWS Lambda supports most major programming languages like Go, Java, Ruby, Python2 and Python3. For this tutorial, we will be using Python3. Even though they are called ‚Äúserverless‚Äù, they actually run inside a variety of runtime environments on cloud server instances. Serverless functions are stateless, i.e. one execution of a function does not maintain a state that subsequent executions can recognise or use. In other words one execution of a Serverless function does not in any way communicate with another execution. Since serverless functions are time- and resource-limited, they are suitable for short-lived tasks. They provide very little flexibility in terms of allocation of memory, CPU, storage etc. One implication of adopting a certain FaaS platform for going Serverless is that you are stuck with the platform‚Äôs vendor for most other cloud services that your serverless functions may interact with. Serverless functions can be used to build a microservices architecture, where your software is built up of smaller, independent microservices that provide specific functionalities. Microservices make it easier for developers to build, test and manage software in an agile way. Microservices are completely separate pieces of your software, so different microservices can be coded, tested and deployed in parallel. It is much easier to pin-point and fix errors in a microservices architecture as you only have to work with the microservice that is malfunctioning. Netflix, for instance, became one of the earliest adopters of a microservices-based architecture when they began moving their software onto AWS cloud in 2009. They currently maintain an API gateway that receives billions of requests daily and is built up of separate microservices for processes like user sign-up, downloading movies etc. By switching to a microservices architecture, Netflix was able to speed up development and testing of its software and easily rollback if errors were encountered. Serverless functions on AWS Lambda or simply Lambda functions can do some really cool things when used in combination with other AWS services, like using Amazon Alexa to turn EC2 instances on and off or lighting bulbs when something‚Äôs pushed onto your CodeCommit (or even GitHub) repository. There are 3 ways you can use Lambda in combination with other AWS services: * When a Lambda function is invoked by another AWS service, Lambda passes specific information from that AWS service to the function using an event object. This will include information like what item in which DynamoDB database triggered the Lambda function. ** Boto3 is a python library (or SDK) built by AWS that allows you to interact with AWS services such as EC2, ECS, S3, DynamoDB etc. In this tutorial we will be using Boto3 to manage files inside an AWS S3 bucket. Full documentation for Boto3 can be found here. Pre-requisites for this tutorial: An AWS free-tier account. An S3 bucket is simply a storage space in AWS cloud for any kind of data (Eg., videos, code, AWS templates etc.). Every directory and file inside an S3 bucket can be uniquely identified using a key which is simply it‚Äôs path relative to the root directory (which is the bucket itself). For example, ‚Äúcar.jpg‚Äù or ‚Äúimages/car.jpg‚Äù. Besides being a powerful resource for developing microservices-based software, Lambda functions make highly effective DevOps tools. Let‚Äôs look at an example of using Lambda functions with S3 buckets in the first two ways mentioned above to solve a simple DevOps problem :) Say you are receiving XML data from three different gas meters straight into an AWS S3 bucket. You want to sort the XML files into three separate folders based on which gas meter the data comes from. The only way to know the data source is to look inside the XML files, which look like this: How would you automate this process? This is where AWS lambda could prove handy. Let‚Äôs look at how to do this. 1 - Creating an S3 bucket Let‚Äôs start by building an empty S3 bucket. All you have to do is to go to the S3 page from your AWS console and click on the ‚ÄúCreate bucket‚Äù button. Make sure you leave the ‚ÄúBlock all public access‚Äù checkbox ticked and click on ‚ÄúCreate bucket‚Äù. Now, add a directory called ‚Äúunsorted‚Äù where all the XML files will be stored initially. Create a .xml file named ‚Äútestdata.xml‚Äù with the following content: 2 - Creating a Lambda function From the Services tab on the AWS console, click on ‚ÄúLambda‚Äù. From the left pane on the Lambda page, select ‚ÄúFunctions‚Äù and then ‚ÄúCreate Functions‚Äù. Select ‚ÄúAuthor from scratch‚Äù and give the function a suitable name. Since I‚Äôll be using Python3, I chose ‚ÄúPython3.8‚Äù as the runtime language. There are other versions of Python2 and Python3 available as well. Select a runtime language and click on the ‚ÄúCreate function‚Äù button. From the list of Lambda functions on the ‚ÄúFunctions‚Äù page, select the function you just created and you will be taken to the function‚Äôs page. Lambda automatically creates an IAM role for you to use with the Lambda function. The IAM role can be found under the ‚ÄúPermissions‚Äù tab on the function‚Äôs page. You need to ensure that the function‚Äôs IAM role has permission to access and/or manage the AWS services you connect to from inside your function. Make sure you add ‚ÄúS3‚Äù permissions to the IAM role‚Äôs list of permissions, accessible via the IAM console. 3 - Adding a trigger for our Lambda function We want the Lambda function to be invoked every time an XML file is uploaded to the ‚Äúunsorted‚Äù folder. To do this, we will use an S3 bucket PUT event as a trigger for our function. Under the ‚ÄúDesigner‚Äù section on our Lambda function‚Äôs page, click on the ‚ÄúAdd trigger‚Äù button. Select the ‚ÄúS3‚Äù trigger and the bucket you just created. Select ‚ÄúPUT‚Äù event type. Set the prefix and suffix as ‚Äúunsorted/‚Äù and ‚Äú.xml‚Äù respectively. Finally, click on ‚ÄúAdd‚Äù. 4 - Adding code to our Lambda function There are 3 ways you can add code to your Lambda function: We will use the first method for this tutorial. On your function page, go down to the ‚ÄúFunction code‚Äù section to find the code editor. Copy and paste the following code into the code editor: Don‚Äôt forget to replace the region name. Make sure the handler value is ‚Äú<filename>.lambda_handler‚Äù . The handler value specifies which function contains the main code that Lambda executes. Whenever Lambda runs your function, it passes a context object and an event object to it. This object can be used to get information about the function itself and its invocation, eg., function name, memory limit, log group id etc. The context object can be very useful for logging, monitoring and data analytics usages. As mentioned earlier the event object is used by Lambda to provide specific information to the Lamda function from the AWS service that invoked the function. The information, which originally comes in JSON format, is converted to an object before being passed into the function. In the case of Python, this object is typically a dictionary. In the code above, you can see that the event object has been used to get the name of the S3 bucket and the key of the object inside the S3 bucket that triggered our function. The code above is simple to understand. It does the following: Now press the ‚ÄúDeploy‚Äù button and our function should be ready to run. 5 - Testing our Lambda function AWS has made it pretty easy to test Lambda functions via the Lambda console. No matter what trigger your Lambda function uses, you can simulate the invocation of your Lambda function using the Test feature on the Lambda console. All this takes is defining what event object will be passed into the function. To help you do this, Lambda provides JSON templates specific to each type of trigger. For example, the template for an S3 PUT event looks like this: To test the Lambda function you just created, you need to configure a test event for your function. To do this, click on the ‚ÄúSelect a test event‚Äù dropdown right above the Lambda code editor and click on ‚ÄúConfigure test event‚Äù. From the pop up menu, make sure the ‚ÄúCreate new test event‚Äù radio button is selected and select the ‚ÄúAmazon S3 Put‚Äù event template. You should be provided with JSON data similar to that in the code snippet above. All we are concerned with is the data that is used in our Lambda function, which is the bucket name and the object key. Edit those two values appropriate to the S3 bucket and the XML file you created earlier. Finally give the test event a name and click on ‚ÄúCreate‚Äù. Now that you have a test event for your Lambda function, all you have to do is click on the‚ÄúTest‚Äù button on top of the code editor. The console will tell you if the function code was executed without any errors. To check if everything worked go to your S3 bucket to see if the XML file has been moved to a newly created ‚Äúgas-meter3/‚Äù directory. As you may have noticed, one downside of testing via the console is that the Lambda function actually communicates with other AWS services. This may cause unintentional changes to your AWS resources or even loss of valuable work. The solution to this is to build and run your lambda functions locally on your machine. Testing Lambda functions locally is not as straightforward. You will need to use tools like SAM CLI and Localstack to do this. 6 - All done! Now your Lambda function should sort any XML files uploaded to the ‚Äúunsorted‚Äù folder on your S3 bucket into separate folders, provided that the XML data is in the format specified in this tutorial. I hope that this article gave you a taste of what is achievable using AWS Lambda and some insight into Serverless Functions in general. Now it‚Äôs your time to get creative with AWS Lambda. Thank you for reading!",58,3,10,Towards Data Science,2021-01-04,2021
https://towardsdatascience.com/python-memory-and-objects-e7bec4a2845,"Python, Memory, and¬†Objects",The basics of memory management for data scientists,6,30,"['Python, Memory, and Objects', 'Basics', 'Stack Memory vs. Heap Memory', 'Memory Optimization', 'More Advanced Data Structures', 'Summary']","As data scientists, normally, we don‚Äôt pay attention to how Python and the underlying operating system handle memory for our code. After all, Python is the most popular language among data scientists, partly because it automatically handles those details. As long as we are working on small datasets, ignoring how Python manages memory (i.e., memory allocation and deallocation) does not impact our code performance. But, as soon as we switch to large datasets (big data) or heavy processing projects, basic knowledge about memory management becomes crucial. As an example, I was working on a data science project regarding indexing human DNA. I used a python dictionary object to keep track of sequences (i.e., sequences of nucleotides) and store their location in a reference human DNA. About 10% into the process, the dictionary object took all my RAM and started swapping between disk and RAM. It made the process super slow (as the disk is much slower in data transmission). As a data scientist, if I knew the basics of Python and memory management, I could prevent it and make much more memory-efficient codes. In this article and an upcoming article, I explain some basic concepts around memory management in Python. At the end of this article, you have good basic knowledge of how Python handles memory allocation and deallocation. Let‚Äôs get started ‚Ä¶ A python program is a collection of Methods or operations are easy. When you add two numbers, you are basically applying the add (or sum) method to two values. References are a little bit tricky to explain. A reference is a name that we use to access a data value (i.e., an object). The most famous references in programming are variables. When you define x = 1 , x is the variable or reference and 1 is its value (more accurate an integer object). In addition to variables, attributes and items are two other popular references in programming. Now, let's get deeper and introduce objects. As a Python programmer, you must have heard that ‚ÄúEverything in Python is an object.‚Äù An integer number is an object. A string is an object. Lists, dictionaries, tuples, pandas data frames, NumPy arrays are objects. Even a function is an object. When we create an object, it will be stored in memory. When we defined references in the previous paragraph, I should have told you that a reference does not point to a value in Python but points to the memory address of an object. For example, in our simple example x = 1 the reference x is pointing to a memory address that the integer object 1 is stored. At the run time, computer memory gets divided into different parts. Three important memory sections are: Code (also called Text or Instructions) section of the memory stores code instructions in a form that the machine understands. The machine follows instructions in the code section. According to the instruction, the Python interpreter load functions and local variables in the Stack Memory (also called the Stack). Stack memory is static and temporary. Static means the size of values stored in the Stack cannot be changed. Temporary means, as soon as the called function returned its value, the function and the related variable will be removed from the Stack. As a data scientist and programmer, you don‚Äôt have access to Stack memory. Python interpreter and OS memory management together take care of this section of memory. As you learned, variables (or references in general) only stores memory addresses of objects. So, where are the objects? Are they in Stack memory? No, they are in a different memory called ‚ÄúHeap Memory‚Äù (also called the Heap). To store objects, we need memory with dynamic memory allocation (i.e., size of memory and objects can change). Python interpreter actively allocates and deallocates the memory on the Heap (what C/C++ programmers should do manually!!! Thanks, Python!!!). Python uses a garbage collection algorithm (called Garbage Collector) that keeps the Heap memory clean and removes objects that are not needed anymore. You don‚Äôt need to mess with the Heap, but it is better to understand how Python manages the Heap since most of your data is stored in this section of the memory. Let‚Äôs find the memory address on the Heap that the variable x points to. To find it out, we can use a function called id(). When we run the first line (x = 1), Python stores integer object 1 in a memory address 140710407579424 on my computer (different from yours). In computer science, we normally show memory addresses in hexadecimal numbers; therefore, I used the hex() function (note: the prefix 0x is used in computer science to indicate that the number is in hex). After storing the int object 1 in Heap memory, Python tells the reference (or variable) x to memorize this address (140710407579424 or 0x7ff9b1dc2720) as its value. Take a look at this example. Here, I defined two variables (x and y). I assigned them an integer object (i.e. 1) to both of them. Surprisingly, the memory addresses that both variables point to are the same. Look at another example. I defined two variables (str1 and str2) and assigned a string object (Python) to both of them. The memory addresses that both variables point to are the same. If you test the same thing for boolean objects, you will see a similar observation. Why? To optimize memory allocation. Python does a process called ‚Äúinterning.‚Äù For some objects (will be discussed later), Python only stores one object on Heap memory and ask different variables to point to this memory address if they use those objects. The objects that Python does interning on them are integer numbers [-5, 256], boolean, and some strings. Interning does not apply to other types of objects such as large integers, most strings, floats, lists, dictionaries, tuples. So far, we have shown examples of simple data structures such as integers, strings, or booleans. What about more complex data structures such as lists or dictionaries. The example clearly shows that the memory address of the list object is different from its items. It makes sense since a list is a collection of objects, each of its items has its own identity and is a separate object with a different memory address. If each item in a list is a single object, does interning (from the previous section) apply to each item in a list? It is easy to check. As you see, both a and lst[0] are pointing to the same memory address due to integer interning. Also, you can see, when the integer number goes beyond 256, both b and lst[1] are pointing to different memory addresses. What happens when we add a new item to a list. Does the memory address change? Let‚Äôs test it. Interesting, the memory address for the list remains the same. The reason is that a list is a mutable object, and if you add items to it, the object is still the same object with one more item. Another important fact about mutable objects is that if you instantiate different variables from an object, all of them will change if you make any change to the object. Let me show it with a simple code. In this example, both variables lst1 and lst2 are pointing to the same mutable object (i.e. [1, 2, 3]). If any of those variables change the object (e.g., append a new item), the value of another variable (which is pointing to the same object) will also change. The only way to get a separate copy of a mutable object is to use .copy() method. As you see, using the .copy() method, we are creating two separate list objects with different memory addresses that changing one of them does not change the other one. Almost anything we said about the list objects also applies to the dictionary objects. There are some subtle differences that are beyond this article. For example, the way their memory size grows after adding a new item will be different. To check if two or more variables are pointing to the same object, you don't need to check their memory address. You can check if two variables are pointing to the same object using is. Here is an example. Remember, is is different from == . is tells you if two objects are the same but == tells you if their content or value is the same. For example, in the previous code, the contents of both lst3 and lst1 are the same (i.e. [1, 2, 3]), but they are two separate and different objects. The following code shows it clearly. This article gave you the basic knowledge about how Python (more accurate the CPython implementation) allocates memory to objects. When working on big data in Python, you need to know these fundamental concepts to write more memory-efficient codes. Follow me on Twitter for the latest stories: https://twitter.com/TamimiNas",328,2,7,Towards Data Science,2021-01-04,2021
https://towardsdatascience.com/estimating-causal-effects-on-financial-time-series-with-causal-impact-bsts-85d3c403f4a0,Estimating Causal Effects on Financial Time-Series with Causal Impact¬†BSTS,A brief introduction to‚Ä¶,8,64,"['Estimating Causal Effects on Financial Time-Series with Causal Impact BSTS', 'Introduction', 'The Problem', 'Default Model', 'Customised Model', 'Conclusion', 'Observations, Criticisms & Further Analysis', 'References']","Note from Towards Data Science‚Äôs editors: While we allow independent authors to publish articles in accordance with our rules and guidelines, we do not endorse each author‚Äôs contribution. You should not rely on an author‚Äôs works without seeking professional advice. See our Reader Terms for details. The ability to quantify and explicate the impact of a known event on a dependent variable is a data science skill whose utility applies to innumerable disciplines. The impact of such analysis, however, and its ability to influence a business decision (notwithstanding the veracity of the analysis itself!) is only as a good as the Data Scientist/Analyst/Quant‚Äôs ability to rationalise the underlying choice of model and its decision, as well as the requisite domain expertise and understanding of the dependent variable. Google‚Äôs Causal Impact library (implemented in both R and Python) can help us accomplish such a task in a very short space of time while providing methods that enable the user to fully explain the underlying modelling process and the model‚Äôs decision. In this article, we are going to briefly explore how we can implement a Causal Impact model to estimate the effect of the Vale dam collapse on the spot price of Iron Ore. You can find all of the code found in this article here. Google‚Äôs Causal Impact library provides a very straightforward implementation of a Structural Time-Series model that estimates the effect of a ‚Äòdesigned‚Äô intervention on a target time-series. This effect is measured by analysing the differences between the expected and the observed behaviour ‚Äî specifically, the model generates a forecast counterfactual i.e. the expected observations of how the dependent variable might have evolved after the event had the event not occurred. Originally developed as an R package, Causal Impact works by fitting a Bayesian Structural Time Series (BSTS) model to a set of target and control time series observations, and subsequently performs posterior inference on the counterfactual. For reference, structural time-series models are state-space models for time-series data, and can be defined in terms of the following pair of equations: In the above expressions: At this stage, it is worth noting one of the key differences between BSTS models and traditional statistical/deep learning variants: Traditional time series forecasting architectures such as Linear Regression models, which estimate their coefficients via Maximum Likelihood Estimation, or, on the more powerful end of the scale, an LSTM which learns a function that maps a sequence of past observations as input to an output observation. BSTS models, on the other hand, employ a probabilistic approach to modelling a time series problem, namely, they return a posterior predictive distribution over which we can sample to provide not only a forecast but also a means of quantifying model uncertainty. Bayesian Structural-Time Series models are particularly powerful models in that they can generalise to a very large class of time series models such as ARIMA, SARIMAX, Holt-Winters to name but a few. You can observe, in the above expression, how we can achieve this by varying the matrices X, Z, T, G and R in order to model distinct behaviours and structure in the observed time-series, as well as adding linear covariates, BetaX, that might also be predictive of the dependent variable. Example - Second-Order AutoRegressive Process: Consider an example where we want to model a time series‚Äôs temporal structure (autocorrelation). We can model the time series as a second-order autoregressive process AR(2), by adjusting the expressions above!: Definition of an AR(2) process: In state-space form: In light of this flexibility to choose any time series model you want to fit your data, one can quickly see how powerful these models can be. But this sounds like a lot of work? Well, yes, but fear not. The wonderful thing about Causal Impact, should you require an expedient initial analysis, is that you don‚Äôt have to explicitly define any of the model‚Äôs structural components. If this is indeed the case, and you do not specify a model as in input, a local level model is built by default and is one that estimates the salient structural components of your time series for you. With the local level model, the target time-series, y, is defined as: Here, a given point in time is modelled as a random-walk component, mu_t (also known as the local level component). Trend and seasonal components, gamma_t, are modelled as unobserved components. The trend is modelled as a fixed intercept and the seasonal components using trigonometric functions with fixed periodicities and harmonics. For a more detailed mathematical explanation, one can refer to the Causal Impact documentation here, and the Statsmodels state-space seasonal documentation, whose logic is followed by that of the Python variant of Causal Impact. In summary, our implementation (in Python) therefore is reduced to a one-line expression!: For those interested in how to build a Bayesian Structural Time-Series in detail using TensorFlow you can see how in this article: https://towardsdatascience.com/structural-time-series-forecasting-with-tensorflow-probability-iron-ore-mine-production-897d2334c72b Otherwise, let‚Äôs take a look at Causal Impact in action. We are going to explore how we can implement Causal Impact in estimating the effect of the Vale dam collapse on the spot price of Iron Ore. Whilst this event does not constitute a ‚Äòdesigned‚Äô intervention, utility still exists in the financial world in providing estimates of price moves in response to future events of a similar nature. Moreover, this is intended to serve as a demonstration of the utility of Google‚Äôs Causal Impact package in estimating the impact of an event on a response time-series. As a reminder, the link to the Python colab notebook can be found here. Vale Dam Incident In this instance, we are estimating the impact of the Vale dam incident that occurred at Vale‚Äôs (the worlds largest producer of Iron ore) C√≥rrego do Feij√£o mine on the 25th January 2019, on the spot price of Iron ore. Caveat: Whilst modelling the outcome of any financial time-series is generally a highly-complicated, non-linear problem requiring far greater consideration and technical application than we are demonstrating, the intention of this demonstration is to illustrate the utility and expediency at which CI can perform such an analysis, and its interpretability ‚Äî a pre-requisite quality in any business environment. We begin by acquiring our spot iron ore price data, then plotting the close price of the spot iron ore time-series. We also create a 21 and 252 day rolling average to give us a directional steer on the current price movements: So far so good. Prior to defining our pre-event and post-event periods, we can get a better idea of the magnitude of the event itself visually by marking the date of the event and plotting it: In the above chart, you can observe two events: As with all forecast problems, it is imperative that we fully consider the assumptions made by any model before applying it to our problem. With the basic information above, let‚Äôs fit the basic model to our spot price data, and examine the output. Implementation simple and straightforward: Note: Causal Impact can (handily) accept date strings when specifying the bounds of our pre and post-event periods. Fitting the Causal Impact model to our data returns an object whose methods when invoked, allow us to check model results, fitted parameters, etc. For now, we shall plot the results: As you can observe in the above chart, by calling the.plot()method on our CI object we can access the results of the fitting process. By default, the plot method renders three separate charts: Additionally, by invoking the CI object‚Äôs .summary()method, we yield a convenient summary report: Examination of the above output reveals the results of the fitted model: A cursory glance at the forecast counterfactual and the point-wise effect suggests that an event of this magnitude has had a significant effect on the spot price. Indeed, the model asserts, with confidence, that the Vale dam incident had an absolute causal effect of $21, varying from $18.04 to $23.98. It is also important to note the p-value here (< .05) to understand whether the observed behaviour is statistically significant or simply occurred by chance. It is immediately apparent, however, that our default model‚Äôs forecast counterfactual doesn‚Äôt look terribly convincing. For the most part, it would appear directionally accurate, however, it has clearly failed to capture the salient price movements, and the forecast appears to lag behind the observed spot price. We can check the fitted model‚Äôs parameters and diagnostics to assess whether the model conforms to its underlying statistical assumptions: Examination of Figure.1 above shows the parameters of the fitted model. A full explanation of the individual results is beyond the scope of this article, however the salient points, namely the model components; sigma2.irregular and sigma2.level and their coefficients show how weakly predictive they are of our target, the spot price of iron ore. Indeed, if we consider figure 2 and the plot of the residuals we can examine the magnitude of the model errors, ‚ÄòStandardized residual for c‚Äô. We can also observe that the errors follow a distinctly non-normal distribution, and exhibit strong autocorrelation. Nevertheless, we have effectively established a baseline model to estimate the effect of the event on our target variable. We have just implemented our first Causal Impact model and estimated the causal effect of the Vale dam incident on our spot iron ore data. So far, we‚Äôve let the package decide how to construct a time-series model for our spot price data, and found, as a result of the fitted model‚Äôs diagnostics, that we cannot be confident of the inferred effect. As mentioned above, a useful quality of structural time-series models is their modularity, affording us the flexibility to model individual behavioural dynamics of our time series such as seasonality, for example. The Causal Impact module offers several options that enable us to accomplish this, which we will exploit in an attempt to improve our inferred counterfactual. In an attempt to improve our model and the forecast counterfactual, we will employ our domain expertise and adjust our model to include a known seasonal component frequently manifest in spot iron ore price action, and incorporate two features that exhibit a known linear correlation with the spot price of iron ore: spot steel scrap and Chinese domestic steel reinforcing bar (rebar): We will begin by trialling the addition of a known seasonal component to our model. For many bulk commodities, prices tend to rise in the Chinese summer and winter ahead of peak periods of construction in the spring and autumn. The same behaviour can typically be observed with that of iron ore. For reference, a seasonal component can be described as a pattern that repeats itself with periodicity i.e. the signal is periodic. By decomposing our time series into its constituent structural components, we observe the degree to which the aforementioned seasonality affects the spot price: Note: Statsmodels seasonal_decompose performs a naive decomposition of our time series ‚Äî more sophisticated approaches would, and should typically be employed, particularly as our time-series is a financial one. For the purposes of this article and demonstrating how to add these as components to our model, however, this shall suffice. Isolating a few examples corroborates our prior belief/domain expertise: we can see that the seasonal component‚Äôs frequency and amplitude correspond with the Chinese summer and winter: It would appear that the signal has a rough periodicity of 146 days, and a harmonic of 1, although these are crude interpretations. We can observe that the amplitude of the winter peak in both examples is of lesser magnitude than that of the summer, therefore the signal itself is not strictly symmetric. For now, we will proceed on our assumptions for the purposes of demonstration. Incorporating seasonal components in Causal Impact is very straightforward: the Causal Impact class accepts a list of dictionaries containing the periodicity of each seasonal signal, and, if known, the harmonics: We can now add the external covariates to our model, spot steel scrap price and Chinese domestic reinforcing bar. Again, adding external covariates to our Causal Impact model is simple. These can be passed as a Pandas data frame along with our target variable. As per the source code, Causal Impact expects the target/label column to be in the first index (0). All subsequent columns are registered as external covariates: We then pass this data frame to our Causal Impact model, along with the same pre and post-period event definitions and our seasonal component. Note that we will not specify the harmonics of our seasonal component*: *model defaults to calculating this as math.floor(periodicity / 2)) Upon inspecting the counterfactual of our new model, we can observe a far more credible result. It would still appear as though we need to account for the temporal (autocorrelative) structure in the time series and can include this in a subsequent model, but the result would appear credible for a first pass. Upon checking the fitted model‚Äôs parameters, we can see that our seasonal component and it‚Äôs definition needs re-evaluating, whereas our external covariates explain a lot of what is observed in our response variable: Finally, we can have our model estimate the impact as a result of the Vale dam incident on our target with the useful method,.summary('report') that returns a detailed explanation of the observed effects: Here, we can see that the model concludes, with significance, that the incident resulted in a +33% increase in the spot price of iron ore ‚Äî and logically, therefore, had the incident not occurred, then the spot price would have traded 33% lower during this period. In this article, we have learned how Google‚Äôs Causal Impact package can be used to estimate the causal effect of an intervention on an observed time-series. Specifically, we implemented a CI model to estimate the effect of the Vale dam incident on the spot price of Iron Ore. Furthermore, we have learned: Many thanks for taking the time to read this article and as always, I welcome all constructive criticisms and comments. 2. https://www.ft.com/content/8452e078-7880-11e9-bbad-7c18c0ea0201 3. https://www.businessinsider.com.au/iron-ore-price-seasonality-2018-1",242,7,15,Towards Data Science,2021-01-04,2021
https://towardsdatascience.com/hyperloglog-a-simple-but-powerful-algorithm-for-data-scientists-aed50fe47869,HyperLogLog: A Simple but Powerful Algorithm for Data Scientists,,10,55,"['HyperLogLog: A Simple but Powerful Algorithm for Data Scientists', 'How Many Unique Visitors in the Exhibition?', 'Flajolet-Martin Algorithm', 'How to improve: LogLog', 'Can we get a better name: SuperLogLog', 'The Ultimate HYPE: HyperLogLog', 'HyperLogLog in Real-world Application', 'Conclusion', 'References', 'Want to Learn More? Weekly I/O!']","HyperLogLog is a beautiful algorithm that makes me hyped by even just learning it (partially because of its name). This simple but extremely powerful algorithm aims to answer a question: How to estimate the number of unique values (aka cardinality) within a very large dataset? This question is called Count-distinct problem in Computer Science or Cardinality Estimation Problem in Applied Mathematics. We will call it Cardinality Estimation Problem in this article because it sounds more impressive. But before we understand how HyperLogLog solves the cardinality estimation problem, we first have to know why we need HyperLogLog to solve this problem. Imagine that you are hosting an art exhibition. Your job is standing at the entrance and counting how many unique visitors so far using only pen and paper. Given that a small error in counting is acceptable, how can you achieve the task? One solution can be: writing down all the visitors‚Äô full names and just check how many unique names on it. But what if you get thousands of visitors a day? Do you have enough papers? And will you be happy with writing down thousands of names? Another solution can be: instead of writing down names, you can write down the last 6 digits of their phone number. Nevertheless, it still requires a lot of work. Right now, let‚Äôs make the task even harder. Can you count the number in real-time or near real-time? And to make it exponentially harder: can you achieve the task only by finger-counting? Yes, you can. You can count thousands of unique visitors in real-time only by finger-counting. Our friends Philippe Flajolet and G. Nigel Martin introduced a brilliant algorithm in their 1984 paper ‚ÄúProbabilistic Counting Algorithms for Data Base Applications‚Äù that may help us solve this task. [1] The solution is: just use your finger to keep track of the longest sequence of leading zeroes you have seen in those 6 digits of phone numbers. For example, if you get 532885, the longest sequence of zeroes is 0. The next you get 042311, the longest sequence now comes to 1. When you see more than 10 people, the longest sequence will more likely be 1. Similarly, when you see more than 100 people, the longest sequence will more likely be 2. It‚Äôs easy to see that in random data, a sequence of K zero will occur once in every 10·¥∑ elements, on average. Now, imagine that your longest sequence right now is 5, chances are that you have seen more than 1,000 people to find someone‚Äôs last 6 digits of phone number start with 00000. You get the idea. Based on probability, the estimation of how many unique visitors will be close to 10·¥∏, given L is the longest sequence of leading zeroes you found in all the numbers. In fact, in our friends‚Äô 1984 article, they hashed the elements first to get more uniformly distributed binary outputs. For example, they may hash an element x1 to 010001 and another x2 to 101000. Therefore, phone numbers that aren't uniformly distributed due to some pattern like area code can also be estimated correctly. Also, because they turned the output into a binary bit array, right now the estimation of cardinalities is 2·¥∏. However, statistical analysis shows that 2·¥∏ actually introduces a predictable bias. So they add a correction factor œï ‚âà 0.77351 to complete the ultimate formula: 2·¥∏ / œï. This algorithm is called Flajolet-Martin Algorithm. What a name! Later we will see the importance of naming and how Flajolet actually improved his naming skills. I know you may think that the estimation seems to be not that robust and reliable. Imagine that the hash value you get from the very first element turns out to be 000000010 ‚Äî jackpot! In real life, some outliers in our data can screw up our estimation. Of course, our friend Flajolet knew that too. How to make our estimation less influenced by the outliers? One obvious solution is to repeat the Flajolet-Martin Algorithm with multiple independent hash functions and average all the results. For example, if we obtain the longest sequence of leading zeroes using m different hash functions, here we denote the value of the longest sequence of leading zeroes as L‚ÇÅ, L‚ÇÇ, ‚Ä¶, L‚Çò, then our final estimation becomes m * 2^((L‚ÇÅ+‚Ä¶+L‚Çò)/m). However, hashing an input with multiple hashing functions can be quite computationally expensive. Therefore, our friend Flajolet and his new friend Marianne Durand came up with a workaround: how about using one single hash function and using part of its output to split the value into many different buckets? To split the value into buckets, they just use the first few bits of the hash value as the index of the buckets and count the longest sequence of leading zeroes based on what‚Äôs left. For example, if we want to have four buckets, we can use the first two bits of the hash value output as the index of the buckets. Assuming we have four elements and get the hash values of them: Hash(x1) = 100101: the 2nd (10) bucket right now with longest sequence of leading zeroes = 1 (0101) Hash(x2) = 010011: the 1st (01) bucket right now with longest sequence of leading zeroes = 2 (0011) Hash(x3) = 001111: the 0th (00) bucket right now with longest sequence of leading zeroes = 0 (1111) Hash(x4) = 110101: the 3rd (11) bucket right now with longest sequence of leading zeroes = 1 (0101) The average of the longest leading zeros of all buckets is (0+2+1+1)/4 = 1. Therefore, our estimation here is 4 * 2¬π. It is not close to the true value because here we only have very few samples, but you get the idea. You can find more detail about the correction factor œï for LogLog in their 2003 paper ‚ÄúLoglog Counting of Large Cardinalities‚Äù.[2] (They denoted the correction factor as Œ± in the original paper.) So this is LogLog, averaging the estimator to decrease the variance. The standard error of LogLog is 1.3/‚àöm, given m is the number of buckets. After coming up with Flajolet-Martin Algorithm and LogLog, our friend Flajolet is unstoppable in terms of tackling the cardinality estimation problem. He decided to push to the extreme of this problem. In the same paper [2] as they introduced LogLog, Durand and Flajolet found out that the accuracy can be largely improved by throwing out the largest values they got from those baskets before averaging them. To be more specific, when collecting the values from the buckets, we can retain the 70 percent smallest values and discarding the rest for averaging. By doing so, the accuracy is improved from 1.3/‚àöm to 1.05/‚àöm. What a miracle! And here they decided to give this method a superior name: SuperLogLog. In 2007, our dear friend Flajolet finally found out his ultimate solution for the cardinality estimation problem. This solution is HyperLogLog, which he referred to as the ‚Äúnear-optimal cardinality estimation algorithm‚Äù.[3] The idea behind it is very simple: instead of using geometric mean to average the result we got from LogLog, we can use harmonic mean! The harmonic mean is the reciprocal of the average of the reciprocals. Reciprocal just means 1/value. Therefore, the formula of harmonic mean is n / (1/a + 1/b + 1/c + ‚Ä¶). For example, the harmonic mean of 1, 2, 4 is 3 / (1/1 + 1/2 + 1/4) = 3 / (1.75) = 1.714 Why using Harmonic means? Because it is good at handling large outliers. For example, considering the harmonic mean of 2, 4, 6, 100: 4 / (1/2 + 1/4 + 1/6 + 1/100) = 4.32 The large outlier 100 here is being ignored because we only use the reciprocal of it. Therefore, we got an averaging method that can be less influenced by large outliers. Again, you can find more detail about the correction factor œï for HyperLogLog in their 2007 paper ‚ÄúHyperLogLog: the analysis of a near-optimal cardinality estimation algorithm‚Äù.[3] By using harmonic mean instead of geometric mean used in LogLog and only using 70 percent smallest values in SuperLogLog, HyperLogLog achieve an error rate of 1.04/‚àöm, the lowest among all. Now we understand how HyperLogLog works. This algorithm can estimate the number of unique values within a very large dataset using little memory and time. In fact, it can estimate cardinalities beyond 10‚Åπ with a 2% standard error while only using 1.5kb memory. Where can we find HyperLogLog in the wild? One example is how Reddit counts how many unique views of a post. In the article View Counting at Reddit, they elaborated that how HyperLogLog satisfied their four requirements for counting views: Some other great examples are databases and data warehouses that have to cope with petabyte-scale data. For supporting an efficient count unique function for data query, those applications use HyperLogLog. Such examples include Redis, Amazon Redshift, Facebook Presto, BigQuery, and Apache Druid. That‚Äôs it. In this article, we see the development and improvement of the ideas from paper to paper. We learn about the Count-distinct problem (Cardinality estimation problem), our friend Philippe Flajolet and his many friends, Flajolet‚ÄìMartin algorithm, LogLog, SuperLogLog, HyperLogLog algorithm, and their applications. As a side note, in the original paper, instead of counting the longest sequence of leading zeroes, Flajolet‚ÄìMartin algorithm actually counts the position of the least-significant bit in the binary. Furthermore, LogLog, SuperLogLog and HyperLogLog actually count the position of the leftmost 1 (so it is 1 + the number of leading 0's). I simplified those details for clarity, but the concepts are all quite similar. Interested readers can read the original papers for more accurate details. Before you leave, you can try to answer these questions on your own as a review of the algorithm. Hope you enjoy the article and thanks for reading. Thanks to Colin Gladue and Ting-Wei (Tiffany) Chiang for reading the draft of this. Weekly I/O is a project where I share my learning Input/Output. Every Sunday, I write an email newsletter with five things I discovered and learned that week. Sign up here and let me share a curated list of learning Input/Output with you üéâ",,0,9,Towards Data Science,2021-01-04,2021
https://towardsdatascience.com/saliency-map-using-pytorch-68270fe45e80,Saliency Map for Visualizing Deep Learning Model Using¬†PyTorch,Interpret the deep learning¬†model‚Ä¶,5,30,"['Saliency Map for Visualizing Deep Learning Model Using PyTorch', 'Motivation', 'Saliency Map', 'The Implementation', 'Conclusion']","When we use machine learning models such as Logistic Regression or Decision Tree, we can interpret which variables contribute to the prediction result. But, this step will be difficult when we are using deep learning model. Deep Learning model is a black-box model. It means that we cannot analyze how the model can predict the result based on the data. As the model gets more complex, the interpretability of the model will reduce. But this doesn‚Äôt mean that the model cannot be interpreted. We can infer the deep learning model, but we have to work on its complexity. In this article, I will show you how to visualize deep learning model result based on gradients. We call this method as saliency map. We will use a framework called PyTorch to implement this method. Without further ado, let‚Äôs get started! Before we get into the saliency map, let‚Äôs talk about the image classification. Given this equation, Where, As you can see on the equation, we multiply the image vector (I) with the weight vector (w). We can infer that the weight (w) can define the importance for each pixel on the image to the class (c). So, how can we get the weight (w) that correspond to score (S) given to the image (I)? We can get the relationship between those variables by looking at the gradients! But wait, what does basically a gradient? I don‚Äôt want to explain so many calculus on it, but I will tell you this. The gradient describes how much the effect of a variable, let‚Äôs say x, can affect another variable result, in this case, y. When we use that analogy, we can say that the gradient describes on how strong for each pixel of the image (I) can contribute to the prediction result (S). The equation for looking the weights (w) looks like this, By knowing the weights (w) for each pixel, we can visualize it as a saliency map, where each pixel of it describes the power of that pixel affects the prediction result. Now, let‚Äôs get into the implementation! In this section, we will implement the saliency map using PyTorch. The deep learning model that we will use has trained for a Kaggle competition called Plant Pathology 2020 ‚Äî FGVC7. To download the dataset, you access on the link here. Here are the steps that we have to do, Now, the first thing that we have to do is to set up the model. In this case, I will use my pretrained model weight using ResNet-18 architecture. Also, I‚Äôve set up the model so we can use the GPU to get the result. The code looks like this, Right after we set up the model, now we can set up the image. To do this, we will use PIL and torchvision libraries for transform that image. The code looks like this, After we transform the image, we have to reshape it because our model reads the tensor on 4-dimensional shape (batch size, channel, width, height). The code looks like this, After we reshape the image, now we set our image to run on GPU. The code looks like this, Then, we have to set the image to catch gradient when we do backpropagation to it. The code looks like this, After that, we can catch the gradient by put the image on the model and do the backpropagation. The code looks like this, Now, we can visualize the gradient using matplotlib. But there is one task that we have to do. The image has three channels to it. Therefore, we have to take the maximum value from those channels on each pixel position. Finally, we can visualize the result using the code looks like this, Here is the visualization looks like, As you can see from the image above, the left side is the image, and the right size is the saliency map. Recall from its definition the saliency map will show the strength for each pixel contribution to the final output. In this case, the leaf on this image has a disease called rust as you can see on the yellow spot on it. And if you look carefully, some pixels has a brighter color than any other images. It indicates that those pixels have a huge contribution to the final result, which is the rust itself. Therefore, we can confidently say that the model has predicted the result by looking at the right information of it. Well done! Now you have to implement your own saliency map for interpreting your deep learning model. I hope you can implement the saliency map for your own cases, and don‚Äôt forget to follow me on Medium! If you want to have a discussion with me about data science or machine learning, you can contact me on LinkedIn. Thank you for reading my article! [1] Simonyan K, Vedaldi A, Zisserman A. 2014. Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps. arXiv:1312.6034v2 [cs.CV] [2] Rastogi A. 2020. Visualizing Neural Networks using Saliency Maps in PyTorch. Data Driven Investor. [3] https://github.com/sijoonlee/deep_learning/blob/master/cs231n/NetworkVisualization-PyTorch.ipynb",55,2,5,Towards Data Science,2021-01-04,2021
https://towardsdatascience.com/how-to-train-an-mt5-model-for-translation-with-simple-transformers-30ba5fa66c5f,How to Train an mT5 Model for Translation With Simple Transformers,Let‚Äôs see how we can train an¬†mT5‚Ä¶,8,56,"['How to Train an mT5 Model for Translation With Simple Transformers', 'Outline', 'Setup', 'Data Preparation', 'Model Training', 'Visualizing Training Progress', 'Evaluating the Model', 'Wrapping Up']","mT5 is a multilingual Transformer model pre-trained on a dataset (mC4) containing text from 101 different languages. The architecture of the mT5 model (based on T5) is designed to support any Natural Language Processing task (classification, NER, question answering, etc.) by reframing the required task as a sequence-to-sequence task. In other words ‚Äî text goes in, and text comes out. For example, in a classification task, the input to the model can be the text sequence to be classified, and the output from the model will be the class label for the sequence. For translation, this is even more straight forward. The text that goes in is in one language, and the text that comes out is in another. Considering the multilingual capabilities of mT5 and the suitability of the sequence-to-sequence format for language translation, let‚Äôs see how we can fine-tune an mT5 model for machine translation. For this article, we‚Äôll be training a translation model to translate between Sinhalese (my native language!) and English. It‚Äôs quite challenging to train good translation models for low-resource languages like Sinhalese because of, well, the low availability of resources (training data). Hopefully, the multilingual pre-training on a huge dataset (which includes Sinhalese, although not a lot of it) will help the mT5 model compensate for inadequate training data in the form of direct Sinhalese-English (and vice versa) sequences. We‚Äôll be using the Simple Transformers library (built on the Huggingface Transformers library) to train the mT5 model. The training and testing data will be obtained from The Tatoeba Translation Challenge. The graphs and charts are generated from Weights & Biases, which is supported natively in Simple Transformers for experiment tracking and hyperparameter optimization. Note: You can find all the code in this article in the examples/t5/mt5_translation directory (link) of the Simple Transformers Github repo. You can find the most up-to-date installation instructions in the Simple Transformers documentation. 1. Install Anaconda or Miniconda Package Manager from here. 2. Create a new virtual environment and install packages. 3. If using CUDA: else: 4. Install simple transformers. The training and test can be obtained from the Tatoeba Translation Challenge data page. You‚Äôll also find datasets for a whole host of other languages there (including Sindarin, a language spoken by Elves in The Lord of the Rings üòÄ). If you want to try training a translation model for another language, you can download the dataset for that language instead of Sinhalese. All the other steps in this article apply to any language dataset. If you are too lazy to search for the dataset, here‚Äôs the direct link. üòú Now, let‚Äôs build the tsv files that we will use to train and test our mT5 model. Running the code above will write the two files, train.tsv and eval.tsv, to the data/ directory. Once we have the data files, we are ready to start training the model. First, we‚Äôll import the necessary stuff and set up logging. Next, we set up our training and evaluation data. Here, we remove the prefix values from the datasets because we expect the model to infer the required task based on the input. If the input is in English, then it should be translated to Sinhalese. If it‚Äôs in Sinhalese, then it should be translated to English. The model shouldn‚Äôt need a prefix to figure this out after the training! You can use a prefix value to tell an mT5 (or T5) to perform a specific task. This is quite useful to train a model which can perform multiple tasks, as shown in the article below. towardsdatascience.com The amount of GPU memory required to train a Transformer model depends on many different factors (maximum sequence length, number of layers, number of attention heads, size of the hidden dimensions, size of the vocabulary, etc.). Out of these, the maximum sequence length of the model is one of the most significant. For the self-attention mechanisms used in the mT5 model, the memory requirement grows quadratically with the input sequence length (O(n¬≤) space complexity). I.e., When the sequence length doubles, the memory required quadruples. Also, mT5 has a much larger vocabulary than T5 (~250,000 tokens to ~32,000 tokens), contributing to mT5 being quite punishing in terms of GPU memory required. The takeaway from all this is that the number of tokens we can input to the model (the maximum sequence length) comes at a hefty premium. Based on this, it‚Äôs wasteful to use even a small number of tokens on the prefix if the model can do without. Now, let‚Äôs get back to training the model! Here, we specify how we want the model to be set up and initialize the pre-trained mT5 model according to model_args. I‚Äôm using a maximum sequence length (max_seq_length) of 96 and a train/eval batch sizes of 20. Generally, larger batch sizes mean better GPU utilization, and therefore, shorter training times. As mentioned earlier, longer sequences require more GPU memory, which means smaller batch sizes and longer training times. The maximum sequence length of 96 allows the model to work with reasonably long text (typically a few sentences) while also keeping the training time practical. Note that you may need to tweak these values to train the model on your own GPU. If you run out of GPU memory (CUDA memory error), try reducing the batch sizes and/or the maximum sequence length. If you want to try the fine-tuned model, you can find it here on the Huggingface model hub. Now, to run the training, we just need to call the train_model() method. As easy as that! The fine-tuned model will be saved to the outputs directory at the end of the training (see docs for more info on model saving). With these settings, the model took a little over 10 hours to complete the training on an RTX 3090 (24 GB VRAM). I probably could have gotten away with slightly larger batch sizes (as you can see below), but I didn‚Äôt want to run the risk of the training crashing as I was running this overnight! Playing it safe with a batch size of 20 meant that the GPU was not fully utilized, but, 80%-ish is not bad! Setting the wandb_project value in model_args tells Simple Transformers to log the training progress to Weights & Biases automatically. You can find all the logged data for this experiment here. The actual loss values here don‚Äôt tell us too much, but the fact that they are decreasing does mean that the model is learning! In fact, it appears as though the model has not yet converged as the evaluation loss is still decreasing. Training for another epoch or two might very well improve the model performance, but, that would take another 10 or 20 hours! To try out the fine-tuned model in a web-based GUI (Streamlit app), use the terminal command simple-viewer. The standard metric used to evaluate and compare machine translation models is the BLEU score, specifically, the BLEU scheme used by the annual Conference on Machine Translation (WMT). The SacreBLEU library can be used to calculate this score. For more information on the BLEU score, please refer to this paper by Matt Post. Since The Tatoeba Challenge also provides the BLEU scores for the benchmark translation models, we can easily compare our model to the benchmark model. Now, let‚Äôs load our fine-tuned model and see how it stacks up! We import the necessary stuff (note the sacrebleu library) and initialize the model just as we did for training, except that we load the fine-tuned model from outputs/ rather than the pre-trained model. We also set some parameters for generating text (decoding) with the model. Here, the max_length is the maximum length for the output from the model rather than the input. If you‚Äôd like to learn more about the decoding process, please refer to the decoding algorithms section in this article and this excellent notebook by Huggingface. Next, we‚Äôll prepare the data for evaluation. Here, we load the evaluation data and prepare separate lists of inputs and true translations (for English to Sinhalese and vice versa). With the model and evaluation data loaded and ready, we can go ahead and do the translations. With Simple Transformers, we just call model.predict() with the input data. Then, we use the sacrebleu tool to calculate the BLEU score. The sacrebleu library should be installed in your virtual environment if you followed the setup instructions. If not, you can install it with pip install sacrebleu. Running this gives us the following scores (rounded off): Both these scores improve upon the scores posted by the translation model in the Tatoeba Challenge! The mT5 model does an excellent job of translating between Sinhalese and English, despite the limited training data availability. mT5 outperforms the scores posted in the Tatoeba Challange. However, it should be noted that the benchmark model in the challenge is trained on several other languages in addition to Sinhalese and English. In addition, the mT5 model requires significantly more compute resources to train and to use. On the other hand, the mT5 model has the potential to improve upon the current scores with more training. Finally, you can find the fine-tuned model on the Huggingface model hub here. You can use it directly with Simple Transformers as shown below.",97,2,8,Towards Data Science,2021-01-04,2021
https://towardsdatascience.com/the-surprising-behaviour-of-distance-metrics-in-high-dimensions-c2cb72779ea6,The Surprising Behaviour of Distance Metrics in High Dimensions,Learn how Distance Metrics can¬†loose‚Ä¶,7,74,"['The Surprising Behaviour of Distance Metrics in High Dimensions', '0) Introduction', '1. What is the Curse of Dimensionality?üëª', '2. How do Higher Dimensions affect our Machine Learning algorithms?üîé', '3. How do Higher Dimensions affect Distance Metrics?üèôÔ∏è', '4. On the Surprising Behaviour of Distance Metrics in a High Dimensional SpaceüíØ', '5. Wrapping everything up ‚úîÔ∏è']","Distance metrics, like Euclidean, Manhattan, Minkowsky and so on, suffer a lot when we increase the number of dimensions (features) of our data. Its like they dilute and loose their meaning. The become unreliable. This is not very easy to understand, and it comes into conflict with the general understanding that more features equals to better Machine Learning models, which is far from being true. Reducing the dimensions of our data is something which should always try to be done in order to remove redundant or noisy features. You can learn all about feature selection in this article. Also, dimensionality reduction techniques like PCA or Kernel PCA are a very clever treatment option to apply to our data when we are going to use an algorithm that computes distance metrics. There is a nominal paper in the field of Machine Learning that speaks about Distance Metrics specially suffering from the Curse of Dimensionality. This paper however, is not amazingly intuitive, and has a lot of complex mathematical formulation, so in this post I will explain in a simple and gradual manner why distance metrics like the Euclidean distance, suffer so much when calculated in high dimensions. Firstly, we will see what the famous Curse of Dimensionality is and how it can affect our Machine Learning algorithms. Then, we will see specifically how it makes distance metrics loose their meaning, and go through the most important parts of the paper. Lastly, we will merge everything together, in a highly visual manner so that the puzzle is solved, introducing other resources and insights. z-ai.medium.com The Curse of Dimensionality refers to certain behaviours or effects that appear when analysing or playing with data in high dimensions (with many features), which do not appear when the number of dimensions is low. Our human intuition and understanding is limited to a three dimensional world. If kept inside this frame of reference, we can visually deduct why things happen, and logically reach some assumptions. We are terrible however, at trying to understand what happens when we are taken away from this limited dimensional space. Because of this, when our data has a lot of dimensions, the effects of that high dimensionality on our models and algorithms can escape our intuition, despite us knowing about the fact that more features doesn‚Äôt always lead to better results in our Machine Learning pipelines. How exactly do more features affect Machine Learning? Most times, we tend to throw our data sets with many features at our models and wait to see what comes out, hoping that if the results are not good enough, more features will do the trick. However, this is far from being true. One of the most important steps in a Machine Learning Pipeline is selecting those features that really improve our models and keep them compact, understandable, and scalable. This is done via a feature selection or dimensionality reduction technique, which can be sort of seen as feature selection. Lets see what happens as we add more features, and therefore dimensions to our data: I have been saying all the time that as we increase the number of Dimensions of our data, distance metrics kind of loose their meaning. However, I have never shown any examples of this being true. Lets zoom out and study this phenomenon from the beginning: a simple straight line ‚ûñ! Lets look at a straight line (1 Dimension) of length 1. The previous figure shows a 1 dimensional space (a line) of length 1. First, if we take 500 randomly generated points along this line, and count the ones that fall within a 10% distance of the limits of the 1 dimensional space (those points from 0 to 0.1 and 0.9 to 1), and calculate the ratio of these points with respect to the total amount of points, we get that approximately 20% of the total points fall within this category (outside points, the red points), and 80% are inside the the rest of the space (inside points, the blue dots). This is a important metric to calculate, and we will see later why. At the moment just keep in the top of your head the proportion of points that fell near the edges or limits of the 1-Dimensional space: about 20%. Secondly, if we take any two random points from these 500 points, calculate the distance in between them, and repeat this process say 10000 times (enough times to ensure we get a representative sample), the average distance in between the two random points picked is of 0.34. This is the second piece of information we are going to consider: the average distance in between any two points of our data, that as we will see, gets bigger as we increase the number of dimensions. Lets take a look at a square ‚¨úÔ∏è! Now, lets check out what happens in a 2 Dimensional space, defined by a square of side 1. As in the previous example, we randomly generate a series of points inside our 2 Dimensional space, in this case 2000. Then, we count how many of these points are near the edges of our 2 dimensional space (outside a square of side 0.8 that shares centre with the whole space, or within 10% of the distance to the sides), and divide this number by the number of total points, to get an idea of how many of the randomly generated samples are near the edges. This time it is about 37%, 17% more than in the 1 Dimensional example. Secondly, we calculate the distance between two randomly selected points 10.000 times, and then average the results, getting an average distance of 0.52 in between any two points of our data set: 0.18 more than in the 1 Dimensional example! You can start to see what is happening here right? As we increase the dimensions the number of points that are close to the edges or limits of our feature space increases, and also the average distance in between any two points increases too. Lets check this out in the last Dimensional space that we are able to visualise: A cube in 3D üèóÔ∏è! To end this route and convince you that this is a real thing, before explaining why it happens, lets check out the numbers for a 1 sided, 3 dimensional space: a cube. This time I will skip how the metrics are calculated and what they mean, as we have already seen it twice. The pattern, as it can be observed is still there: the ratio of points that are not inside the ‚Äòinside‚Äô cube is of 49%, 12% higher than in the square, and the average distance in between two randomly selected points is of 0.65, 0.13 higher than in our two dimensional example. You might be wondering. Does this always happen? If we calculate how many points are not close to the limits of an N dimensional space with N ranging from 1 to 8 Dimensions, we get the following output: As you can see, as we increase the number of dimensions of our data, more and more randomly created samples (which could be anywhere), go to the limits or extremes of the space. If we had an infinite dimensional space with the shape of a hypersphere, all the points would be in the surface of this ‚ôæÔ∏è Dimensional space: If it was an orange, every point would be situated at the skin, and we would have no juicy interior. Also, the distance in between any two points becomes more and more similar, therefore evaluating how close two points are using a distance metric (like those needed for K-means for example) makes little sense, as pretty much every point will be equally close to all the others. The following table summarises what we have just seen with the previous examples for 1, 2 and 3 dimensions. Lets check out what the paper says in layman terms, and then wrap it all up removing the veil from this mysterious phenomenon üîÆ! The paper that we spoke about (which you can find here) tackles this issue from the perspective of norm K distance metrics. L1 norm is the Manhattan Distance, L2 norm is the famous Euclidean distance, and so on. It first introduces the Curse Dimensionality, going into how affects Distance Metrics in a special way. Then, it discusses and provides evidence that higher norm metrics suffer more form this curse than lower curse metrics. Every page of the paper is covered in ugly mathematical formulas like the following, which scare away the fearful reader. Their conclusions however, are easily explainable, and highly influential: These last results can be visualised in the following graphs: These graphs show the distance between the point that is furthest away and the closest point as the dimensions of our data (x axis) increase. We can see that for k = 3, this difference starts heading to 0. For k = 2, this difference seems converges to a constant value as we increase the dimensions, and for k = 1, this differences diverges to infinity. This means that higher norm metrics (higher values of k) provide a poorer contrast between the point that is most far away and the closest point (as when we increase the number of dimensions these distance in between these two points converges to 0). After proving this the paper goes on to explore Fractional distance metrics (0<k<1), showing that these could provide even better results and more resilience to higher dimensionality than the previously studied distance metrics. The paper ends showing that in High Dimensions (20 for synthetic data, 168 for the Musk Dataset, 32 for the breast cancer, 34 for Ionosphere), fractional norm distance metrics work better than higher norm distance metrics in a K-means clustering algorithm that tries to identify the classes of the data as different clusters. Awesome! Having explored a bit what the paper says, lets explain why all this happens and wrap everything up in an awesome and insightful conclusion. Alright, so why does this happen? What are the actual implicit conclusions we can derive from all of this? Why do our distance metrics get lost in high dimensional spaces? Why do our points go closer to the limits of the N-dimensional space as we increase dimensions? Why does the distance in between two random points increase? Why is our data more sparse in a high dimensional space? Lets go back to the square. The inside of the space that the square defines has a dimension of L*L (the area of the square). This is where our data points live. The limits of this space, for the case of the square, are its perimeter: 4L. If we calculate the ratio of limit/inside we get 4L/L*L which is 4/L for the square. Lets follow this same reasoning for the cube. The space inside the cube is of L*L*L, and the limits of the space are 6L*L, defined by the 6 squared faces of the cube. Therefore, if we perform the same calculation, and get the ratio of limit/inside on the cube, we get 6L*L/L*L*L, which is 6/L, higher than for the case of the square! If we keep going like this, and calculate the ratio of limit/inside for an N-dimensional cube, this number keeps getting larger and larger, always with an L in the denominator, but an increasingly big nominator. What does this tell us? Easy: as we increase the dimensions of our feature space, the proportion of space that is at the limits gets larger with respect to the total feature space. This is why when we randomly created points for a line, square, and cube, we saw that an increasingly big proportion of these points fell near the limits. Voila! Question answered. üëå I challenge you to do the same for a circle and a sphere. See if you can reach a similar conclusion! Little hint: if you imagine your infinite-dimensional spherical space as an infinite-dimensional orange, all of the points of your data would be near the skin, and none would be swimming in the tasty fruit that is inside. üçä Next. Why is our data more sparse in a high-dimensional space? Basically, if we increase dimensions or features, we are giving our data a higher chance to differentiate itself from other data points. Imagine you have a data set about people. If you only have the age of the people, you can basically make four or five groups: very young people, young people, middle aged people, seniors, and elderly. If now you add another dimension, lets say, gender, you double the possible combinations. If you add a third one, like the height, you triple the possible number of groups if we consider that we group people by small, normal, and tall. See where I‚Äôm going? More dimensions gives our data a higher chance of being different, of being unique, and that is why it becomes more sparse. This is also the answer to why the distance in between two random points increases with more dimensions: every data point is becoming increasingly separate and different from the rest! Lastly, why does the distance in between the point that is most far away, and the point that is closest get smaller and smaller until it is 0? This is partly related to what we just saw about our data becoming more unique and sparse. As we increase dimensions, we make our data points more different in between them, and give them a higher chance of differentiation. Our data becomes more and more sparse as we increase the number of dimensions. This means that the proportion of the feature space that is occupied diminishes. As the space gets larger and the data becomes more sparse the difference between the closest and the most far away point becomes smaller and smaller, until it reaches 0 for an infinite number of dimensions. Taran! Data really does behave wierd in high dimensions. Bottom line: when computing distance metrics, try to keep your features to a moderate amount, using a combination of feature selection or feature importance and dimensionality reduction techniques. You will see that many times this causes model performance increase, aside from facilitating interpretability and reducing training time. z-ai.medium.com That is all! For more resources on Machine Learning check out the following repo, I hope you love it! Stay tuned for my next posts! Until then, take care, and enjoy AI! Thanks for reading!",304,3,13,Towards Data Science,2021-01-04,2021
https://towardsdatascience.com/the-8-most-popular-coding-languages-of-2021-b3dccb004635,The 8 Most Popular Coding Languages of¬†2021,"And more importantly, how‚Ä¶",10,55,"['The 8 Most Popular Coding Languages of 2021', '1. The most popular coding language for absolute beginners: Python', '2. The most popular coding language for people who don‚Äôt want a programming job: R', '3. The most popular coding language to increase your salary: Perl', '4. The most popular coding language for mobile app development on iOS: Swift', '5. The most popular coding language for lateral thinkers: Ruby', '6. The most popular coding language for mobile apps and web development: JavaScript', '7. The most popular coding language to quickly increase your salary: Go.', '8. The most popular coding language in 2022: Rust', 'Final Thoughts']","How can you decide what the most popular coding language is? It‚Äôs like trying to pick the most popular ice cream flavor ‚Äî everyone has a favorite. The truth is that different coders prefer different coding languages for different reasons, and just when you think you can say a single coding language reigns supreme, a new one crops up, or an older one becomes relevant for a new application. The most popular coding language of 2021 will be based on what the coder in question wants to accomplish and what they‚Äôve already learned or done. For experienced coders hoping to increase their salary, the most popular coding language will be different than for programmers who are just starting out and want an entry-level coding job after a coding bootcamp. With so many constantly-shifting languages it‚Äôs hard to know where to start, especially as open source languages change all the time, with new packages and frameworks. No matter what your interest or need is, if you want to know what the most popular coding languages will be in 2021, you‚Äôll find them on this list. No matter what list you check, Python is almost always listed as the most popular coding language for beginners ‚Äî on Qvault‚Äôs post on best programming languages for beginners, GitHub‚Äôs ranking, Stack Overflow‚Äôs developer survey, and is even the top-most language universities are teaching to computer science majors. It‚Äôs not a fast-grower like Rust, it‚Äôs not an old institution like JS. But it is an unstoppable juggernaut of a language. It‚Äôs been around for 3 decades now and it‚Äôs experienced steady growth in use and popularity long enough to make it top just about anyone‚Äôs chart. The great thing about Python is that it‚Äôs written with the developer experience in mind. In practical terms, this means it reads like English ‚Äî easy especially for people with no coding background to pick up. It‚Äôs also very fast to build a basic prototype of anything, which makes it extremely empowering and rewarding for beginners who can produce functional code with a good tutorial in just a few minutes. Finally, it‚Äôs versatile. No matter what your need is ‚Äî data science, machine learning, web development ‚Äî you can probably use Python to do it. In summary, its syntax, ease of progression, and versatility makes Python the most popular coding language for absolute beginners. Python‚Äôs the most popular coding language for beginners because it comes with a robust support network for brand-new coders. You can begin coding using free tutorials on Python.org which is geared towards beginners. You should also take advantage of the rich and supportive online community of Python users and lovers. Most Pythonistas will remember their own days of learning Python and gladly lend a hand to beginners. Check out the Python subreddit, read and post Python questions on Stack Overflow, and see if you can find a coding buddy on a Discord group or a Slack channel. When I worked as a customer success manager, my job involved absolutely no coding. However, I still found it incredibly beneficial to be able to run analyses in R ‚Äî looking at retention, churn, amount of communication and more. R is another open-source coding language, less popular than Python but still very active and beloved in the data science community. If you‚Äôre looking to get a job in anything that isn‚Äôt programming, R is the most popular coding language for this. It‚Äôs replacing SQL and SAS which are closed-source, paid languages. As enterprises both want to reduce costs and want to hire people who are able to run analyses no matter if they‚Äôre coding in their day job or not, they are turning to R. In their R versus Python tutorial, Datacamp writes that R is used by ‚Äústatisticians, engineers, and scientists without computer programming skills. It‚Äôs popular in academia, finance, pharmaceuticals, media, and marketing.‚Äù R is the most popular coding language for people who don‚Äôt code in their jobs for a few very valid reasons. First, it‚Äôs open-source. Like Python, there‚Äôs no need to pay any money. It also has an integrated development editor, RStudio, that makes it even easier to use. It has a robust ecosystem of open source packages that make it very simple for anyone to run statistical analysis in a few lines of code and create a publication-ready graphic in just a few more lines. Especially for folks who don‚Äôt have or want a job in programming, I find it‚Äôs best to find a project you really care about and set out a specific goal. You don‚Äôt have the necessity from your job ‚Äî you can get away with not knowing how to code, for now anyway. You don‚Äôt have any prior experience coding, so the learning curve will be steep. You need something that you‚Äôre deeply passionate about. Only that will get you over the frustrating hurdles, knowledge gaps, and user errors that come with every coder‚Äôs beginner journey. Perl is one of the most contradictory languages on this list in that it has the highest global salary ($75k median annual salary), but is also the most dreaded language (71.4%), according to Stack Overflow‚Äôs survey. But if you want to get a higher salary in your programming job, there is no more popular coding language. It‚Äôs known for being the predecessor to the more popular PHP, and also for being a bit of a flaming dumpster heap of a language. The blog Some Dude Says writes in his blog post ‚ÄúPerl in 2020: Is It Still Worth Learning? ‚Äú that, ‚ÄúPerl tried to be too much for too many people. Snippets of terrible code floated around and were pulled in without a second thought on many projects. Script kiddie after script kiddie cobbled together their abominations and let them loose on the world. They threw the source online for the world to see for free too. Books were also rife with trash and republished even when they had long since become obsolete,‚Äù But despite being hated by many developers, many employers find it a useful coding language both for new projects as well as maintaining existing infrastructure and projects. That‚Äôs why it‚Äôs still the most popular coding language to increase your salary. Opensource.com lists Amazon, Boeing, BBC and Northrop Grumman among the many big-name employers looking for Perl developers. It‚Äôs a popular language for employers. Like R and Python, it‚Äôs open-source which means that it‚Äôs low-cost and low-risk to use. Many users (for example, on this subreddit) describe it as a language with a wide scope, and limited functions, making it simple and functional. The main reason it‚Äôs hated is perhaps a legacy of the factors Some Dude Blogs listed above ‚Äî a place where the open-source nature let this language down and allowed its good name to be dragged through the mud. However, with the latest releases of Perl that fixes many user experience issues and the upcoming release of Perl 7, it may be experiencing a slow climb in popularity again, especially given its high-demand nature among employers who, despite all its flaws as a programming language, find Perl to be an excellent skill to hire for, which earns it a spot on this list of most popular coding languages. The learn.perl.org website is probably the best place to start learning Perl. Unlike R and Python where many users learn from googling and copy-pasting code chunks, it may be best to stay away from places like Stack Overflow where bad code snippets may still be floating around. Modern Perl is also a relatively recent document that may avoid a lot of out-of-date opinionated tutorials, while the Perl Cookbook stands up as a tried-and-tested resource. R, Python, and Perl were all developed last century. Swift, meanwhile, was developed only in 2014 specifically for the purpose of being an Apple programming language. As the name implies, it has a reputation for being a speedy way to build iOS apps, quickly overtaking Objective-C which was originally built for that purpose. Apple.com itself says Swift is 2.6x faster than Objective-C and 8.4x faster than Python. Even though it‚Äôs a young language, it‚Äôs the 9th most-loved language on Stack Overflow‚Äôs developer survey of 2020. To write an iOS app, there‚Äôs no other most popular coding language. Dummies.com writes that ‚Äú[d]eveloping iOS apps can be the most fun you‚Äôve had in your career in years, with very little investment of time and money (compared with developing for platforms like Windows).‚Äù For people who want to code, building apps is a great way to showcase your skills or even earn some money on the side. Compared to Android apps, iOS has a more robust developer program and handles a lot fo the stickier sides of creating and hosting an app on a store. It‚Äôs also faster to develop an iOS app compared to Android. For those reasons, Swift is the most popular coding language for those who want to develop mobile apps for iOS. Apple obviously has a vested interest in helping developers learn Swift, so it‚Äôs open source. In a rather meta turn of events, Apple has actually developed an app called the Swift Playground, created to help beginner coders learn the basics of Swift, along with several other resources to help users learn. If you‚Äôre more advanced at coding or want to go off-piste to learn Swift, After that, the best method is simply to get your feet wet and design your first app using Swift. Ruby is one of the most popular coding languages for startups ‚Äî it‚Äôs a language where there‚Äôs more than one way to do things, with a very simple syntax that enables the ‚Äúmove fast and break things‚Äù ethos of many startups favored by lateral thinkers. Ruby on Rails, a full-stack web application framework that runs Ruby, is also very popular due to its ease of building a web app in very little time. For many beginner coders, it can feel limiting to work with a language like Python where there‚Äôs frequently just one way to do things. Ruby‚Äôs simple syntax allows for flexibility in approaches, which is a boon to individuals who are learning a second coding language, or who are more lateral thinkers and enjoy coming at things from alternative angles. This alternative angle puts Ruby on the list of the most popular coding languages in 2021. Because of Ruby‚Äôs dynamic nature, there isn‚Äôt a single method to learn things. While it‚Äôs important that you go into it understanding core coding concepts like variables, data structures and conditional statements, the simplicity of Ruby and Ruby on Rails means that once you‚Äôve grasped the basics, the next step should be trying to build a simple web app of your own. JavaScript is the most popular coding language for the web, responsible for interactive websites. Developed in 1995, it‚Äôs used by 95% of websites as the dominant client-side scripting language today. As Node.js was developed, many people began using JavaScript on the server-side, too. Along with CSS and HTML, it‚Äôs what builds what you see anytime you hop onto the World Wide Web. According to Stack Overflow‚Äôs Developer Survey for 2020, it‚Äôs the top-most used language for the eighth year in a row. As long as websites exist, JavaScript will be useful for any coder to learn. For any web developer, it‚Äôs obviously a must-have. And even if you don‚Äôt want to be a web developer, the ability to build your own website ‚Äî often used as resumes and portfolios nowadays ‚Äî is an attractive skill to showcase. For web development and mobile apps, the fact that JavaScript is the most popular coding language is remarkable, given it was created in only ten days as a response to the very first Browser War. It‚Äôs such a popular coding language because it‚Äôs ubiquitous, but it‚Äôs also good to understand why it‚Äôs ubiquitous. First, it‚Äôs usable for just about any both frontend and backend web development, but there have also been several frameworks developed to take it a step further. For example, JavaScript is also for desktop apps like Slack and Skype, which use Electron.js. Vue.js, Angular.js, and React.js are separate JavaScript web frameworks used to build user interfaces built by ex-Google employee Evan You, Google, and Facebook respectively. It‚Äôs also standardized, meaning that updates and releases with new versions frequently come out. No matter where you are in your coding career, this language has something to offer you. If you search ‚Äúlearn JavaScript,‚Äù it‚Äôs easy to become overwhelmed with the quantity of information, tutorials, and guides. It‚Äôs hard to even know which frameworks and libraries you need. That‚Äôs why I recommend a hierarchical approach that lets you systematically and consistently progress with learning the most popular coding language for web developers. Some examples include JS: The Right Way, and Qvault‚Äôs Basic Intro to Coding. Go was developed at Google, influenced by coding language giant C, but built to avoid the pitfalls of C++, which was universally despised by the developers of Go. The aim was to build a language that was fit for purpose in an era of enormous code bases. It‚Äôs now used by several big companies ‚Äî Google is obviously among them, but Uber, Twitch, and Dropbox also feature in the list. On Stack Overflow‚Äôs Developer Survey, it‚Äôs 3rd both on the list of most wanted languages (17.9%) as well as the highest median salary worldwide (74k). There are several reasons it‚Äôs the most popular coding language for coders who want to quickly increase their salary. First, it‚Äôs a language that‚Äôs built for big projects. Unlike Ruby, for instance, which is quick to build but hard to scale, Go was intentionally created by Google to help them with truly tremendously-sized projects and tasks faster. It‚Äôs intentionally created to reduce time spent reading and debugging code to help make these tasks doable. This makes it an attractive language for many big companies that are aiming to achieve projects on this scale. It‚Äôs also reputed to be faster and easier than Perl to learn, which tops the list of highest salaries. Perl comes with several decades of history and opinions to wade through, while Go, created only in 2009, has a smaller and more modern syntax. Unlike many coding languages with bloated vocabularies, Go is small enough to ‚Äúfit in your head,‚Äù to paraphrase Samuel Jones, a data engineer who wrote up a review of Go after using it to build an API. This reduces time searching for answers and syntax online and in reference books. It‚Äôs also possible to learn it just by reading, since the syntax is clear enough that non-Goers and even non-coders can look at it and understand what‚Äôs happening. As a language built by Google for the internet, you can imagine there are several free web-based resources that can help you on your way to learn. First, several sources recommended checking out A Tour of Go where you learn to use Go. It‚Äôs interactive and you run your own code snippets on the website itself. It‚Äôs divided into modules, which makes it easy to keep track of where you are and reference back if necessary. Once you‚Äôve grasped the basics, some other great resources include Go By Example and Go Mastery, where you‚Äôll learn to find examples of code for typical or common tasks. Rust is the most-loved language for the fifth year running and is the fifth-most wanted language (14.6%) according to the 2020 Stack Overflow‚Äôs Developer Survey. So why is it not top of the list despite being objective the most popular coding language of all? Because according to the very same survey, 97% of those survey respondents had never used Rust. It also has an uncertain future. Mozilla sponsored the development of Rust in 2009 and announced it in 2010. It is viewed as an alternative to other systems programming languages, like C or C++, built to ‚Äúbe a language for highly concurrent and highly safe systems‚Äù according to its Wikipedia page. What this means for developers is that they can write safe code quickly and efficiently. However, due to Covid, Mozilla laid off a large part of their Rust team to focus on commercial products. While they‚Äôve announced that there will be a foundation created to take ownership of the future and costs of Rust, it‚Äôs unsure yet how that will shape up. Assuming Rust is able to overcome current difficulties, I expect it will become more popular year-on-year. It has a dedicated fan base of current developers, with a growing chunk of coders interested in learning it. While it may not be top of the list in 2021, I believe it may be the most popular coding language of 2022 or beyond. Many coders compare it favorably to C++ in terms of ease of learning. The priority on safe code can be frustrating for many coders who type in code only to get annoying error messages, but this may be a shift away from the ‚Äúmove fast and break things‚Äù mentality and moving more towards a safer, more structurally sound codebase. It has applications for both long-standing necessities of development as well as more futuristic endeavors. Mozilla‚Äôs page on it describes the applications as ranging from, ‚Äúgame engines, operating systems, file systems, browser components and simulation engines for virtual reality.‚Äù For these reasons, Rust may be the most popular coding language of the future. Because Rust is so focused around safety and structure, developers have spent a lot of time and resources ensuring error messages are user friendly, unlike many other coding languages. This makes it especially rewarding for beginners to learn, as when they make a mistake, it‚Äôs easy to correct. The Rust website offers three paths to try ‚Äî reading from what‚Äôs known as ‚Äú The Book,‚Äù trying out the Rustlings Course with small projects to help get you up and running, and Rust By Example, which illustrates the concepts and libraries that underpin Rust. All these resources are free. As one Redditor points out, it‚Äôs a relatively new language in that many of the answers to questions you‚Äôll have, haven‚Äôt yet been posted and answered on places like Stack Overflow. For this reason, they recommend joining the Discord channel as a way to gain mentorship and a supportive community to learn Rust. There are plenty of languages to choose from, and as you‚Äôve seen from this list, many are recent. While some are objectively better than others for specific tasks, most serve a good purpose for someone. If you want to learn the most popular coding language of 2021, you first have to decide what you want from learning a coding language. It‚Äôs always good to stay on top of trends and make sure you‚Äôre at the top of your coding game, no matter where you sit. More than 70% of developers at all levels of professionalism learn a new coding skill at least once a year. Why not start 2021 right and prioritize your future skill set with one of these most popular coding languages of 2021? This list will help you choose the one(s) you can start with.",,0,15,Towards Data Science,2021-01-04,2021
https://towardsdatascience.com/6-cool-python-tricks-you-should-know-bab1b6a42446,6 Cool Python Tricks You Should¬†Know,Go beyond the¬†usual,1,24,['6 Cool Python Tricks You Should Know'],"Data science has experienced monumental growth in recent years. Python, being the most common programming language in data science domain, increased its popularity as well. In this article, I will mention 6 Python tricks that I think are pretty cool. They will also make your life easier by providing practical ways of accomplishing certain tasks. To become a good programmer, it is not enough just to write code to accomplish a given task. Your program should be efficient in terms of time and computational complexity. Furthermore, the code should be clean, easy to read and debug, and maintainable. The tricks in this article will help you achieve the ultimate goal of writing clean and efficient code. Slices are objects so they can be stored in variables. Some data structures allow for indexing and slicing such as lists, strings, and tuples. We can use integers to specify the upper and lower bound of the slice or use a slice object. The slice s represents a slice from the fourth element to the sixth element. We apply the same slice object to a list, string, and tuple. We are likely to encounters cases where we need to swap the values of two variables. One way is to use an intermediate variable to temporarily hold values. This way is kind of tedious. Python provides a much better way for swapping. Consider we have a list of lists. We can sort the list based on the first or second items of the inner lists by using the sort function with a lambda function. The list is sorted based on the second items. We can do the same with the first items just by changing the 1 to 0. Consider we have a function that multiplies the given numbers. This function works fine if we need to multiply just three numbers. It must be given exactly three numbers. We can make the function more flexible by using argument unpacking. Now the mult function is able to multiply any number of values. Argument unpacking is very commonly used in Python. You must have seen *args and **kwargs if you read the documentation of a package or library. Assume we have a function that returns a tuple of two values and we want to assign each value to a separate variable. One way is to use indexing as below: There is a better option that allows us to do the same operation in one line. It can be extended to a tuple with more than 2 values or some other data structures such as lists or sets. It is a common practice to add variables inside strings. F strings are by far the coolest way of doing it. To appreciate the f strings more, let‚Äôs first perform the operation with the format function. We specify the variables that go inside the curly braces by using the format function at the end. F strings allow for specifying the variables inside the string. F strings are easier to follow and type. Moreover, they make the code more readable. We have covered 6 simple yet highly practical tricks. They cause small changes but small improvements add up. You will eventually be writing code that is more efficient, easier to read and debug, and more maintainable. Thank you for reading. Please let me know if you have any feedback.",281,2,4,Towards Data Science,2021-01-04,2021
https://towardsdatascience.com/overcoming-data-preprocessing-bottlenecks-with-tensorflow-data-service-nvidia-dali-and-other-d6321917f851,"Overcoming Data Preprocessing Bottlenecks with TensorFlow Data Service, NVIDIA DALI, and¬†Other",,9,57,"['Overcoming Data Preprocessing Bottlenecks with TensorFlow Data Service, NVIDIA DALI, and Other Methods', 'Data Preprocessing Bottleneck', 'Sample Use Case', 'Identifying the Bottleneck', 'Prepone Operations to Data Preparation Phase', 'Optimize the Data Preprocessing Pipeline', 'Offload Operations to the GPU', 'Offload Data Preprocessing to Other Machines', 'Summary']","In a previous post, I spoke about the importance of profiling the runtime performance of your DNN training sessions as a means to making the most of your training resources, accelerating your training, and saving money. I described a typical training pipeline, (see the diagram below), reviewed some of the potential performance bottlenecks, and surveyed some of the tools available for identifying such bottlenecks. In this post I would like to expand on one of the more common performance bottlenecks, the CPU bottleneck, and some of the ways to overcome it. More specifically, we will discuss bottlenecks that occur in the data preprocessing pipeline, and ways to overcome them. In the context of this post, we will assume that we are using TensorFlow, specifically TensorFlow 2.4, to train an image processing model on a GPU device, but the content is, mostly, just as relevant to other training frameworks, other types of models, and other training accelerators. A CPU bottleneck occurs when the GPU resource is under utilized as a result of one, or more of the CPUs, having reached maximum utilization. In this situation, the GPU will be partially idle while it waits for the CPU to pass in training data. This is an undesired state. Being that the GPU is, typically, the most expensive resource in the system, your goal should always be to maximize its utilization. Without getting into too many technical details, a CPU bottleneck generally occurs when the ratio between the ‚Äúamount‚Äù of data pre-processing, which is performed on the CPU, and the ‚Äúamount‚Äù of compute performed by the model on the GPU, is greater that the ratio between the overall CPU compute capacity and the overall GPU compute capacity. For example, if both your CPU cores and GPU are maximally utilized, and then you upgrade to a more powerful GPU, or downgrade to a system with fewer CPU cores, your training runtime performance will become CPU bound. Naturally, your first instinct will be to simply switch over to a machine with a more appropriate CPU to GPU compute ratio. But, sadly, most of us don‚Äôt have that freedom. And while cloud services, such as Amazon SageMaker, offer a variety of training instance types, with different CPU-compute to GPU-compute ratios, you may find that none of them quite fit your specific needs. Assuming that you are stuck with the system that you have, what steps can you take to address your performance bottleneck and speed up the training? In the next sections we will propose four steps for addressing the preprocessing data bottleneck. In order to facilitate our discussion, we will build a toy example based on Resnet50. In the code block below, I have built a model using TensorFlow‚Äôs built in Resnet50 application. I have added a relatively heavy data pre-processing pipeline which includes dilation, blur filtering, and a number of TensorFlow pre-processing layers. (See the documentation for the advantages of using such layers.) The raw data input is stored in TFRecord files, which I created from the CIFAR-10 dataset, (using this script). I have created this example so as to artificially create a performance bottleneck. I would not, under any circumstances, recommend using it for actual training. All tests were run on an Amazon ec2 p2.xlarge instance type using an Amazon Deep Learning AMI. There are a number of different tools and techniques for evaluating the runtime performance of a training session, and identifying and studying an input pipeline bottleneck. Let‚Äôs review just a few of them: The first thing to check is the system resource utilization. There are a number of different ways to do this. The Linux top command shows the CPU utilization. To see how the utilization breaks down per CPU core, type ‚Äò1‚Äô while top is running. To measure the GPU utilization, you can use nvidia-smi. When training in Amazon EC2, you can use Amazon CloudWatch to monitor system metrics. While the GPU metrics are not included by default, you can add these by using the gpumon utility. Below is a sample graph of the CPU and GPU utilization captured across several different experiments. In the use case we introduced above, the average reported GPU utilization caps out under 50% with long periods of idle time. At the same time, the CPU is highly utilized, with some of the cores reaching maximum utilization. To dive into the next level of detail of how the training is performing, you can use a performance profiler. TensorFlow Profiler: The built in TensorFlow profiler includes a wealth of performance analytics, and in particular tools for analyzing the performance of the input pipeline. You can view using TensorBoard by installing the TensorBoard profile plugin. One way to enable the profiler, is to program the training loop with the TensorBoad callback. Below is the profiling overview page for our use case example on which the data input bottleneck is glaringly apparent. The trace-viewer tool allows you to drill down into the details of the pipeline execution, and study the flow of data in between the CPU and GPU. In our example, you can clearly see long periods of GPU idle time, due to the data input bottleneck. Amazon SageMaker Debugger: If you are training in the Amazon SageMaker environment, you can take advantage of the profiling features that are built into Amazon SageMaker Debugger. Here is an example of how a severe bottleneck in the input pipeline will appear in Amazon SageMaker Studio. Linux Profilers: General purpose Linux performance profilers are also often helpful in analyzing training bottlenecks. For example, using the Linux perf utility we are able to see that our CPU spends a large chunk of its time on an internal linear algebra function: Being that the objective of our analysis is to accelerate the training runtime, it is only natural that we would use this metric as a measure of our performance. In our example, we will use the average runtime of a single (100 step) epoch as our primary performance metric, and measure how different changes to the model affect this value. The average runtime of a single epoch of the model above, is 122 seconds. A useful technique (described here) for measuring what the runtime would be, if it were not for the data input bottleneck, is by caching the first processed input batch and using the same cached batch for all subsequent steps. This essentially shuts off the preprocessing pipeline, and enables us to calculate the ideal epoch runtime. To implement the technique, we simply tack on the following line of code at the end of our dataset creation: By applying this technique to our example, we are able to reduce the runtime to 58 seconds. In other words, were it not for the data input bottleneck, we would be able to speed up training by more than a factor of 2. In the next sections we will walk through a number of proposed steps for solving a bottleneck in the input pipeline. We will demonstrate some of the steps on our toy example, keeping in mind the target runtime we have just calculated, 58 seconds per epoch. The first thing to do in order to address the data preprocessing bottleneck, is to identify any operations that can be preponed into the, raw, data record creation phase. The more operations we can move into the data creation phase, the more we can free up CPU cycles during training. Any operations that are run in the beginning of the pipeline, in a deterministic fashion (have no random component), that do not depend on a hyper-parameter, and do not excessively increase the size of the data, are good candidates for preponement. In our toy example, the dilation operation, (assuming it does not depend on a hyper parameter), fits this criteria. So the first thing we will do is knock off the dilation operation, and assume that the TFRecords contain the image data after it has already undergone appropriate dilation. In our specific implementation, the blur filter might have also been a good candidate for preponement, but since, in most cases, blurring is applied randomly, we will leave it in. By removing just the dilation operation, our runtime decreases to 115 seconds per epoch. This is less than our starting value of 122 seconds per epoch, but we still have a long way to go to get to our target of 58 seconds per epoch. One thing to take note of, is that certain operations might change the size of your data records, and thus, might impact the overall size of your dataset, as well as the amount of network traffic during training (if the training set is stored remotely). If you choose to prepone operations that increase the size of the data, excessively, you might run the risk of replacing one bottleneck with another, i.e. a network IO or data-loading bottleneck. Once we have moved as many operations as possible to the data creation phase, the second step is to identify ways in which to optimize the performance of the remaining pipeline. Often times, some small tweaks to the input pipeline setup, could reduce the performance overhead. Here are just a few things you could try: Make sure that your TensorFlow binaries were configured (and compiled) to take full advantage of your CPU, and CPU extensions. For example, if you are using a modern x86 ISA CPU, (such as Intel or AMD), make sure to use TensorFlow binaries that are optimized to use the CPU‚Äôs advanced vector extensions (e.g. AVX2). Intel, in general, offers a wide variety of binaries that are specifically optimized to run on Intel CPUs, including intelpython, and TensorFlow-mkl. Note that one of the advantages of using a cloud based solution for training, is that the cloud learning environment is, (presumably,) configured to take full advantage of the cloud system resources. When you have a CPU bottleneck in a multi-CPU core system, you might find that, while one or more of the CPU cores are at full utilization, other are not. This is actually quite common. One thing that you could try, is to improve the load balancing between the CPUs so that the overall CPU utilization increases. You could try this by using the tf.config.set_logical_device_configuration API to separate the CPU compute into multiple logical devices, and the tf.device API to specify where each operation should be run. You can also try to improve the load balancing by playing around with different options for the num_parallel_calls argument of the tf.data.Dataset.mapfunction, (instead of relying on TensorFlow‚Äôs autotune feature). In any case, keep in mind that this is likely to be a tedious, pain-staking, effort, and that even the slightest change to your model, will, likely require recalculating the load balancing. As in our example, you might find that even after you have exhausted all options for preponing operations to the data creation phase, and optimizing the CPU code, you continue to face a data preprocessing bottleneck. The next option to consider is to modify the load balancing between the CPU and the GPU, by moving some of the preprocessing operations onto the GPU. The downside to this approach is that we are almost certain to increase the runtime of a GPU step. Also, since we are increasing the size of the computation graph that is running on the GPU, we may need to free up some GPU memory by running with a smaller batch size. As a result, it is highly unlikely that we will be able to achieve the target throughput we calculated above. But if it reduces the overall train time, then it is totally worth it. Let‚Äôs explore a few ways to offload preprocessing operations onto the GPU. In most cases, the best way to offload from the CPU, is by moving operations that are performed at the end of the preprocessing pipeline unto the GPU. By targeting these ‚Äútail‚Äù operations, rather than operations in the middle of the pipeline, we avoid the overhead of data transfers between the GPU and the CPU. If the ‚Äútail‚Äù operations are performed on the model input, we can place them at the head of the model. If they are performed on label data, we can modify our loss function to perform these operations before applying the actual loss. In our example, we have removed the augmentations from our input pipeline, and instead applied them to the beginning of our GPU computation graph: By applying this change, we are able to reduce the runtime down to 108 seconds per epoch. By wrapping operations with a tf.device(‚Äò/device:GPU:0‚Äô) statement, we can force certain operations to run on the GPU. The downside to this method, is that it requires transferring data to and from the GPU device. In our example, we chose to apply this technique to the blur function, by modifying it as follows: When running the blur function on the GPU in this manner, while leaving the augmentations on the CPU, we attain an epoch runtime of 97 seconds. When combining both techniques, the epoch runtime is 98 seconds. Using the TensorFlow profiler trace-viewer, we are able to see how the tf.device technique increases the data traffic between the CPU and the GPU: By comparing the highlighted streams in this experiment, to the same streams in the trace-viewer capture above, we see that there are significantly more memory copies to and from the GPU. We also see that the GPU is far more active. Another way to verify that the blur function is indeed running on the CPU, is to set tf.debugging.set_log_device_placement(True). You can run the example, once with the blur function on the CPU, and once with the blur function on the GPU, and see how it impacts the output of the log device placement routine. NVIDAI DALI is a framework for building highly optimized preprocessing pipelines. In particular, using NVIDIA DALI, you can program parts of your pipeline, or your entire pipeline, to run on the GPU. A DALI pipeline is built from DALI operations. DALI comes with a list of supported operations, as well as APIs for creating custom operations. Using the TensorFlow DALI plugin, DALI pipelines can be wrapped with the, tf.data.Dataset API compliant, DALIDataset, as shown here. In addition, DALI supports loading from TFRecord files as shown here. Unfortunately, as of the time of this writing, the documented support for DALI is limited to version 1 compatible TensorFlow. (Those of you who have read my previous blogs, should already know how I feel about using legacy code.) In addition, NVIDIA DALI was designed for NVIDIA GPUs. It will not run on other machine learning accelerators. Another consideration is distributed training. While DALI does support multi-gpu training, depending on how you implement distributed training, (e.g. with Horovod or a TensorFlow distribution strategy, with model.fit() or a custom training loop), integrating a DALI pipeline will vary between being slightly more difficult, and much more difficult. If you feel strongly about using the latest TensorFlow features, or if you want your code to be compliant with other accelerators, (AWS Tranium, Habana Gaudi, TPU, etc.), or if converting your pipeline to DALI operations would require a lot of work, or if you rely on the high level TensorFlow distributed training APIs, NVIDIA DALI might not be the right solution for you. Using DALI requires use of the TensorFlow DALI plugin python package. See the documentation for installation steps. In the code block below, I show how to convert the pipeline from our use case to NVIDIA DALI. I have left out some of the random augmentations, as there were no built-in, corresponding, DALI operations. I ran the script in TensorFlow 2.3 (as it would seem that, as of the time of this writing, DALI has not been updated to support TensorFlow 2.4). The resultant runtime of a 100 step epoch was 77 seconds. While this trial did not include the augmentations, it is clear that DALI offers potential for significant runtime improvement. As I mentioned above, offloading operations to the GPU might require freeing up some memory be reducing the size of the training batch. It turns out that this was not required in our toy example. (This probably means that we could have started out with a larger batch size.) This finding will not necessarily carry over to other models, especially if you are making sure to maximize your GPU memory and batch size. The final option we explore, is to offload some of the preprocessing activity to other machines. Rather than moving preprocessing computation onto the GPU, we will move it to CPU cores on auxiliary machines. We will explore this approach using the, relatively new, TensorFlow data service feature. Introduced in TensorFlow version 2.3, tf.data.experimental.service provides APIs for defining dedicated worker machines for performing data preprocessing. A dispatch server is responsible for distributing preprocessing tasks to one, or more, worker servers, each of which load the raw data directly from storage, and send the processed data to the GPU device. By applying tf.data.experimental.service.distribute to your dataset, you can program the dataset to run all preprocessing operations up to the point of application, on the dedicated workers. The number and types of worker services to use, and where in the pipeline to apply the service, should be determined by considerations, such as the severity of your bottleneck, the availability and cost of auxiliary machines, the manner in which the preprocessing operations impact the size of the data, and how this impacts the network traffic. For example, if you choose a remote worker machine with a low network bandwidth, and program a preprocessing operation that blows up the size of the data to run on the worker, you might not see any performance improvement. Let‚Äôs demonstrate the use of this API on our toy example. For this demonstration, I have chosen a single, auxiliary Amazon EC2 c5.4xlarge instance with 16 CPU cores, and with the same Amazon Deep Learning AMI. The communication between p2.xlarge and the c5.4xlarge will use the grpc network protocol, so you need to make sure that both instances are in a security group which allows inbound traffic of the relevant protocol, one from the other. On the worker device we run the following script, where ‚Äú10.0.1.171‚Äù is the ip adress of the auxiliary device: Note that we are running the dispatch server and worker server on the same machine. We also make sure that the TFRecord files are copied over to this machine, as the workers will load the raw data from those files. On the GPU machine we have modified the train script as follows: Note, that we have programmed just the record parsing, and heavy blur function to run on the worker. The batching and augmentations remain on the primary device. The results of running this setup could not be better! The runtime per epoch is 58 seconds, meeting the target we set for ourselves above. By using an auxiliary CPU device, and the TensorFlow data service to offload preprocessing computation, we have completely solved the CPU bottleneck! And indeed, we find that the average GPU utilization in this case is up around 97%. In the table below we summarize our findings on our toy, resnet50 model: In this post we have surveyed a number of ways to address a performance bottleneck in the data input pipeline. In particular, we have shown how the TensorFlow Data Service can be used to completely solve this bottleneck. This survey is not intended to be all comprehensive. There are likely to be additional tools and techniques available. While we have demonstrated how to apply these techniques to a toy Resnet50 model, their impact is certain to vary across models and datasets. Please don‚Äôt hesitate to share your own tools, techniques, and experiences.",73,4,17,Towards Data Science,2021-01-04,2021
https://towardsdatascience.com/bayesian-ab-testing-part-i-conversions-ac2635f878ec,Bayesian AB Testing‚Ää‚Äî‚ÄäPart I‚Ää‚Äî‚ÄäConversions,"How to model, use and analyse conversion based metrics¬†in‚Ä¶",7,57,"['Bayesian AB Testing ‚Äî Part I ‚Äî Conversions', 'Series Structure', 'Experiment Context', 'Choosing a Prior Distribution', 'Setting a loss threshold', 'Experiment Results', 'References']","In a previous blog post, I discussed the advantages of using Bayesian AB testing methods rather than frequentist ones. In this series of blog posts, I will be taking a deeper dive into the calculations involved and how to implement them in real world scenarios. The structure of the series will be as follows: So without further ado, let‚Äôs jump into how to model, use and analyse conversion based test metrics in bayesian product experiments. Following on from the example used in the previous post, let‚Äôs assume we‚Äôve recently changed the messaging on an upsell screen and want to AB test it before releasing to our wider user base. We hypothesise that the changes we‚Äôve made will result in a significantly better conversion rate. Similar to frequentist methods, we can model each conversion on the upsell screen, ùëã, as Bernoulli random variable with conversion probability ùúÜ: While under the frequentist methodology we‚Äôd assume that ùúÜ is a constant, under the bayesian methodology we model it as a random variable with its own probability distribution. Our first step is to choose an appropriate approximation for this distribution using past data. We call this our prior distribution of ùúÜ. We then set our loss threshold, which is the largest expected loss we‚Äôre willing to accept if we were to make a mistake. As with any statistical modelling, bayesian experimentation methods are built on approximations of real world data. As such, there is always a chance that we draw the wrong conclusions from the test. This loss threshold allows us to say that even if we did draw the wrong conclusions, we can be confident that the conversion rate wouldn‚Äôt drop more than an amount we‚Äôre comfortable with. Finally, we draw samples in the form of a randomised experiment and use these to update the distribution, and thus our beliefs, about ùúÜ under the control and treatment versions of the upsell screen. We can use these posterior distributions to calculate the probability that treatment is better than control and expected loss of wrongly choosing treatment. So our first step is to choose a prior distribution of ùúÜ. To do this we can look at the data we‚Äôve recently (last few weeks) gathered about this conversion. I‚Äôve generated a sample prior data set which we can use for this exercise. Since this data set is artificially generated, it‚Äôs already in the ideal format for this exercise. In the real world, it‚Äôs likely that we‚Äôd need to perform some ETL operations in order to get the data in this format. However this is outside the scope of this post. We see that we have a sample size of 5268 users, and for each user we can see whether they converted on this screen or not. We can go ahead and work out the prior conversion rate. We see that our prior data gives us a conversion rate of ~30%. We now use this to choose a prior distribution for ùúÜ. Choosing a prior is an important aspect of bayesian experimentation methods, and deserves a post on its own. I will be diving into it in more depth in part 4 of this series. However, for the purposes of this post, I will be using a rough method of choosing a prior. We‚Äôre going to use the beta distribution to model our conversion rate since it‚Äôs a flexible distribution over [0,1] and is also a good conjugate prior. It will make our calculations easier when we work out posteriors with experiment data. When choosing a prior distribution for our conversion, it‚Äôs best practice to choose a weaker prior than the prior data suggests. Once again I will explore this in more depth in part 4 of this series, but essentially, choosing too strong a prior could result in our posterior distributions being wrong and could therefore lead to wrong calculations and conclusions. With that in mind, let‚Äôs look at potential priors of varying strength. Here we can see three prior distributions which have a mean conversion rate of ~30%, similar to our prior data, and are all much weaker than the true distribution of the prior data. Let‚Äôs choose a prior that is in between the weakest and mid priors in the diagram ùêµùëíùë°ùëé(7,15) where ùêµ(ùëé,ùëè) is the beta function defined as and has the property Now that we‚Äôve chosen our prior, we need to choose our ùúñ which is the highest expected loss we‚Äôre willing to accept in the case where we mistakenly choose the wrong variant. Let us assume that this is an important conversion for us so we want to be pretty conservative with this ùúñ. We aren‚Äôt willing to accept a relative expected loss of more than 0.5%. So we set ùúñ=0.005‚àó0.3=0.0015. We have a prior and a threshold for our expected loss, so we can start running our experiment and gathering data from it. Let‚Äôs assume that we‚Äôve left our experiment running for a couple of weeks and want to check whether we can draw any conclusions from it. In order to do this we need to use our experiment data to calculate our posterior distributions, which we can then use to calculate the probability of each variant being better, and also the expected loss of wrongly choosing each variant. For the purposes of this exercise, I‚Äôve generated a sample experiment data set. Let‚Äôs start off by exploring it and aggregating it to find out the conversion rates of each variant. We see that the data set is similar to the prior data set with an extra column for specifying which the group the user was allocated to and therefore which variant they saw. Once again it‚Äôs worth noting that since this data set is artificially generated, it‚Äôs already in the ideal format for this exercise without the need for additional ETL operations. We can now go ahead and aggregate the data. We can tell by inspection that treatment had a better conversion rate, but we need to perform further calculations in order to update our beliefs about the respective conversion probabilities ùúÜ_ùëê and ùúÜ_ùë° of the control and treatment variants. With our experiment data, we can now calculate our posterior distributions under each variant. But before we do let‚Äôs explore the maths behind how this is possible from just the information that we have. We‚Äôre going to use a theorem[1] that states the following: Suppose we have the prior Suppose a variant was displayed to ùëõ visitors and ùëê converted, then the posterior distribution of the variant is given by Let‚Äôs go ahead and prove this. By bayes theorem we have the following Since we modelled each conversion as a Bernoulli RV with probability ùúÜ, given ùúÜ we can model the result of showing a variant to ùëõ visitors as a Binomial RV. So we have And thus, using the definition of our prior Let‚Äôs now just consider the coefficients Using the definition of the beta function we can say Substituting (3) and (4) back into (2) Substituting (5) back into (1) Since ùëì(ùúÜ;ùëé+ùëê,ùëè+ùëõ‚àíùëê) is a distribution over [0,1] the denominator is 1 and we have the result we were after. Now that we‚Äôve sufficiently convinced ourselves of the maths, we can calculate our posterior distributions. Now that we‚Äôve got our posterior distributions, we can go ahead and calculate the joint posterior. Since randomised experiments are built on the idea of random assignment of a user to a variant, we can assume that these two distributions are independent. Note that this isn‚Äôt always the case. For example, there might be some cases where network effects might be in play and we‚Äôd need to take that into consideration. This assumption is also dependent on the procedure of random assignment working properly. Let us assume that our method of random assignment has worked properly and that there are no network effects. Under this assumption we can say the following: Let‚Äôs use this to calculate our joint posterior distribution. The blue line in this graph is the line that represents ùúÜ_ùëê=ùúÜ_ùë°. Since the joint posterior is above this line, we can use this as a visual indication that the treatment is in fact better. If the joint posterior was below the line then we could be pretty sure that control was better. If any part of the joint posterior was on the line then there would be more uncertainty on which variant was better. In order to quantify this, we need to calculate ùëù(ùúÜ_ùë°‚â•ùúÜ_ùëê) and ùê∏[ùêø](ùë°), the expected loss of wrongly choosing treatment. From the simulations we see that ùëù(ùúÜ_ùë°‚â•ùúÜ_ùëê)=0.9718 so treatment has a 97% chance of being better than control. Now that we‚Äôve calculated the probability of treatment being better, we need to calculate ùê∏[ùêø](ùë°). The loss function of each variant is given by So the expected loss of each variant is given by We use this to calculate the expected loss[2] From running simulations we see that: ùê∏[ùêø](ùë°) = 0.0001369 < 0.0015 = ùúñ Since the expected loss of one of the variants is below the threshold that we set at the start of the test, the test has reached significance. We can conclude with high confidence that the treatment is better, and that the expected cost of mistakenly choosing treatment would not be greater than what we‚Äôre comfortable with. So we can strongly recommend that the treatment variant of the upsell screen should be rolled out to the rest of our user base. I hope this case study was useful in helping you understand the calculations required to implement bayesian AB testing methods. Watch this space for the next parts of the series! [1] VWO Whitepaper by C. Stucchio [2] Bayesian A/B testing ‚Äî a practical exploration with simulations by Blake Arnold ‚Äî I‚Äôve used the logic from Blake‚Äôs code for calculating expected loss Also found The Power of Bayesian A/B Testing by Michael Frasco very helpful in understanding the technical aspects of bayesian AB testing methods My code from this post can be found here Thanks for reading this article! I hope it helped you get a better understanding of how to implement bayesian AB testing methods for conversion metrics. If you enjoy reading my articles, would like to support my writing and are thinking of getting a Medium subscription, feel free to use my referral link below. I‚Äôd get a percentage of your subscription fees. medium.com",242,0,11,Towards Data Science,2021-01-04,2021
https://towardsdatascience.com/modelling-the-probability-distributions-of-dice-b6ecf87b24ea,Modelling the probability distributions of¬†dice,A tutorial in statistical analysis and mathematical‚Ä¶,1,48,['Modelling the probability distributions of dice'],"If you have ever played the game of Catan you‚Äôll quickly realise that when rolling two dice the number 7 is very common! This brought up a question, which is as follows: What is the true probability of rolling a sum of 7 with two 6-sided dice? Moreover, what is the probability of rolling a sum of any number with n 6-sided dice? Here is what I did to find the answers. Let‚Äôs first run a few dice experiments (ie. simply roll some dice). Lucky for us we can use computers to produce millions of dice rolling simulations in a few minutes instead of rolling them ourselves for a few years. Just by eye-balling the experimental data in Figure 1 we can see a familiar shape emerging as the number of dice increases, a bell curve, also known as a normal or Gaussian distribution. We can work with this, and try to fit a Gaussian to the experimental data. A Gaussian distribution is mathematically expressed as . The two parameters Œº and œÉ correspond to the mean and the standard deviation of the probability distribution, they define the central position and the width of the distribution respectively. But what are the best values of these parameters to fit our n-dice experimental data? Well, we can infer the most likely parameter values via statistical analysis. We can see from Figure 1 that the probability distributions are symmetric. Using this symmetry we can define the means of the experimental data by simply locating the maximum positions of each distribution. Figure 2 below is a plot of these maximum positions for an increasing number of dice. It can be seen from the figure that a linear correlation exists between the mean Œº and the number of dice n with a line of best fit of Œº=3.5n. Now with values for the means we can use the method of least squares to find the values for the standard deviation œÉ that correspond to the best fitting Gaussians to the experimental data. The method of least squares defines a metric to compare how similar two sets of data are, this metric is known as the mean squared error (MSE) which is mathematically expressed as . MSE is the mean squared difference between the experimental data (X·µ¢ ) and the Gaussian fitting (P(x·µ¢)) for a given œÉ value, where n denotes the number of bins in the histogram data and i denotes each bin. So finding the œÉ value that minimises MSE corresponds to minimising the difference between the Gaussian fitting and the experimental data ie. The best fit. Figure 3 shows the minimisation process for identifying the most likely parameter values (which were Œº = 14 and œÉ = 3.36) for four dice. Using the mean squared error to infer the most likely œÉ values for a range of dice we can plot œÉ as a function of the number of dice n. Figure 4 shows the plot of œÉ(n), we can see that the experimental data traces out a ‚àön looking curve. Again, using the method of least squares we can find the best fitting ‚àön curve to the experimental data, this resulted in œÉ(n) = 1.75‚àön. So, given n-dice we can now use Œº(n) = 3.5n and œÉ(n) = 1.75‚àön to predict the full probability distribution for any arbitrary number of dice n. Figure 5 and 6 below shows these fittings for n=1 to n=17. We have so far shown that with a very quick empirical effort we can predict the data amazingly well using Gaussians. However, despite the success in fitting the larger n cases our Gaussian fit is still only an approximation. This is why our fitting is worse for smaller n and the first two cases (1 and 2 dice) are not captured very well by our Gaussian approximation. Let‚Äôs see if we can derive the exact solution for the probability of a given total sum for n dice! Warning: This will involve some maths‚Ä¶ Let us first look at an example of a simpler version of this problem. Let‚Äôs consider a two-sided dice, this is essentially what a coin is. So with a coin we have two possible outcomes heads or tails. Now let‚Äôs say we have three coins and we ask what are the possible outcomes if we flip these? the possible outcomes are shown in Table 1 below. There are 8 possible outcomes when the coins are distinguishable and 4 possible outcomes when we consider the coins to be indistinguishable. This indistinguishable distribution is known to be captured by the binomial . Here, the term H¬≥ represents (H,H,H) and H¬≤T represents the case where two heads and one tails is observed, the coefficients represent the number of combinations that give rise to that given observation, ie. 1 for H¬≥ and T¬≥, and 3 for H¬≤T and T¬≤H. Each coefficient of the binomial can be determined by a neat little equation where k denotes the number of heads that get flipped and (n-k) is the number of tails. For example, 2 heads and 1 tails (n=3,k=2) . Combining the binomial coefficient with the probability of flipping each combination gives us the probability of that observation , where H represent the probability of flipping a heads (H=1/2) and T represents the probability of flipping a tails (T=1/2). The general binomial probability distribution of n coins and flipping k heads and (n-k) tails is given by . The full binomial for n-coins (H+T)‚Åø can be expressed as a series . So, with this knowledge now lets get back to considering 6-sided dice, where we label each side as x‚Å± (i corresponds to the number on that side). Similar to the 2-sided coins, n lots of 6-sided dice can be fully described by the multinomial . Note, we can use the exponents to express the total sum of each possible outcome. For example, x¬πx¬π =x¬≤. This is useful for us because now we have a way of identifying the total sum of each combination. For example x¬πx‚Å∂ = x‚Å∂x¬π=x¬≤ x‚Åµ= x‚Åµx¬≤ = x‚Å¥ x¬≥ = x¬≥x‚Å¥= x‚Å∑. These all represent a total sum of 7, and therefore the probability of rolling a 7 with 2 dice is a combination of the probability of rolling any of these: (1,6),(6,1),(2,5),(5,2),(4,3),(3,4). As in the coin scenario the coefficients of each possible factor of the multinomial f(x, n) multiplied by the probability of getting that factor (ie. (1/s)‚Åø, where s=the number of dice sides) tells us the probability of rolling that factor. We now want to use this to tell us what the probability of getting any given total T as a function of dice n. We can express f(x, n) as Where s denotes the number of sides the dice have (s=6). Notice that the sum here is a geometric series, geometric series can be alternatively expressed as So let‚Äôs replace the sum with this alternative expression . The resulting two terms are binomials and can be expanded as binomial series. We saw early when considering the coin that a binomial can be expanded to a series form as So (1- x À¢)‚Åø in series form is . Now (1-x)‚Åª‚Åø is a little less intuitive to expand as the exponent is negative. We can expand this by considering the Maclaurin series expansion of the function, this is a Taylor expansion around the point (x=0). The Taylor expansion of a general function g(x) is . Using the chain rule we can find the derivatives for the case g(x)=(1-x)‚Åª ‚Åø . Following the pattern of the series we can generalise for the l·µó ∞ derivative of g(x=0), . So, the full Taylor series of g(x) is and expanded around x=0 we have . Putting the series expressions derived for (1- x À¢)‚Åø and (1-x)‚Åª ‚Åø together we have an expanded series expression for the full multinomial . Recall that the total sum of the n-dice is the exponent of x, so collecting all x terms here the total sum T is given by T = n+sk+l. Lets substitute T into our series expression for f(x,n) with the aim to substitute out l for l = T-n-sk , clear up the numerator to get . However, the condition T-sk-n>0 must always be true which means T-n>sk and ((T-n)/s)>k, this inequality flipped is k<((T-n)/s), so this is the upper limit of the summation ie. . This substitution means the second summation, is now redundant as the only varying parameter in the summation is k which is already taken into account with the first summation. Remember dice are discrete so this means that ((T-n)/s) must only take integer real values, and so we must apply the floor function ‚åä‚åã which rounds the result of ((T-n)/s) to the greatest integer less than ((T-n)/s). The equation below allows us to determine the collection of coefficients that produce a total sum of T with n s-sided dice. Assuming the dice are fair we can express the probability of rolling this total sum T as the product of the multinomial coefficient and the probability of each dice side to the power of the number of dice rolled n , . This is the function we have all been waiting for! Using this function P(n, s, T) we can plot the exact distributions for n-dice, these are shown below in Figure 7 and 8 as the green lines. It can be seen from Figures 7, 8 and 9 that our analytical solution fits that data better than our previous Gaussian approximation. We can now capture the exact probability distributions, even for n=1 and n=2 cases! Some might argue that dice probabilities are mundane mathematical problems, and they might well be right! However, what we have covered here contains three important lessons at the heart of all mathematical modelling and data science problems. First, we have demonstrated the power of computers to produce and process experimental data. For example, we produced millions of dice rolling simulations in a few minutes. Second, we demonstrated the effectiveness of statistical inference and regression analysis. We showed this through estimating the experimental data as Gaussian distributions (a proxy model). This approximation allowed us to use a simple form of regression (the method of least squares) to find the parameters for the best fitting Gaussians. We then extended and abstracted our proxy model to find how the parameters change as we increase the number of dice. With this information we could extrapolate to predict the distributions for very large numbers of dice without running simulations. Third, and finally, we dived deep into the mathematics of the problem and derived an exact and general solution for the probability distributions. The similarity between these exact probability distributions and our simpler Gaussian approximation further highlights and is proof of the usefulness of statistical inference and regression analysis in modelling data. Thanks for reading, now back to the game of Catan! References [1] Piaggio HT. Introduction to Mathematical Probability. By JV Uspensky. Pp. ix, 411. 30s. 1937.(McGraw-Hill). The Mathematical Gazette. 1938 May;22(249):202‚Äì4. [2] Weisstein, Eric W. ‚ÄúDice.‚Äù From MathWorld ‚Äî A Wolfram Web Resource. https://mathworld.wolfram.com/Dice.html [3] Weisstein, Eric W. ‚ÄúMaclaurin Series.‚Äù From MathWorld ‚Äî A Wolfram Web Resource. https://mathworld.wolfram.com/MaclaurinSeries.html",45,0,11,Towards Data Science,2021-01-04,2021
https://towardsdatascience.com/5-python-coding-questions-asked-at-faang-59e6cf5ba2a0,FAANG Ask These 5 Python Questions in¬†2021,A must-read for data scientists and data engineers!,9,50,"['FAANG Ask These 5 Python Questions in 2021', 'Question 1: Who wins first? By Microsoft', 'Question 2: Maximum 69 Number, by every major company', 'Question 3: Valid Perfect Square. By Facebook', '# Question 4: Factorial Trailing Zeroes. By Bloomberg', 'Question 5: Perfect Number, by Amazon', 'Takeaways', 'My Data Science Interview Sequence', 'Enjoy reading this one?']","The complete Python code is available on my Github. For any data related interviews, programming in Python is an essential skill and a must-prepare area! There are four types of programming questions, including Data Structure and Algorithm, Machine Learning Algorithms, Math and Statistics, and Data Manipulation (Check this wonderful article by Emma Ding @Airbnb). I elaborated the topic on Data Manipulation and String Extraction in a related post. I focus on Math and Statistics in today‚Äôs post and live-code five Python programming questions asked by major tech companies, particularly FAANG. This type of question gives you a business setting and ask for statistical solutions via simulations. towardsdatascience.com Amy and Brad take turns in rolling a fair six-sided die. Whoever rolls a ‚Äú6‚Äù first wins the game. Amy starts by rolling first. What‚Äôs the probability that Amy wins? This is a hard-core simulation question, and there is no better way than simulating the process a considerable amount of times and check the probability that Amy wins. Amy rolls first. If the result is a 6, then game over and Amy wins. Otherwise, it‚Äôs Brad‚Äôs turn to roll, and wins the game if it is a 6. If not, the turn shifts back to Amy. Repeat the process until someone ends the game with a 6. Here, the key is to understand the logic flow: who wins and under what conditions. Does Brad have to roll if Amy gets a 6? No. Check line 11: A_6 is the data distribution for Amy, and if she has a 6, her count +1. Otherwise, Brad can roll the dice. At the end (line 25), the final result should be the number of occasions that Amy won divided by the total number of Amy and Brad won. A common mistake is to divide A_count by the total number of simulations. This is incorrect because there are iterations when both Amy and Brad fail to roll a 6. Let‚Äôs test out the above algorithm. As it turns out, Amy has an upper hand in winning this game as she starts rolling before Brad. Amy has a 53% probability of winning in 10,000 times of simulation. - Given a positive integer num consisting only of digits 6 and 9.- Return the maximum number you can get by changing at most one digit (6 becomes 9, and 9 becomes 6).- https://leetcode.com/problems/maximum-69-number/ Given a positive integer, there is only one way to make the value bigger, which is to turn a ‚Äò6‚Äô into a ‚Äò9‚Äô, not the other way around. Also, we have to change the leftmost 6; otherwise, it wouldn‚Äôt be the maximum number. For example, we have to change ‚Äò6996‚Äô to ‚Äò9996‚Äô, not ‚Äò6999. I have come up with several variations of this question: you either change once or change all ‚Äò6‚Äôs. In line 3, we turn the integer into a string and replace the first ‚Äò6‚Äô into a ‚Äò9‚Äô; turn it back to an integer using int(). For the second scenario, we don‚Äôt have to specify k as the replace() method changes all suitable values by default. I specify the k value for the pedagogical purpose. Another variation of the question could be to replace the first two, or three, ‚Äò6‚Äôs to make the number maximum. - Given a positive integer num, write a function that returns True if num is a perfect square else False.- Follow up: Do not use any built-in library function such as sqrt.- https://leetcode.com/problems/valid-perfect-square/ It‚Äôs quite straightforward: check if a positive integer number has a perfect square root and return True if there is one, which can be accomplished in two steps. The tricky part is that we have to use a built-in library (e.g., math, Numpy) to calculate the square root, which is an easy question at LeetCode. If we can‚Äôt use these libraries, it becomes more challenging and iterative, a medium level question at LeetCode. The algorithm easily passed the test case. It should be pointed out that we need to use the int() method to only obtain the integer part of the square root and leave out any decimal parts. For perfect squares, it won‚Äôt make any differences, and the equation holds. For non-perfect squares, the equation won‚Äôt hold and return False. (Special thanks go to Han Qi for catching the mistake!) If no library is allowed, we adopt a binary search. LeetCode contains a detailed explanation (here), and I have another post on the topic as well (here). In brief, we create two pointers, left and right, and compare the average value of these two numbers to the original number: if it is smaller than the number, we increase the value; if it is bigger, we decrease it; or, return True if they match. These conditions are automatically checked in the while loop. - Given an integer n, return the number of trailing zeroes in n! - Follow up: Could you write a solution that works in logarithmic time complexity? - https://leetcode.com/problems/factorial-trailing-zeroes/ There are two steps to this question: For the first step, we use a while loop to iteratively loop over the n factorial and stop the loop until 1. For the second step, things become a little bit trickier. The question asks for the trailing, not the total number of, zeros. There is a huge difference. The 8! is 40,320, which has 2 zeros but only 1 trailing zero. We have to be extra careful with the calculation. I‚Äôve come up with two solutions. The first part of calculating the product is pretty self-evident. As for the second part, we use an str() method to transform the product into a string and then read it backward: if the number is 0, we add 1 to the count; otherwise, we break the loop. The break command is essential here. As discussed, the above function calculates the total number of zeros without the break command. The first part is the same as solution 1, and the only difference is we use a while loop to calculate the trailing number: for the product to be evenly divided by 10, the last digit must be 0. So, we use a while loop to constantly update the while loop until the condition does not stand. BTW, solution 2 is my favorite way of calculating zeros. (Special thanks to staniec for the improved solution. Please see the comment section for the code). - A perfect number is a positive integer that is equal to the sum of its positive divisors, excluding the number itself. - A divisor of an integer x is an integer that can divide x evenly.- Given an integer n, return true if n is a perfect number, otherwise, return false.- https://leetcode.com/problems/perfect-number/ This question can be divided into three steps: Steps 2 and 3 are self-evident and no more than one-liners. However, the tricky part is to find the positive divisors. To do so, we can adopt the brutal force method and iterate over the entire sequence from 1 up to the integer. It should work for a small integer theoretically but exceeds the time limit if we run for large numbers. It‚Äôs not time efficient. This method won‚Äôt work well for large values. Your interviewers may ask for a more efficient solution. To find its divisors, we don‚Äôt have to check the values up to the integer. For example, to find divisors for 100, we don‚Äôt have to check the numbers from 1 to 100. Instead, we only have to check until the square root of 100, which is 10, and all the other available values have already been included. This is an optimal algorithm that saves us a ton of time. The complete Python code is available on my Github. Medium recently evolved its Writer Partner Program, which supports ordinary writers like myself. If you are not a subscriber yet and sign up via the following link, I‚Äôll receive a portion of the membership fees. leihua-ye.medium.com towardsdatascience.com towardsdatascience.com towardsdatascience.com Please find me on LinkedIn and Youtube. Also, check my other posts on Artificial Intelligence and Machine Learning.",230,4,7,Towards Data Science,2021-01-04,2021
https://towardsdatascience.com/anomaly-fraud-detection-a-quick-overview-28641ec49ec1,Anomaly & Fraud detection: A quick¬†overview,"Anomaly detection, fraud detection, and outlier detection‚Ä¶",1,35,['Anomaly & Fraud detection'],"Anomaly detection, fraud detection, and outlier detection are the terms commonly heard in the A.I. world. While having different terms and suggesting different images to mind, they all reduce to the same mathematical problem, which is in simple terms, the process of detecting an entry among many entries, which does not seem to belong there. For example, credit/debit card fraud detection, as a use case of anomaly detection, is the process of checking whether the incoming transaction request fits well with the user‚Äôs previous profile and behavior or not. Take this as an example: Joe is a hard-working man who works at a factory near NY. Every day he buys a cup of coffee from a local cafe, goes to work, buys lunch, and on his way home, he sometimes shops for groceries. He pays bills with his card and occasionally spends money on leisure, restaurants, cinema, etc. One day, a transaction request is sent to Joe‚Äôs bank account to pay for a $30 payment at a pizza hut near Austin, TX. Not knowing whether this was Joe on a vacation or his card has gone missing, does this look like an anomalous transaction? Yes. What if someone starts paying $10 bills with Joe‚Äôs account on a ‚ÄúCard-holder-not-present‚Äù basis, e.g. online payment? The banking institute would want to stop these transactions and verify them with Joe, by SMS or Email. These are obvious examples of anomalous transactions that seem identifiable by the naked eye. But as with every statistics problem, there are always non-trivial cases. How do we approach detecting them is the question I want to talk about. Note that, there is no definite and certain answer to an anomaly detection problem, the answers are probabilistic and always depend on the perspective from which we are looking at the data. I would classify the mathematical approaches to this problem into three categories: Easily explainable statistical methods, somewhat explainable classic machine learning methods, and the black-boxed deep learning methods. The process of feature engineering sets up the frame we are going to look at the data. It defines what we think is important and it is the process in which we introduce our intuition to the models. This step highly depends on the problem at hand but I am going to get deeper into it for the bank account example which we discussed earlier. What can help us decide whether it is Joe being Joe or his card or online credentials have gone missing? Here is a suggestive list: Quoting this book, by Baesens et. al., A fraudulent account activity may involve spending as much as possible in a short space of time. These are some examples that would need the banking institute to derive from different tables on their database on Joe. Of course, there are many other possible features for every problem, try and find them for your case. The name might be a bit misleading since everything we are about to do is a statistical method right? But here I am focusing on simple statistics that can be explained in 5 minutes to for example a stakeholder, who might not understand complicated methods. A drawback of these methods is their incapability to handle categorical data, like the hour of the day feature. So in order to implement them, I would suggest applying them separately to each category. So we would be comparing during day transactions with each other and overnight transactions with each other. Z-score is has a very simple idea behind it, how many standard deviations is this data point away from the mean of others? The higher it is, the more anomalous the data point. This definition has limits, it assumes the data is normally distributed and it is prone to outliers and would trigger if Joe decides to spend a little more than usual once in a while. Therefore we turn our looks to the modified Z-score, also recommended by Iglewicz and Hoaglin. Modified Z-score uses median absolute deviation and is defined as follows: The authors suggest labeling the points with a modified Z-score of 3.5 or higher as anomalous. 2. Interquartile range: As seen in boxplots visualizations, the distribution of data in a range can be visualized in quartiles, a nice description is available here. In this method, the points between the first and the third quartile are normal points and the points outside them would be tagged as anomalous. You would be able to modify this to e.g. ‚Äúintertentile‚Äù range, where instead of quartiles, you can use nth and mth tentile to label the data. 3. Histogram bins: While being a famous way of data visualization, histograms can also be used in outlier detection. By calculating the bins for each sample and acquiring the histogram, we can flag the outlier points as anomalous. This is somewhat related to the Z-score metric. In contrast to the methods described above, ML methods are far more sophisticated, a bit complicated, and able to handle categorical data (via preprocessing methods such as one-hot encoding, for example get_dummies in Pandas). kNN is a widely used ML algorithm which the fundamental logic behind it is the following: ‚Äúthe similar observations are in proximity to each other and outliers are usually lonely observations‚Äù. Using this, we can detect the points in a high-dimensional feature space that are the most ‚Äúlonely‚Äù. It is greatly covered in sklearn library. By calculating the average distance of each data point from others, we can set a threshold to classify a certain proportion of them as anomalous, or even run Z-scoring on these distances and find the outliers. 2. One-class SVM SVMs are a strong weapon in an ML toolkit. In short, they are hyperplanes in the feature space which divide points to different classes. In the context of anomaly detection, One-class SVMs learn what is ‚Äúnormal‚Äù and detect outliers and anomalous data based on that. Here is a thorough and complete article on the math behind it. One-class SVMs are available in sklearn‚Äôs SVM toolkit. 3. DBSCAN DBSCAN is an unsupervised algorithm that detects densely packed regions of the space and marks the data points in low-density areas as anomalous. It is a widely used clustering method which has two hyperparameters to tune: Epsilon and min_samples, as defined in sklearn‚Äôs implementation. Epsilon is the measure of how close the data points should be to each other to be part of one cluster, and min_samples the minimum number of points in a cluster. 4. LOF (Local outlier factor) While having a similar logic to kNN and DBSCAN, LOF assigns a metric (LOF) to each datapoint, normal points would have a score somewhere between 1 and 1.5 while outliers having a higher score. It is also present in sklearn. 4. Isolations Forest Isolation forest or iForest, is a very strong, probably the best method in big data, tool for anomaly detection. It is easily scalable and there is a great explanation here. It is the way-to-go for big data anomaly detection. Finally, the fancy A.I. regions, where we end up with black boxes which perform well for the reasons we don‚Äôt know and judge the data based on reasons we can not interpret. The most famous DL anomaly detection method is the use of autoencoder networks. Autoencoders Autoencoders are networks which consist of two, well actually three, parts: The encoder, the latent space, the decoder. In mathematical terms, autoencoders learn the identity function (simply put: f(x)=x) on the dataset. Let‚Äôs say I input a large set of Joe‚Äôs transactions which I believe are not fraudulent. The network trains on taking transaction a, encoding it to the latent, lower-dimensional space, and then decoding it back to a space with the cardinality equal to the input space. For example, an input with 100 features would be reduced to a latent space with 30 features (this is encoding), then turn back into a 100 features representation. The neural network trains on minimizing the difference between the input and the output. So it basically learns to give back whatever it has received. Now, after the training phase, if we show a normal entry to it, the network would be able to reconstruct it with low error, as it is similar to what we have trained the network with. But what if I input a slightly different entry? The reconstruction error would be higher, meaning that the network has not been able to reconstruct it very well. We can decide based on this metric, whether a data point is anomalous or not. This was a quick review of the famous, available methods for anomaly detection. I would go deeper on some of them in the future and provide a hands-on example for detecting fraud in a sample dataset. Let me know what you think if there are any comments in your mind.",229,0,7,Towards Data Science,2021-01-04,2021
https://towardsdatascience.com/15-awesome-python-and-data-science-projects-for-2021-and-beyond-64acf7930c20,15 Awesome Python And Data Science Projects For 2021 And¬†Beyond,15 Cool Python and Data¬†Science‚Ä¶,5,115,"['15 Awesome Python And Data Science Projects For 2021 And Beyond', 'Beginner', 'Intermediate', 'Advanced', 'Conclusion:']","Implementation of awesome and cool projects to revolutionize the modern generation is the best part of Python and Data Science. My favorite part about the fields of Python programming and Data Science is the numerous amounts of wonderful projects you can build with them. You can construct groundbreaking, innovative, and revolutionary projects which can benefit the entire society as well as change the landscape of the entire world to reach great feats. Tech giants and major companies are heavily investing their resources in Data Science due to the vast potential the innovations of this subject possess. With a wide array of spectacular projects that are built each day by creative data scientists and programmers, it would great to have a look at what we as individuals can achieve. In this article, we will discuss fifteen awesome Python and Data Science projects that you can enjoy implementing. These projects are guaranteed to provide you with the best possible experience for understanding most of the essential Python and Data Science concepts in further detail. Along with the immense knowledge and experience you gain from these projects, you can also showcase them in your resumes for better job opportunities or just as a sign of self-pride! The article is divided into three major sections targeted at audiences of all levels. The categories are beginner, intermediate, and advanced. In each of these categories, we have exactly five projects mentioned. This adds up to a total of fifteen fabulous projects that you can build from scratch. You can pick which category or which particular project you want to choose. However, it is highly recommended that you glance through all the project ideas provided in this article for more innovative ideas. So, without further ado, let us start looking at each one of these project ideas, and analyze them accordingly. To start things off with a simple project that I recently covered in one of my articles is the reminder application that will consistently notify you about the various tasks you have to complete throughout the day. The notifications will be reminded according to the time scheduler which you have programmed the script to perform. The project makes use of only two modules for the completion of the task. It utilizes the time module, which comes pre-installed with Python, and the plyer library, which will be used for alerting us about the timely notifications for the completion of the particular task at hand. The following project is extremely simple for a beginner-level introduction to understanding Python and the basic concepts related to the subject. Despite its simplicity, it can be very useful for improving your overall productivity. The link provided guides you through the entire process of building this project from scratch. towardsdatascience.com The first project is fairly simple, and the estimated time to complete this project should range anywhere from 30 minutes to 2 hours, depending on the programmer‚Äôs interest and skill. However, the difficulty range from the next projects mentioned will gradually increase. An important aspect of python and machine learning is understanding the math behind these concepts and knowing how some of the code in machine learning libraries. To have a better grasp of these concepts, it is essential to practice the ideas implemented in scientific modules like numpy and scikit-learn by ourselves. One such programming application is performing the matrix multiplication operation without using any ML libraries. To perform this task, the main requirement is knowledge of how matrices works. The complete explanation and guide can be obtained from my article below. However, if you are just interested in the basic gist of this coding problem and want to try to solve this on your own, then use the next reference paragraphs to help you get started. towardsdatascience.com My approach to this problem is going to be to take all the inputs from the user. These are the number of rows and columns of both the first and second matrix. Also, based on the number of rows and columns of each matrix, we will respectively fill the alternative positions accordingly. The first step, before doing any matrix multiplication is to check if this operation between the two matrices is actually possible. This can be done by checking if the columns of the first matrix matches the shape of the rows in the second matrix. This can be formulated as: ‚Üí no. of columns in matrix 1 = no. of rows in matrix 2 This should be a great starting point for you to get started. From here you can compute the problem statement with various ways by utilizing your own techniques. The house price prediction is one of the best ways for a beginner to get started with various machine learning algorithms. The best part about trying out this project is that you can gain a superior understanding of the scikit-learn (also referred to as sklearn) library, which is an extremely significant module for Machine Learning tasks. The scikit-learn module is one of the best tools for machine learning and predictive data analysis. It offers a wide range of pre-built algorithms such as logistic regression, support vector machines (SVM‚Äôs), classification algorithms like K-means clustering, and a ton more operations. This is the best way for beginners to get started with machine learning algorithms because of the simple and efficient tools that this module grants access to. Using this module you can access The Boston Housing Dataset. The Boston Housing Dataset is a derived from information collected by the U.S. Census Service concerning housing in the area of Boston MA. The dataset is small in size with only 506 cases. The following describes the dataset columns: In the next few blocks of code, we will discuss how we can utilize this module to access the dataset as well as some of the additional libraries required for analyzing and solving this simple machine learning task. It will be a quick guide on how you can get started and will cover the basic requirement. After understanding these concepts, you should be able to implement some machine learning algorithms on the following dataset. The first step is to import all the essential requirements for solving this task. It is recommended that you use the following modules while trying to implement the various machine learning algorithms. The block of code is a simple representation of some of the code lines that might be required to solve the task. (It is for trying a decision tree or a random forest approach.) After importing all the essential libraries required for performing this task, you can load the Boston dataset and proceed to assign separate variables for the data and the target variable. The price which is to be predicted is the target variable, while the other important features are the information of the dataset. This converts the problem into a machine learning prediction task. You can do this from the code provided below. Finally, we can quickly visualize this data using the pandas data frame structure. This can be constructed from the simple code block as mentioned below. The above image is a representation of the dataset. You can add the feature names for the respective columns if you like. However, this should be a good starting point for most beginners to get started. Kaggle and GitHub are your best friends for solving these machine learning tasks. Check out the Kaggle Website for further information from the link here. Email spam, also referred to as junk email, is unsolicited messages sent in bulk by email (spamming). Email filtering is the processing of email to organize it according to specified criteria. The term can apply to the intervention of human intelligence, but most often refers to the automatic processing of incoming messages with anti-spam techniques ‚Äî to outgoing emails as well as those being received. Various Classification Algorithms can be used for completing the email spam detection task. The various machine learning algorithms like Naive Bayes, support vector machines, K-nearest neighbors, and random forests among many other algorithms can be used for filtering spam messages and classifying if the received email is a spam or not. Advanced spam detection can be performed using techniques like neural networks or optical character recognition (OCR) which is also used by companies like Gmail for spam filtering. Assume we have a dataset of 30,000 emails out of which some are classified as spam and some are classified as not spam. The machine learning model will be trained on the dataset. Once the training process is complete, we can test it with a mail that was not included in our training dataset. The machine learning model can make predictions on the following input and classify it correctly if the input e-mail is spam or not. Various anti-spam techniques are used to prevent email spam (unsolicited bulk email). No technique is a complete solution to the spam problem, and each has trade-offs between incorrectly rejecting legitimate email (false positives) as opposed to not rejecting all spam (false negatives) ‚Äî and the associated costs in time, effort, and cost of wrongfully obstructing good mail. As an example, the Naive Bayes classifiers are a popular statistical technique of e-mail filtering. They typically use bag of words features to identify spam e-mail, an approach commonly used in text classification. Naive Bayes classifiers work by correlating the use of tokens (typically words, or sometimes other things), with spam and non-spam e-mails and then using Bayes‚Äô theorem to calculate a probability that an email is or is not spam. Naive Bayes spam filtering is a baseline technique for dealing with spam that can tailor itself to the email needs of individual users and give low false positive spam detection rates that are generally acceptable to users. It is one of the oldest ways of doing spam filtering, with roots in the 1990s. A quick guide for the following can be obtained from the link here. Sentiment analysis (also known as opinion mining or emotion AI) refers to the use of natural language processing, text analysis, computational linguistics, and biometrics to systematically identify, extract, quantify, and study affective states and subjective information. Sentiment analysis is widely applied to voice of the customer materials such as reviews and survey responses, online and social media, and healthcare materials for applications that range from marketing to customer service to clinical medicine. A basic task in sentiment analysis is classifying the polarity of a given text at the document, sentence, or feature/aspect level ‚Äî whether the expressed opinion in a document, a sentence or an entity feature/aspect is positive, negative, or neutral. Advanced, ‚Äúbeyond polarity‚Äù sentiment classification looks, for instance, at emotional states such as ‚Äúangry‚Äù, ‚Äúsad‚Äù, and ‚Äúhappy‚Äù. However, you can choose to negate the other reviews and only classify them as good or bad. For example, for a movie review, anything rated 1‚Äì2 stars is marked as negative, a rating of 4‚Äì5 is marked as positive, while a neutral rating of 3 can be ignored accordingly. Solving the Sentiment Analysis problem is one of the best beginner-level projects for you to start with due to the wide range of options that you have available to solve the following task. You can choose any method that you prefer for solving this question. Machine learning Algorithms like logistic and Naive Bayes can be easily used to solve such kinds of tasks. Many approaches can be used for obtaining a solution to this problem, including methods of deep learning. However, even the simplest methods can be used to solve this task, depending on how complicated you decide to make the problem. My suggestion for detailed understanding of the concepts of natural language processing and sentiment analysis is the link here. You can audit the course if you like. I would also recommend checking out the article below for further information on this topic. towardsdatascience.com The outdated GIF you guys can see above is one of my first projects I ever did with the help of pygame about three years ago. If you want a more concise guide on how you can build this from scratch with python then do let me know. But the main idea here is to build a game with python from scratch on your own. Start off with something simple like a snake game, or tic-tac-toe, and proceed towards a more advanced one like flappy birds with reinforcement learning. The idea behind accomplishing this task is more of personal opinion and preference. I believe that one of the best ways to get a good hold of any programming language is to start with a project that is fun and enjoyable. I am also a bit of a gaming nerd. To get started with gaming projects related to python, I would highly recommend the use of the Pygame library module for the execution of such programs. With the pygame module, you can build some simple, fun games with python. However, don‚Äôt expect anything too fancy as it has its limitations. Regardless, it is a fantastic way to get started, and below is the starter code to dive in. Just install pygame with a simple pip command and then use the following import pygame command. The following message will greet you upon the successful importing of the module. The versions might differ depending on the time on installation, so don‚Äôt worry too much. Just use the updated versions always in any scenario. I will go over some basic commands you should know and how they work. Below is the complete code block for all the important aspects you need to know to get started with pygame. I would highly recommend you check out some YouTube videos for better understanding and learning to build some games. The documentation for the pygame module, albeit a little lengthy, is probably one the best resources to learn more about this module. Computer vision is an interdisciplinary scientific field that deals with how computers can gain high-level understanding from digital images or videos. From the perspective of engineering, it seeks to understand and automate tasks that the human visual system can do. RGB are the 3 most significant layers of coloring for computer vision tasks. The composition of these three colors, namely red, green, and blue can be used to compose almost any other color. Mixing them in the right proportion allows us to frame any other desired color. This concept has existed since the cathode ray televisions a few decades ago. Each of these colors has an 8 bit integer value. This means a matrix of these could range from 0 to 255. The reasoning for this is because ¬≤‚Å∏ is 256 and 0‚Äì255 consist of 256 values. Each of these colors will have a value of this range and since we have a 3-Dimensional image, we can stack each of these upon each other. OpenCV module is by far the best library for the execution of complex machine learning, deep learning, and computer vision tasks. It offers simplicity and high standards for the analysis and performance of the models being built. It is an open-source library and can be integrated with other python modules such as NumPy to accomplish complicated real-time applications. It is supported for a wide range of programming languages and runs remarkably on most platforms such as Windows, Linux, and MacOS. Working and dealing with images is an essential aspect of computer vision projects for AI and Data Science. The reading, displaying, and writing of images is an essential part of computer vision as you have to consistently deal with images. The best part about opencv apart from the previously mentioned advantages is that it grants you access to a variety of image formats as well. So, we can work on all these image formats without facing any major issues. The article link mentioned below is a concise guide to master the basics of computer vision from scratch. I have covered all the essential aspects for a beginner to get started and achieve an overall detailed understanding of the OpenCV module and how you can operate on a variety of images. towardsdatascience.com The next intermediate level we will be focusing on is one of the coolest aspects of having python programming knowledge. Complicated tasks such as text-to-speech conversion and optical character recognition of python can be completed just with the help of understanding the python library modules created for this purpose. The text-to-speech (TTS) is the process of converting words into a vocal audio form. The program, tool, or software takes an input text from the user, and using methods of natural language processing, understands the linguistics of the language being used, and performs logical inference on the text. This processed text is passed into the next block, where digital signal processing is performed on the processed text. With the use of the many algorithms and transformations, this processed text is finally converted into a speech format. This entire process involves the synthesizing of speech. Optical Character Recognition is the conversion of 2-Dimensional text data into a form of machine-encoded text by the use of an electronic or mechanical device. The 2-Dimensional text data can be obtained from various sources such as scanned documents like PDF files, images with text data in formats such as .png or .jpeg, signposts like traffic posts, or any other images with any form of textual data. There is a wide range of interesting applications for optical character recognition. Below is the list of two articles that will be extremely useful to get you acquainted with the Google Text-To-Speech module for speech translation and the pytesseract module for optical character recognition. Refer to these below articles for a comprehensive guide for getting started with them and perform a project using them together. towardsdatascience.com towardsdatascience.com Face recognition is the procedural recognition of a human face along with the authorized name of the user. Face detection is a simpler task and can be considered as a beginner level project. Face detection is one of the steps that is required for face recognition. Face detection is a method of distinguishing the face of a human from the other parts of the body and the background. The haar cascade classifier can be used for the purpose of face detection and accurately detect multiple faces in the frame. The haar cascade classifier for frontal face is usually an XML file that can be used with the open-cv module for reading the faces and then detecting the faces. A machine learning model such as the histogram of oriented gradients (H.O.G) which can be used with labeled data along with support vector machines (SVM‚Äôs) to perform this task as well. The best approach for face recognition is to make use of the DNN‚Äôs (deep neural networks). After the detection of faces, we can use the approach of deep learning to solve face recognition tasks. There is a huge variety of transfer learning models like VGG-16 architecture, RESNET-50 architecture, face net architecture, etc. which can simplify the procedure to construct a deep learning model and allow users to build high-quality face recognition systems. You can also build a custom deep learning model for solving the face recognition task. The modern models built for face recognition are highly accurate and provide an accuracy of almost over 99% for labeled datasets. The applications for the face recognition models can be used in security systems, surveillance, attendance systems, and a lot more. Below is an example of a face recognition model built by me using the methods of VGG-16 transfer learning for face recognition after the face detection is performed by the haar cascade classifier. Check it out to learn a more detailed explanation of how exactly you can build your very own face recognition model. towardsdatascience.com The link above is an example for a high accuracy face recognition system using deep learning with transfer learning methods to grant access to authorized users and deny permission to unaccredited personnel. Using methods of image data augmentation and transfer learning models, the face recognition model on the authorized user‚Äôs faces predicts with a high accuracy level. A unique aspect of working on Data Science projects is the ability to create awesome predictive type models. The Google search bar, WhatsApp messages, etc., among many other applications, use the methodology of Next Word Prediction to predict the appropriate suggestion after each new word has been typed. This is similar to Autocomplete, or word completion, which is a feature in which an application predicts the rest of a word a user is typing. In Android smartphones, this is called predictive text. In graphical user interfaces, users can typically press the tab key to accept a suggestion or the down arrow key to accept one of several. This project idea is a fantastic choice for transitioning from intermediate-level projects to fairly advanced ones. This project idea uses major concepts of natural language processing and will require a decent amount of skill to solve. You can use a variety of machine learning algorithms and techniques to solve this task. However, I would recommend and encourage all of you to try out some innovative deep learning methods for solving this project while aiming to achieve top-notch results. The next word prediction for a particular user‚Äôs texting or typing can be awesome. It would save a lot of time by understanding the user‚Äôs patterns of texting. This could be also used to create a bigger virtual assistant project to complete certain sentences. Overall, the predictive search system and next-word prediction is a very fun concept to implement. You can check out my article below, which covers the deep learning methodology to predict the next words. towardsdatascience.com The resource mention above uses LSTM based deep learning model which takes an input word or sentence and predicts the next appropriate word. This deep learning model uses the concept of long short-term memory with natural language processing for the pre-processing of the corpus and text data. Uses a custom sequential model for the prediction of the appropriate next word. It has a wide array of applications in social media for the next word prediction. This computer vision project could easily be considered a fairly advanced one but there are so many free tools and resources that are available that you could complete this task without any complications. The object detection task is the method of drawing a bounding box around the recognized object and identifying the recognized object according to the determined labels and predict these with specific accuracies. the object tracking is slightly different in comparison to the object detection, as you not only detect the particular object but also follow the object with the bounding box around it. Object detection is a computer vision technique that allows us to identify and locate objects in an image or video. With this kind of identification and localization, object detection can be used to count objects in a scene and determine and track their precise locations, all while accurately labeling them. An example of this can be either following a particular vehicle on a road path or tracking a ball in any sports game like golf, cricket, baseball, etc. The various algorithms to perform these tasks are R-CNN‚Äôs (Region-based convolutional neural networks), SSD (single shot detector), and YOLO (you only look once) among many others. I am going to mention 2 of the best resources by two talented programmers. One method is more so for embedded systems like the raspberry pi and the other one is for PC related real-time webcam object detection. These two below resources are some of the best ways to get started with object detection/object tracking and they have YouTube videos explaining them in detail as well. Please do check out these resources to gain a better understanding of object detection. github.com github.com The demand for high-quality chatbots is increasing every day. The main reason why chatbots are so popular now is because they can provide automated responses about the website or a particular topic. They can answer frequently asked questions and help new users on the website by welcoming them and briefing them about the particular site. A well-trained chatbot can even converse with the user similar to how a human assistant would. Chatbots are also able to engage in conversations and help the user perceive what the site is about while promoting the website or social media web page. They can also advertise and garner better interaction from the user. All these factors make chatbots extremely important for any small start-up or any big website as it saves a lot of human effort and resources. Deep Learning algorithms with neural networks and natural language processing (NLP) are the most popular chatbot methods which are being used today. There are a lot of other machine learning algorithms also which can be used. In deep learning, more popularly LSTMs are used and the sequence to sequence models with attention is preferred. Below is an example of an innovative chatbot built by me from scratch. towardsdatascience.com The resource mentioned above is for an Innovative Chatbot with 1-Dimensional Convolutional Layers. A 1-Dimensional text classification-based chatbot that replies with sarcastic responses to commonly asked questions. This chatbot model is an integral component of the virtual assistant project that will respond to the user with witty responses and keep the user engaged in funny conversations. The chatbot model is also perfect for casual talks and appealing to a foreign audience. It also has a high-quality prediction system. Machine translation, sometimes referred to by the abbreviation MT, is a sub-field of computational linguistics that investigates the use of software to translate text or speech from one language to another. On a basic level, MT performs mechanical substitution of words in one language for words in another, but that alone rarely produces a good translation because recognition of whole phrases and their closest counterparts in the target language is needed. Not all words in one language have equivalent words in another language, and many words have more than one meaning. These types of problems can be solved with AI and Data Science technologies. These usually work on concepts such as natural language processing, machine learning, and deep learning. A sequence to sequence (Seq2seq) mechanism with attention can be used to achieve higher accuracy and lower losses for these predictions. Seq2seq is a family of machine learning approaches used for language processing for applications that include language translation. Seq2seq turns one sequence into another sequence. It does so by use of a recurrent neural network (RNN) or more often LSTM or GRU to avoid the problem of vanishing gradient. The context for each item is the output from the previous step. The primary components are one encoder and one decoder network. The encoder turns each item into a corresponding hidden vector containing the item and its context. The decoder reverses the process, turning the vector into an output item, using the previous output as the input context. Zero-shot and one-shot learning methods even exist for natural language processing. The same methods can be used for better training of the model to improve the overall performance and avoid repeated training procedures which can be a really big hindrance in some real-life applications and scenarios. Hence, one-shot learning is a great alternative for deployment and working in other embedded systems with lower training capacities. Machine Translation is an awesome advanced level project to try out and have fun with. A great resource for accomplishing this task is the official website of TensorFlow that deals with Neural machine translation with attention. They work on a toy dataset and provide great insides on how to perform the following complex problem. You can check out the link from here. This project uses computer vision and deep learning to detect the various faces and classify the emotions of that particular face. Not only do the models classify the emotions but also detects and classifies the different hand gestures of the recognized fingers accordingly. After distinguishing the human emotions or gestures a vocal response is provided by the trained model with the accurate prediction of the human emotion or gesture respectively. The best part about this project is the wide range of data set choices you have available to you. The project is a fairly advanced computer vision task, which will be awesome to fit on your resume of widely accomplished projects. Working on this fabulous project will also provide you with some must needed experience to complete complicated and complex problems related to deep learning and computer vision. The below links are a reference to one of the deep learning projects done by me by using methodologies of computer vision, data augmentation, and libraries such as TensorFlow and Keras to build deep learning models. I would highly recommend viewers to check the below 2-part series for a complete breakdown, analysis, and understanding of how to compute the following advanced computer vision task. Also, make sure to refer to the Google text-to-speech link provided in the previous section to understand how the vocal text conversion of text to speech works. towardsdatascience.com towardsdatascience.com The links provided above represent a computer vision and deep learning model to recognize human emotions and gestures. The model also provides a vocal response and classifies the respective emotion or gesture accordingly. Uses deep learning technology with a custom-built sequential model to achieve high accuracy on human emotions, and transfer learning based on VGG-16 architecture for gesture recognition. GANs developed and originated in 2014 by Ian Goodfellow, and his colleagues have gained immense popularity recently. GANs are perceived to be the future of deep learning with their amazing ability to create visuals and images that have never even existed. Generative Adversarial Networks are the current peak of deep learning with an exceedingly improving curve. GANs are an undeniable future trend that will revolutionize artificial intelligence forever. GANs is a slightly complicated topic, and I will be covering it extensively in the upcoming article‚Äôs part by part. However, for the purpose of this post, it is essential to note that two networks, a generator, and a discriminator go to war against each other and have a small dispute. The generator tries to create realistic fake images to bypass the elemental checking of the discriminator, while the role of the discriminator is to catch the fake copies. This cat and mouse chases leads to the development of unique samples that have never existed, and it is realistic, far beyond human imagination. I am not going to mention any specific project with GANs as there is a wide variety of unique and awesome applications as well as other innovative projects you can create with them. The popularity of GANs is on the rise, and it can create new artistic and realistic images out of absolutely nothing. GANs are being even explored to generate music from various sub-fields and Genres. Hence, they are not limited to images only. A famous example of Generative Adversarial Networks (GANs) can be observed from the website called ‚Äúthispersondoesexist.com.‚Äù Upon refreshing or re-visiting this site, you will encounter new faces of individuals who actually don‚Äôt exist. This project is constructed and imagined by a type of GANs (generative adversarial network) called the StyleGAN2 (Dec 2019) developed by Karras et al. and Nvidia. The construction of a variety of new projects with Python and Data Science will enable you to understand and grasp the concepts you have studied much better. The passion to build new applications with Python and Data Science is an important quality for being successful in this field. Theoretical understanding of the intuition behind machine learning concepts and math for the various topics of data science is crucial. To appreciate the true beauty of data science, you need to try out lots of projects. The wide array of tasks and the problems you can solve are absolutely fantastic, which leads to a sense of accomplishment when you find their respective solutions. However, you also need to know how you can implement the following projects in a real-life practical scenario. Don‚Äôt be afraid to get your hands dirty with some code and implement these projects on your own. Experiment with different parameters and try to achieve better solutions by trying out numerous algorithms and methods. In this article, we discussed 15 awesome Python and Data Science projects that you can experiment with and try out. These ideas would fit perfectly for anyone‚Äôs resume as it includes a wide array of unique and cool projects that you have built. It will help you to improve your overall profile as well as help you in clearing the initial selection process more effectively. With the implementation of these projects, you will also gain more practical knowledge and a deep understanding of the concepts that you work on. If you have any queries related to the topics discussed in this article, then feel free to let me know in the comments section below, and I will try to get back to you with a response as soon as possible. Check out some of my other articles that you might enjoy reading! towardsdatascience.com towardsdatascience.com towardsdatascience.com towardsdatascience.com towardsdatascience.com Thank you all for sticking on till the end. I hope you guys enjoyed reading this article. I wish you all have a wonderful day ahead!",457,0,25,Towards Data Science,2021-01-04,2021
https://towardsdatascience.com/best-cities-to-work-as-a-data-scientist-a295ff60c6ef,"<strong class=""markup--strong markup--h3-strong"">Best Cities to Work as a Data Scientist</strong>","<em class=""markup--em markup--h4-em"">Silicon Valley isn‚Äôt your only option. Data Scientists are¬†in</em>",3,33,"['Best Cities to Work as a Data Scientist', 'English Speaking Cities', 'French Speaking Cities']","The job landscape for Data Scientists is promising. According to the US Bureau of Labour Statistics, by 2026, there will be roughly 11.5 million job openings [1]. These numbers suggest that companies outside Silicon Valley recognise the importance of data professionals to their business. As a result, both experienced professionals and those in a career change to Data Science can expand their horizons. Although, Silicon Valley is still the number one area for data professionals ‚Äî and with the highest average salaries ‚Äî it‚Äôs not the only option. Based on the growing demand in different industries, the list of cities below are on a positive trend for Data Scientists. Some of the factors influencing the list below include the number of hiring companies, government investment, collaboration between academia and industry as well as salary. Interestingly, leading cities are no longer exclusively English-speaking. Investments in Data Professionals and Artificial Intelligence are also directed to French-speaking areas. The French-speaking trend might gain thrust now that the UK has left the European Union. So, here are the best cities to work as a Data Scientists beyond Silicon Valley, split between English and French language. Boston is well-known for its high concentration of universities, such as MIT and Harvard, and insurance and cybersecurity companies [2]. According to the management consultancy KPMG, Boston is among the top ten cities globally to become the ‚Äúleading technology innovation hub outside of Silicon Valley over the next four years‚Äù [3]. Let alone several startups on AngelList that are hiring [4]. The average annual salary is $141,000, which delivers an above-average purchasing power given the living cost. India is one of the fastest-growing tech hubs globally with Delhi leading the way with young entrepreneurs [5]. More US companies are seeking to acquire local startups to enter this complex yet necessary market. Although the currency exchange rate drags salaries down compared with western countries, living costs are also low. Therefore, Delhi turns out to be an excellent testing market for startups and innovative ideas. Also, there is a massive opportunity to target Indian consumers, as there are around 340 million people with internet access. Not surprisingly, global companies, such as Alibaba and Google, have capitalised on the country‚Äôs digital scene. London is the world‚Äôs hub for the link between artificial intelligence (AI) and the Financial Technology industry (FinTech). The UK government has settled a ¬£1 billion deal, roughly $ 1.35 million, with over 50 tech companies around the world specialised in AI [6]. The city is also home to the Alan Turing Institute. London often hosts international summits such as the Deep Learning Summit, AI Summit, ODSC‚Äôs European Conference, and the Strata Data Conference. There is no shortage of opportunity to network. london.intelligenthealth.ai The cost of living is high, mainly because of rent, which can take up to 60% of one‚Äôs monthly income. However, a Data Scientist‚Äôs average salary being ¬£61,543 is ten per cent more than the average national Data Scientist job‚Äôs salary [7]. Also, London is just 50 min away from Cambridge, home of the University of Cambridge and considered one of Europe‚Äôs leading biomedical research hub. The ‚Äòtriangle‚Äô (Raleigh, Durham, and Chapel Hill) is known for both research and technology centres. The triangle is home to Lenovo, Citrix and Cisco among other top companies. Raleigh-Durham has a younger and thriving population with a median age of 36. This mid-sized group of cities has been raking as one of the top locations for data professionals. Also, its citizens have a far lower cost of living compared with San Francisco. The demand for Data Scientists has been powered mostly by healthcare IT and tech research labs. Most data professional jobs in Toronto are driven by financial institutions such as TD Bank, HSBC, Royal Bank of Canada, Scotiabank, etc. An entry-level data scientist position‚Äôs average annual salary sits between CAD 80,000 to $85,000 [8]. The University of Toronto is among the top universities in the world in computer sciences and fosters an interdisciplinary research environment [9]. As a result, the city has attracted attention for contributions in a wide range of scholarly disciplines and commercial innovations involving Big Data. Toronto is also the home of the Vector Institute, which drives research and leadership in AI, fostering economic growth and improving Canadians‚Äô lives. Despite Switzerland being one of the most expensive cities to live in Europe, if you work as a Data Scientists in Geneva, than that might not be an issue. The city attracts talented Data Scientist with increasing financial rewards, as the average net salary in Geneva can be 75% higher than those in London [10]. Top tech companies have offices in Geneva including Dell, HP, IBM, Microsoft, Google and Oracle. But if you would like to work in a startup environment, here is a list for you to watch in 2021. The city of Montr√©al might come as a surprise to many Data Scientists and AI professionals outside Canada. However, the Montr√©al has emerged as an AI powerhouse. According to Forbes Magazine and author of Deep Learning, Professor Yoshua Bengio, says that: ‚ÄúMontr√©al has the combination of great universities, innovative companies (including multiple of Silicon Valley companies who have established offices in the city), and the Canadian ethos of cooperation‚Ä¶ unlike the competitive, individualistic culture of much North America.[11]‚Äù Also, another reason to work here is the Montr√©al Institute of Learning Algorithms (MILA). The institute is similar to a large startup and has become the centre of the AI development in Montr√©al. MILA combines machine learning researchers of both McGill University and the University of Montreal. It has over 500 professionals dedicated to machine learning innovation and offers many training programmes. Last but not least, Paris has a lower cost of living than some other European hubs, like Geneva. Since 2014, it has been selected by notable tech companies such as IBM and Amazon to develop innovation labs, while Deepmind has long-term plans to develop further their AI lab (video below). Paris is definitely a historic and vibrant city, making it even more attractive. It features an average annual salary of about $55,000, which might increase as the French government commits to invest in technology and research [12]. Also, Paris is just a 2-hour train from central London and has easy access to Geneva. Data professionals are in demand and will continue to be at least for the next years. Working as a Data Scientist in Silicon Valley might be a dream and can remains as such. Nevertheless, plenty of cities worldwide hire data professionals, invest Machine Learning and create AI research hubs. As a result, it seems that French-speaking cities have emerged as potential centres and consequently turning French a second language for data professionals. This is an exciting opportunity as the French language can open a new route for experienced professionals and those in a career change who, otherwise, would not be able to work in an high tech and AI companies like those in Silicon Valley. Thanks for reading. Here are some articles you will like it: towardsdatascience.com towardsdatascience.com medium.com References: [1] https://economictimes.indiatimes.com/magazines/panache/11-5-mn-job-openings-by-2026-sky-high-salaries-why-data-science-is-booming/articleshow/74667347.cms?from=mdr [2] https://www.bizjournals.com/boston/blog/bbj_research_alert/2016/04/meet-massachusetts-fastest-growing-cybersecurity.html [3] https://www.builtinboston.com/2016/10/11/boston-cybersecurity-companies [4] https://info.kpmg.us/content/dam/info/en/innovation-enterprise-solutions/pdf/2019/technology-innovation-hubs-2019.pdf [5] https://angel.co/role/l/data-scientist/boston [6] https://www.insightssuccess.in/india-the-fastest-growing-tech-hub-in-the-world/ [7] https://www.adzuna.co.uk/jobs/salaries/london/data-scientist [8] https://www.adzuna.co.uk/jobs/salaries/london/data-scientist [9] Article by Taesun Yoo https://medium.com/@yoots1988 retrieved from https://towardsdatascience.com/data-career-market-insights-in-toronto-indeed-ca-1e50cdb88458 [10] https://www.numbeo.com/cost-of-living/compare_cities.jsp?country1=United+Kingdom&city1=London&country2=Switzerland&city2=Geneva [11] https://www.forbes.com/sites/peterhigh/2017/11/06/why-montreal-has-emerged-as-an-artificial-intelligence-powerhouse/?sh=66ef582b23bd [12] https://www.aiforhumanity.fr/en/",105,4,6,Towards Data Science,2021-01-04,2021
https://towardsdatascience.com/state-values-and-policy-evaluation-ceefdd8c2369,State Values and Policy Evaluation,An Introduction to Reinforcement Learning: Part¬†1,14,93,"['State Values and Policy Evaluation', 'Introduction', 'Contents', 'The Terminology of Reinforcement Learning (RL)', 'Basic Mathematics', 'Policy Evaluation', 'Iterative Policy Evaluation', 'Iterative Policy Evaluation Code', 'Policy Improvement', 'Discounted Rewards', 'Summary', 'What‚Äôs Next?', 'Footnotes:', 'References']","Once upon a time there was a Baby Robot who got lost in the mall. Using the strategies from Multi-Armed Bandits he was able to recharge, in the shortest time possible, and is now ready to start looking for his mum. Unfortunately he can‚Äôt quite remember his way back to her, so will need our help to guide him. We‚Äôll do this using Reinforcement Learning, to help him find his way and ensure that he gets back safely. In very simple terms, Reinforcement Learning can be thought of as learning from trial and error. An agent, that interacts with its environment, receives rewards that reflect its ability to accomplish some predefined goal. By evaluating how the agent‚Äôs actions impact on its performance, and then modifying those actions to improve performance, Reinforcement Learning can progressively move towards an agent that gives the maximum amount of reward and that solves the task at hand. Reinforcement Learning can therefore be considered to consist of two distinct parts: In this part we‚Äôll mainly consider the prediction problem. Then, once we are able to measure how actions relate to the amount of reward received, we can turn our attention to improving the policy, to maximise those rewards. The standard reinforcement learning book or syllabus will begin with a description of the full reinforcement learning system and the derivation of the equations used to describe that system. Only then, once they have all the theory in place, will they show how that theory can be applied to practical applications. In this article we‚Äôll take the opposite approach. We‚Äôll start with very simple methods of practically solving very simple problems and gradually build on these, adding the bits of theory as required, until we‚Äôre able to solve the full reinforcement learning problem. So we‚Äôll end up in the same place as a traditional course, but by using a top-down approach, rather than the standard bottom-up method. We want to get Baby Robot back to his mum as quickly as possible, so don‚Äôt have time to learn all the theory up-front, instead we‚Äôll add it as we go! In this post we‚Äôll cover the following Reinforcement Learning topics: A Jupyter Notebook version of this article can be found on github. This contains all of the code used to create the grid levels and RL algorithms described in this article. github.com Consider this level where Baby Robot initially finds himself: On entering the level he has two possible choices: he can either go North or South. Which of these will get him to the exit the quickest? The fundamental concept in Reinforcement Learning (RL) is the concept of a reward: a single numerical value that is used to measure how well the task at hand has been performed. It is the reward that drives learning, allowing a problem solving strategy to be optimised to give the maximum reward. In problems such as this one, in which Baby Robot must find his way out of a level, a reward could be given simply for reaching the exit, however that doesn‚Äôt fully describe what we want to achieve. We actually want to get to the exit as quickly as possible. Giving a reward just for reaching the exit doesn‚Äôt encourage this behaviour. Baby Robot could spend days walking around the level before finding the exit and would receive exactly the same amount of reward as he would for proceeding there directly. Therefore a better reward system would be one that encourages taking the shortest route and that discourages wandering around aimlessly. Effectively we want to express the reward as a penalty, that increases with the number of steps taken to reach the exit. However, we also still want to stick with RL‚Äôs central idea of maximising rewards and so we introduce the idea of negative rewards. For each step taken we give a reward of -1. In this way, a route that takes a long time to find the exit will accumulate a large negative reward and one that goes there directly will have a small negative reward. In terms of getting the most reward, the direct route will be better since it will be less negative. With our reward system, each time Baby Robot moves from one square to the next he will be given a reward of -1. In RL terminology, each of these squares represents a state, where a state is defined to be a unique, self-contained, stage in the environment that we‚Äôre working in. So, in this case, each state describes a position within the grid that forms the level. In a game, such as chess, the state would describe the current board position. In the case of a self-driving car the state could describe properties such as the position on the road, the direction and speed of the car and details of other traffic. In each case the state defines the current situation. The state is self-contained in the respect that it is independent of any previous states. For example, in chess, the state given by the current board position is independent of all other moves that have been made up to that point. To choose the next move you don‚Äôt need to know which moves were made in the past. Similarly, for Baby Robot to move from one square to the next in the grid, he doesn‚Äôt need to know which states he was previously in. When a state is independent of the prior states it is said to satisfy the Markov Property. Since we know that a penalty (reward) of -1 is incurred each time Baby Robot moves from one state to the next, and we also have the luxury of being able to see the map of the level, we can help Baby Robot to make his choice by calculating the value of each state, where the value defines how good it is to be in a particular state. Obviously, for this grid level, it‚Äôs better to be in a state that‚Äôs close to the exit than it is to be in one that‚Äôs far away. In RL the value of a state is defined to be the expected reward that can be obtained when starting in that state and then proceeding to choose actions that are defined by a plan, or policy, in all future states. Let‚Äôs add this information to the level. Beginning at the exit (which in RL is referred to as the Terminal State, where the episode ends and which, by definition, is given a reward value of zero), and working our way around the level, incurring a penalty of -1 each time we move to a new square, gives the following values for each state: There are a few points to note here: We can show the policy on our level diagram, where the arrows now point in the direction of the action that should be taken in each state. Note how for this greedy policy the arrows always point in the direction that will give the maximum reward from the current state. So, from Baby Robot‚Äôs starting position, if he follows this optimal policy he will arrive at the exit with an accumulated total reward of -4. Since we give a fixed reward when moving from one state to the next, under this optimal policy the expected return, and therefore the value of each state, is simply the number of steps to the exit, multiplied by the reward of -1 that is given for each step. In the description of the initial level, where Baby Robot finds himself, we‚Äôve already covered most of the basic concepts of Reinforcement Learning. We can now add the simple mathematical terms that go with these concepts and then build on these as we go along. Firstly we‚Äôve said that Baby Robot receives a reward when he takes an action in a state. Unsurprisingly we use the first letter of each of these terms to refer to its corresponding value, with lower-case being used to refer to specific values for each of these quantities, which gives us: Additionally, when Baby Robot takes an action he‚Äôll most likely move from his current state to another state. The next or successor state is denoted by s‚Ä≤ (read as ‚Äús prime‚Äù): The rewards, states and actions are actually random variables: there‚Äôs a probability of getting a certain reward, taking a specific action or being in a certain state and these probabilities are referred to using capital letters. Tying all these terms together gives us the expected reward for a state-action pair: So, at time ‚Äòt -1‚Äô, starting in a particular state ‚Äòs‚Äô and taking action ‚Äòa‚Äô, the expected reward that‚Äôs received at the next time step is a function of the current state and action. The reason it‚Äôs an expected reward is because the amount of reward that‚Äôs received, when repeatedly taking a particular action in a particular state, may not always return a constant value and so this effectively defines the mean value that will be obtained. Using these terms we can create equations for the basic properties we‚Äôve defined: Return ‚ÄòG‚Çú‚Äô: the total amount of reward accumulated over an episode, starting at time ‚Äòt‚Äô.In our case an episode refers to all the time steps that occur between entering and exiting a level. (Things obviously get a bit more complicated in long-running or continuous tasks, but we‚Äôll come back to that later). So, starting at time ‚Äòt‚Äô, the return is just the sum of the future rewards. Value: the value of a state is just a measure of how good it is to be in that state. This can be expressed in terms of the amount of future reward or, in other words, the return that you‚Äôre likely to receive if you start in that state. Obviously, no matter which state you start in, the rewards you‚Äôll receive will depend on the actions you choose and these actions are determined by the Policy, which is commonly denoted by the symbol ‚ÄòœÄ‚Äô. So the value for state ‚Äòs‚Äô under policy œÄ is simply the expected return: Since, in our simple case, we always get the same reward of -1 for taking an action, the value of a state is simply the immediate reward plus the value of the next state: For example, the value of the start state of the level is -4. If the optimal policy is followed from this state Baby Robot will have accumulated a total reward of -4 by the time he reaches the exit. Similarly, if he chooses to take a single step South from the initial state, he‚Äôll receive a reward of -1 for taking the action and the value of the next state is -3, so again this gives the value of the initial state as -4. Note how the calculation of the state value is split into two parts; the immediate reward received for taking the action and the value of the state where that action takes you. This technique, of splitting a problem into sub-problems, is known as Dynamic Programming. Using this, state values that have already been calculated can be reused to calculate the values of other states whose actions lead there. This greatly simplifies the problem, since you don‚Äôt need to work out the reward that will be given at every state when calculating the total return obtained between a starting state and the end of the episode. Instead you only need to look ahead one step. Baby Robot‚Äôs mum told him to never trust strangers, so he‚Äôs a bit nervous about following our policy. What happens if we‚Äôre lying to him and it‚Äôs not the optimal policy? So, rather than using our policy, he instead decides to toss a coin and use that to decide which way to go. Every time he enters a new state he‚Äôll flip the coin. If it‚Äôs heads he‚Äôll go forwards, tails he‚Äôll go backwards. Therefore each of these actions now has a 50% chance of being selected. What happens to the value of each state under this new policy and how do we go about calculating it? This can all be summarised by the following equation: As with equation 4, for a deterministic policy, the value of any action is given by the reward ‚Äòr‚Äô obtained for taking that action ‚Äòa‚Äô, plus the value of the next state ‚Äò s‚Ä≤ ‚Äô where that action leads to. However, since under a stochastic policy there can be more than one action, the action‚Äôs reward is multiplied by the probability of taking the action: œÄ(a|s) represents the probability of taking action ‚Äòa‚Äô from state ‚Äòs‚Äô under policy ‚ÄòœÄ‚Äô. These values are then summed, over all the actions for the state, which gives the expected reward value for state ‚Äòs‚Äô. Effectively, the combination of the sum and the probability of taking an action gives the average value of the rewards returned from the actions. Under Baby Robot‚Äôs new policy each of the 2 actions (forwards or backwards) are taken with a probability of 0.5 and the reward for taking any action is still -1. Therefore the value of any state will be: Where s·∂† is the state moved to when choosing the forwards action and s·µá is the next state when taking the backwards action. For this simple level, calculating the optimal policy was easy. We simply started at the exit and worked our way backwards, adding another -1 reward every time we moved to the next state. But how do we go about calculating the state value when the chance of moving to the next state is random? (In the language of RL the chance of moving to the next state is referred to as State Transition Probability). We can do this by taking a similar approach to the calculation of the optimal policy values, except now, rather than being able to find the value of each state in a single sweep through all of the states, we‚Äôll need to do multiple sweeps. Each of these will give us a slightly better estimate of a state‚Äôs true value. Initially, since we don‚Äôt know the value of any of the states, let‚Äôs assume that none of them return a reward, so we‚Äôll set all initial values to zero. By definition the reward of the exit, the terminal state, is also zero since this is where the episode ends. To start the iterative process we can begin in any state but, for simplicity, let‚Äôs begin at Baby Robot‚Äôs current location, the entrance to the level. Baby Robot tosses his coin. Heads he goes north, tails he goes south. So the probability of each action is 0.5, the reward for either action is -1 and value of the next state is currently 0 for both actions. Therefore, using equation 6 above, the current value of the current state is: To keep things simple we‚Äôll take the initial value of each state at the beginning of the sweep, rather than its updated value, to avoid the condition where some states have been updated and some haven‚Äôt (although doing this is a perfectly reasonable thing to do and can often lead to a faster convergence of the state values; using the updated values is referred to as ‚Äòin-place‚Äô updating). Therefore, keeping the value of each next state at zero for this sweep, we can repeat the above procedure for the remaining states. This results in a value of -1 for all states at the end of the first pass. At the end of the first pass, the value of each state looks like this: Once the value of each state has been calculated, the process can be repeated, using the newly calculated state values to calculate the state values of the next iteration. The progress in calculating the state values, over the first 10 iterations, is shown below (ignore the blue arrows for now, we‚Äôll come to those shortly). If this process is repeated for long enough the state values eventually stop increasing and are said to have reached convergence. In theory convergence is truly only reached ‚Äúin the limit‚Äù or, in other words, when the number of time-steps is equal to infinity. Obviously this is rather impractical and so we instead define convergence to have occurred when the maximum difference between a state value at one iteration and the next is less than some threshold value. In our experiments we use a threshold of 1e-3 (=0.001) and, with this, it takes 206 iterations for the state values to converge for this policy. Now it can be seen that, under this policy of choosing the next state by tossing a coin, the values of each state are a lot more negative than under the optimal policy of going straight to the exit. The values do however still represent the expected number of steps from any state to the exit, except now Baby Robot is following a random trajectory that will lead to many more states being visited. This is shown below, for one of his shorter trips from the start to the exit of the level: The code used to evaluate the policy is shown below. This repeatedly calculates the state values for every state until convergence is reached. (This snippet of code has been taken from the full notebook for this article, which can be found on github) This code consists of three main parts that implement the Iterative Policy Evaluation routine: Unsurprisingly, deciding which move to make based on the toss of a coin isn‚Äôt a very good strategy. As shown above, under this policy it takes much longer to reach the exit. Additionally, the value of each state is much lower than under the optimal policy. However, although the state values are much worse, in terms of the reward that can be obtained, they do still give one important bit of information: the relative goodness of each state. Looking back at the final, converged, state values, it can be seen that the expected reward that can be obtained from the starting square is -32. So, on average it will take 32 moves to reach the exit from this point. Similarly, for the square immediately to the North of the start position, the expected reward is -35 and to the South it‚Äôs -27. Therefore its easy to see that, to get to the exit in the shortest number of steps, its better to head South from the starting square. By repeating this one-step look ahead, and acting greedily with respect to the value of the next state, we can modify the stochastic coin toss policy to create a policy that moves in the direction of the greatest reward. In this manner we can improve the policy, to produce one that gives increased rewards. Indeed, after a single iteration of policy evaluation on this level, acting greedily with respect to the state values gives us the optimal policy. This is shown by the blue arrows, which can be seen to point in the direction of greatest reward and lead directly from the entrance to the exit of the level. One other interesting observation, when acting greedily with respect to the calculated state values, is that it may not be necessary to wait for the values to converge before the policy can be improved. Look again at the first few iterations for this level (conveniently copied here to avoid you having to scroll!): Although, during policy evaluation, it takes 206 iterations for the state values to fully converge, it can be seen that by the 5th iteration greedy selection has already found the optimal policy. Indeed, for the start square, which is all we‚Äôre really interested in, the optimal policy has been found by the 4th iteration. All future iterations are therefore redundant in terms of improving the policy. We‚Äôll make use of this observation as we look at more efficient ways of finding the best policy. So far we‚Äôve evaluated the state values for the optimal policy, which we were able to easily determine for this very simple level, and for a stochastic policy, where all actions were selected with a random probability. In both cases a series of actions existed that would ultimately lead from the start of the level to the exit. But what would happen if this wasn‚Äôt the case and if none of the policy‚Äôs actions ever lead to the terminal state? For example, consider the deterministic policy shown below: In this policy an action is defined for every state, to specify the direction that should be moved from that state. The important point to note about this policy is that none of the actions ever lead to the exit and, as a result, the episode will never terminate. Obviously this will cause problems when evaluating the policy. The state value represents the total reward that can be obtained from a state. As we‚Äôve seen, this is calculated as the sum of all the rewards that will be obtained, starting in the state and then following the policy thereafter. Since our initial policy never reaches the terminal state, at each iteration of policy evaluation, this sum will just continue to grow. To prevent this from happening we introduce the idea of discounted rewards. Now, rather than the return simply being the sum of all the rewards that are accumulated from a state until the end of an episode, we progressively reduce the contribution of rewards. The further a reward is into the future, the less weight it will be given when calculating the state‚Äôs return value. The formula for calculating the return now becomes: In this new discounted formula for the return value, ‚ÄòŒ≥‚Äô (gamma) is the discount factor, where 0 ‚â§ Œ≥ ‚â§ 1. So the reward from each time step is multiplied by an increasing power of ‚ÄòŒ≥‚Äô. When the value of the discount factor is less than one, this will act to progressively reduce the value of rewards from future time steps, until eventually their contribution to the overall return is effectively zero. For example, a value of 0.9 is commonly used as the discount factor. With this we can calculate the return value of the initial state, as follows: Clearly, applying a discount factor progressively decreases the future return values and it doesn‚Äôt take long before they‚Äôre down close to zero. However, as we‚Äôve seen, it would be impractical to calculate a state‚Äôs value by considering the reward from all future states. Instead we use Dynamic Programming to simplify the problem into one that just uses the immediate reward and value of the next state. The value of the next state represents the return that will be obtained from the next state and therefore we can just change equation 7 to apply the discount factor to the next state‚Äôs value: In other words: the value of a state, when following policy ‚ÄòœÄ‚Äô, is equal to the sum, over all actions from that state, of the probability of taking each action, times the immediate reward for that action plus the discounted value of the next state where we end up after taking the action. Using the discounted state value function, with the discount factor set to 0.9, we can now calculate the value of our deterministic policy: With discounted rewards, the value of each state, rather than continually decreasing, now converges to our threshold in 67 iterations. In this case, all of the states (other than the exit) has a value of -10. The reason that no state is better than an other is because, under this policy, its never possible to reach the terminal state and therefore all states are equally bad. In a future part we‚Äôll look at how we can use these state values to improve the policy, to give one that does allow Baby Robot to escape from this level. One important point to note about the new, discounted, state values is the following: After all the running through mazes Baby Robot is pretty tired (as I‚Äôm sure you are too!), so we‚Äôll take a break here. We‚Äôve managed to successfully get Baby Robot through the very simple initial level. In the process we covered nearly all of the main foundations of Reinforcement Learning: Although we‚Äôve covered a lot of ground, we‚Äôre still missing a few of the core concepts of Reinforcement Learning. In particular, Markov Decision Processes and Bellman Equations. In simple terms these are, respectively, the mathematical framework used to model Reinforcement Learning problems and the set of equations used to calculate the values of states and actions. As you‚Äôve seen, we‚Äôve already used some equations to calculate the state and action values. These are actually partial forms of the full Bellman Equations. We‚Äôll fully describe both of these topics in the next post. Additionally, we‚Äôve only really looked at the Prediction Problem, in which we evaluate the value function for a given policy. While we were able to act greedily with respect to these calculated values, and thereby help Baby Robot find his way through the simple grid level, we need to expand on this for more complicated problems. We therefore need to examine the Control Problem, in which we find the optimal policy based on the policy evaluation. Furthermore, Baby Robot has not yet had to actually explore any of the levels where he‚Äôs found himself. All of the information about the level, such as the rewards and state transition probabilities, was already given and this was used to derive the optimal policy for each level. When all the information is given up front it‚Äôs known as a model-based system. A more realistic scenario is when these values aren‚Äôt available and some exploration is required to solve the problem. Unsurprisingly, problems of this nature are called model-free problems and we‚Äôll take a look into these in future posts. towardsdatascience.com Top-Down Learning: This approach to learning is probably most famously used (at least in the machine-learning world) by Jeremy Howard in his excellent Fast.ai courses. In academic studies it‚Äôs been shown to help give a better understanding of the overall concepts involved in a subject. For a full theoretical breakdown of everything covered in this article, check out the bible of Reinforcement Learning: ‚ÄúReinforcement Learning: An Introduction‚Äù, Sutton & Barto (2018) For the Baby Robot‚Äôs Guide to Multi-Armed Bandits, start here: towardsdatascience.com",,0,23,Towards Data Science,2021-01-04,2021
https://towardsdatascience.com/machine-learning-model-development-and-deployment-with-mlflow-and-scikit-learn-pipelines-f658c39e4d58,Machine Learning Model Development and Deployment with MLflow and Scikit-learn Pipelines,,6,21,"['Machine Learning Model Development and Deployment with MLflow and Scikit-learn Pipelines', 'Pipelines', 'MLflow', 'Execution Using MLflow Project', 'Model Serving', 'Summary']","The Victorian Gasolier is an example of an industrial gas pipeline terminating in an elegant fixture. Similarly, our machine learning pipeline needs to be functional, compatible with other systems, and attractive for both developers and users. This post contains an example of python machine learning model development using Scikit-learn pipelines and deployment with MLflow. The steps include: Why would you want to use this approach? This combination of methods eliminates the need for additional model serving logic, allows traceability to original model development code, incorporates both industry standard and custom code based upon domain knowledge, and allows for easy model deployment. AI needs to be customized for your business context. You can‚Äôt just download an open-source package and apply it to your problem. ‚Äî Andrew Ng By combining domain knowledge with a reusable code base, we can leverage open source solutions to similar problems and build customized systems. Pipelines have a number of advantages. The Scikit-learn pipeline is also compatible with other modeling packages such as Keras and XGBoost. For this pipeline we will import the Cleveland Clinic heart disease dataset from the UC Irvine Machine Learning Repository. A very similar dataset is also available on Kaggle. Features related to laboratory and physiological testing include: Let‚Äôs read in the data, set up our target variable, and take a look at the data. In order to demonstrate the use of a custom transformer we will create a new feature based upon the ratio of the resting blood pressure to the maximum blood pressure. This feature will be created as a new class and saved into a separate file so it can be output to the MLflow tracking server to be used during deployment in combination with the saved model. Now, let‚Äôs split our data up into train, validation, and test datasets. We are now ready to import our custom class and define the pipeline. Now that we have our pipeline, the next step is to save the resulting model to the MLflow tracking server. The MLflow Project is a framework-agnostic approach to model tracking and deployment, originally released as open source in July 2018 by Databricks. MLflow is now a member of the Linux Foundation as of July 2020. It is also possible to deploy models saved on a MLflow tracking server via Seldon into Kubernetes. This allows the usage of the model tracking functionality of MLflow with the expanded deployment capabilities of Seldon. Although MLflow has a scikit-learn ‚Äúflavor‚Äù for models, due to the usage of a custom transformer we will need to instead use the generic ‚Äúpython function flavor‚Äù. Usage of MLflow with scikit-learn pipelines allows us to save the model definition and all preprocessing steps and transformations into a single object, plus our custom transformer code. First set up the model output class and the conda environment that is needed to execute the model in the future. Next we will output the model and some metrics to the tracking server. Using MLflow Project and Github, the model saved to the tracking server can be linked to the Github code. You can also use the MLproject file to execute locally. Here is an example of what our very simple MLproject file looks like. We can either execute this based upon the conda.yaml file to set up a new conda environment, or execute with the ‚Äî no-conda option. Now we can review the details of our run_id in the MLflow tracking server, by executing mlflow ui at the anaconda prompt, of our activated environment that contains MLflow. We find the tracking server at http:\\localhost:5000 The model can now be served from the Anaconda prompt. You will get a message, after building a conda environment and activating it, that the model is serving on http://localhost:5000. We can now provide data to the model and receive a prediction. We will use python here to demonstrate but this API could also be invoked to score data from a stream. Writing python based production ready deployment code for data science models is facilitated by the usage of Scikit-learn pipelines and MLflow model tracking servers. With this method the data scientist can produce an API from a model without additional ETL of an input data stream. This can facilitate deployment and allow software engineers to focus upon infrastructure and systems integration, including continuous integration and delivery instead of maintaining a modeling related code base. It also makes redeployments as easy as referencing a new run_id on the MLFlow tracking server.",113,1,6,Towards Data Science,2021-01-04,2021
https://towardsdatascience.com/why-you-are-betting-on-the-wrong-nba-teams-39e2bf98588,Why you are betting on the wrong NBA¬†teams,,6,33,"['Why you are betting on the wrong NBA teams', 'How accurate are betting odds?', 'Step 1: Getting data', 'Step 2: Finding differences in implied probability', 'Step 3: Simulating the strategies', 'Conclusion: Find the big-time underdogs']","You might be surprised to learn that the NBA teams that win the most games do NOT win bettors the most money! In this post, I will analyze NBA betting data and run simulations to show you why this has historically been true ‚Äî and explain which teams you should bet on instead. You won‚Äôt even need basketball knowledge to successfully implement this strategy. Although you don‚Äôt need to be an NBA expert, this post assumes you have some knowledge of sports betting concepts and terminology. If you aren‚Äôt familiar with them, check out this short guide for a primer. Let‚Äôs start with the basics ‚Äî when two NBA teams play each other, one team is considered more likely to win (the favorite) while the other more likely to lose (the underdog). If you were to place a moneyline bet on the underdog, you‚Äôd win more money than if you had bet on the favorite; after all, you deserve a bigger reward for picking the less likely winner. Based on the payouts set by the sportsbook, you can calculate the implied probability of each team winning the game. In theory, the implied probability of winning your bet should be identical to the probability of your team winning the game. In reality, sportsbooks don‚Äôt set their odds that way. Instead, they invite action on both sides so the amount of money at stake is balanced, thus reducing their risk and maximizing their profit. For example, let‚Äôs say the Milwaukee Bucks (the team with the best record last year) are playing against the New York Knicks (a team with‚Ä¶ not the best record). If bettors rush to bet on the Bucks, the sportsbooks may get nervous about paying out a lot of money if the Bucks win. So they decide to reduce their payout for the Bucks and increase their payout for the Knicks, thus incentivizing more money to be placed on the Knicks‚Äô side. These payouts could swing such that implied probability of a Bucks win is 90%, when in reality their ‚Äútrue win probability‚Äù might only be 80%. In this case we have a value bet, where it would be profitable in the long-run to bet on the Knicks. Of course, the big challenge that all bettors face is finding the true win probability of each bet AND figuring it out before everyone else does. It‚Äôs impossible to know these win probabilities for certain, but can we find games where the implied odds are more likely to be inefficient? This could help us spot opportunities for value bets. First things first, we need a large dataset for our analysis. We‚Äôll need data for the following: I ended up using Sportsbook Review, a site that aggregates historical betting odds from many different sportsbooks. From there, I found an open-source repository with a script that can effectively scrape betting data from any historical NBA game. I modified the script to also scrape the final score of each game and to run for an entire NBA season, which I did for three regular seasons: 2017‚Äì18, 2018‚Äì19, and 2019‚Äì20 (only including games before the pandemic suspended the season). Here‚Äôs a snapshot of what the dataset looks like: There are a lot of different ways to place bets on NBA games, like moneylines, point spreads, point totals, etc. For the sake of this post, I‚Äôll focus exclusively on the profitability of moneylines. Similarly, there are many different sportsbooks you can use to bet. For the sake of simplicity, I will focus exclusively on Pinnacle, which is regarded as having some of the most accurate odds in the industry. The goal here is to examine the implied odds of past NBA games and determine if they‚Äôve been historically accurate. If there are big discrepancies, then there could be an opportunity to make money. I calculated the implied win probability for each bet based on their moneyline odds. You might notice that the sum of the win probabilities in each game is greater than one, which shouldn‚Äôt be possible! However, sportsbooks do this on purpose to profit from the total betting action. To adjust for this, I normalized the win probabilities to add up to one, which results in the REAL implied win probability for each bet. Next, I created ‚Äúbins‚Äù so that all bets with similar implied win probabilities are grouped together. Why do this? Suppose we have a bet with an implied win probability of 11.7% (or +755 moneyline). It‚Äôs hard to find many other bets that have this exact moneyline. But if we include it in a bin of all bets from 10% to 15%, then we have quite a few data points to look at in each bin. We then calculate each bin‚Äôs actual win rate (number of real-life wins divided by total number of games) and expected win rate (average implied win probability of all bets in the bin). From there, we can take the difference between the actual win rates and expected win rates ‚Äî which I‚Äôll call the residual ‚Äî and see if there are any large discrepancies. Here are the results when dividing all bets into 20 bins. Each bin covers an implied win probability interval of about 5 percentage points. It turns out that the implied win probabilities (and therefore the moneylines) are pretty accurate! In general, the actual and expected win probabilities don‚Äôt differ by more than 5%. However, there is a slight negative correlation between residual and expected win rate. It appears that huge underdogs have been slightly underrated, while huge favorites have been slightly overrated. Now, it‚Äôs time to put my (imaginary) money where my mouth is. In the last section, we found that huge underdogs might actually be slightly undervalued. What happens if we simulate betting on underdogs over the last three NBA seasons? We‚Äôll backtest with actual game results and Pinnacle moneylines from the 2016‚Äì17 to 2019‚Äì20 seasons. I wrote a function that simulates a betting strategy and tracks our winnings over time. We first must set a bet amount, which will be $100 every time for the sake of simplicity. We must also set a ‚Äúwin probability threshold,‚Äù which determines the underdog teams we‚Äôll bet on. If we set it to 0.5, then we bet on any team with a win probability less than 50% (aka the underdog of every game). If we set it to 0.2, then we only bet on the big underdogs of lopsided games, where the win probability is less than 20%. We‚Äôre ready to go now. What happens if we run the simulation with a threshold of 0.5, which places a $100 bet on the underdog of every game? Oh no, we lost money! We lost $3,903 after three seasons, with an especially brutal stretch from bets 600 to 1,000. It‚Äôs worth noting that the expected value of every bet is negative. As I mentioned earlier, the sportsbook takes a cut of every bet through the virgorish or ‚Äúvig.‚Äù Pinnacle has a vig of about 2‚Äì3%, which is actually quite low. Any strategy that is profitable must be at least 3% better than break-even! Next, let‚Äôs try the exact opposite strategy and bet on the favorite of every game. After a tiny tweak to my simulator function, we get the following results over time: What an absolute nightmare! With this strategy, we lost $10,352. Comparing this graph with the previous one, we can see that the trends move in opposite directions (as they should), but the winnings are completely outweighed by the magnitude of the losses. Finally, let‚Äôs test our hyped-up strategy of betting on huge underdogs. What happens if we run the simulation with a threshold of 0.2? We make a total profit of $7,182!! Not bad at all! One really important caveat is that we lost nearly $3,000 before making profits afterwards. In order to survive with this strategy, you will need to have a large bankroll and/or make small-sized bets. Otherwise, it‚Äôd be easy to go on a long losing streak and completely run out of cash. If you‚Äôd like to check out my complete Jupyter notebook, you can find it here. Based on this analysis, there has historically been a profitable strategy by betting on big-time underdogs. You could think of every bet as a lottery ticket with a high likelihood of losing but a large upside. I do think it‚Äôs feasible that these underdogs are relatively ‚Äúunderpriced‚Äù while the heavy favorites are ‚Äúoverpriced.‚Äù There may be a psychological explanation for a lot of bettors; people might want to win more often at the cost of long-term monetary returns! In summary, this underdog strategy requires patiently enduring long losing streaks, placing small-sized bets, and having a big enough bankroll. However, if the bottom-feeders of the NBA can pull off enough rare Ws, you just might be able to make cash. And by the way, the Knicks already beat the Bucks this season, despite having just a 12% implied win probability for that game! Perhaps a sign of things to come. Note from Towards Data Science‚Äôs editors: While we allow independent authors to publish articles in accordance with our rules and guidelines, we do not endorse each author‚Äôs contribution. You should not rely on an author‚Äôs works without seeking professional advice. See our Reader Terms for details.",183,3,8,Towards Data Science,2021-01-04,2021
https://towardsdatascience.com/why-working-in-consulting-is-so-valuable-for-data-scientists-710f3a4cc2d0,Why working in Consulting is so valuable for Data Scientists,Career insights that might help you¬†to‚Ä¶,4,18,"['Why working in Consulting is so valuable for Data Scientists', 'Why media?', 'Why consulting?', 'Conclusion']","This article is different from the tech-heavy Data Science tutorials that I usually write. Today I am looking back at the time when I was working as a Data Science Consultant in a large communication company and discuss the reasons why it was worth it. I do not pretend my opinion to be exhaustive and universally applicable, but I hope it will be helpful for those who are facing a career choice or are at the very beginning of their professional journey. All views presented in this article are my own. To begin with, I provide a couple of words about myself for context. I started as a software engineer and worked in various data and analytics fields, mostly sales and marketing related. In 2014 I decided to completely switch to Data Science and got a Master's degree in Germany in a related field. After that, I joined one of the largest marketing and communications companies globally as a Data Science Consultant, where I spent the next three years in two different business units: marketing (media) and consulting. You might ask why a young Data Science graduate should consider joining a Media company in 2021 when there are such fascinating fields like medical research or autonomous driving attracting the brightest talent. To answer this question, I would like to cite Taleb‚Äôs response to the question of why he took interest in stock trading (who in his turn was citing Mandelbrot): The reason is data, a gold mine of data. In fact, media has lots of data, tons of data, mountains and oceans of data, and the best part about this data is that it is real-world data. Meaning that it is dirty, sparse, imbalanced, and broken in all possible ways. 99% of these data are not very insightful, but it is real data about real human behavior. Finding the 1% gold in these data is the challenge and blessing for a curious Data Scientist. Working with a large amount of poorly organized data is the best way to learn how to efficiently process data coming from different sources, in different formats, often in real-time. There are different points of view on what a Data Scientist should do. However, it never hurts to see a bigger picture beyond the application of Machine Learning algorithms. Understanding how to build a real-life working application that will generate revenue is at least as valuable as deep Data Science knowledge. Not many companies take this extra step of production deployment and maintenance, as most of the Data Science projects in non-digital industries never make it through the Proof of a Concept phase. When in 2019 I was at an Industry 4.0 conference, my colleagues from non-digital industries were discussing that it might take more than a year to get approval to buy sensors, install them and then collect enough data to train the first classification model. In media there are no such constraints. There is simply too much data which is very cheap to collect. Surely media, as well as stock trading, can not offer a self-fulfilling mission that you would be happy to share with your friends. Instead, it can offer you a priceless experience. Consulting teaches you to focus on and assume ownership for results. When attending the Data and AI Meetup in Cologne, I was talking with a couple of students from a local university who were challenging me, saying that their professor thinks that consulting is not a place to practice true Data Science. Frankly speaking, the professor was partly right, but just partly. You should not expect that when you join consulting someone will teach you how to write Python or run A/B tests. That likely will not happen. What will happen is that you will be thrown on a project and will need to figure out the technical part yourself. Surely there will be some people around that you can ask but most of the decisions you would need to take on your own. That is a great motivation to continuously work on your technical skills in order to feel more confident during client conversations. However, improving your qualification and keeping up with the new developments will be your own responsibility. At the same time, a variety of clients, projects, and people around will create a perfect training environment to practice fundamental soft skills that are not easy to learn by yourself. The truth is that you can develop your career for a while if you have sharp technical skills, but after some point, the soft skills really start to matter, and consulting is the best place to gain them. Consulting teaches you integrity. Remember my passage from the ‚ÄúWhy Media?‚Äù part about 99% of not insightful data? That still holds true. Sometimes data are not what we expected or, even worse, sometimes the data are stopping us from delivering what we have promised. The rule of thumb in this situation is to be honest with yourself, your colleagues, and your clients. It is very easy to lose a client‚Äôs trust by holding back facts and it is much more difficult to gain it back. Probably the most important rule of not just consulting, but life in general is: Meaning that if you found a problem with the data, infrastructure, etc., find and propose a solution, never just address the problem. There is always some workaround that you will enjoy finding yourself. That rule applies to the relationships with colleagues, too. If you want to be successful in a consulting career, do not bring the problems you are facing to the managers you are reporting to, propose solutions. That seems to be a very obvious piece of advice, but it happens all the time that team members think that people managers exist to solve the problems of their managees. In fact, that is not their raison d‚Äô√™tre. A right specialization choice that is relevant for your local market will secure your employment for the years ahead. Big consultancies usually have a variety of clients from multiple industries. Hence, there is an opportunity to work on a number of projects, be it building a recommendation system, automatic entity extraction algorithm, or time series forecasting. While working in consulting, you have a unique opportunity to find the area that interests you the most. Besides that, you will learn the needs of the industry in your local market. Thus you can choose your future specialization based not only on personal preferences but also on the market demand in your particular region. For example, my local market is Germany which is dominated by four sectors: automotive, mechanical engineering, chemical, and electrical industries. Most of the clients I was working with were from the automotive industry, sometimes from the chemical industry that was supplying the automotive industry. What automotive clients need is a variety of models built with structured data such as demand forecasting, inventory optimization, recommendation systems, etc., and models that are built on unstructured data such as objects and sound recognition models. However, I happened to make my Master's in semantic web and knowledge graphs. Therefore, I decided to change my qualification to time series analysis and machine learning for structured data to fit the needs of the companies in the market context described above. Looking back at the past years I am very happy with the career choices I made. It is hard to find another industry that would provide that many learning and growth opportunities. What made it especially valuable is access to almost unlimited data and infrastructure, a wide variety of clients and projects, and an environment that enables to work on communication skills. I would not want to miss all of these experiences as they have enabled me to take on my new dream job.",211,2,6,Towards Data Science,2021-01-04,2021
https://towardsdatascience.com/variational-inference-for-neural-networks-a4b5cf72b24,Variational Inference for Neural¬†Networks,An introduction to‚Ä¶,1,28,['Variational Inference for Neural Networks'],"Bayesian analysis is a great way to quantify the uncertainty of a model‚Äôs predictions. Most regular machine learning modelling tasks involve defining a likelihood function for data given a model and its parameters. The aim is to maximise that likelihood with respect to the parameters in a process known as Maximum Likelihood Estimation (MLE). MLEs are point estimates of the model parameters, this means a single prediction is made MLE parameters are used in inference. In Bayesian modelling, in addition to the likelihood function, we must also define prior distributions for the model parameters. We use Bayes‚Äô rule to find the posterior parameter distribution. The posterior distribution and model can be used to produce a probability distribution of the prediction (shown below). This is what allows us to quantify uncertainty when using Bayesian modelling methods. If we‚Äôre no longer bound to point estimates, we can choose whether or not to trust the model's prediction based on the spread of the distribution. That‚Äôs all very well but what‚Äôs the catch? Bayesian inference is often a very expensive endeavour. Calculating the posterior distribution is often either intractable or extremely analytically difficult. Furthermore, even if there is a closed analytical form for the posterior, calculating the integral over all parameters is basically impossible for models of any reasonable complexity. There are ways to mitigate this difficulty: towardsdatascience.com Variational Inference aims to approximate the posterior with a ‚Äúwell behaved‚Äù distribution. This means that integrals are computed such that the better the estimate, the more accurate the approximate inference will be. Let‚Äôs take a look at some maths and then we‚Äôll see how this applies to neural networks. First, we need to define a distance measure between probability distributions that we can use to minimise. For this, we choose a distance measure called the Kullback-Liebler (KL) divergence. The reason we choose the KL divergence over other distance measures will become clear in a moment but it‚Äôs basically because of it‚Äôs close ties to the log-likelihood. KL has the following form: If we substitute the p(x) term for the posterior and do a bit of rearranging we get‚Ä¶ Now we can use the fact that the KL divergence is always positive in the following way‚Ä¶ The F(D,q) term is called the variational free energy or the Evidence Lower Bound (ELBo). Importantly, maximising the ELBo minimises the KL divergence between the approximate posterior and the true posterior. The form of the free energy in the last line is the form that‚Äôs most useful to us for the purpose of optimization. We brushed over a detail before, I said we need to approximate the posterior with a ‚Äúwell behaved‚Äù distribution but what constitutes ‚Äúwell behaved‚Äù? One popular choice is to approximate the joint posterior of all parameters as the product of independent distributions (often Gaussians). The independence condition allows for a number of optimisation methods to be used to maximise the ELBo including coordinate ascent and gradient ascent. Gaussians are chosen for a number of reasons including the fact that they are a conjugate prior and the KL between Gaussians has a clean closed-form. So how can we apply VI and the mean-field approximation to neural networks? The transition from a non-Bayesian to a variational-Bayesian network is quite a smooth one. Normally we would create a dense layer with weights in it but these are just point estimates, now we want to model each weight as an approximate posterior distribution. So let‚Äôs say each weight has a Gaussian posterior with mean Œº and standard deviation œÉ. To maximise the ELBo we need two things, the mean likelihood over the approximate posterior (q) and the KL between q and the prior. To compute the mean likelihood we draw Monte Carlo samples from q and estimate the mean likelihood by taking a forward pass of a minibatch (like we would do normally). The KL between q and the prior has a nice closed form because we chose everything to be Gaussian. Then we can use just use gradient descent as normal right? Well not quite, there is a small subtlety, you can‚Äôt take gradients of something stochastic, it just doesn't make sense. So here‚Äôs another reason to choose a Gaussian, you can parameterise a Gaussian in the following way: Now we can take gradients with respect to Œº and œÉ! It‚Äôs worth noting that we‚Äôve doubled the number of parameters in our model since we now have a separate weight for the mean and standard deviation of each model parameter. This increases the complexity of the model quite substantially without improving the model‚Äôs predictive power. Well, it looks like this‚Ä¶ There are a few important things to note with this implementation. Firstly, we don‚Äôt create weights for the variance directly. Instead, we create weights such that œÉ = log(1+exp(w)). We do this for numeric stability during optimisation. The second thing is that we accumulate the KL loss for each layer and as you‚Äôll see in a moment, we pass that loss forward to the next layer. We can do this because the KL term doesn‚Äôt depend on the data and it helps us to keep tabs on the total KL loss if we just add it up as we go. Now let‚Äôs put this into a model: Great question! Let‚Äôs find out! Let‚Äôs take MNIST, train a model to classify handwritten digits and see what the results look like. I haven‚Äôt included the training loop code here because it‚Äôs all pretty boilerplate, nothing fancy, just train a model on MNIST for about 5 epochs. One thing that is worth noting is that when we use this model to make predictions, we want to predict multiple times using samples from q. That way we unleash the real power of Bayesian networks which is the ability to predict uncertainty. Knowing when your model is confident about a prediction can help us to include a human in the loop and improve our overall accuracy by only accepting predictions that the model is confident with. To create this figure all points are ordered based on the uncertainty in the prediction. We then iteratively discard predictions based on their uncertainty and evaluate the new model accuracy. If we predict on all of the data, regardless of our certainty, we can expect a validation accuracy of about 93%. By flagging only 5% of the data we can boost model accuracy by more than 2%! In general, the higher the proportion of predictions that we flag for review, the higher the accuracy achieved by the model. We could use this method to calculate a threshold uncertainty that should be used to decide if a prediction should be flagged for review or not. We can also take a look at some samples that the model isn‚Äôt confident about‚Ä¶ In this case we predicted a 3 but the label is a 5‚Ä¶ to be honest I don‚Äôt know about you but I can see where the confusion comes from! We‚Äôve hopefully achieved 2 main objectives in this post. Firstly, we understand what variational inference is and why it‚Äôs useful and secondly, we know how to implement and train a deep neural network that leverages VI. So next time you‚Äôre designing a run-of-the-mill neural net for classification or regression, consider making it Bayesian instead!",,0,7,Towards Data Science,2021-01-04,2021
https://towardsdatascience.com/how-to-connect-to-a-heroku-postgres-database-with-pgadmin4-using-docker-6ac1e423ae66,How to Connect to a Heroku Postgres Database With pgAdmin Using¬†Docker,Learn about the world‚Äôs¬†most‚Ä¶,4,24,"['How to Connect to a Heroku Postgres Database With pgAdmin Using Docker', 'Setup', 'Wrap Up', 'Resources']","Heroku is a cloud platform to build, deploy, monitor, and scale applications. It supports a wide range of programming languages, including Ruby, Java, Python, and many more. On top of that, you can add many add-ons to your app. Heroku Postgres is one of them that supports PostgreSQL. You can connect to the PostgreSQL database using the command line or a graphical interface. For the latter purpose, we can use pgadmin4. It‚Äôs an open-source administration and development tool for PostgreSQL. This post teaches you how to create a Heroku app with a Heroku Postgres add-on. After that, we will run pgadmin4 on our local machine using Docker. Finally, we will connect pgadmin4 to the PostgreSQL Database. We will use the cloud platform Heroku, Docker desktop for mac, and a Docker compose file for our setup. You‚Äôll need a Heroku account for the step. Now, go to the Heroku dashboard. From there, click the New drop-down menu and select Create new app. Type the app name and click the Create app button. Navigate to your app's Overview tab and click the Configure-Add-Ons link. You should see Heroku Postgres as you type into the search bar. Select Hobby Dev ‚Äî Free. It can store 10,000 rows of data and has a limit of 20 Connections. Find more about plans & pricing here. First, you will need to install Docker. Now download the docker-compose file from here, or you can copy and paste it to a file called docker-compose.yml. I‚Äôve talked about the individual elements of the docker-compose file. Have a look if you‚Äôre interested. towardsdatascience.com For the environment tag, the required variables are PGADMIN_DEFAULT_EMAIL and PGADMIN_DEFAULT_PASSWORD. Read more about it here. Let‚Äôs run the following command from the same directory where the docker-compose.yml file is located. You can now access pgadmin4 via your favorite web browser by visiting the URL http://localhost:5050/. Use admin@admin.com for the email address and root as the password to log in. Congratulations! you have successfully deployed a PostgreSQL database and run pgadmin4 on your machine using Docker. Now we need to connect them so we can do database administration from our local machine. Click the Heroku Postgres link and navigate to the Settings tab. Now click View credentials. As you can see, there are multiple values, but we are only interested in Host, Database, User, and Password. From pgadmin4, click Servers > Create > Server to create a server. For the name, use any name. Now move to the Connection tab. Use the value of Host for Hostname/address. Use the value of Database for Maintenance database. Use the value of User for Username. Use the value of Password for Password, and also tick the Save password? box if you don‚Äôt want to type the password every time you log in to pgadmin4. Now for the SSL tab, select Require for SSL mode. Finally, move to the Advanced tab and use the value of Database (Maintenance database) for the DB restriction. Click Save, and you‚Äôre done! This part is optional if you want to import some test data for testing. Click Servers > testdb > Databases > Database name > Schemas > Tables. Right-click on tables and select Query tool. Copy-paste the SQL query from here to Query Editor and click the play button. Now you should have two tables called students and marks with some test data. Now you know how to connect to a Heroku Postgres database with pgadmin4 using Docker. If you find this post helpful, check out the posts below where I‚Äôve talked more about Docker. towardsdatascience.com towardsdatascience.com towardsdatascience.com",14,0,5,Towards Data Science,2021-01-04,2021
https://towardsdatascience.com/performance-metrics-in-machine-learning-part-2-regression-c60608f3ef6a,Performance Metrics in Machine Learning‚Ää‚Äî‚ÄäPart 2: Regression,Using the right performance metric¬†for‚Ä¶,3,39,"['Performance Metrics in Machine Learning ‚Äî Part 2: Regression', 'Regression', 'Summary']","In the previous post of this three-part series, I went through the most common performance metrics that every Data Scientist should know when working on Classification tasks. You can check the previous part of this series here. In the second part, I am going through the performance measures that are most applicable to Regression tasks. These are the most common tools to be able to effectively evaluate whether a model is actually well-performant and ready to be brought into Production or it still needs some fine-tuning. Before getting deeper into the performance metrics, it is best to highlight some key introductory concepts. Error is a pretty intuitive measure that needs to get little formal definition as it is a widely known concept. In terms of Machine Learning performance, it is key to define that when we talk about errors we specifically refer to the difference, the ‚Äúdelta‚Äù, between the actual target value and the predicted value. Refreshing our memory from the previous post: When evaluating the performance of a classification model, two concepts are key, the real outcome (usually called ‚Äòy‚Äô) and the predicted outcome (usually called ‚Äò≈∑‚Äô). For instance, a model can be trained to predict whether a person will develop a particular disease. In this case, it is trained with samples, e.g. a person‚Äôs data, containing predictive information, such as Age, Gender, etc. and each person is labelled with a flag stating whether the disease will develop or not. In this case, the label can be whether the disease will happen (y=1) or will not happen (y=0). A Machine Learning model aims at making sure that every time a sample is presented to it, the predicted outcome corresponds to the true outcome. The more the model‚Äôs predictions are the same as the true values the higher is the performance of the model. There are many different ways of evaluating a model‚Äôs performance which are here mentioned, but in general, models make mistakes, lowering performance. The higher the difference between the real outcome ‚Äòy‚Äô and the predicted outcome ‚Äò≈∑‚Äô, the more ‚Äúoff‚Äù the model is from being an accurate representation of the phenomenon; the closer the values and the better the performance of the system. The Mean Squared Error measures the average of the errors squared. It basically calculates the difference between the estimated and the actual value, squares these results and then computes their average. Because the errors are squared, MSE can only assume non-negative values. Due to the intrinsic randomness and noise associated with most processes, MSE is usually positive and not zero. Like the variance, MSE has the same units of measurement as the square of the quantity being estimated. Similarly to the Variance, one major disadvantage of Mean Squared Error is that it is not robust to outliers. In case a sample has a ‚Äúy‚Äù and associated error which is way larger than the other samples, the square of the error will be even larger. This, paired to the fact that MSE calculates the average of errors, makes MSE prone to outliers. Similarly to the Mean Squared Error, RMSE calculates the average of the squared errors across all samples but, in addition, takes the square root of the result, effectively taking the square root of MSE. By doing so, RMSE provides an error measure in the same unit as the target variable. For instance, if our target y is next year‚Äôs sales in dollars, RMSE will give the error in dollars, while MSE would be in dollars squared, which is much less interpretable. The Mean Absolute Error does not take the square of the errors. Instead, it simply calculates the absolute value of the errors and then takes the average of these values. The MAE takes the absolute value as we are not interested in the direction in which the estimated and actual target values differ (estimated > actual or vice-versa) but on the absolute distance. This also avoids errors to cancel each other out when calculating the MAE. Differently from MSE, MAE does not penalise larger errors more than smaller ones, because the formula for MAE does not apply the square to errors. Another advantage is that MAE does not square the units, similarly to RMSE, making the results more interpretable. The Mean Absolute Percentage Error measures the error between actual and forecasted values as a percentage. It achieves so by calculating it similarly to MAE, but also dividing it by the actual value, expressing the result as a percentage. By expressing the error as a percentage, we can have a better understanding of how off our predictions are in relative terms. For instance, if we were to predict next year‚Äôs spending, an MAE error of $50 could be both a relatively good or bad approximation. For instance, if the $50 error was made with respect to an actual spending of $1 million, we could safely say that the prediction is pretty good. Instead, if the error was on a $60 cost prediction, it would be pretty far off from the actual value. In relative terms, an error of $50 against a $1 million prediction is a 0.005% error. If this error was made on a $60 prediction it would mean that the error is 83% of the predicted value (leading to basically a range of $10 to $110, almost reaching double the actual value). Using MAPE, in this case, shows a more accurate representation of the error with respect to the absolute values. R Squared (R¬≤) represents the proportion of the variance for the dependent variable y that‚Äôs explained by the independent variables X. R¬≤ explains to what extent the variance of one variable explains the variance of the second variable. So, if the R¬≤ of a model is 0.75, then approximately 75% of the observed variation can be explained by the model‚Äôs features. R¬≤ is calculated by taking one minus the sum of squares of residuals divided by the total sum of squares. R¬≤ compares the fit of the chosen model with that of a horizontal line, which acts as a baseline. If the chosen model fits worse than a horizontal line, the ùëÖ¬≤ is negative. Because of the formula of ùëÖ¬≤, even though the ‚Äúsquare‚Äù is involved, it can have a negative value without violating any rules of math. ùëÖ¬≤ is negative only when the model does not follow the trend of the data and fits worse than a horizontal line. One of the drawbacks of R¬≤ is that the more features are added to a model, the more the R¬≤ increases. This happens even though the features added to the model are not intrinsically predictive. For this reason, the Adjusted R¬≤ was introduced. It takes into account the features used in the predictive model. Doing so, the more predictive features are added to the model, the higher the Adjusted R¬≤. However, the more ‚Äúuseless‚Äù features are added to the model, the lower the Adjusted R¬≤ value, differently from what would happen with R¬≤. For this reason, the Adjusted R¬≤ is always less or equal the R¬≤ value. Where n is the number of data points and k is the number of features in the model. Overall, it is usually important to report both an error measure, e.g. RMSE and an R¬≤ measure. This is because R¬≤ expresses the relation between the features X in the model and the target variable y. Error measures, instead, express how much spread out the data points are with respect to the regression fit. Reporting both Adjusted R¬≤ and RMSE, for instance, allows for a better comparison of the model against other benchmarks. After having gone through the Classification Performance Metrics in the previous post, we examined Regression metrics. We first highlighted the meaning of error and then focused specifically on the most common metrics that every Data Scientist should know. Error Metrics R¬≤ Metrics Check out other articles on performance metrics such as: To read more articles like this, follow me on Twitter, LinkedIn or my Website.",106,0,7,Towards Data Science,2021-01-04,2021
https://towardsdatascience.com/8-data-science-project-ideas-from-kaggle-in-2021-83a3660e0342,8 Data Science Project Ideas from Kaggle in¬†2021,,4,46,"['8 Data Science Project Ideas from Kaggle in 2021', 'Motivation', 'Project Ideas', 'Conclusion']","Finally, we are in year 2021 üéâ It's a new chapter of life üê£ For me, as a data scientist, I wanted to use this opportunity to summarize a list of interesting datasets that I found on Kaggle in 2021. I also hope that this list can be useful to the people who are looking for data science projects to build their own portfolio. After taking many different pathways trying to learn data science, the most effective one I found so far is to work on projects from real datasets. However, it sounds simple but actually it‚Äôs quite challenging to build a data science portfolio from scratch. Data Science is a broad subject. New learners can easily feel lost even with so many resources free online. Learning new concepts passively cannot guarantee that you are able to solve a similar problem next time facing it. In the end, I feel that the ability to design your own learning map is important to make sure that you are in an active learning mode. It requires your passion, logic, diligence and an overall understanding of data science. To become an active learner, in any subject, Interest is your best Teacher. üòº Therefore, I summarized some most recently updated datasets from Kaggle. The tasks vary from sentimental analysis to building a predictor. I have also tried to add some extended readings as more options to explore. ( I picked the datasets based on their date & votes. ) www.kaggle.com The dataset is imbalanced. It requires some strategies to fix an imbalanced dataset. towardsdatascience.com towardsdatascience.com www.kaggle.com We can simplify the NLP process by utilizing the Hugging Face package. github.com www.kaggle.com A good dataset to understand precision and recall. The top priority in this business problem is to identify customers who are getting churned. Even if we predict non-churning customers as churned, it won‚Äôt harm our business. But predicting churning customers as Non-churning will do. So recall (TP/TP+FN) need to be higher. towardsdatascience.com towardsdatascience.com www.kaggle.com This is a dataset with a lot of potential. As listed in the tasks, this dataset is suitable for a recommendation engine, trend analysis, popularity predictor and unsupervised clustering. towardsdatascience.com www.kaggle.com Although this task asks us to perform sentiment analysis, I feel that it‚Äôs also suitable to build a word cloud based on the text data. towardsdatascience.com towardsdatascience.com towardsdatascience.com www.kaggle.com This can be a time series analysis task. towardsdatascience.com towardsdatascience.com www.kaggle.com For restaurants recommendation, a heat map might help as well. towardsdatascience.com www.kaggle.com I probably want to focus on some creative data visualizations for this classic project. medium.com medium.com I really enjoyed myself learning while summarizing above resources into a reading list. Some of the projects might be challenging but efforts will always pay off. For me, a difficult project idea makes me have more willingness to learn more than a simple one does. Meanwhile, a complex dataset usually contains more features that enable us to complete a project in depth. Efforts will always pay off. üèÜ I hope you find some project ideas that really interest you in this article! Happy New Year Happy Learning See you Next Time",59,0,5,Towards Data Science,2021-01-04,2021
https://towardsdatascience.com/5-reasons-data-scientists-must-be-data-engineers-in-2021-and-beyond-805c6d1a1e03,5 Reasons Data Scientists Must be Data Engineers in 2021 and¬†Beyond,Our field is progressing rapidly‚Ä¶,1,31,['5 Reasons Data Scientists should Have Engineering Skills'],"Our field is progressing rapidly, consequently the market is demanding more from us. Gone are the days when you could sit in ivory towers and build fancy models with little to no practical utility. Simply put, things are expected to work ‚Äî and at scale. I‚Äôve written this article from my experience in advanced analytics across three major banks and my master‚Äôs program at university. I hope to enlighten the reader about the current state of data science. Roughly a decade ago I was starting my master‚Äôs degree in operations research, applied statistics and risk. Like many master‚Äôs programs it was highly theoretical. I still recall my professor detailing the arduous process of creating a consumer credit scorecard using a combination of excel spreadsheets, SAS and logistic regression. A final year project had us build an algorithm from first principles to optimise the travelling salesman problem using VBA (yuk!). Success was based more on theoretical understanding than practical utility. Fast forward a few years to my first analytics job at a major bank in the UK. SAS was the main software used for analytics, all the data was relational and warehoused. Data science primarily took the form of complex SQL queries consisting of multiple joins followed by some model building. SAS scripts were often tens of thousands of lines long to do quite simple things, and excel spreadsheets were a standard to articulate models. Modelling was a long and iterative process requiring extensive SAS knowledge but it rarely went beyond ones own laptop. Do any of these experiences sound familiar to you? I would hazard a guess that many data professionals have been through this ‚Äúexperience pipeline‚Äù ironically never having built a data pipeline (or knowing what one is). Maybe you have solid theoretical foundations in statistics, mathematics and can even code up a decent ML model, but you do not see how you can take these beyond your own laptop. Or you‚Äôre interested in breaking into the field and need to direct your learning. Either way this article is for you. I hope by the end you will have an overview of the data science landscape so that you may guide your studies and/or professional ambitions adequately. For me it was a humbling realisation, to be useful as a data scientist you probably need to be a data engineer too. Here are five reasons why: Practical data science is about providing data driven decision making at scale AND as a service to customers or clients. Fraud detection systems, recommendation engines (think Instagram, Netflix) and credit scoring are all examples of this. If you look around you‚Äôll notice these systems everywhere. Auditability, governance, accuracy of decisions quality of service are just some the challenges that present themselves in data systems. Given all these challenges, how do companies provide us with these services consistently to a high standard of quality? They hire people that can systematise data solutions. To meet the market‚Äôs demand for scalable, data driven decisioning we will need to think in terms of systems. Data engineering principles give us the framework to think about systems and go beyond modelling to build end to end data solutions. Anybody that is familiar with SciKit Learn will tell you building machine learning models can be done in just a few lines of code. In addition, AutoML tools have become a popular way to build ML models with speed and efficiency to the frustration of many purists. In short machine learning is becoming more accessible to the layperson. Now I‚Äôm not suggesting that one does not have to understand the models that they‚Äôre working with, I‚Äôm simply pointing out that these models are becoming easier to implement. The ease of use of these tools is significant for the future of data roles. For example, a software engineer has the potential to pick-up data science tools and implement practical end to end solutions far quicker than ever before. Saying all of this I believe that data scientist and analysts will need engineering skills to compete with software engineers for jobs. Most of what we do now is being tracked. My Fitbit records biometric data from my workouts, my smart meter records my energy usage, my budgeting apps record my spending. Data whether unstructured or structured is constantly being captured, and with the internet of things expanding to include devices like cars big data is bound to get ‚Äî well, bigger. Successfully implementing data science solutions hinges on making use of big data. Previously this would have required a deep software or data engineering skillset, but tech has changed the landscape; conveniently this leads on nicely to the next point‚Ä¶ Essentially cloud computing platforms have made it easier to build end to end data solutions. Firstly, they‚Äôve democratised computing resource and storage. Some cloud computing platforms boast the ability to automatically scale compute resources and storage needs. Prior to this storage and compute resources would have been manually managed on either onsite servers or a private cloud. Resource management like this is complex and requires specialist engineers and often a high capital investment. Secondly, data processing is more accessible making it easier to build end to end solutions and make use of big data. Cloud platforms have bespoke tools for building data pipelines to handle a multitude of use cases including streaming and batch processing. Access to most of the cloud computing platforms is available at relatively inexpensive rates making it possible to build machine learning systems from one‚Äôs own laptop. It is easier than ever for a data scientist to engineer end to end data solutions. Many businesses have cottoned on to the data science landscape. Anybody that has recently applied for a data science position may have noticed that building AND deploying scalable machine learning models is now commonly expected. Silicone valley tech companies are ahead of the curve with this. In 2016 there were more data engineer jobs posted on job boards in the valley than data science and data analyst positions combined 1. At Google all technical staff are referred to as engineers further highlighting the importance of this position 1. I see this trend expanding outside of silicone valley with tech hubs sprouting globally. Companies are recognising the advantages of hosting data services on cloud platforms. An understanding of the theory and the ability to implement solutions end to end will make you an asset. A friend of mine hiring analysts for his tech start-up said‚Ä¶ ‚ÄúHe‚Äôs been looking for analysts but what he really needs are analyst-engineers‚Äù. Final Thoughts It‚Äôs safe to say that the role of a data scientist is evolving, we should adapt or be left behind by the winds of change. This is an exciting opportunity for those that are willing to learn and really want to make an impact whether with an entrepreneurial venture or in an existing business. With new tech at our finger tips the focus will be less on the theoretical underpinnings of models and more on solving novel business problems. References 1 Valliappa Lakshmanan, (2019), Data Science on the Google Cloud Platform p.5‚Äì6 www.linkedin.com",172,1,6,Towards Data Science,2021-01-04,2021
https://towardsdatascience.com/embarrassingly-shallow-autoencoders-ease-for-recommendations-f91117f02851,Embarrassingly Shallow Autoencoders (EASE) for Recommendations,Recommendations without the¬†Ruffles,1,36,['Embarrassingly Shallow Autoencoders (EASE) for Recommendations'],"Recommendations without the Ruffles Background I always enjoy learning about Recommendation Systems that are both simple and effective. Too often I hear stories about complex engines that have so many moving parts, and/or are incredibly expensive to train, which then fail when in a production environment. Two authors that I consistently see on the Leaderboards of PapersWithCode.com are Steffen Rendle (Google) and Harald Steck (Netflix). The amazing thing about their research? I am often guilty of jumping on the bandwagon of complexity when seeing a new deep learning architecture. It can be fun and exciting to exist on the ‚Äúcutting edge‚Äù of whatever domain you are interested in, adding layer upon layer of complexity. Do these actually produce the best results, though? If you read my Graph-based recommender article, you may have noticed I referenced a great paper by Dacrema et al. (2019). In many domains/datasets, the best algorithms can be simple approaches: Let‚Äôs embrace Occam‚Äôs Razor and stick to algorithms that produce great results without all of the rest of the fluff. This is where EASE comes into play. It is essentially an item-item similarity model, but with the provision that we enforce an item to not be similar to itself. This makes the model more generalized and can produce results that are both interesting and diverse. Example I found out about EASE while browsing through PapersWithCode.com . It is the best algorithm by a far margin on the Million Song Database, as well as in the Top 5 for a number of other datasets. The implementation was written in Numpy, and while it worked pretty well, I wanted to speed it up a bit with PyTorch. So I refactored it while attempting to learn how it worked, and you, dear reader, can follow me on this journey. Let‚Äôs make a dummy example to see how this all works before we try to run it on a real dataset. We can make a few users + item (ratings optional) pairs and store them in a DataFrame. For each user and item, we will use an integer token to identify it. This will be our X matrix. Now, we could store this as a Dense PyTorch Tensor, but it probably will be very sparse in a real world application. Instead, we are going to keep it as a sparse tensor until we need it: From here, we need to build our B matrix, which is our item to item weight matrix that will be the source of our predictions: P(user, item) = X[user]*B[item] Steps The EASE paper explains all of the steps needed to form this closed-form solution, but here are the basics: Personally, I think it is easier to see these steps when trying to understand what they are doing: Step 1 Step 2 Step 3 Steps 4 , 5 This is our weight matrix! Now we just multiply any vector that contains all of the interactions for a given user by B, and the output will be a scored list of predictions. These predictions should make some intuitive sense. High scores are assigned to items the user has already interacted with. The 4th item has not been interacted with, and has the next largest score. We should recommend it! We can recommend a block of users by multiplying our entire X matrix by B (sparse.to_dense()@B) to get the entire set of predictions. Intuition I think of the matrix multiplication step like this: Here I am using the term similar in a broad way: The Matrix and Wall-E may not be similar in genre, tone, etc., but it is possible that these two items are generally both watched by similar users, a weighted co-occurrence. If you liked Wall-E, other items watched by Wall-E watchers might be relevant to you as well. Possibly. Now, we can go through our predicted scores, select the TopK items for each user, and then return those item names. Simple! We did not even have to use gradient descent or chain rule once. What other reasons might we use an approach like EASE, apart from how simple it is to get SOTA results? The paper‚Äôs author makes the case: For me, I personally enjoy being recommended something rare and interesting that I would enjoy, rather than something popular that I was probably aware of already. The novelty and joy of discovery of something wonderful is uniquely satisfying. For those of you that may be asking ‚ÄúWhy is this an autoencoder? Where is the dense layer here?‚Äù That‚Äôs why this is a shallow AE! There is no need for a dense layer to compress the user vector. Rather, we store the weights of B for the output layer. Then why is this Embarrassing? I think the only embarrassment here is the fact that such a simple set of transformations remains so powerful as to be SOTA, while complex neural nets and ensembles lag behind. Ratings EASE can also handle explicit user feedback as well (e.g., five star ratings) to make predictions. I have not fully tested that part of the code, but it should work in theory! More work to be done. Metrics Typically for these types of systems, I like to use HitRate@K as my metric of choice. It‚Äôs simple to understand (predicted [‚Äú1984‚Äù, ‚ÄúAttack Surface‚Äù, ‚ÄúGuards! Guards!‚Äù] with actual [‚Äú1984‚Äù] being a Hit). Plus, it aligns nicely with most business use cases. As a test case, I grabbed the GoodReads dataset, filtered books in English, and trained EASE (lambda = 250.0) on 5 million book ratings. I then made 20 predictions for 100k users and compared the results: HitRate@20 of 9.1%! Not too bad. If I just predicted the most popular books on the GoodReads data, I would have only gotten 6%, so a definite improvement along with some personalization. The entire pipeline took less than a minute to run! Conclusion I would encourage you all to check out my repo here, as well as the original Numpy version here. The paper itself is worthy of a read, as it is short and to the point. Tuan also just posted an article on other simple recommendation systems that you might want to check out.",21,0,6,Towards Data Science,2021-01-04,2021
https://towardsdatascience.com/mongodb-in-the-cloud-three-solutions-for-2021-ee87c4ef4ef,MongoDB in the Cloud: Three Solutions for¬†2021,An overview of pricing and compatibility for¬†MongoDB‚Ä¶,6,31,"['MongoDB in the Cloud: Three Solutions for 2021', 'What is MongoDB?', 'Cloud providers', 'MongoDB Atlas', 'Azure Cosmos DB', 'AWS DocumentDB']","By: Edward Krueger and Douglas Franklin. This article will investigate the costs and features offered by MongoDB‚Äôs MongoDB Atlas, AWS Amazon DocumentDB and Azure Cosmos DB. We assume that we aren‚Äôt talking about mega-scale, enterprise solutions. We are looking for a more typical app. One reason this is relevant is that MongoDB, the company, decided to end mLab services late this year. This is to migrate users to the companies newer platform, MongoDB Atlas. However, there are many cloud providers for MongoDB today. We will discuss MongoDB Atlas, Azure Cosmos DB, and AWS DocumentDB. MongoDB is a document-oriented database that stores data as BSON objects or documents organized into collections. These documents can be retrieved as JSON‚Äôs. This means you can store records without using the column and row structure found in RDBMS. A MongoDB cloud database is a great solution for content management, product catalogs, geospatial data applications, and any project with rapidly changing data requirements. There are many MongoDB cloud solutions available. What solution a development team or company ultimately decides on will likely come down to features, pricing, and integration with your existing architecture. All the options discussed here are managed cloud services. This means that all of the DB administration and security tasks required in modern databases will be handled by the service provider. Ignoring what the solutions have in common, let's break down some differences. MongoDB, founded in 2007, is under pressure to get more users and make more money. The company built Atlas in 2016, acquired mLab in 2018 for $68 million, then deprecated mLab late in 2020. MongoDB Atlas is the solution offered by MongoDB the company. As such, MongoDB Atlas is the most complete MongoDB solution. With MongoDB Atlas, you will have access to all the MongoDB features and methods, including map-reduce. Additionally, you will be able to run the v4.4, v4.0, or v3.6 API. Another benefit of Altas DB is that you can be flexible with other cloud service providers (AWS, Azure, and GCP). We were easily able to convert our app to one running on GCP with MongoDB Atlas. MongoDB Atlas does have a free tier. However, Cloud Atlas Pricing is about $57/month for 10GB storage with 2GB ram and 1 vCPU if you need more robust computing. For more information on setting up MongoDB Atlas or migrating to MongoDB Atlas, check out this article. towardsdatascience.com Azure Cosmos DB is the Microsoft Azure solution for Document Databases. Azure Cosmos DB uses RUs or resource units, making pricing slightly more opaque than the other cloud platforms. However, there are RU calculators you can use to convert to dollars and estimate costs. Here is an Azure DB Monthly Cost Calculator. Azure does have a free tier that grants the first 400 RU/s and 5 GB of storage in the account. Azure Cosmos DB free tier makes it easy to get started, develop and test your applications, or even run small production workloads for free. The Free tier comes with all the benefits and features of a regular Azure Cosmos DB account. Cosmos DB uses the MongoDB v3.6 API and does not support map-reduce. After your free credits are used up, Cosmos DB costs are lower than competitors. For 10GB data with 1.5GB ram with minimal use will cost about $25/month. Additionally, Azure now offers a serverless option. This is great for inconsistent use or low use apps, like internal business apps or proof of concepts. There is no minimum charge involved when using Azure Cosmos DB in serverless mode. Azure Cosmos DB serverless only bills you for the RUs consumed by your database operations and the storage consumed by your data. This can reduce costs drastically for infrequently or irregularly accessed apps. That being said, serverless is not a good solution for regular data loads. Serverless does provide automated scaling; however, costs can be high. For more information on setting up an Azure Cosmos DB or migrating to Cosmos DB, check out this article. towardsdatascience.com Amazon Web Services solution is Amazon Document DB. AWS DocumentDB has no free tier. The cheapest instance available, db t3.medium, has 10GB storage with 4GB RAM and 2 vCPUs is about $60/month. Notice that this is about double the RAM offered by Azure and Atlas. With AWS DocumentDB, you have the option of using the v3.6 or v4.o API. This platform lacks the map-reduce functionality available in Atlas. Vendor lockin is another thing to keep in mind with AWS. If your company already uses AWS services over other cloud providers, this solution can synergize well. Vendor lock-in is not impossible to get around, but it is additional work and expense. For Example, to use AWS DocumentDB with a Heroku app requires you to get Heroku enterprise, which is expensive. Amazon DocumentDB is a VPC-only service and doesn‚Äôt support public endpoints. As a result, you can‚Äôt connect directly to an Amazon DocumentDB cluster from an environment outside AWS. However, you can connect from a machine outside AWS using an SSH tunnel. Teams will need to consider features, pricing, and their existing architecture to choose their best cloud MongoDB solution. Some of the decisions will be easy; for example, if you need map-reduce, choose MongoDB Atlas. If you are going to make a low/sporadic use app use Azure Cosmos Serverless to save on costs. For predictable loads, Azure serverless can be rough. People often choose serverless options because they scale very easily. This issue is that mega scaling serverless can cost be unexpectedly high. However, if every amount of scaling means additional revenue, you have variable costs corresponding to variable revenue. Consider an online shop of black Friday, for example. Failure to accommodate extra traffic via auto-scaling is a disaster for lost revenue. If your application is hosted on Heroku (not enterprise), choose MongoDB Atlas or Azure Cosmos. With MongoDB Atlas, you can choose any cloud provider you like and use the latest MongoDB API features. This flexibility is not available with Amazon Document DB or Azure Cosmos DB. These options locking you into their respective cloud vendors while running older versions of the DB API. If your company has some existing projects using AWS and or you like the platform, AWS DocumentDB can be a great solution. Just keep in mind compatibility issues with other cloud services. AWS Amazon Document DB and Azure Cosmons support most of the features you need to create an application. The only feature we note that is lacking is the map-reduce method. So if you are using MongoDB to use this feature, Document DB or Cosmos may not be a good solution for you; Atlas will better serve your needs.",136,0,5,Towards Data Science,2021-01-04,2021
https://towardsdatascience.com/precision-recall-curves-how-to-easily-evaluate-machine-learning-models-in-no-time-435b3dd8939b,Precision-Recall Curves: How to Easily Evaluate Machine Learning Models in No¬†Time,,8,38,"['Precision-Recall Curves: How to Easily Evaluate Machine Learning Models in No Time', 'Reviewing Confusion matrix, Precision, and Recall', 'Precision', 'Recall', 'Dataset loading and preparation', 'Comparing Precision-Recall curves', 'Conclusion', 'Learn more']","Precision-Recall curves are a great way to visualize how your model predicts the positive class. You‚Äôll learn it in-depth, and also go through hands-on examples in this article. As the name suggests, you can use precision-recall curves to visualize the relationship between precision and recall. This relationship is visualized for different probability thresholds, mostly between a couple of different models. A perfect model is shown at the point (1, 1), indicating perfect scores for both precision and recall. You‚Äôll usually end up with a model that bows towards the mentioned point but isn‚Äôt quite there. Here‚Äôs what the article covers: Before diving deep into precision, recall, and their relationship, let‚Äôs make a quick refresher on the confusion matrix. Here‚Äôs it‚Äôs most general version: That‚Äôs great, but let‚Äôs make it a bit less abstract by putting actual values: You can calculate dozens of different metrics from here, precision and recall being two of them. Precision is a metric that shows the number of correct positive predictions. It is calculated as the number of true positives divided by the sum of true positives and false positives: Two terms to clarify: You can now easily calculate the precision score from the confusion matrix shown in Image 2. Here‚Äôs the procedure: The value can range between 0 and 1 (higher is better) for precision and recall, so 0.84 isn‚Äôt too bad. High precision value means your model doesn‚Äôt produce a lot of false positives. Recall is the most useful metric for many classification problems. It reports the number of correct predictions for the positive class made out of all positive class predictions. You can calculate it with the following formula: Two terms to clarify: Sure, it‚Äôs all fun and games when classifying wines, but the cost of misclassification can be expressed in human lives: a patient has cancer, but the doctor says he doesn‚Äôt. Same principle as with wines, but much more costly. You can calculate the recall score from the formula mentioned above. Here‚Äôs a complete walkthrough: Just as precision, recall also ranges between 0 and 1 (higher is better). 0.61 isn‚Äôt that great. Low recall value means your model produces a lot of false negatives. You now know how both of these metrics work independently. Let‚Äôs connect them to a single visualization next. You‚Äôll use the White wine quality dataset for the practical part. Here‚Äôs how to load it with Python: Here‚Äôs how the first couple of rows look like: As you can see from the quality column, this is not a binary classification problem ‚Äì so you‚Äôll turn it into one. Let‚Äôs say the wine is Good if the quality is 7 or above, and Bad otherwise: Next, let‚Äôs visualize the target variable distribution. Here‚Äôs the code: And here‚Äôs the visualization: Roughly a 4:1 ratio, indicating a skew in the target variable. There are many more bad wines, meaning the model will learn to classify bad wines better. You could use oversampling/undersampling techniques to overcome this issue, but it‚Äôs beyond the scope for today. You can make a train/test split next: And that‚Äôs it! You‚Äôll train a couple of models and visualize precision-recall curves next. The snippet below shows you how to train logistic regression, decision tree, random forests, and extreme gradient boosting models. It also shows you how to grab probabilities for the positive class: You can obtain the values for precision, recall, and AUC (Area Under the Curve) for every model next. The only requirement is to remap the Good and Bad class names to 1 and 0, respectively: Finally, you can visualize precision-recall curves: Here‚Äôs the corresponding visualization: As you can see, none of the curves stretch up to (1, 1) point, but that‚Äôs expected. The AUC value is an excellent metric for comparing different models (higher is better). Random forests algorithm did best on this dataset, with an AUC score of 0.83. To summarize, you should visualize precision-recall curves any time you want to visualize the tradeoff between false positives and false negatives. A high number of false positives leads to low precision, and a high number of false negatives leads to low recall. You should aim for high-precision and high-recall models, but in reality, one metric is more important, so you can always optimize for it. After optimization, adjust the classification threshold accordingly. What‚Äôs your approach to model selection? Let me know in the comment section. Loved the article? Become a Medium member to continue learning without limits. I‚Äôll receive a portion of your membership fee if you use the following link, with no extra cost to you. medium.com Originally published at https://betterdatascience.com on January 4, 2021.",106,1,5,Towards Data Science,2021-01-04,2021
https://towardsdatascience.com/10-basic-shell-commands-every-programmer-should-know-d5d43b009d20,10 Basic Shell Commands Every Programmer Should¬†Know,Work with your computer faster and¬†more‚Ä¶,12,28,"['10 Basic Shell Commands Every Programmer Should Know', '‚Ññ1: Listing all files in a directory', '‚Ññ2: Changing directories', '‚Ññ3: Creating a new directory', '‚Ññ4: Creating Files', '‚Ññ5: Locating files', '‚Ññ6: Moving Files (mv)', '‚Ññ7: Removing Files and Directories', '‚Ññ8: Getting the current path', '‚Ññ9: Displaying user information', '‚Ññ10: Clearing the shell screen', 'Takeaways']","We all ‚Äî probably ‚Äî started our data science or programming journey using some GUI. A tool or an app with everything built up, and we just have to write code and click on some button to compile and run it. Voila, the results show up, and we are done. Although there is absolutely nothing wrong with GUIs, as you advance in your career, you will need a better, faster, and more efficient way to control your computer and get the job done. That‚Äôs using shell commands ‚Äî command prompt in Windows ‚Äî to control your workstation. Using shell commands is an essential skill you will need to be comfortable with if you want to take your programming skills and your career to the next level. Using shell commands, you will be able to harness your computer's real power and accomplish more tasks in less time. Once you master shell commands, you will be able to write few commands to update your systems, configure your webserver or database remotely, and much, much more. towardsdatascience.com All of that power starts with some basic commands to perform simple yet essential tasks. In this article, I will go through 10 basic shell commands that any programmer must be familiar with regardless of their work field. The most basic and useful command is the ls command. We often arrange our source code files and other essential files in specific directories in the system. The ls command will display all folders in the current directory. The current directory is where you‚Äôre running the ls command. Running this command will display something like the following. Using the ls command is good if you just want to know what‚Äôs in your current directory. But, what if you need to access another directory? Here where the cd command comes in. The cd command is short for ‚Äúchange directory.‚Äù The way to use this command is by writing cd and then the directory's name or path you want to move to. There are some special uses of the cd command, mainly typing cd .. will take you back in the directory tree one step while typing the cd with no directory name will take you back to the home directory. To create a new directory ‚Äî folder ‚Äî you need to use the make directory command (mkdir). The mkdir command simply creates a new folder; you need to give it a name and a path to do that. The mkdir will then create a folder with the given name in the given path. If you didn‚Äôt provide the command with a path, it would create the folder in your current directory. Once the folder is created, you can see it by running the ls command. So far, we went through how to change directories, create them, and display their content. What if you want to create a file? Then you will need to use the touch command. This command can be used to create all kinds of files. That‚Äôs why the file extension must always be included when the command is used. If you want to search for a file in your system, you can use the locate command. Which is the faster way to find files within a directory tree. You can ask the locate command to search for a filename with or without case sensitivity by adding or excluding the -i option. Moreover, you can use the * to widen your search. You can even lookup a regex if you add the -r option. If you want to change the location of a file or a set of files, you can use the move (mv) command to do so. The mv command needs two pieces of information, the file you want to relocate and the new intended location. Say you want to remove/ delete a file or directory; you can use the remove (rm) command. If you want to remove files, you can use the rm command followed by the name or the path to the file you want to remove. However, if you want to remove a folder, then you need to add the -r option when using the rm command. A little weaker variation of the rm command is rmdir. This command is used to remove empty folders/ directories. Sometimes you get deep into your directory tree that you get lost and don‚Äôt know your exact directory anymore. Although the header of the shell command should tell you your current location, you can get a complete one using the pwd command. pwd stands for print working directory. You can also use the shell to display the information about the currently logged-in user. To do that, you can use the command whoami, which does exactly as it says. After some commands, your shell screen will get cluttered and crowded. You can clear the old commands and their results from the screen and start clean using the clean command. As a programmer, whether you are a data scientist, an app developer, or a web developer, sooner or later, you will find yourself in need to master and use shell commands to control your computer. Using the shell will allow you to use the full power of your computer and optimize your workflow. In this article, I went through the core 10 shell commands every programmer should be familiar with. Of course, there are way more commands, and during your career, you will learn more and more commands. towardsdatascience.com Mastering the shell is a must step to evolve and advance in your career. The good news is, you don‚Äôt actually need to memorize these commands; you will automatically do by using them daily. The world of shell commands is vast, and the commands themselves vary based on the system you use. The ones mentioned in this article are Unix/ Mac commands; their Windows equivalent is slightly different. The could make things somewhat confusing, but with practice and time, it will become second nature to you.",62,0,6,Towards Data Science,2021-01-04,2021
https://towardsdatascience.com/a-beginners-guide-to-hadoop-s-fundamentals-8e9b19744e30,A Beginner‚Äôs Guide to Hadoop‚Äôs Fundamentals,A non-technical guide to Hadoop‚Äôs big data analytical‚Ä¶,6,57,"['A Beginner‚Äôs Guide to Hadoop‚Äôs Fundamentals', 'Hadoop 1 Architecture', 'Hadoop Distributed File System (HDFS)', 'Hadoop 2.0', 'What is MapReduce?', 'Conclusion']","Literally, Hadoop was the name of a toy elephant ‚Äî specifically, the toy elephant of Doug Cutting‚Äôs (Hadoop‚Äôs co-founder) son. But you‚Äôre not here to learn how, or from where, Hadoop got its name! Broadly speaking, Hadoop is a general-purpose, operating system-like platform for parallel computing. I am sure I do not need to mention the severe limitations of a single system when it comes to processing all the big data floating around us ‚Äî it is simply beyond the processing capacity of a single machine. Hadoop provides a framework to process this big data through parallel processing, similar to what supercomputers are used for. But why can‚Äôt we utilize supercomputers to parallelize the processing of big data: Hadoop comes to the rescue as it takes care of all the above limitations: it‚Äôs an open-source (with strong community support and regular updates), operating system-like platform for parallel processing that does not rely on specific hardware vendors for ongoing hardware support (works with commodity hardware) and does not require any proprietary software. There have been three stable releases of Hadoop since 2006: Hadoop 1, Hadoop 2, and Hadoop 3. Let‚Äôs now look at Hadoop‚Äôs architecture in more detail ‚Äî I will start with Hadoop 1, which will make it easier for us to understand Hadoop 2‚Äôs architecture later on. I will also assume some basic familiarity with the following terms: commodity hardware, cluster & cluster node, distributed system, and hot standby. Following are the major physical components of the Hadoop 1 architecture: Master Nodes: Each of the above nodes represents an individual machine in a production environment, working in the master mode, that are usually placed in different racks in a production setup (to avoid the failure of one rack bringing down multiple master nodes). Slave Nodes: The slave nodes cannot function without the master nodes and are fully dependant on the instructions that they receive from the master nodes before undertaking any kind of processing activities. To ensure continuous uptime, slave nodes send a heartbeat signal to the name node once every three seconds to confirm that they‚Äôre up and active. All the above master and slave nodes are interconnected through networking infrastructure to each other to form a cluster. In terms of processing capacities, Job Tracker is more powerful than the Name and Secondary Name Nodes with neither requiring substantial storage capacity. However, the data nodes are the most powerful machines within the cluster with substantial RAM and processing capabilities. Following are the three primary deployment or configuration modes supported by Hadoop: A Job in the Hadoop ecosystem is analogous to a Python script/program that one can execute in order to perform a certain task(s). Just like a Python script, Hadoop‚Äôs job is a program(s), typically as a JAR file, that is submitted to the Hadoop cluster in order to be processed and executed on the input (raw) data that resides on the data nodes and the post-processed output is saved at a specified location. Now let‚Äôs move on from Hadoop‚Äôs physical infrastructure to its software components. The core software components of Hadoop are: MapReduce is further comprised of: Just to be clear, Hadoop is a parallel processing platform providing the hardware and software tools to allow parallel processing) that then makes available the MapReduce framework (i.e., a bare-bone skeleton that can be customized based on the user requirements) for parallel processing. But MapReduce is not the only framework supported by Hadoop ‚Äî Spark is another. HDFS is the file-management component of the Hadoop ecosystem that is responsible for storing and keeping track of large data sets (both structured and unstructured data) across the various data nodes. In order to understand the working of HDFS, let consider an input file of size 200MB. As explained earlier, in order to facilitate parallel processing on data nodes, this single file will be broken down into multiple blocks and saved on the data nodes. The default split size (that is a global setting and can be configured by the Hadoop administrator) in HDFS is 64MB. Therefore, our sample input file of 200MB will be split into 4 blocks ‚Äî where 3 blocks will be of 64MB and the 4th block will be 8MB. The splitting of the input file into individual blocks and saving them on specific data nodes is taken care of by HDFS. One critical aspect to take note of here is that the splitting of the input file by HDFS happens on the client machine that is outside the Hadoop cluster and the name node decides the placement of each data block into the specific data nodes, based on a specific algorithm. So the client machine directly writes the data blocks to the data nodes once the name node has provided it with the block placement strategy. The name node, acting as the Table of Contents of a book, remembers the placement of each data block within the various name nodes, together with other information, e.g., block size, hostname, etc., in a file table called the File System Image (FS Image). So what happens in case of failure of a data node? The failure of even one data node will result in the entire input file being corrupted ‚Äî since one piece of our puzzle has gone missing! In a typical production setup, where we are usually dealing with data blocks of hundreds of gigabytes, it is highly inefficient and time-consuming to push the original data file back into the Hadoop cluster. To avoid any potential data loss, backup copies of data blocks on each data node is kept on an adjacent data node. The number of backup copies to be made of each data block is controlled by the Replication Factor. By default, the replication factor is set at 3, i.e., every block of data on each data node is saved on 2 additional backup data nodes so that the Hadoop cluster will have 3 copies of each data block. This replication factor can be configured on a per-file basis at the time of pushing the source data file into HDFS. The backup data node will kick-in as soon as any data node fails to send a heartbeat signal to the name node. Once a backup data node is up and running, the name node will initiate another backup of the data block so that the replication factor of 3 holds throughout the cluster. In Hadoop 1.0, the Secondary Name Node acts as a backup of the Name Node‚Äôs FS image. However, and this was one of Hadoop 1.0‚Äôs primary limitations, the Secondary Name Node does not operate in a hot standby mode. Therefore, in the event of the Name Node‚Äôs failure, the entire Hadoop cluster will go down (data will be present in the Data Nodes, however, it will be inaccessible since the cluster has lost the FS image), and the contents of the Secondary Name Node need to be manually copied to the Name Nome. We will go over this later ‚Äî but this limation was addressed with the release of Hadoop 2.0, where the Secondary Name Node acts as a Hot Standby. Hadoop 2.0 is also sometimes known as MapReduce 2 (MR2) or Yet Another Resource Negotiator (YARN). Let‚Äôs try to understand the salient architectural differences between Hadoop 1.0 and Hadoop 2.0. Remember that, in Hadoop 1.0, the Job Tracker acts as a centralized job scheduler that splits up a specific job into multiple jobs before passing them on to individual data nodes ‚Äî where the individual tasks on the data nodes are monitored by the Task Tracker that then reports back the status to the Job Tracker. In addition to its job scheduling responsibilities, the Job Tracker also allocates the system resources to each data node in a static mode (that is, the system resources are not dynamic). Hadoop 2.0 replaces the Job Tracker with YARN while the underlying file system remaining as HDFS. In addition to MapReduce, YARN also supports other parallel processing frameworks, e.g., Spark. YARN can also support up to 10,000 data notes, compared to only 4,000 data nodes supported by Hadoop 1.0‚Äôs Job Tracker. YARN has 2 components: Scheduler and Applications Manager. Both these tasks were managed single-handedly by the Job Tracker in Hadoop 1.0. Separating these distinct responsibilities into YARN‚Äôs individual components allows better utilization of system resources. Further, the task trackers on each data node were replaced by a single Node Manager (that works in the slave mode) in Hadoop 2.0. Node Manager communicates directly with YARN‚Äôs Applications Manager for resource management. As alluded to earlier, in addition to a Secondary Name Node, Hadoop 2.0 also has a Hot Standby Name Node that seamlessly kicks-in in case of Name Node‚Äôs failure. The Secondary Name Node comes in handy in case of the failure of both the Name and the Hot Standby Name Node. As the name suggests, MapReduce is comprised of the following 2 stages with each stage having 3 further sub-stages: All 3 sub-stages of the Map stage are performed or acted upon in each of the data blocks residing in the individual data nodes ‚Äî this is where parallelization kicks-in within Hadoop. Record Reader The Record Reader is pre-programmed to process one line at a time from the input file and produces 2 outputs: Mapper Mapper is programmable to process each key-value pair output from the Record Reader one at a time based on any required logic or the problem statement. It outputs additional Key-Value pairs based on a user-defined function it was programmed to perform. Sorter The output from Mapper is fed into the Sorter that lexicographically sorts (obviously! üòä) the keys from the Mapper‚Äôs output. In case the keys are numeric, then the Sorter will perform a numerical sorting. The Sorter is pre-programmed and the only configuration possible is to implement sorting on values. At the end of the Map stage, we will have multiple Mapper outputs, one from each of the data nodes. All these outputs will be transferred to a separate, single data node where the Reduce operation will be implemented on them. The 3 sub-stages of the Reduce operation are: Merge Intermediary outputs from each Map operation are appended to one another to result in a single merged file. Shuffler Shuffler is another pre-programmed built-in module that aggregates together the duplicate keys as present in its input, resulting in a list of values for each unique key. Reducer Shuffler‚Äôs output is fed to the Reducer, which is the programmable module of the Reduce stage ‚Äî similar to the Mapper. Reducer produces output in key-value pairs based on what it is programmed to perform as per the problem statement. I will use a very simple, non-ML problem statement to try and explain the mechanics and the workflow of MapReduce. Consider an input file with just 2 statements as follows: Our task is to find the frequency of words in the input file, the expected output being: Going through the MapReduce stages explained above: Certain industry use case of MapReduce include: Right, so this was a very high-level and non-technical introduction to the world of Hadoop and MapReduce. Obviously, there are several other Hadoop components that I have not even touched upon here, e.g., Hive, Zookeeper, Pig, HBase, Spark, etc. Please feel free to reach out to me if you want to discuss the above content, or that in any of my previous posts, or anything in general related to data analytics, machine learning, and financial risk. Till next time, rock on!",40,0,11,Towards Data Science,2021-01-04,2021
https://towardsdatascience.com/understanding-pytorch-loss-functions-the-maths-and-algorithms-part-1-6e439b27117e,Understanding PyTorch Loss Functions: The Maths and Algorithms (Part¬†1),A step-by-step guide to¬†the‚Ä¶,5,18,"['Understanding PyTorch Loss Functions: The Maths and Algorithms (Part 1)', 'Introduction', 'Regression Loss Functions', 'Classfication Loss Functions', 'Conclusion']","You can find part 2 here. Just like humans, a machine learns from its past mistakes. These ‚Äúmistakes‚Äù are formally termed as losses and are computed by a function (ie. loss function). If the prediction of a machine learning algorithm is further from the ground truth, then the loss function will appear to be large, and vice versa. Thus, the objective of any learning process would be to minimize such losses so that the resulting output would closely match the real-world labels. This post will walk through the mathematical definition and algorithm of some of the more popular loss functions and their implementations in PyTorch. Choosing the best loss function is a design decision that is contingent upon our computational constraints (eg. speed and space), presence of significant outliers in datasets, and the types of inputs/outputs we have. Nonetheless, one of the first questions that we need to ask when choosing between different loss functions would be the kind of data we are presented with. Generally, loss functions can be neatly grouped based on the specific tasks that we are dealing with: either a regression or classification problem. Regression deals with continuous set of data, such as when predicting the GDP per capita of a country given its rate of population growth, urbanization, historical GDP trends, etc. On the other hand, classification problem concerns with finite, discrete categories, such as predicting whether a satellite imagery is experiencing any rainy event or not. Mean Absolute Error (MAE) sums up the absolute difference between the truth (y_i) and its corresponding prediction (y_hat_i), divided by the total number of such pairs. Algorithms: MAE PyTorch Implementation: MAE Similar to MAE, Mean Squared Error (MSE) sums up the squared (pairwise) difference between the truth (y_i) and prediction (y_hat_i), divided by the number of such pairs. MSE generally penalizes prediction that is far away from the truth by applying a squared operator, but is less robust to outliers since it tends to exaggerate such observations. Algorithms: MSE PyTorch Implementation: MSE If you only have two labels (eg. True or False, Cat or Dog, etc) then Binary Cross Entropy (BCE) is the most appropriate loss function. Notice in the mathematical definition above that when the actual label is 1 (y(i) = 1), the second half of the function disappears. In the case where the actual label is 0 (y(i) = 0), the first half of the equation is dropped. In short, we are just multiplying the log of the actual predicted probability for the ground truth class. This idea is useful when we generalize BCE to handle more than 2 classes in Categorical Cross Entropy (discussed in the last section). Algorithms: BCE PyTorch Implementation: BCE The Categorical Cross Entropy (CCE) loss function can be used for tasks with more than two classes such as the classification between Dog, Cat, Tiger, etc. The formula above looks daunting, but CCE is essentially the generalization of BCE with the additional summation term over all classes, J. Algorithms: CCE PyTorch Implementation: CCE That‚Äôs it for our introduction to PyTorch‚Äôs more popular loss functions, their mathematical definitions, algorithm implementations, and PyTorch‚Äôs API hands-on. The next part of this series will deal more with other less popular albeit useful loss functions. Do subscribe to my Email newsletter: https://tinyurl.com/2npw2fnz where I regularly summarize AI research papers in plain English and beautiful visualization.",127,1,5,Towards Data Science,2021-01-04,2021
https://towardsdatascience.com/kubeflow-not-yet-ready-for-production-9037ce2f3d35,Kubeflow: Not Yet Ready for Production?,Why we abandoned Kubeflow for our machine learning reference‚Ä¶,9,29,"['Kubeflow: Not Yet Ready for Production?', 'Kubeflow: The good parts', 'Kubeflow: The shortcomings', 'Problems with the initial installation', 'Problems with integrating components', 'Problems with documentation', 'The future of Kubeflow?', 'Picking and choosing Kubeflow components?', 'Have you used Kubeflow in production settings?']","We‚Äôre building a reference machine learning architecture: a free set of documents and scripts to combine our chosen open source tools into a reusable machine learning architecture that we can apply to most problems. Kubeflow ‚Äî a machine learning platform built on Kubernetes, and which has many of the same goals ‚Äî seemed like a great fit for our project in the beginning. We tried it for several weeks, but after facing several challenges, we‚Äôve now decided to drop it completely. This article describes our Kubeflow experience. Our goal is to help others see ‚Äî earlier than we did ‚Äî that Kubeflow might not be everything it claims to be quite yet. To be clear: Kubeflow has some shortcomings that prevented us from relying on it for this project. That said, we still respect Kubeflow‚Äôs goals, and we hope that as the project matures and addresses some of these issues, we can revisit the idea of using it in the future. We‚Äôll start by looking at why we were drawn to Kubeflow in the first place. All production machine learning projects consist of many components, which can be broadly divided into three categories: If we look at the components for machine learning projects as described by Andreessen Horowitz, Kubeflow can be used in across all three categories ‚Äî at least in theory. Having one cohesive tool to perform these different tasks is definitely attractive on paper. Since we were already set on using Kubernetes and wanted to use only open source tools, Kubeflow seemed like it would be a great fit. Unfortunately, Kubeflow turned out to be finicky to set up, unreliable, and difficult to configure. It also relied on many outdated components and libraries. Finally, a lot of the documentation was broken or out of date, and we weren‚Äôt able to integrate it nicely with AWS and GitHub without relying on some hacky workarounds. We go into each of these issues in detail below. Sign up to our Newsletter: Get our next article on what we used instead of Kubeflow straight to your inbox. Even before we adopted Kubeflow, we already knew there‚Äôd be a steep learning curve. But we had plenty of Kubernetes experience on the team, and we figured we‚Äôd be able to get an initial installation up and running fairly quickly. Days after our initial attempt, we were still struggling with bugs related to KFDef and Kustomize manifests. The manifests provided failed many times with no clear error messages, so we had to check every install component manually and try to figure out which ones were broken. Our goal was to integrate Kubeflow with GitHub authentication, but the manifest provided for AWS and OpenID Connect (OIDC) also contained a bug involving the Kubeflow ingress point. We had to update the ingress manually to use the required OIDC information in order to resolve this. Overall, while Kubeflow runs on top of Kubernetes and is meant to be cloud-agnostic, but we ran into many issues running Kubeflow on AWS. It‚Äôs likely this process would have been smoother if we‚Äôd gone with GCP instead. Because Kubeflow is built by Google, it often defaults to GCP and doesn‚Äôt play nicely with other cloud providers yet ‚Äî especially when it comes to authentication and permissions management. Kubeflow consists of many different loosely coupled components. This loose coupling is nice because it theoretically allows us to choose which components to use. But it comes with disadvantages too. Different components rely on different versions of the same dependencies, which causes more trouble. During our test runs, we discovered that upgrading one component would often break a different one. For example, upgrading the KFServing component required upgrading Istio ‚Äî the mesh service platform that Kubernetes services use to share data with each other. This upgrade broke access to the dashboard because the newer Istio version was incompatible with AWS authentication. The result was a set of incompatible versions, and the only way to recover was to reinstall Kubeflow all over again. We also had to create our pipelines directly from notebooks, but even after using some hacky workarounds, this turned out to be impossible ‚Äî there were still unresolved issues with Kubeflow. As one AWS engineer said on GitHub, ‚Äúin-cluster communication from notebooks to Kubeflow Pipeline is not supported in this phase.‚Äù Many of the documentation pages are labeled ‚Äúout of date,‚Äù including those for significant Kubeflow components such as Jupyter Notebooks. Even worse, some of the documentation pages that were also written for older versions of Kubeflow aren‚Äôt labeled ‚Äúout of date.‚Äù So it was hard to know when to trust the documentation as we were debugging ‚Äî trying to figure out when we‚Äôd done something wrong versus when the problem was outdated documentation. This slowed everything down. Many of the links in the documentation also return ‚Äúpage not found‚Äù or ‚Äúthis page does not yet exist‚Äù errors, which made the experience frustrating overall. In October, an article titled ‚ÄúIs Kubeflow Dead?‚Äù noted that Kubeflow development seemed to be slowing down, with some of the lead engineers abandoning ship to take up roles at other companies. As this article observes, part of the reason for the perceived slowdown is that development is moving out of the main repository and into sub-repositories. That said, we also found many of our own concerns and experiences mirrored by others in the community. Luke Marsden says: ‚ÄúI‚Äôm having a tough time with Kubeflow 1.1 and IMO it‚Äôs really lacking a focus on end user experience, which is way harder than it needs to be.‚Äù And Clive Clox says: ‚ÄúKubeflow is an ecosystem and some projects are more used than others. I think they are finding it challenging to bring everything into a cohesive whole.‚Äù Kubeflow Pipelines is Kubeflow‚Äôs main focus, and it would be possible to use only this component without the others. But even when we tried to use smaller pieces, we ran into issues ‚Äî it‚Äôs not always clear which components are necessary and which are optional. Given the state of the documentation, it‚Äôs a time-consuming and error-prone process to figure this out by trial and error. That‚Äôs why we‚Äôve decided not to integrate any Kubeflow components for now. We haven‚Äôt decided exactly what we‚Äôll replace it with yet. Since Kubeflow has such broad goals, we‚Äôll probably need to use several different tools in its place. It‚Äôs likely we‚Äôll use Prefect as a workflow tool, and we‚Äôll write another article on our experiences with that. If you‚Äôve had similar or contradictory experiences with Kubeflow, we‚Äôd love to hear about them ‚Äî or any other feedback you have on this article.",348,10,6,Towards Data Science,2021-01-04,2021
https://towardsdatascience.com/how-to-be-a-data-analyst-data-viz-with-google-data-studio-5cda4ad475f2,How to be a Data Analyst‚Ää‚Äî‚ÄäData Viz with Google Data¬†Studio,A Demo Report on Customer Churn by a¬†Data‚Ä¶,8,53,"['How to be a Data Analyst ‚Äî Data Viz with Google Data Studio', 'We will discuss‚Ä¶', 'About Google Data Studio', 'Why Data Analysts Need to Know Data Viz', 'The Process of (Diagnostic) Analytics', 'The Anatomy of an Effective Report', 'Now it‚Äôs your turn!', 'Closing']","Are you an aspiring data analyst or data scientist looking to build up your portfolio and visualization skills? Or are you a data analyst who wants to improve your visualization skills and business intelligence capabilities? Or are you frustrated with the high cost of Tableau and is looking for a free alternative? Today, I will show you my thinking process of creating data visualization using the (free!) Google Data Studio, and how you can create your own report for your portfolio ‚Äî all without needing to code. I will be using a demo report that I created on analyzing customer churn to run through this process. Google Data Studio lets you tell your story in data. In particular, it is a handy tool that allows you to ‚Äî Not only that, it has the following awesome features. There are a wide variety of data visualization or business intelligence software that one can explore. These software include Tableau, Qlikview, FusionCharts, Highcharts, Datawrapper, Plotly and Sisense. Each of these has its own merits but is generally expensive. If you would like to get your hands dirty using a free software, then Google Data Studio is right up your alley. Note, however, that the software you use at work would be dependent on the tools available. Note, google data studio offers awesome (free!) tutorials via its Analytics Academy. As an analyst in a tech firm, my responsibility is to perform different types of analytics and communicate insights. The four types of analytics include ‚Äî Arguably, all of these types of analytics involve some form of data visualization. Particularly, effective descriptive and diagnostic analytics are used by almost all stakeholders in a data-driven company and involve some heavy-lifting in data visualization. These two forms of analytics can be performed easily on business intelligence or data visualization software, which allows analysts to create visualizations and stakeholders to access them. As such, it is extremely helpful if a data analyst is well-versed in such software and can efficiently create visualizations that communicate insights. The report I will be sharing today is one of diagnostics analytics. 1. Understand the problem. An understanding of the business problem is important because it allows a data analyst in providing an analysis that is in line with the business needs. Executing the request without an understanding of the problem is a futile exercise at best. This report is created for the following mock scenario. You are a data analyst at AT&B Telecommunications company. Recently, the business team identified a spike in the customer churn. However, it is unsure of the characteristics of these customers. Your role is to inform the business team which customers are likely to churn. 2. Plan the report. Creating a mock-up of the report that you are about to create can speed up the analytics process tremendously. Having a plan will reduce distraction and help you to focus on the task on hand. In this step, some questions you might want to ask yourself include: Applying this to our case on hand, we know that the most important metric that we have to explore is ‚Äòcustomer churn‚Äô. Some of the hypotheses that can explain customer churn include: 3. Create a mock-up After answering these questions, we can move on to create a mock-up of the report. This will help the analyst in visualizing the data required in step 4. In this case, I have created the following mock-up. As you might notice, this is not the neatest nor the most aesthetically appealing ‚Äî and that‚Äôs perfectly fine! It‚Äôs just a draft, after all. 4. Extracting and cleaning the data Now that we have the hypotheses in mind, we can extract the relevant data that we would like to explore. In the typical case, one would use SQL to extract the relevant data. The data set used in this dashboard is available on Kaggle. It is a data set of a Telecommunications company of 7,043 customers. It contains the following features: In this case, the data set is clean. We can now move on to put our plan in action ‚Äî using Google Data Studio! A report should include the problem statement that you are solving and the findings. This is outlined in Page 1 of the report. In order to help your stakeholders understand the report, proper documentation on the source of the data and the explanation of the data should be included. This is in Page 2 of the report. The findings are the most important part of the report. A finding section contains a heading, a visualization, an analysis and possible actions. This is in page 3 to 8 of the sample report, and can be seen as follows: That‚Äôs quite a lot to unpack. Let‚Äôs break the findings section down into its anatomy. The Summary Not all stakeholders have the time to digest all the visualizations created. Therefore, the header of each finding should be descriptive and summarize the finding from the graph. The Visualization An appropriate visualization should be made to support the finding. The visualization should also be clutter-free to not distract the reader from the message. The Analysis At times, it is helpful to describe the graph to guide the reader in interpreting the graph. Whenever possible and appropriate, the analyst should also establish the statistical significance of the finding to increase one‚Äôs confidence in the finding. The Takeaway The analyst can also attempt to make further hypothesis on the finding and possible further explorations. This portion can spark an interesting discussion with relevant stakeholders and initiatives to address the problem at hand. I have deliberately left some analysis untouched so you can get a chance to practice. Do leave comments down below on what kind of analysis that you would include in this report. If you would like to learn more, Data visualization and reporting is an essential skill set of a data analyst and a data scientist. It will be a bonus if you can build a report of your own and include it in your resume if you are looking for a role as an analyst or a data scientist. Enjoyed this post? You might like this ‚Äî towardsdatascience.com towardsdatascience.com Feel free to connect with me on LinkedIn too. I will be happy to receive feedback, answer questions or just connect with like-minded individuals. www.linkedin.com",164,4,7,Towards Data Science,2021-01-04,2021
https://towardsdatascience.com/exploratory-spatial-data-analysis-esda-spatial-autocorrelation-7592edcbef9a,Exploratory Spatial Data Analysis (ESDA)‚Ää‚Äî‚ÄäSpatial Autocorrelation,Explores Toronto neighborhoods‚Ä¶,7,30,"['Exploratory Spatial Data Analysis (ESDA) ‚Äî Spatial Autocorrelation', 'Spatial Autocorrelation', 'Load Dataset', 'Spatial Weights', 'Global Spatial Autocorrelation', 'Local Spatial Autocorrelation', 'Closing Notes']","In exploratory data analysis (EDA), we often calculate correlation coefficients and present the result in a heatmap. Correlation coefficient measures the statistical relationship between two variables. The correlation value represents how the change in one parameter would impact the other, e.g. quantity of purchase vs price. Correlation analysis is a very important concept in the field of predictive analytics before building the model. But how do we measure statistical relationship in a spatial dataset with geo locations? The conventional EDA and correlation analysis ignores the location features and treats geo coordinates similar to other regular features. Exploratory Spatial Data Analysis (ESDA) becomes very useful in the analysis of location-based data. ESDA is intended to complement geovizualization through formal statistical tests for spatial clustering, and Spatial Autocorrelation is one of the important goals of those tests. Spatial autocorrelation measures the correlation of a variable across space i.e. relationships to neighbors on a graph. Values can be The two most commonly-used measures of spatial autocorrelation are spatial similarity and attribute similarity. Spatial similarity is a representation of the spatial structure of a dataset by quantifying (in spatial weights) the relative strength of a relationship between pairs of locations. Neighborhood relations are defined as either rook case, bishop case or queen (king) case ‚Äî see the figure below. These are rather simple and intuitive as the names suggest. You can read Contiguity-Based Spatial Weights for more in-depth explanation of spatial weights. On the other hand, attribute similarity, measured by spatial lags, is obtained for each attribute by comparing the attribute value and its neighbors. The spatial lag normalizes the rows and takes the average value in each weighted neighborhood. We use Toronto Airbnb listings in 2020/12 as our main dataset. We also grab Toronto neighborhoods Polygon dataset so that we can map our spatial listing data points to different areas. Our goal is to investigate if there is any spatial correlation in Airbnb average listing price between neighborhoods, i.e. hot area, cold area, or random. The following Python libraries are used for manipulating the geo data: Examples of Airbnb listings Airbnb data needs to be converted to GeoDataFrame. We need to define the CRS (Coordinate Reference Systems) with the dataset. We use EPSG:4326, which defines latitude and longitude coordinates on the standard (WGS84) reference ellipsoid. In total there are 140 neighborhoods in Toronto. Coordinates in the geometry column defines multi-polygon boundaries of the neighborhood. Here are some examples: Next, we want to find out the average price per night for each neighborhood. Given the latitude/longitude coordinate of the listings and polygon boundaries of the neighborhood, we can easily join the two datasets using the sjoin function provided in GeoPanda. Here we use op='contains' in sjoin, meaning we want the records from the left DataFrame (i.e. neighborhood data) that spatially contain geo records from the right DataFrame (i.e. listing data). In the result dataset, column index_right is the index from the listing records and price is the corresponding list price. We take the average of listing prices per neighborhood, and here is the aggregated dataset: We create an interactive Choropleth map using Folium. You can see the average listing price when hover over the neighborhood. Click here to explore the interactive map. So far so good. Although we can visually see which neighborhoods have listings with higher or lower price, it is not quite obvious to tell if there is any spatial pattern and what the patterns are, given the irregular polygons of different sizes and shapes. Spatial autocorrelation is a good solution for answering the question. As the aforementioned, to investigate spatial autocorrelation we need to compute spatial weights. We use PySal library for the calculation. Here we use the Queen contiguity for spatial weights. Global spatial autocorrelation determines the overall clustering in the dataset. If the spatial distribution of the listing price was random, then we should not see any clustering of similar values on the map. One of the statistics used to evaluate global spatial autocorrelation is Moran‚Äôs I statistics. Moran‚Äôs I values range from -1 to 1, and 1 indicates a strong spatial autocorrelation. In our example, we have a Moran‚Äôs I value 0.23 and p-value of 0.001 which is considered statistically highly significant. Therefore, we would reject the null hypothesis of global spatial randomness and in favor of spatial autocorrelation in listing prices. We can also visualize this on a Moran‚Äôs I plot. The global spatial autocorrelation analysis is great for telling if there is a positive spatial autocorrelation between the listing price and their neighborhoods. But it does not show where the clusters are. The Local Indicators of Spatial Association (LISA) is intended to detect the spatial clusters and put them in 4 categories: We calculate the Moran Local score. Plot it using Moran‚Äôs scatterplot. Now let‚Äôs plot the LISA results in a map using the lisa_cluster function. Neighborhoods in red are areas that have high prices with high prices in nearby neighborhoods (HH). Blue colors show areas that have low prices surrounded by low prices (LL). It is also interesting to notice the yellow and light blue colors show the areas that have big price differences from their neighborhoods (HL and LH). You can use the interactive neighborhood price map above to verify the LISA categorization. After some further investigation, it seems there is a listing in the Rustic neighborhood (the red block surrounded by gray) with a price $2,000, much higher than the average of the area $60. It could be a data issue or a different type of property. Using the spatial autocorrelation analysis, we analyze the global and local spatial autocorrelation of Toronto Airbnb prices in relation to their nearby neighborhoods. A LISA analysis is very useful to identify statistically significant clusters of high values and detect outliers. It can help in many geo-based data analysis, including social, economic, epidemiological studies. All codes can be found on Google Colab. Happy Machine Learning! Originally published at https://ai-journey.com on January 4, 2021. Thanks for reading. If you have any feedback, please feel to reach out by commenting below, contacting me on my blog https://ai-journey.com/ or messaging me on LinkedIn.",84,0,7,Towards Data Science,2021-01-04,2021
https://towardsdatascience.com/deploy-code-with-streamlit-90090b611f3c,Deploy Code With Streamlit,Why Streamlit and a few tips on deploying a data dashboard app from¬†Python.,8,32,"['Deploy Code With Streamlit', 'The Valley of Death', 'Deploy Large Data With Compression', 'Pickle Protocol 5 ValueError', 'Unsupported Locale Setting Error', 'KeyError with OS and PWD', 'Conclusion', 'Resources']","I recently built and deployed a data application from my Mac using Python and did not write one line of HTML, CSS, or JavaScript ‚Äî all made possible with a nifty package called Streamlit. I recently deployed an early alpha version of a data dashboard with Steamlit. Although the app is in its infancy, I‚Äôm happy to have it in some kind of production state and I am excited to continue improving on it. Based on my experience, this article contains some of the biggest gotchas and tips on breaking through a few final issues that you may encounter with a Streamlit deployment. This article is not a full guide, for that, just check out the Streamlit site which does a good job at the step-by-step process. For the longest time, I‚Äôve been wondering about the best way to deploy my data analysis. For example, we do all this work and nug away at code, but there always seems to be a gap. It is almost as if there is a valley where projects go to die before we can share them. There is some valley where projects go to die before we can share them. Possible solutions include Jupyter notebooks and Deepnote which are great for sharing code and analysis. Another one is ObservableHQ which is pretty good at pushing D3 visuals out the door. There are dozens of other ways to get the job done as well. However, despite the options, I‚Äôve never been able to connect with a deployment method with just the right features. Fortunately, my mentor gave me some advice to try a combination of Plotly and Streamlit. Although I knew about Plotly, I never heard of Streamlit before. Billed as the ‚Äúfastest way to build and share data apps,‚Äù Streamlit makes the promise of turning your code into a web app within minutes with zero work on the front-end. I thought it was too good to be true, but as it turns out, Streamlit works. If this all sounds interesting, check out the Streamlit page. The documentation and instructions are pretty clear; however, there are a few important design features that I think are worth considering in your code. Read on for more. Sometimes you have a ton of data, 500 thousand or 1 million rows of data ‚Äî how best to deploy a data app that points to such a large dataset? Although there are plenty of answers, to include avoiding having a single massive table in the first place, let us proceed with solving this specific problem. Now what? There are three main problems with handling a single, large data table. First, on disk, a file of this size may range in the hundreds of MBs. Second, if deploying with Streamlit, you have to use GitHub which means a hard limit of about 50 MBs. Third and lastly, loading a large dataset can crash the app. As a result, consider compressing DataFrames to compressed BZ2 pickle files instead of CSV files. Big Data, Big Problems: 245 MB .csv file or 20 MB .bz2 file? Inserted on 13 January 2020 ‚Äî although this compression method works for large data as a means to get to production, as a next step, you should certainly take additional steps to reduce the size of each file as much as possible. More about that and optimization in a follow-up article linked below. towardsdatascience.com To handle large DataFrames, after reading the initial csv file, design your workflow to read and write pickled data with BZ2 compression. The BZ2-Pickle combination does good for you in two big ways. First, the pickled format retains the state of all your columns and takes up less disk space than CSV files. Second, BZ2 efficiently compresses data to tiny bytes along with pretty great decompression on the back end. Fortunately, Pandas makes this an easy task. All you need is to have ‚Äò.bz2‚Äô as the file extension. Like most projects, the workflow starts with data in CSV format; however, internal to the app, I designed the program to read and write DataFrames in pickle formats. A major reason to use pickled DataFrames is that the serialized file retains various metadata ‚Äî if you set dtypes to strings, integers, or category, those dtypes are retained every time you read in the data. On the other hand, with CSV files, you have to re-process the data back to a suitable DataFrame. Although pickle files are pretty sweet, during the last few steps of deployment with Streamlit, I ran into a brick wall of an error with the pickle protocol. Apparently, the Pandas to_pickle() method defaults to a protocol of version 5 which is not universally supported. As a result, although a standard Pandas pickled DataFrame may work in testing on your local machine, deployment to a server is another story. When faced with an error, I sometimes go with an alternate solution, i.e., something works just as well to produce the same result. However, I had little choice with the data file because of various constraints. While searching for an answer, I discovered that the solution is fairly simple ‚Äî change the Pandas to_pickle() protocol from default to version 2. When Pandas pickle is combined with BZ2 compression, the result is a super small, super convenient, and very compatible data file. When Pandas pickle is combined with BZ2 compression, the result is a super small, super convenient, and very compatible data file. In addition to visualizations, the app presents the user with a narrative of the dataset‚Äôs high-level numbers. For example, there are a few sentences that explain the counts and frequencies in a written format. To keep the reader in the flow of the narrative, I wanted to format numbers as strings with thousands separators instead of pure integers. One way to format numbers as text is with the Locale library. Unfortunately, while this method works just fine on my local machine, the deployment drops dead with an unsupported locale setting error. For whatever reason, locale throws a massive wrench into the deployment. Although there may be some other option to resolve locale settings, I realized there may be a better alternative. The solution is always f string, a fantastic and nifty invention that will help you when you need help the most. The f string is a fantastic invention. Although the syntax may seem intimidating (it takes some getting used to), it works. To return an integer formatted as a neat number with thousands separators try the following syntax. Managing file paths and directories are a pain. For example, the syntax might change depending on whether you are a Mac, a PC, in a virtual environment, or some other wild combination. To deal with the uncertainty and like many others, I developed a habit of coding file path separators with the OS library to ensure compatibility across different environments. For whatever reason, although the app works on my local machine, the online Streamlit deployment returns a KeyError when I try to access ‚ÄòPWD‚Äô or print working directory. Although I am not sure why os.environ[‚ÄòPWD‚Äô] fails to work, I ended up changing the approach to rely on relative paths. If Streamlit runs from the project root, i.e. main.py or app.py, then consider simply referencing sub-folders with a relative path instead. Consider the following set-up where streamlit runs app.py (from the root directory). In this scenario, if the app needs to read a file from the data sub-folder, work with os.sep.join() with relative location in mind. While testing this solution, I noticed that this works just fine both on my local machine and in the deployment. With Streamlit, it is possible to deploy a Python-based project as a web app with little to no front-end work. Although the process takes longer than just a few minutes, it is doable within a few hours or a day of work. I am sharing this article to pass along the word about Streamlit and to share a few tips on working through some issues. As for the issues, I wish I had designed around these roadblocks from the start. Although each issue was fairly minor, each one prevented the app from deploying and required a significant amount of time to research and refactor. Hopefully, these notes on my mistakes can benefit your next project. Thanks for reading. If you are considering deploying with Streamlit or are troubleshooting, I hope this article helps.",263,0,9,Towards Data Science,2021-01-04,2021
https://towardsdatascience.com/sql-performance-tips-2-128a31e8ecb,SQL Performance Tips¬†#2,Avoiding running on the heap and CTEs vs Temporary Tables,4,17,"['SQL Performance Tips #2', 'Replacing CTEs with indexed temporary tables', 'Avoiding running queries on the heap', 'To sum up']","This post is the second in a series of small groups of two tips on improving the performance of SQL queries, in addition to useful concepts usually overlooked in the modern SQL world: Similarly to previous iterations, an introductory note is required: I am by no means an SQL expert, feedback and suggestions are more than welcome. CTEs can be a major help in most queries, simplifying the development process and its maintenance, providing a single source of truth. However, their impact drastically depends on the SQL engine. For instance, PostgreSQL 12+ automatically materializes (allocates the output into memory) CTEs which are called more than once. Microsoft SQL Server is known to not materialize CTEs but, instead, running it as a view each time it is called, which can even result in different outputs per call ‚Äî depending on the serialization level. But even if the engine materializes the CTE like PostgreSQL, what about its indexes? Well, typically the engine will make use of the underlying source table‚Äôs indexes. CTEs will be perfect for many situations, however sometimes, you look at the query optimizer and realize a full table scan is being made, simply because the query is not relying on the underlying existing indexes. Or worse, Microsoft SQL Server, for instance, tends to have concurrency races when a CTE is being called on itself, often resulting in deadlocks. Another useful way of recognizing the potential benefit of a temporary table is to use the SQL engine‚Äôs statistics to spot excessive I/O operations visible through the number of logical reads and similar statistics. Hint: the fewer reads, the better. Consider the Stack Overflow database creating a query to: The CTE‚Äôs execution plan‚Äôs highlights are: The temporary table‚Äôs execution plan‚Äôs highlights are: So when to choose CTEs vs Temporary Tables? CTEs tend to be a suitable choice when: Temporary tables on the other hand may be a good choice when: Note: This section has been heavily retrieved from Brent Ozar‚Äôs blog. Make sure to follow it as he understands SQL Server like no other. Link below. www.brentozar.com This is one of the SQL 101 lessons: never, under any circumstances, run queries on (clustered) unindexed tables unless you are forced to do so. Even if they are temporary tables, make sure to add at least a primary key, ideally, a composite key including the queryable columns. You can further boost up your performance (although this does come at its own cost) by specifying a non-clustered key on your filterable columns if you are not able to do so using a primary key. Running large operations, whether a filter or a join, on an unindexed table, will reflect in a full table scan, which is exactly what it sounds, scanning the entire table, for each row and operation. Sounds inefficient? That‚Äôs because it is. But wait, I would never ever have a table without indexes, and certainly not run a query on it! It‚Äôs actually not that simple. Like I mentioned in the first paragraph if your query does not make use of the existing indexes (e.g. using a column in a where predicate where the column is not indexed), then a full-table scan will be performed. This is where the execution plan can become your best friend, hinting at cases where a full-table scan is being performed, which signals the potential need for additional or adapted indexes.",37,0,5,Towards Data Science,2021-01-04,2021
https://towardsdatascience.com/static-vs-dynamic-in-application-security-testing-36687a0c55c5,Static vs Dynamic in Application Security¬†Testing,Explore the key differences between SAST and¬†DAST,5,25,"['Static vs Dynamic in Application Security Testing', 'Static Application Security Testing', 'Dynamic Application Security Testing', 'Conclusion', 'References']","I have covered an article previously on Weighing the Pros and Cons of Static Application Security Testing. My previous article covers the Top 10 most critical web application and API security risks faced by the developers all over the world based on the report produced by OWASP: a non-profit organization that aims to educate people on security vulnerabilities. In this tutorial, we are going to explore and compare the differences between Static Application Security Testing (SAST) and Dynamic Application Security Testing (DAST). Static Application Security Testing (SAST) is one of the method for reducing the security vulnerabilities in your application. Another method is Dynamic Application Security Testing (DAST), which secures your application. Let‚Äôs have a look at the differences between both methods. SAST is a form of white-box security testing which has full access to the underlying source code and binary. It will test your program via an inside-out approach. Specialized SAST software such as GitLab, Klockwork or AppThreat will scan your source code automatically either during the coding process, or after you have committed the code to your pipeline. For example, for Klockwork‚Äôs users, once you have established a link between your project and Klockwork Desktop, it allows you to code your program normally using any IDE of your choice as long as it is open in the background. Each time you save the file, Klockwork Destop will update the code automatically and perform a scan on the spot. If it detects any security issues, it will display them on the user interface. SAST is typically conducted at the early stage of the system development life cycle, usually during or after the development stage. This allows you to identify any form of security vulnerabilities before going into the testing or quality assurance phase. It is a lot easier to fix problems when they are discovered early. Besides, most SAST executions will flag lines of code with the vulnerabilities. This can be extremely useful and serve as pointers to developers when fixing vulnerabilities. It also costs less to maintain and develop the project. SAST methodology is prone to high number of false positives compared to DAST. As a result, a situation might arise where developers have wasted valuable time and resources fixing imaginary problems in their system. Such a downfall can be costly in the long term if there are hundreds or thousands of false positives. SAST is language-dependent in terms of analyzing the underlying source code and binary. Most SAST tools specialize only in a few computer languages. You will not be able to find a one-size-fits-all SAST tool for every programming language used in your projects. As a result, scaling and maintaining a project build with different computer languages will be an enormous task. SAST is not capable of detecting any form of run-time vulnerabilities as it only scans the static code and binary. If you have incorrectly configured your system, a SAST tool will not be able to identify run-time issues leading to a false sense of security among the developers. On the other hand, DAST is termed dynamic because it does not have any access to the underlying static code or binary. Tests are conducted from the outside-in. You can think of it as a hacker trying to test the security vulnerabilities of your system. Unlike SAST, it analyzes the running application without any knowledge of the technology used to develop the system. Once it detects any potential vulnerabilities and risks, it logs the issues for the developers. The following example illustrates how Gitlab made a comparison on the vulnerabilities between the source and target branches. The information will be displayed on each merge request. DAST is usually conducted at the end of the system development life cycle. It typically takes place during the testing phase, right before the user acceptance tests. It serves as the final gateway to secure your application before deploying it out to your users. Most of the time, issues that are detected under DAST will not be fixed immediately unless they are deemed critical. This is mainly due to lack of time available before the UAT or deployment phase. So, most of the issues are pushed into the next development cycle. Since tests are conducted directly on running applications, DAST is not tied to any computer languages. It is a lot easier to scale and maintain your test since it is independent of the programming languages used in the development of your system. However, most DAST tools only support specific types of applications, like web applications or web services. You might need a few different DAST tools if your project consists of web applications, desktop applications and mobile applications. For example, let‚Äôs have a look at the DAST report produced by Gitlab. It highlights the Anti CSRF Tokens analysis performed on a web application. Behind the hood, it uses an open-source web app scanner called OWASP ZAP (Zap Attack Proxy) for scanning your running application. One main advantage of DAST over SAST is that it is capable of finding run-time vulnerabilities. This includes configuration issues, authentication issues and system memory issues. You will be able to identify many more issues from the user perspective. Let‚Äôs recap what we have learned today. We started off with a brief explanation of the Top 10 security vulnerabilities reported by OWASP. Then, we moved on to explore the key differences between Static Application Security Testing (SAST) and Dynamic Application Security Testing (DAST). We learned that SAST is a form of white-box testing while DAST is a form of black-box testing methodology. While SAST is usually done at the early stage of system development life cycle and is language-dependent, DAST is not tied to any computer languages and is usually conducted at the end of the cycle. Thanks for reading this piece. Hope to see you again in the next article!",6,1,5,Towards Data Science,2021-01-04,2021
https://towardsdatascience.com/fasten-python-objects-using-zip-88a1e7a68c7,‚ÄúFasten‚Äù Python Objects Using¬†Zip,A Python trick makes the usage of multiple iterables easier,5,30,"['‚ÄúFasten‚Äù Python Objects Using Zip', 'Basic Usage', 'Zip with Unpacking', 'Zip Longest To Overcome The Bucket Effect', 'Summary']","Like most of the other programming languages, Python has many common iterables such as list (usually called array in other languages), set and dictionary. However, there is such a ‚ÄúPythonic‚Äù iterable that is not quite common, which is ‚Äúzip‚Äù. This ‚Äúzip‚Äù is not for compressing, but should be understood as its shallow meaning: fasten the objects in two or more iterables (such as lists). In this article, I‚Äôm going to introduce To begin with, let‚Äôs create two lists with some objects. Please be noticed that any iterables will work with zip, so does the list. Here is the most basic usage of zip, which is to fasten the two lists together. Suppose that the two lists are the two metal strips of the zip, each object in them then will be the ‚Äúzipper tooth‚Äù. After we ‚Äúpull‚Äù the slide along with them, the ‚Äúteeth‚Äù are locked together correspondingly. Then, each ‚Äúrow‚Äù will have the two objects from both lists. Note: The number ‚Äú33‚Äù is an integer while the other elements are strings in the values list. Therefore, the Python zip does not care about the types of elements in the iterables, which is very flexible! Rather than loop and print the values, we can easily put the results into a list as follows. In fact, the most convenient usage of this is to generate a dictionary on top of the zipped lists. Sometimes, it is not necessary to put the results into another iterables. If we want to use the zipped results without the annoying tuples, we can simply get both the values out of the zip as follows. What if we put a Python dictionary into a zip? The values of the dictionary will be simply ignored and only the keys will be considered as the ‚Äúmetal strip‚Äù. Let‚Äôs build a dictionary, as well as an extra list just for the experiment. Then, let‚Äôs give it a try. Exactly like what I mentioned, only the keys are taken and the values are ignored. To solve the problem, simply pass the keys and the values of the dictionary separately into the zip. It also can be seen that zip supports more than two iterables. Unpacking is another unique concept in Python. If you‚Äôre not quite familiar with it, you might still have seen the asterisk sign * in some Python code. This is the ‚Äúunpacking‚Äù. Let me explain this using an example. Suppose we have such a two-dimensional list, which may also be treated as a matrix conceptually. We all know that the matrix is a list with 3 elements, and every element is also a list with 3 digits. Now, suppose we want to zip the 3 ‚Äúsub-lists‚Äù and use the digits at the corresponding position, zip with unpacking is the best solution. It is actually not difficult to be understood. The unpacking simply takes the 3 ‚Äúsub-lists‚Äù out of their parent list. I have an idea that guarantees you can understand. See the code below, which produces exactly the same results. There is also a limitation of zip. That is, the length of the zipped results depend on the shortest one among all the iterables. This is very similar to the classic ‚Äúbucket effect‚Äù. It generally means that how much water a bucket can contain depends on the shortest wood board. Let‚Äôs again have an example to reproduce this problem. In the above example, our keys list has 3 elements but the values list only has two. In the result of the zip, we have lost the third key message because there is no more element in the values list can be extracted to match it. To solve the problem, we can use another method in the itertools library, which is a built-in Python library so that it comes with Python by default. Cool! Now we have the key message. Since there is no corresponding value in the other list, it was just filled with None. It is worth to be mentioned that there are new features released in Python 3.10 (see PEP-618) www.python.org A new flag of zip called strict has been proposed and accepted in PEP-618 that will force to check the length of the iterables in zip. A ValueError will be raised if the lengths are different. docs.python.org In this article, I have introduced ‚Äúzip‚Äù which is one of the most Pythonic and useful tools in Python. Its basic usage would be very useful, and we can see it is widely used in many Python projects. Besides, the unpacking technique is also commonly used together with zip. Finally, I have demonstrated one of the constraints of Python zip which can be referred to as ‚Äúbucket effects‚Äù, but it can be solved within Python built-in libraries. medium.com If you feel my articles are helpful, please consider joining Medium Membership to support me and thousands of other writers! (Click the link above)",276,3,6,Towards Data Science,2021-01-04,2021
https://towardsdatascience.com/a-comparison-of-seaborn-and-altair-640b11ca2e7b,A Comparison of Seaborn and¬†Altair,Two popular data visualization libraries for¬†Python,1,21,['A Comparison of Seaborn and Altair'],"Data visualization is a substantial part of data science. It helps to better understand the data by unveiling the relationships among variables. The underlying structure within a dataset can also be explored using well-designed data visualizations. In this article, we will compare two popular data visualization libraries for Python: Seaborn and Altair. The comparison will be based on creating 3 sets of visualizations using both libraries. The focus of comparison is on the syntax, the structure of figures, and how these libraries handle the relationships between variables. We will use a marketing dataset to create the visualizations. Let‚Äôs start by importing the dependencies and reading the dataset into a Pandas dataframe. The first set of visualizations includes basic histogram plots. In Seaborn, we pass the name of the dataframe and the name of the column to be plotted. The height and aspect parameters are used to modify the size of the plot. Aspect is the ratio of the width to height. In Altair, we start with a top-level Chart object which takes the dataframe as argument. Then we specify the type of plot. The encode function takes the columns, relations, and transformations to be plotted. Anything we put in the encode function needs to be linked to the dataframe passed to the Chart. Finally, the properties function adjusts the size of the plot. Altair is quite efficient at data transformations. Let‚Äôs explain the transformation done to create the above histogram. What this line does is that it divides the amount spent column into bins and count the number of data points in each bin. Both Seaborn and Altair provides simple ways to distinguish the data points of numerical variables based on different groups in categorical variables. In Seaborn, this separation can be achieved by hue, col, or row parameters. Following code will return a scatter plot of the amount spent and salary columns and separation will be done according to gender. In Altair, we use the color parameter inside the encode function. It is similar to the hue parameter of Seaborn. In some cases, it is more appealing or informative to use multiple plots in one visualization. Each subplot can be used to emphasize a certain feature or relationship so the overall visualization conveys more information. There are many ways to create multi-plot visualizations. In Seaborn, we can use the pyplot interface of Matplotlib to manually create a grid by specifying the number of Axes objects in a Figure. Then we can generate a plot for each Axes object. Another way is to use the FacetGrid or PairGrid to automatically generate a grid of plots. Based on the given variables (i.e. features), FacetGrid creates a grid of subplots which allows for transferring the structure of the dataset to the visualization. Row, col, and hue parameters can be considered as the three dimensions of FacetGrid objects. We first construct the pattern using the hue, col, and row parameters. Hue parameter uses different colors for separation whereas col and row parameters use x axis and y axis, respectively. Then the map function is used to specify the type of plot along with the variables (i.e. columns in dataframe) to be plotted. In Altair, the logic for creating such grids is quite similar to Seaborn. In addition to the color parameter, we use the column and row parameters in the encode function to add dimensions to the returned visualization. Both Seaborn and Altair are popular data visualization libraries for Python. Either one will be enough for most of the data visualizations tasks. There are small advantages of one over another in different aspects. I think the syntax of Seaborn is a little simpler and easier to read than the syntax of Altair. On the other hand, Altair is superior to Seaborn with respect to data transformations. Altair also provides interactive visualizations. Thank you for reading. Please let me know if you have any feedback.",50,0,4,Towards Data Science,2021-01-04,2021
https://towardsdatascience.com/measuring-success-ef3aff9c28e4,Measuring Success of Machine Learning¬†Products,Business Performance vs Model Performance,5,10,"['Measuring Success of Machine Learning Products', 'Defining and Quantifying Success', 'Business Performance', 'Model Performance', 'Closing Remarks']","Failure is part of the learning process. Unfortunately, it frequents being part of the machine learning development process far too often. ML projects can be doomed from conception due to a misalignment between product metrics and model metrics. Today, there are many skilled individuals who can create highly accurate models, and poor modelling capabilities is not a common pitfall. Rather, there is a tendency for accurate models to be developed that are not useful for a product, thus failing to meet business objectives. In defining success, it is important to consider the differences between business performance and model performance. The most simple way to put this is that business performance is a function of many variables, not just model performance. With poor model performance, business performance will be inadequate, but good model performance does not guarantee good business performance! To evaluate business performance, it is necessary to start with a product or feature goal. For example, increasing revenue of an e-commerce site. Once this is defined, a product metric should be assigned to evaluate success. This metric needs to be separate from any model metrics, only quantifying the product‚Äôs success. Product metrics can vary, with metrics such as number of users a feature attracts or the click-through rate (CTR) of recommendations both representing valid examples. At the end of the day (and fiscal period), product metrics are what matter. They represent the goals of the product. Any other metrics are simply to be considered tools available to optimize product metrics. Typically projects only aim to improve a single product metric, but their impact is frequently quantified with respect to numerous metrics. Some of these include guardrail metrics, which represent metrics that are not to fall below a certain point. For example, an ML project can have the objective of increasing a product metric like number of users while maintaining the stability of other metrics like average user session. Measuring the effectiveness of an ML approach requires the tracking of model performance. Prior to deployment of a product utilizing ML, it is not possible to quantify product metrics. During the building of the ML product, offline metrics or model metrics are useful for defining success. To consider an offline metric to be of quality, it should be evaluated without exposing the ML model to users. Furthermore, it should be highly correlated with product metrics and business goals. Suppose you were developing a feature to offer suggestions to users while typing queries to an online retail store. The success of this feature can be measured using CTR (product metric). To create these suggestions, a model which predicts the words a user will type and displays these predictions can be built. By measuring the word-level accuracy (calculating how often the model predicts the correct next set of words), the performance of the model can be defined. In this scenario, the model would be required to have extreme accuracy to increase the product‚Äôs CTR, as a single error in word prediction would render a suggestion useless. A (better) approach would involve training a model that can take user input and perform classification into categories of your catalog, suggesting the top most likely predicted categories. Here, the number of categories in a catalog is significantly less than all the words in the English language, making this a much easier model metric to optimize. Moreover, the model only needs to correctly predict one category to generate a click. Business performance seems to often get lost in the hype of ML model performance, but it is always important to realize what metrics need to be optimized. Beyond just business performance and model performance, other metrics for success are required. Considerations include model freshness and speed. Model freshness is an important consideration as models age and data distributions change. Speed is always an important consideration with any software. Autonomous vehicles could never be achieved if it took even several seconds to process data and generate predictions. For more on this topic, I highly recommend Building Machine Learning Powered Applications by Emmanuel Ameisen. It covers the skills necessary to design, build, and deploy applications powered by machine learning.",76,0,4,Towards Data Science,2021-01-04,2021
https://towardsdatascience.com/json-and-textual-data-in-python-5aa7c376a0d4,JSON and textual data in¬†Python,How to incorporate the immensely popular JSON data format in¬†Python‚Ä¶,6,23,"['JSON and textual data in Python', 'Why JSON', 'The JSON basics for Python', 'JSON files', 'JSON & HTML entities', 'Modular JSON writing function']","In this story, we‚Äôll look at JavaScript Object Notation or JSON, probably the world‚Äôs preferred data-interchange format and a sure-fire upgrade from more traditional file storage options (XML, CSV, TSV). We‚Äôll cover the basics for creating and loading JSON files, file storage, and newline delimited JSON storage and take a look into a more specific use-case of working with textual data and JSON. JSON is widely used in web applications as the preferred way to interchange data, especially to and from front-end to back-end middleware. Other than that it has four advantages, specific for textual data. It is often better compared to more traditional textual data storage formats such as comma-separated (CSV) or tab-separated files (TSV), because your textual data can never contain the delimiters of the file with these files. I expect my output to have three columns: text, age, surname, but because of the compound sentences, I now have 5 columns. A workaround is using a delimited that never occurs in your texts. A pipe | is much more uncommon, but still, you would never be 100% sure that a text does not contain a pipe. CSV and TSV files convert everything to strings, but JSON objects can contain anything: lists, strings, dictionaries, floats. Perfect for data storing! JSON consists of a key-value mapping, as Python dictionaries do. This enables us to label our attributes. Nested JSON objects are also very common. I believe that JSON is superior to Pandas DataFrames, especially for textual data. Often there are multiple (nested) levels to your data, for instance in the case of Named Entity Recognition, a dataset might look like this: But when loading this dataset in a Pandas DataFrame, what do I do with the entities? Do I keep them in a single column, or do I unpack them in different columns? What if there are multiple entities in a single sentence? Do I still unpack all of them? JSON with appended s in the functions is for encoding/decoding strings and without the appended s for file encoding/decoding. What JSON does is convert Python objects to a format inspired by JavaScript object literal syntax. The default conversion of Python objects to this format is shown in the image below. It supports all standard objects in Python. JSON expects each key in the key-value mapping to be unique and there is no easy way to circumvent this. Non-unique keys, either get summed as shown in the example below or overwritten when working with other data types. JSON objects are very similar to Python key-value dictionary mappings and as such when saved to a file are just a very long string representing the dictionary on one line. An issue that arises because of this is that the document becomes very heavy to open for text editors such as Sublime or Pycharm and it reduced human-readability. An option to reduce the load and improve readability is to use the indent argument when dumping JSON. Another useful argument is sort_keys, that well.. sorts the keys before saving the file. A very common way to save JSON is to save data points, such as dictionaries in a list, and then dumping this list in a JSON file as seen in the nested_text_data.py. Another, more recent option is to make use of JSON newline-delimited JSON or ndjson. This format saves each JSON data point on a new line. A huge advantage is that you can iterate over each data point and do not need to load the whole object (list) in memory. There is a library in Python jsonlines to write these files. Have you ever seen the ASCII-format representations of & > and < in your JSON data &amp; &gt; &lt; or how about the non-breaking space &nbsp;? Oftentimes data comes in JSON format as a response from an API or web-service. As a result of this, textual data is (1) in ASCII-format and (2) non-Unicode characters are converted to Unicode representations \u2014: This StackOverflow thread suggests using the ensure_ascii=False flag when dumping this JSON. After reading both the json.dump() docs and this StackOverflow thread I understood that ensuring ASCII is important, especially if you‚Äôre unsure of the format of your JSON, which is often the case if you use a web-service API. Also, it did not solve my issue with the HTML entities‚Ä¶ The solution is html.unescape() from the HTML library, which unescapes all HTML entities from your string. I make use of a custom function to write JSON objects. I like this function because it helps me with static file management: adding the date and time to the file name and providing logging feedback where I saved the file. As you can see I also included the unescape.html() function in a wrapper unescape_all_html() for my most-used JSON data format: a list of dictionaries. Find the wrapper below. That‚Äôs it! I hope this guide gave you an introduction to working with JSON (and text) in Python! Happy coding :)",17,0,5,Towards Data Science,2021-01-04,2021
https://towardsdatascience.com/extracting-information-from-unstructured-sources-b9d8bd14e70a,Extracting Information from Unstructured Sources,Searching for‚Ä¶,6,43,"['Extracting Information from Unstructured Sources', 'Problem Statement', 'The First Approach', 'The Second Approach', 'Conclusion and Further Steps', 'Acknowledgments']","Recently, one of our clients has contacted us with an interesting problem. It was an investment company, which needed to calculate potential risks for their objects of investment. They required a solution that would help generate a list of all addresses (or GPS coordinates) that were associated with a specific company name. The problem our client faced was that a lot of companies could have many different subdivisions with different names in different languages. Plus, a lot of companies have departments all over the world. Also, the company could have different hidden locations, associated with them, which are not stated directly on their legal documents. We started with a small Proof of Concept to check if there is an approach that can solve one or a few of these problems. The result had to be presented as a graph of geo-locations, with described relations to other vertices and a given company name. The interesting challenge was that we were not provided with any data or directions. So the first problem we focused on was data sources. After a short research, we divided all available and relevant data sources into three major categories: Considering drawbacks of each datasource we decided to use the last category for our PoC, since registries and social networks are hard to interact with. In addition to being hard to find, registries provide data in various formats. This makes it hard to use even a few of them for a short PoC. The approach we chose in the end was not an easy fruit. We have to try a few approaches which didn‚Äôt work for several reasons. We will briefly describe what we did and how to point out the most promising results and causes of failure. Our main idea for solving this problem was based on using spaCy. This framework has a built-in ability to tag organizations and locations in a text, the ability which we found very useful for this problem. As soon as we chose a data source, we started to analyze web-pages related to a few randomly chosen organizations. After having tagged a few web-pages texts with spaCy, we found patterns, which appeared mostly when a company address was involved: With this pattern in mind, we started working on the following idea: We search the web for a term that looks like ‚Äú<Company> address‚Äù. The returned pages are proclaimed to be depth 1 pages. Then, we look for the above-stated pattern on each of the returned web-pages and extract a corresponding text ‚Äî organization name and its address. After this, we grab all the links on those pages, proclaim their pages to be depth 2 pages and continue in a similar fashion with these pages. This process gives us a graph with vertices, corresponding to web-pages and organizations (with their addresses). The first ones are connected, while the second ones are only connected to the first ones. The implementation of the process described above was a simple scrapy web-crawler. After receiving an initial list of links (depth 1 pages), the crawler looked for a pattern, collected links and proceeded to other pages, saving all results in a CSV-file. To check the pattern we used a tag-by-tag sliding window approach, which simply meant counting the number of specific tags or numerical tokens fitting into a sliding window. Despite the simplicity of this approach, it had instantly failed due to many reasons. First, even for a few initial links (say 100), the number of web-pages linked to them was measured in gigabytes. The process took too much time and resulted in a very small number of useful texts. Second, there were a lot of texts that contained neither the organization‚Äôs name, nor its address but still fit into our pattern filter. This was mostly because many words related to addresses were tagged as organizations or persons by spaCy or were not tagged at all. We decided to analyze the collected data and label them. Only ~20% of collected texts contained both an organization name and its address. The other ~80% had a lot of extra text, missing an organization name or a meaningful part of the address. This kind of data is hard to work with. However, while analyzing this data we saw that spaCy works well with organization names themselves. But our pattern was ignoring the majority of such occurrences. Another thing we found out was that almost every country has a different address template. Even although that we used corresponding spaCy models for different languages, such a simple pattern could not cover even a few address templates. This has driven us to the conclusion that such an approach is not working well for the given problem. Having taken into account all this information, we decided to simplify our PoC and focus only on US-based organizations and English pages. Plus, we decided to divide the problem into two parts: looking for related organizations and extracting the address for a single organization. To solve the first problem, we decided to try this: if some organization is mentioned on the web-pages related to a given company, then this organization is somehow connected to this company. For a moment, we would omit the question of how exactly an organization is connected to a company (opponents, subsidiaries, the same foundation group, etc.) since it appears to be a more complex problem to solve. To strengthen the observed relation, we assumed, that the company must be also mentioned on some company‚Äôs web-pages. We started with a simple search for related organizations using web-search and spaCy. We used Bing, however, you can use any other search engine available. We will show you the pieces of code in Python, which we used to collect data. Note that our code does not strive for elegance since it was written for a PoC and of course, can be enhanced or optimized. First, we searched for a term that looks like this: ‚Äú<given company name> companies‚Äù. Thus, if a given company is Samsung, then the search term would be: ‚ÄúSamsung companies‚Äù. That is how we received ten search results, enough for a start. Then, we extracted each corresponding web-page, tagged its text with spaCy (we didn‚Äôt need to tag HTML tags, scripts, attributes and so on), and extracted only those that were named ORG. As we already mentioned, we also strengthened the power of the observed relation by checking if a given company was mentioned on the pages related to the found named entities. For this purpose, we performed a search for each found named entity. Then, we preprocessed all collected data and saved them into csv files for further use: Now, to check if a given company is related to found named entities, we implemented a scrapy crawler. Using a list of URLs and a company‚Äôs name it processed each URL and counts all mentions of a given company. Please find it‚Äôs main method without the whole initialization, which mostly deals with files infrastructure: We simplified the code removing some needed try ‚Ä¶ except statements or other very specific parts of the code. Also, the decision to count the number of mentions appeared to be not completely correct: it would fail in some (not all) complex cases when the given company name consisted of two or more words. The more proper way to do this would be using the spaCy tokenizer and comparing tokens. Once the crawler had finished the work, we got a CSV file with IDs of only those potential organizations, that had a strong observed relation to a given company. A list of nearly a thousand found named entities for Samsung decreased to a list of just nearly 6 hundred. What remained looked like this: At first sight, it looked good. However, if we take a closer look at this list, we will find that a lot of named entities there are not organization names at all. For example, Samsung Annual Report is perhaps just a header of some text, but not an organization name. And there is nothing to blame spa–°y for: all these named entities do look like organizations. We labeled the collected data manually. And what we saw was that this approach gives a pretty good recall. In fact, for the Samsung company, our list contained nearly 80 of their different subdivisions, legally related organizations as well as their opponents in different areas. However, it had a low precision rate: nearly 60% of those named entities were not valid organization names (nor parts of full organization names) So we needed some kind of a model for filtering out what was not a valid organization name. The approach we settled for was a little bit complex and too big to describe it here. We wrote a separate article on this topic. Additionally, we tried to build a model for classifying organizations by their relationship to a given company. That model was supposed to be based on correlations between specific words and the number of mentions of a given company alongside some named entities. This approach, however, did not work. With the above-mentioned model, we filtered out what was not an organization name. Now that we have a list of organizations related to a given company, we need their addresses. To achieve this goal, we performed a search for each organization with a search term that looked like ‚Äú<organization name> address USA‚Äù. It gave us a list of web-pages that potentially contained USA addresses of related organizations. In general, extracting addresses from an HTML page is quite tricky. The process must include finding different patterns that are common for separate countries, dealing with a list of predefined words ( like country names, states, regions, etc.), and understanding what piece of text is an address. This is harder when you need to extract an address that is written in a different language or is presented on a web-page in the form of a table with rows like Street, House #, State, etc. As for the PoC, we decided to focus on the simplest case ‚Äî USA addresses written in a single p tag. For this we built a simple regex which took into account the order of spaCy‚Äôs named entities, numbers, states and, perhaps, the country. An interesting point here was that we needed to include spaCy‚Äôs organization (ORG) tag in it, since very often parts of an address where tagged with this tag. Also, we needed to build a custom regex for the US states. Even despite the fact that most of the states had a proper context around them, they were still rarely tagged by spaCy: For the address_re here: n ‚Äî is a small number, followed by up to 4 spaCy ORG, LOC, FAC or GPE tags ((l|f|g|o){0,4}), then state(s) and zip(z) and, perhaps GPE tag(g?), which would most likely by country. From this text we extract addresses in the following way: All the code above we use in our scrapy crawler to collect addresses. Again, its main method looks like this: With this, we passed through each search result and extracted addresses of organizations. For sure, this solution is not perfect. A lot of extracted texts were not addresses at all or contained plenty of extra text. However, this happened relatively rarely compared to the amount of extracted data. All these actions resulted in a list that looks like this: We used geopy Python package to get GPS coordinates of the address. Note that those geocoders don‚Äôt play well when you perform multiple requests sequentially. That‚Äôs why we added a random delay, just for fun :-). Now, having visualized all collected data, we received a map with specified locations of organizations related to some given company. Here is a list of further steps that have the highest priority: Perhaps, some specific language systems or translators will be needed for countries like China, Japan, Korea, and others. To sum up, we now have data to build a two-layer graph of entities related to a given company. The first layer of the graph would be a layer of organizations, somehow related to a given company. It can include different types of subsidiaries, opponents, legally connected companies, etc. The second layer is a layer of addresses of those organizations. This graph can be used for building a general company profile, calculating investment risks, company stock price, and much more. I want to thank my colleagues Andy Bosyi, Mykola Kozlenko, Volodymyr Sendetskyi, Viach Bosyi and Nazar Savchenko for fruitful discussions, cooperation, and helpful tips as well as the entire MindCraft.ai team for their constant support. Alex Simkiv, Data Scientist, MindCraft.ai Information Technology & Data Science",,0,10,Towards Data Science,2021-01-04,2021
https://towardsdatascience.com/get-started-with-sql-joins-87835422644b,Get Started with SQL¬†JOINs,"Learn left join, inner join, self join using¬†examples",4,36,"['Get Started with SQL JOINs', 'Why We Need to Learn SQL JOIN', 'How to Create JOIN', 'Take-Home Message']","To perform advanced analytical processing and data discovery, one table is often not enough to bring valuable insights, hence combining multiple tables together is unavoidable. SQL, as a tool to communicate with relational database, provides the functionality to build relationships among tables. This article introduces how to use SQL to link tables together. If you want to learn more about the basics of SQL, I suggest have a read of my first article about learning SQL in everyday language. It gives a comprehensive SQL introduction for absolute beginners. towardsdatascience.com Maybe you haven‚Äôt even realized, we frequently come across joining in Excel as well. This is achieved by VLOOKUP function as shown below. VLOOKUP function allows us to perform matching of one column and return the corresponding reference values from another column. In this case, we are able to find the country_name related to each criterion by matching the country_code to the fips_code in Country Code table. We may encounter many situations like this when the information in one table cannot suffice. SQL join uses the exact same logic, yet it is more powerful since it is matching and merging two or more tables together rather than two columns. In the following section, we will dive deeper into how to use SQL to address this problem. Another very practical and realistic reason for learning SQL join is INTERVIEW! That‚Äôs right, SQL join is almost an inevitable question in any data scientist or data analyst interview. Some questions are more theoretical and explicit, ‚ÄúCan you tell me the difference between LEFT JOIN and INNER JOIN?‚Äù; others are more practical and implicit ‚ÄúCan you write down a SQL statement to find the name of this employee‚Äôs manager?‚Äù So it gives you another reason to learn and distinguish the implementation of each kinds of join. SQL join follows this syntax and I break it down into three components: 1) select the attributes and tables; 2) determine the join condition; 3) choose the appropriate join type. Just like other SQL statements, it is necessary to specify the attribute and table name in the form of SELECT <attribute> FROM <table>. But the difference is that more than one table is required to join. If same attribute names exist in more than one table, then simply referring to the attribute by name will be ambiguous since the database is uncertain about which table you are selecting this attribute from. To solve this problem, we need to use the table name as the attribute prefix. For example, if ‚Äúname‚Äù attribute exists in both Customer and Country tables, and we only select the name of customers. Then we refer to the attribute as ‚ÄúCustomer.name‚Äù. Tables are joined together by at least one common attribute. This common attribute is often referred to as a foreign key. As demonstrated in the excel example, VLOOKUP function also takes advantage of this shared attribute. We use this shared attribute (foreign key) as the matching point where we can find corresponding information of each row in another table. This common attribute needs to be explicitly indicated as the join condition in the SQL statement. Let‚Äôs continue with this country code example. The aim of this exercise is to find the country name of each criterion in the ‚ÄúGoogle_Ads_GeoTargets‚Äù table. The datasets are from Google Public Dataset. Have an exploration and try to implement it in the BigQuery if you are interested. In the left table, we have already got the country_code for each criterion and the Country_Code table provides us with the country name of fips_code. Therefore, the logic is to match the country_code in the GeoTarget table to the fips_code in the Country_Code table and find the corresponding name. country_code and fips_code are the common attributes that build the relationship between two tables. We write down the following statement to indicate the join condition. Notice that it is better to use the table name as the attribute prefix and don‚Äôt forget the keyword ‚ÄúON‚Äù: There are more five major join types: left join, inner join, full join, self join, and cross join. In order to communicate to the database, we need to explicitly or implicitly indicate which join type in the statement. This is achieved by using keywords ‚ÄúLEFT JOIN‚Äù, ‚ÄúINNER JOIN‚Äù or ‚ÄúFULL OUTER JOIN‚Äù etc. Each type has its distinct use cases. Hopefully, the comparison below will help you to distinguish their subtle differences. 1. Left Join Left join is the most similar to VLOOKUP in excel. The table on the right can be seen as a reference table or a dictionary from which we are extending the existing knowledge stored in the left table. Therefore, left join is adopted to return all records in the left table and reference the corresponding values from the right table. Left join can also provide us with more insights on why some values failed to find a match. E.g. Is it a result of incorrect records or typos in the left table or is it because of inexhaustive data in the right table? We use this statement to generate LEFT JOIN result shown in the picture: All rows from the Google_Ads_Geo_Targets are returned as the result even though some failed to match the Country_Code table (consequently, null is returned). You may wonder why it is not necessary to use right join most of the time. This is because right join can be achieved swapping the directions of table and perform left join. Besides, right table is often regarded as a reference book. In this instance, the Country_Code table is seen as a reference or a dictionary. There is no point returning the matching for all fips_codes. Just think of the situation we come across a new word. We would find the meaning of this word in a dictionary rather than go through each word in dictionary to match the words we enountered. The scope would be too broad and vague. 2. Inner Join Inner join is useful when we are interested in the intersection of tables. Since it only shows the records that exist in both tables, inner join usually has the least number of rows returned. As shown in the result below, all null values are filtered out. We indicate this join type using the INNER JOIN keyword: 3. Full Outer Join Full outer join covers every row from both tables regardless if a match has been found. It is used to have a thorough grasp of information stored in both tables and pick up any mismatches. In this example, the first row shows as null for country_name and fips_code because there is no match of fips_code = ‚ÄúAZ‚Äù in the Country_Code table. On the flip side, the last row has no criteria_id and country_code because there is no criteria with country_code = ‚ÄúZA‚Äù in the Google_Ads_GeoTargets table. To write down a FULL OUTER JOIN statement: 4. Self Join Self join means linking the table to itself and we apply it to address unary relationship. It is powerful to create hierarchical relationships, for example, employee and manager, category and subcategory etc. Eventually, it still can be seen as joining two tables together. I am using an Employee example to explain self join. In this example, if we want to find the manager name of each employee, we need to use a self join to find the name of each employee‚Äôs manager, and we use managerID to derive the name. managerID is indicated as the foreign key in the ERD diagram because it is borrowed from the employeeID. As demonstrated in the ERD, We can think of Manager table as a subset of Employee thus this self join is equivalent to left join Employee Table to its subset Manager. The join condition is that the manager ID in Employee table needs to be equal to employeeID in the Manager table, hence e.managerID = m.employeeID. For self join, we still need to consider what is the join type. In this case, we want to show all employee records even though they don‚Äôt have a manager (e.g. employee 3 and 6), thus a left join is needed. It is worth noting that renaming the tables is required to clearly indicate which attributes from which table you are referring to. Also, we can use the AS keyword to rename attributes into something readable. 5. Cross Join I would say that cross join is the most irrational join, because it doesn‚Äôt have any joining condition. Cross Join matches each row in one table to all rows in the other table, just as in the example below. The Geo Target criteria with criteria_id = ‚Äú1000998‚Äù matches to all records in the Country_Code table. The table below is not exhaustive but we can calculate that there are 42 rows in total (7 rows of Google Ads criteria x 6 rows of country code). We may accidentally create a cross join when there are duplicated rows in tables. Therefore, it is essential to eliminate duplicated rows while preprocessing the data. I will explain this in more detail in the upcoming article, stay tuned if interested :) Cross join is high in computation cost and should be avoided when possible. The only use case I can think of is to populate combinations of all records in two tables, for example, size x color. Please comment down below if you have any suggestions on cross join use cases. The main take away from this article is to boil down SQL join into three steps: Hope that this article helps you to upgrade your basic SQL skills to more advanced analysis through combining tables. www.visual-design.net www.visual-design.net Originally published at https://www.visual-design.net on January 4th, 2021.",11,0,9,Towards Data Science,2021-01-04,2021
https://towardsdatascience.com/xrv-the-final-update-cb7b09192ace,xRV: The Final¬†Update,"This article details my final update to xRV, my pitch-level run value estimator‚Ä¶",3,46,"['xRV 3: The Final Update', 'Recap: Previous Iterations', 'Final Model: Version 3.0']","After documenting my attempts to create a pitch-level run value estimator for MLB pitches on this blog all summer, this will be my final update for the foreseeable future. This article contains information about how I got here, then an in-depth breakdown of the steps I took to improve my pitch-level expected Run Value metric. At the end, I provide some fun analysis, graphics, and leaderboards. Again, I am making all of my reproducible code for this project available to the public here. Information about QOS+, the sibling metric of xRV, is at the bottom of this article. In March, I wrote about my first attempt to quantify how good or bad every MLB pitch was in the 2019 season here. For this, I used a model called a K-Nearest Neighbors Regressor where each pitch of the season was given the average run value of its 100 most similar pitches from 2019. When adding up the predicted pitch value for every pitch, good pitchers tended to score well and less good pitchers tended to score poorly. I wrote an article on this, believing it was evidence that my model was pretty good. Only recently, on a Nick Wan Twitch stream, did I learn that nearly every pitch level metric (good or bad) will aggregate pretty well. The final RMSE of this metric, 0.22, was the same as the standard deviation of pitch run values in 2019. In other words, the model was not very good at all. This was likely because KNN is not a very robust model type for this kind of problem and I did zero feature selection or model validation, two very important steps in the model building process. So, I tried again in September. In the next version of my pitch quality metric, I made sure to document all of my steps meticulously and make the code publicly available to increase accountability. Just in case this model was as bad as Version 1.0, I wanted to get feedback much sooner. In this model, written up here and created using 2020 data, I pivoted to using Random Forests which is a non-parametric model type much more suitable for this non-linear data. Each pitch was given a prediction by one of 16 models specific to that pitch‚Äôs pitch type group (Fastball or Non-Fastball) and platoon split (RvR, RvL, LvL, LvR). Version 2.0 was an objective improvement upon Version 1.0 as the final RMSE dropped from 0.22 to about 0.14. I also improved as an analyst, this time performing feature selection and validation and generally having a more sound model building process. However, the code was quite inefficient, forcing me to group pitches into only 2 groups instead of the more typical 7 or 8 for the sake of running time. My code for this project is posted above, so you can decide for yourself if you agree with my self-assessment. While making Version 2.0, I learned a lot about the model building process and that is where a lot of the value of this project was for me. Still, I knew there were some improvements to be made, and that has been in the back of my mind for a while. Enter Version 3.0. This model made a few key improvements over the last two versions. Namely, I created models for each of six pitch type groups (FF, SI, FC, CH, SL, CU) instead of just two (FB, non-FB) thanks to more efficient code (functions!!!). Being able to select features for each pitch type instead of for broad pitch type groups greatly improved the model‚Äôs performance by RMSE. I will break down this version in terms of the traditional data science workflow, touching on data acquisition, data cleaning and feature creation, feature selection, model creation, model validation, results, and a touch of analysis. I acquired all of my data for this project using Bill Petti‚Äôs baseballr package and the scrape_statcast_savant() function, getting every pitch from the 2020 season. (Data from other seasons would have worked as well, but I simply did not have the computing power to make predictions for multiple seasons.) This gave me a dataset of over 250,000 observations and about 90 variables for each observation. Huge thanks to baseballsavant.com for making this data public and making this project possible. The biggest problem in creating a pitch-level quality metric is finding a good response variable. I‚Äôve seen some similar metrics try to predict the probability of a swinging strike or try to predict CSW%. From the start, I wanted to predict Run Value, the number of runs that each pitch is worth on average. Unsurprisingly, this metric is not as easy to acquire, so I calculated it myself with the help of the Fangraphs Glossary post on Linear Weights. Most of this section of the project was taken from earlier projects I had done that focused on deriving the Run Value for every pitch of the season. Here is where I made several important choices like treating pitches resulting in a walk as having the value of a ball (not a walk) and pitches resulting in a strikeout as having the value of a strike, for example. These choices are stylistic and can be quibbled with or easily adjusted. In addition to creating my response variable Run Value, I created a few new variables called velo_diff, hmov_diff, and vmov diff that represent each non-fastball pitch‚Äôs velocity, horizontal movevment, and vertical movement difference from the pitcher‚Äôs fastball averages. These variables were only considered for non-fastball models. Note: Throughout this process, I am using six pitch type groups. These are Four Seam Fastballs (FF), Sinkers (Si), Cutters (FC), Changeups (which includes pitches tagged to be Changeups CH and Splitters FS), Sliders (SL), and Curveballs (pitches tagged Curveball CU or Knuckle-Curve KC). Philosophically, it pains me a bit to use MLBAM pitch classifications, but I think it is okay in this context. Other pitch types like Knuckleballs, Eephuses, and Pitch Outs where removed. Lastly, I removed rows with missing values in important columns (which was only a couple hundred observations) and flipped a few measurements for Left Handed Pitchers to be on the same scale as those for RHPs. As baseball fans and analysts, we know that the aspects that make a pitch valuable or not can vary greatly by pitch type. For this reason, it is important to take pitch type into account when deciding which features to use in our model. I created a function to tell me which features were most important to predicting Run Value (using the Boruta algorithm). Here is the code for non-fastballs: I then split my data frame into six, one for each pitch type group, and ran this function for each pitch-type-specific data frame, storing the results for later. Now that we know which variables we will use to model Run Value, we need a model! Version 2.0 of this metric used random forests, and so does Version 3.0. I experimented a bit with using extreme gradient boosting instead, but an initial XGBoost model did not represent an improvement over Version 2.0, so I stuck with random forests. I created a function that took in a data frame and a set of features as input and fit a random forest model, returning the model itself as output. Next, I created a function to validate these models. The function randomly splits the input data set into training (70%) and testing (30%) sets, fits a model for data with each combination of pitcher and batter platoon (LvL, LvR, RvL, RvR), applies the model predictions to the test set, and outputs the RMSE. I ran this validation function for each of the pitch-type-specific data frames, creating 24 random forest models in total. The validation RMSE for each pitch-type-specific data frame was between 0.08 and 0.11, meaning each Run Value prediction was off from its actual Run Value by an average of 0.08 to 0.11 runs. In a perfect world, I would use K-fold cross validation here instead of validating each model only once. However, these values are almost always within this range when I run the code and cross validation would simply take more time than I am willing to spend. When I aggregate all of the test data back into one dataset, my total validation RMSE is about 0.10 runs, a very similar number to the validation RMSE of Version 2.0. Looking good. The last step here was to apply the models to all of the pitches in 2020. Again, I created a function to do this, greatly lowering the runtime of this step! I passed each pitch-type-specific data frame and corresponding list of features from the feature selection step into the function and got the final predictions. The total aggregated RMSE of the predictions was also about 0.10 runs. Version 2.0 had a final RMSE of about 0.14, so I am happy with the improved performance of Version 3.0 in comparison. Finally, I joined the predictions back onto the original data frame for analysis. Aside from leaderboards, which I will get to in a second, I was able to make some fun graphics with these predictions. The first was simply a plot of the predictions (called expected Run Value or xRV) on a hex plot of the strike zone: Clearly, blue means the pitches in that region had a negative xRV which is good for the pitcher and red hexagons had a positive xRV which is good for the offense. It is cool to see that pitches in the strike zone are typically good for the pitching team, and middle-middle pitches are better than guaranteed balls way outside the zone! I broke this plot down to be specific to pitchers and pitch types of interest (thanks to the patchwork and stat_summary_hex() functions!), like the pitchers with the best changeups‚Ä¶ ‚Ä¶and curveballs‚Ä¶ ‚Ä¶of 2020. The same could be done for an individual pitcher, like Jacob deGrom. Leaderboards I know some readers just want to know who rates well by this metric, so here are some leaderboards. I don‚Äôt think I can embed these tables into Medium, so enjoy these screenshots! Here I am also including a rescaling of xRV called xRV+ where 100 is average for that pitch type and each additional 10 points (110 xRV+) is one standard deviation above average. Remember that xRV is a rate stat when aggregated to the pitcher or pitch type level, meaning the units are ‚Äúruns allowed per 100 pitches.‚Äù Interpretation Example: ‚ÄúNik Turley‚Äôs fastball had an xRV of -1.43 runs, so we would expect him to have prevented 1.43 runs per 100 fastballs on average.‚Äù All leaderboards are min. 150 pitches. Top 2020 Fastballs: Top 2020 Sinkers: Top 2020 Cutters: Top 2020 Changeups: Top 2020 Sliders: Top 2020 Curveballs: I‚Äôve learned that self-evaluation is incredibly important in the research space and in general, so here are my thoughts on where this model falls short. Overall, I do think there is plenty of room for improvement on this project. I don‚Äôt anticipate being willing or able to take this project any further, but hopefully by writing this article and providing my code here, I can be a jumping off point for the next person or people who want to give this a try! Once again, my code base for this project can be found here. I made a video where I find some of the best and worst pitcher pitches and individual pitches by xRV and do some analysis live using baseballsavant.com in a more fun and informal format. That link is here! A sibling metric of xRV called Quality of Stuff Plus (QOS+) has been used on several articles by Eno Sarris at The Athletic this season, found here. Along with the improvements to xRV detailed in this article, subsequent (and similar) adjustments have been made to QOS+ as well. There are a few key differences between the metrics, but the most relevant one is that QOS+ does not consider the plate location of a pitch in its predictions. For any future uses of QOS+, expect these changes to be reflected.",31,0,11,Towards Data Science,2021-01-04,2021
https://towardsdatascience.com/level-up-your-machine-learning-workflow-with-a-thunderbolt-3-ssd-5682bcce8773,Level-Up your Machine Learning Workflow with a Thunderbolt 3¬†SSD,Are you running out of space on¬†your‚Ä¶,1,12,['Level-Up your Machine Learning Workflow with a Thunderbolt 3 SSD'],"I have recently started training models in TensorFlow on some rather large datasets. My already-taxed internal SSD soon ran out of what little space it had left, prompting me to find another solution. I had an old external SSD lying around, so I tried moving my 20 GB dataset onto that. Unfortunately, training times nearly doubled. Running a quick disk speed test on this drive vs the internal drive in my computer clearly identified the problem. The external SSD may have similar raw performance to the SSD inside my MacBook, but the external drive only supports data transfer over a USB 3.0 interface. This limits the maximum practical transfer speeds to around 500 MB/s. My internal SSD, in contrast, is connected to the motherboard through a PCIe bus, which has a practical limit of tens of gigabytes per second. The maximum speed this can achieve is solely limited by the performance of the flash memory. My 2016 MacBook Pro, along with almost all new Apple computers and many Windows computers, has a Thunderbolt 3 (TB3) interface. This external connection supports transfer rates of up to 5 GB/s. Such a fast speed enables a whole host of interesting peripherals, such as external GPUs and fast external storage. I was curious to see if an external SSD that supports Thunderbolt 3 could solve my storage woes, so I ordered one. The drive I chose is a Plugable 1 TB Thunderbolt 3 SSD. For $300 it is a good bit more expensive than regular USB external SSDs, but if the faster link didn‚Äôt produce a bottleneck I deemed it worth it. This drive has a theoretical maximum read speed of 2400 MB/s and a write speed of 1800 MB/s. I formatted the drive using Apple‚Äôs new Apple Filesystem (APFS) format, and ran a speed test. To my surprise, this drive was actually faster than the one inside my computer! I suspect that this is due to the age of my internal SSD. The external one I ordered likely uses newer flash technology. I was also pleasantly surprised that I could at least come close to reaching the advertised speeds, which isn‚Äôt always the case. The real test for my drives, however, was how well they performed in my machine learning workflow. I ran a test on my old USB external SSD, my new TB3 SSD, and my internal SSD. I trained a convolutional neural network in TensorFlow on a 20 GB image dataset for 10 epochs, and compared how long it took with the dataset on each drive. I did this test on my external RX580 GPU, which also uses a Thunderbolt 3 interface. My computer has two Thunderbolt controllers (one for the left-side ports and one for the right), so I put my GPU on one side and my external drives on the other. I don‚Äôt know if it‚Äôs possible to saturate the bandwidth of the Thunderbolt controller, so I wanted to be safe. The USB 3.0 SSD was by far the slowest, completing 10 epochs in 10 minutes and 29 seconds. My Thunderbolt 3 SSD completed the test in 6:12, and my internal SSD completed the test in 5:42. I was surprised that my internal SSD was still faster given that it has a slower transfer speed than my external drive. My guess is that this is due to the nature of my dataset, which has many small files. My internal SSD has a direct connection to my computer‚Äôs PCIe bus, so the latency induced by starting a file transfer is likely much lower than that of my external drive. The external drive uses the same NVMe interface as the internal drive, but there are two Thunderbolt controllers (one in the drive and one in the computer) between the drive and the PCIe bus. If I instead were training on much larger files I‚Äôm guessing that the external drive would have the edge due to its faster overall speed. I was still pleasantly surprised with my the performance of my new hardware. I‚Äôll definitely keep it around, as I can now liberate close to a hundred gigabytes on my internal drive used for various datasets. I suspect that this tip will help others in a similar boat! I hope you found this article useful. Check out more of my writing here, and my website www.alexwulff.com.",42,1,4,Towards Data Science,2021-01-04,2021
https://towardsdatascience.com/image-segmentation-change-the-color-of-your-car-step-by-step-guide-ba9aa16ee52e,Image segmentation: change the color of your car! Step-by-step guide.,,2,25,"['Image segmentation: change the color of your car! Step-by-step guide.', 'Gathering images and creating mask annotations']","Have you ever wondered what your car would look like in another color? Or are you just interested in training a deep learning model for image segmentation tasks with no theoretical knowledge required? Or even both? Then this post is for you! There will be two main parts: first how to create a dataset for an image segmentation task and then how to create and train a model. If you‚Äôre only interested in the second part, you can read it directly! I provided a dataset for it. First things first, we need to gather car side view images. There are many ways to collect such data, I can suggest the following methods: Unsplash is a great website that shares a lot of images freely available with nice permissive copyright terms. It was especially useful to me because I wanted to publicly share some of the pictures, in this blog for example. The con here is that there are not many pictures specifically of car side views. Google Chrome and Firefox have a nice extension letting you download all images of your current active tab. Very useful if you google for example ‚Äúcar side -sidecar -sketch -cartoon‚Äù and go to the images tab. the ‚Äú-‚Äù symbol means that you want to exclude those keywords from your search. If you have a good camera, you also can simply go around and take pictures yourself! Make sure to blur people and number plates if you want to make your dataset publicly available. I personally chose to adjust my dataset in this manner for copyright reasons. You might need to collect around 200 images to get satisfying results with our deep learning model. A mask annotation is an image with pixel values representing classes, as the image below shows. It can follow different formats: black-and-white PNG, colorful PNG, COCO style JSON, etc‚Ä¶ There are also many tools available for annotating images. I decided to use VoTT, which can be downloaded from their GitHub page here. It is really useful if you want to create bounding boxes or polygons for image segmentation. Polygons aren‚Äôt directly understood by all models for image segmentation, but there is software capable of translating polygons into actual masks for us. We will be using Intelec AI. Once you have downloaded VoTT, start it up. Set up a project, where ‚ÄúTarget connection‚Äù is where you want to save your annotated images and ‚ÄúSource connection‚Äù is the location of your image dataset. Click on the polygon option in the top left corner and start your annotation journey! Once you have finished drawing your polygon, press escape and choose your label. If you struggle with this software, I advise you to watch this quick tutorial. When you are finished with annotating, there is one last and important thing to do! On the left panel, go to export settings and select VoTT format. Then on the top panel, click on export. It should yield a big JSON file, something like [project-name]-export.json. This file is very important: it contains all annotations in a format that Intelec AI can understands and translates for us. As the screenshot above shows, I personally decided to segment the cars into 4 different categories: the car itself, to know where it is on the picture, the wheels, the front and back lights (labeled with lights) and finally the windows, rear windows and windshields (labeled with window) If you want to try to train a model without annotating your own dataset, I provided mine here! For this demo I chose to present you Intelec AI because it is easy to use and often yields good results. They propose different models for different tasks and do most of the data wrangling for us. You will actually not see a single line of code in this tutorial, thanks to VoTT and Intelec AI! Intelec AI can be downloaded from this link. You will need to have Docker. Its installation is well explained in their page. I won‚Äôt explain how Docker works in detail here, but if you are a Linux user like me, I advise you to setup docker to be used without sudo, as this page suggest. Once Docker and Intelec AI are installed, we can create a ZIP folder with the same structure as suggested by one of the screenshots below. You need to be careful about the structure: the name of the folder and the JSON file should match exactly as the screenshot below suggests. If you annotated image yourself, be careful to rename your [project-name]-export.json into masks.json Then, go to the ‚ÄúFile explorer‚Äù tab of Intelec AI and upload your zip folder. Unzip it by right-clicking on it. Now we can simply go to the training page and create an image segmentation trainer! Link it to the data we just uploaded, choose a shrink factor (I would advise a factor of 2 if you have a powerful GPU, otherwise go for a factor of 5) and press ‚Äútrain‚Äù. You can grab a coffee and come back in a few moments to see your results. To me, it took less than 5 minutes to train 22 epochs with one GPU! Intelec AI offers a summary as shown below: If you are curious to see how well the model is training through epochs, I saved some intermediate results as we can see below: Bottom right shows that it already begins to understand the difference between the two main components of a car: the body and the wheels. Bottom left shows nice improvements when segmenting around the actual car. Top right shows that it begins to correctly guess where windows are as well. Top left shows how precise it became with windows and also started to detects back lights. It still has troubles with the front light though. If this is what you would actually be trying to detect, it might need more epochs as it is smaller than other objects. Intelec AI lets you train extra epochs easily: just press the ‚Äústart‚Äù button again. I was curious to see whether the model was overfitting, so decided to try out segmenting my own car! I deployed my model in the Intelec AI software, took a picture of my beautiful Fiat Seicento sport and uploaded it in the ‚ÄúDeployed models‚Äù tab. Here is the result! I have to be honest, I really didn‚Äôt think the model would be able to do do such an awesome job! Now it is easy to use a sprite of a wheel you wanted to try out on your own car, because its location is automatically detected by the model with very good accuracy. As you have now seen, nowadays it is not difficult anymore to do deep learning: you don‚Äôt need great skills of coding, nor the deep understanding of all the mathematics and subtleties of neural networks.",17,1,6,Towards Data Science,2021-01-04,2021
https://towardsdatascience.com/food-item-search-using-recipe-embeddings-a-simple-embedding-based-search-engine-using-gensim-29631fcf5953,"<strong class=""markup--strong markup--h3-strong"">‚Äòfood-item‚Äô search using recipe embeddings</strong>",,1,47,"['‚Äòfood-item‚Äô search using recipe embeddings : A simple embedding based search engine using gensim, fastText and ElasticSearch']","This is an introductory lesson in building a search ML product. Where we‚Äôll be using a tool (genSim) to train a language model (fastText), and then index the data onto a scalable search infrastructure (ElasticSearch), and write a custom search functionality to try the embedding based search. Purpose In this tutorial we‚Äôll learn how to create a simple search application for a certain use case. Search is ubiquitous, every app has multiple of search bars and algorithms all serving a different purpose. Imagine you‚Äôre creating a food delivery app, or more specifically a cooking app, and you want to let your users post recipes and search your inventory real-time, and hence you want a search bar on the home-page as the first search bar which the user encounters when they come to your platform. We‚Äôre doing this as a tutorial to learn. The code is available in my Github. github.com Notebook(s): https://github.com/arnab64/food-search-recipe-embeddings/tree/main/src Goal Display a list of food-items given a query in decreasing order of similarity from the inventory. Data Data source: The dataset is a public domain dataset sourced from Kaggle. 6000 Indian Food Recipes Dataset Data exploration and preprocessing: All the necessary preprocessing has been done as required for text fields, both in order to train the embeddings and to use them. More details are available in the notebook. I mainly used the two columns Ingredients and Recipe in order to be able to train word vectors on them. Stack used Gensim, ElasticSearch, Pandas recipeEmbeddings : a fastText language model on food recipes build using GensimSince this dataset is a recipe dataset, we can train a linguistic model on the recipes. Dishes are the result of execution of a sequence of certain steps using certain ingredients. In fact, recipes are made sequential structure which makes it good for sequential tasks on food. Here, we are trying to build a food/dish-suggestion application, and we want embeddings which would do that. We are trying to suggest dishes, and we have the recipe of each. Hence, the nature of the input field is already in a sequential manner, the output we want will be a list of dishes in decreasing similarity. We can use embeddings trained on the recipes of the dish, and then represent each Food-Item/Dish using the embeddings of its constituent ingredients or recipe. We refer to these as food-item-embeddings. Because, all food items will be uploaded to website by vendor only once, and since the context doesn‚Äôt change, these embeddings for every Dish can be precomputed once, and everytime we have a new linguistic model, and indexed for faster retrieval. For our purpose, we train a fasttext model on the recipe column. For more details about training language models and to learn more on word2vec and fastText embeddings, check out this article. For details on my implementation of all the stuff mentioned here check out my repository: arnab64/food-search-recipe-embeddings Now, what is our task? In this report I have performed the second task mentioned, i.e. given an explicit query at runtime, I want to use the embeddings to suggest food-items. Some of the results of similar food-items, or as we call dish here, can be seen below. The training of the fastText model was very simple, and done in gensim. How do we get to food-item-embeddings from word vectors? A recipe is a collection of instructions, and a instruction is a collection of words (ingredients, actions, etc). Now, we have trained our language model to be able to learn n-dimensional representations of a word. recipe-vectors : Simple averaging over the word vectors of the words in the recipe, as I did not spend too much time on it. You can do the same thing on just the title of the food-item, but a title does not provide as much information about the food, as a recipe. Hence, it might not be that useful. Nevertheless, I tried out other methods as well, but not mentioning here to keep it short. Added food item-vectors based on recipeEmbeddings to existing data The newly computed vectors for every food-item based on it‚Äôs recipe and ingredients have been added as additional column in pandas. For non-user textual features like food-items it is best to store embeddings pre-computed, and update with change in data. After we have the vectors in the dataset, we can use these vectors + the other existing features to perform a classification task, like here we can try to predict the type of cuisine given a name, and hence evaluate how good the embeddings are. But predicting cuisine doesn‚Äôt seem like an interesting problem to dedicate time to now. Using fastText we are using only the contextual information and not the sequential information, i.e. these embeddings can most likely not differentiate if you added the onions after the tomatoes or vice versa. We could train our custom language model using attention based methods to capture the sequential information of recipes. Then we‚Äôd have to train the custom language model in torch or tensorflow and then use the model to add vector based embeddings. fastText is used to be able to train word vectors without wasting time, or we can use pre-trained word-embeddings trained on billions of params, and figure out a way to use them directly to power apps thus saving engineering effort and achieving similar results. Indexed the food-dataset with the new dense-vectors onto ElasticSearch We are instead going to index the data into a ElasticSearch and explore the features there, and most importantly we wanted to build a search engine, ElasticSearch makes it super easy, and scalable, and with dense-vector based operations, many smart applications can be built which will be very fast. elasticSearch can handle almost all kinds of data, it‚Äôs massively scalable and fast, easy to deploy your search model to production, and experiment with. Here, I indexed the data from pandas to ElasticSearch using the ElasticSearch client API in Python. Define the schema in python and wrote the custom data ingestion function from pandas to JSON, for all the columns needed in the index. Dense-vector based Search on ElasticSearch Elastic search makes the data available to be searched over API once indexed. The embedding for the search query is derived in the same way as we do for a item with recipe. We read the pre-trained recipe word model and get embeddings for the words in the processed search query and take an average. Observations The following results were interesting to see and proves a basic utility of the recipe data, and it can be combined with other datasets to make the embeddings and models better. Recipes follow a universal format, which globally, and in India recipes follow a structure. This is also a multilingual problem, as same recipes may exist in different languages on the web, and these kinds of embedding based methods make it easier to be able to build a application that works for any language, certainly helpful for apps in a multicultural market like India. My insights: This is a really small dataset that the embeddings are trained on (about 6000 instances), we might have real data which is huge and hence better, or trains smarter embeddings. However, despite using this we can build a workable search application which doesn‚Äôt look too bad. Since we are directly using recipe based vectors for every food item search, I believe our application would work great when we provide search queries which are essentially ingredients. Like: ‚Äúflour oil bake tomato cheese olive oregano‚Äù -TO- Pizza or breadsticks with tomato salsa etc. For food name search, we might have to train another model mapping queries to recipes. ‚Äúpizza‚Äù ‚Äî ‚Äúfour cheese and mushroom pizza, chicken pesto pizza, Cheesy muffin‚Äù Here (example-3) we see see that we have retrieved a lot of soup dishes for multiple cuisines. Users don‚Äôt list ingredients are queries, they are more likely to search for the name of the dish. This method works for that too but our embeddings aren‚Äôt designed to be optimized for that. For example, the results for ‚Äúchicken tandoori‚Äù look like this. [‚ÄòPaprika Chicken Skewers‚Äô, ‚ÄòBaked Paneer Corn Kebab‚Äô, ‚ÄòChicken Tikka Taco Topped With Cheesy Garlic Mayo‚Äô, ‚ÄòBaked Fish Crisps (Fish Fry In Oven)‚Äô, ‚ÄòBeetroot Chicken Cutlets‚Äô, ‚ÄòChicken Malai Kabab‚Äô, ‚ÄòPotato Roulade‚Äô, ‚ÄòRosemary And Thyme Chicken‚Äô, ‚ÄòSpicy Kiwi Salsa with Feta Cheese‚Äô, ‚ÄòCrispy Vegetable Tempura‚Äô] The results aren‚Äôt bad, and actually quite diverse but similar. Hence, to be able to use food name based search, we‚Äôll just have to add another model that maps from search queries to unique ‚Äúfood-recipes‚Äù, and maybe even use a sequential kind of training. Evaluation and next steps As of now I do not have a way to evaluate the goodness of these vectors. I left the work at this point. But this was a good introductory learning experience to know about the data science problems in apps dealing with food. I‚Äôm pretty sure the actual problems are a lot more complicated and at a bigger scale. As for next steps, I will try to combine other datasets, figure out approaches to come up with better way of embedding information and solve another task. Feel free to contribute or connect for exciting stuff. Thank you!",24,0,8,Towards Data Science,2021-01-04,2021
https://towardsdatascience.com/do-this-to-make-your-website-interactive-in-minutes-c11b7495d088,Do This to Make Your Website Interactive in¬†Minutes,Leveraging the power of data visualization to‚Ä¶,9,24,"['Do This to Make Your Website Interactive in Minutes', 'Data Sources', 'Step 1: Including the library', 'Step 2: Preparing the HTML', 'Step 3: Creating an outline in JavaScript', 'Step 4: Inserting data', 'Step 5: Formatting and styling', 'Other types of visualizations', 'Key things to remember']","Data tells meaningful stories, and with visualizations to complement it, we can glean valuable insights to find, support, and share an idea effectively. But static images often don‚Äôt enable the full capacity of data visualization. What could we unlock if we added user interactivity with data to a website? Whether on a project, personal website, or blog post, allowing a user to customize and explore hands-on allows for a more useful and memorable experience. With Chart.js, you can insert stunning tables and graphs quite easily and become a more impactful storyteller. This post will guide you through the tool, including setup and creating several types of figures on a page. Chart.js is a free, open-source library used to animate graphs within JavaScript files and HTML canvas elements. It includes eight different types of graphs: line, bar, donut, pie, scatter, bubble, radar, and polar area, but it‚Äôs also possible to mix-and-match multiple figures on the same page. We‚Äôll go through how to set up Chart.js, create initial graphs regarding world demographics, and style them. Here are links to all the data sources used in this tutorial. I‚Äôve put together a GitHub repository of everything we‚Äôll cover in this tutorial. If you open the index.html file, you should see a page of graphs and tables pop up in your browser. Now, open up index.html in a text editor for code. To easily import the latest version of Chart.js, we‚Äôll use use this CDN link (ending in ‚ÄúChart.min.js‚Äù). Then, place the link in a script tag as shown above. To format our charts, include CDN links to Bootstrap CSS and Bootstrap JS. If you think more information on Bootstrap would be helpful, check it out here. One main thing we need to prepare before we can start visualizing our data is to define an area in our HTML where we want to draw the graph. We must add canvas elements for each graph we want on our webpage. Here‚Äôs how this works. In our code snippet above, we give our graph (in this case, a line graph) an id , which is line, to the canvas element that we can later use to reference our designated graph element in JavaScript. You can name this element anything you want; the only thing that matters is that you use the same id when referring to the graph within all your files. Keep in mind that every id must be unique. To keep my data easily organized, I choose to place each graph‚Äôs code in a separate JavaScript file. Feel free to follow a different format, but it is what I find works best. Create a JavaScript file (and place it into a new folder called .js if you prefer). Let‚Äôs break down what‚Äôs happening in line.js. First, we locate the canvas element added earlier to our index.html file (notice the id line. Then, using that canvas element, we create a line chart (type: 'line'), and pass in our data. Compile all the data you need for the graphs. For this sample line graph, we‚Äôll use The World Bank‚Äôs population growth statistics. Download the data (or enter it in manually if you wish). Then, copy-and-paste the data into an array format as shown below. This is an integral part of the overall structure of the JavaScript files. We can give the dataset a label as well. If you save and refresh the HTML page, you‚Äôll see a new line graph. However, we can take steps to customize the visuals, including axis tick marks and labels, chart titles, point and line colors, hover effects, and much more! We can also add multiple datasets to one graph, which helps depict more trends. This is currently commented out in the code below. For every line that we want to create, we add another object to the datasets array. Each object can have a range of adjustments; we can not only pass new data to draw the line, but we can change the name, behavior, and looks of the line. These features can be edited as depicted in the finished line.js file. Read more detailed descriptions about all the customizations you can choose from in the Chart.js documentation! So far, we‚Äôve walked through how to create, personalize, and view a line graph on a webpage. Let‚Äôs look at some other visualizations offered by Chart.js. These are all included in the repository linked at the beginning of this tutorial. Chart.js offers several other types of charts such as tables, bar charts, pie charts, scatter charts, and more. Here is the JavaScript code for more samples. This code is included in index.html, rather than JavaScript. Keep these tips in mind as you review your code.",108,0,5,Towards Data Science,2021-01-04,2021
https://towardsdatascience.com/20-data-science-buzzwords-and-what-they-really-mean-3514ce0fde64,20 Data Science Buzzwords and What They Really¬†Mean,Choose your words wisely! Unlock the¬†full‚Ä¶,3,11,"['20 Data Science Buzzwords and What They Really Mean', '20 Data science buzz words and what they really mean', 'Concluding thoughts']","As a data scientist in a Tier One Consultancy, I crave unlocking value for clients. My role is to bring value by applying core data science techniques. Doing so, I sometimes face cases where data science solutions are excluded. The techniques are simple still referred to as black-box methods. This might at times be true but often it is a result of the lack of effective communication. We have failed as data scientists when people see every method as black-box methods The field of data science should be for everyone. It is our job to communicate core techniques and results for everyone to understand. Historically, technical departments primarily served as a support function and help desk. They were in big need when having technical issues or problems but not a part of the team. As a data scientist, you can only perform if included in the full process. Here the siloed mentality is no longer ideal. Let‚Äôs end the era of siloed processes together. By respecting the end user we can be more inclusive to non-technical people. This will unlock the full potential of data science and analytics! Ensuring transparency by adapting communication depending on the end-user is key. By doing so, the organizations will have higher beliefs in the selected methods used. If we push this together I am sure data scientists will be in even higher demand going forward. What are your thoughts? Has this changed your view on the importance of adapting communication? Feel free to leave your thoughts in the comment section. If you enjoyed this article, you might like my free newsletter here ‚Äî I‚Äôll email you when there‚Äôs a new post. You can also reach me on Twitter at @juel_sofie. Thanks for reading!",28,0,3,Towards Data Science,2021-01-04,2021
https://towardsdatascience.com/animated-bar-plot-in-python-for-time-series-data-8809dbdf9bc,Animated Bar Plot in Python for Time Series¬†Data,Implement an animated racing bar chart in¬†python,3,12,"['Animated Bar Plot in Python for Time Series Data', 'Conclusion:', 'References:']","Python has achieved popularity in a very short span of time, due to the presence of a large number of open-source libraries, still, it falls short when coming to create dynamic plots. Exploratory Data Analysis is one of the essential key components for an end-to-end machine learning project. Development of plots for data visualization is required for the EDA of the dataset. There are various libraries such as seaborn, matplotlib, ggplot, etc that create great plots for data visualization. When it comes to dynamic plots or animated plots, these libraries fail to develop animated plots. bar-chart-race is an open-source python library that can create dynamic or animated plots for the time series dataset. Pip and Conda installation of the bar_chart_race library: bar_chart_race library is an open-source library that creates animated bar plots for time-series datasets. There is a list of certain quickstart guides to follow before the implementation of animated bar plots: Below mentioned sample dataset is a correct example of the required data frame format to implement a bar plot. The above sample dataset is the number of covid-19 cases of India for each state. The columns of the dataset refer to states of India and the values are the number of covid-19 cases for a particular date. The index of the dataset is reset to the Date column. Below is the python code to implement an animated bar plot for covid time series data. Rest all the parameters are kept as default which can be altered to change the presentation of the plot. To read how to create an interactive pivot table using PivotTable.js read the below article: towardsdatascience.com Animated bar plot is a better visualization technique than static bar plots. In this article, we have discussed how to plot animated or dynamic bar plots for a time series dataset. Matplotlib animations can also be used to develop animated bar plots but it requires a lot more line of code to implement. bar_chart_race library is built on top of the matplotlib library. [1] Bar-chart-race documentation: https://pypi.org/project/bar-chart-race/",36,1,3,Towards Data Science,2021-01-04,2021
https://towardsdatascience.com/pandas-python-vs-data-table-r-278386a96f2a,Pandas (Python) vs Data.table (R),Practical guide for comparing two popular data manipulation‚Ä¶,1,29,['Pandas (Python) vs Data.table (R)'],"Data science ecosystem is full of highly effective and practical tools and frameworks. New ones are introduced and the existing ones are improved continuously. Having a variety of selections is a good thing and likely to increase the efficiency in most cases. However, it might also make it hard to decide which one to pick. In this article, we will compare popular data manipulation libraries in arguably the two most commonly used programming languages in data science domain. We will see how basic operations are done in Pandas (Python) and Data.table (R). The goal is not to determine if one is superior to or better than the other. I just want to make you familiar with the syntax and show similar or different approaches. There are many options to use these packages. I‚Äôm using R-studio IDE for R and VSCode for Python. We start with importing the libraries and creating the basic data structures by reading data from a csv file. In Pandas, we use the read_csv function to create a dataframe from a csv file. In Data.table, the equivalent of the read_csv is the fread function. The head function displays the top n number of rows. The name of the function is same for both packages but the syntax is slightly different. The default n value is 5 for both so they will display the first 5 rows by default. We usually check the size of data in terms of the number of rows and columns. Both libraries have intuitive ways of obtaining the size related information. In some cases, we just need a certain set of columns from a table. Both Pandas and Data.table provides convenient ways for doing this operation. The same operation is done with Data.table as follows: The rename and setname functions can be used to rename columns in Pandas and Data.table, respectively. Pandas: Data.table: As we see from the examples, the columns method on dataframe returns a list of columns. Same operation is done with the names function in Data.table library. We can filter a dataframe or table based on row values. In the following examples, we create a subset that contains customer who are older than 40. Pandas: Data.table: One of the most common operations in data analysis is to comparing numerical variables based on different values in a categorical variable. In Pandas, this kind of transformations and calculations are done with the group by function. For instance, we can calculate the average charge of data points in each category based on the gender and smoker columns. It is possible to apply multiple aggregate functions. For instance, we may want to see both the number of observations in each group in addition to the average charge value. In Data.table, the same calculation is done as follows: Just like with Pandas, Data.table provides a straightforward way to apply multiple aggregations. Both the number of observations and average charge value are calculated as follows: We have covered some simple tasks in Pandas and Data.table libraries. Due to the complexity level of operations, the syntax seems quite similar for both libraries. However, as the complexity increases and we do more advance data analysis and manipulation operations, the difference between them become more noticeable. The best way to learn how to use these libraries is through practice. As you adapt them in your data analysis tasks, you will have some preference for using a particular one in certain tasks. In general, I think both are more than enough to perform typical data analysis tasks. Thank you for reading. Please let me know if you have any feedback.",33,3,5,Towards Data Science,2021-01-04,2021
https://towardsdatascience.com/can-shap-trigger-a-paradigm-shift-in-risk-analytics-c01278e4dd77,Can SHAP trigger a paradigm shift in Risk Analytics?,Written by Chandan Durgia and Prasun¬†Biswas,1,40,['Can SHAP trigger a paradigm shift in Risk Analytics?'],"Written by Chandan Durgia and Prasun Biswas If one ranks economic theories by the variety of applications, ‚ÄúGame Theory‚Äù surely would be in the top one percentile. Like others, Game theory has evolved in its interpretation and applications. Starting with a basic postulate of ‚Äúhow agents would choose between different options considering the competitor‚Äôs view‚Äù to the Nobel prize-winning ones like ‚ÄúNash equilibrium‚Äù (John Nash ‚Äî 1994) and ‚ÄúStable allocations and practice of market design‚Äù (Shapley and Alvim Roth ‚Äî 2012). From areas like politics, gaming, marketing, philosophy etc. game theory has been instrumental in defining how people make better decisions given the interaction with other cooperative/competitive players. Before we delve further, let‚Äôs understand the problem which Shapley Et al. (called Shapley henceforth) tried to solve using Game theory. In simple words, Shapley devised a unique solution (termed as ‚Äútheorem‚Äù or ‚ÄúShapley theorem‚Äù henceforth) to answer a question ‚Äúgiven a ‚Äòwork‚Äô what the share of rewards/costs are when the associated people have different or similar objectives‚Äù. In other words, what is the marginal contribution of the rewards/costs to the players of a game with certain objectives. Some interesting problem statements which could leverage the theorem are: 1. Airport problem: How much each agent should pay for the common runway? 2. Taxi Fare problem: In case of a shared ride what exactly should be the correct contribution of each passenger? 3. What is the most optimum way to allocate the total team‚Äôs bonus to the team members? :) (If a free-luncher eats your credit, you so badly want this.) The use cases for the theorem are vast. However, what had been fascinating is how this has left an indelible mark even in the space of Artificial Intelligence/Machine learning (AI/ML). From modelling perspective, the Shapley values help in attributing the ‚Äúcontribution‚Äù of each of the features and the ‚Äúdirectionality‚Äù of the feature impacting the dependent variable. For a given value model, Shapley values attributes the reward/cost among the agents per the following equation: Conceptually, Shapley is quite intuitive. For a given model, the marginal contribution of a feature is calculated by: ¬∑ analyzing the difference in the output by including and excluding the feature ¬∑ averaging over all N! possible orderings ¬∑ with all subset of remaining features. All these three components are highlighted in the formula above. Additionally, these Shapley values must have a couple of key properties: 1. Additive: The Shapley values should sum up to 100%. i.e. the contribution is divided amongst all players in a fashion that the sum of total marginal contribution is 100%. 2. Monotonicity: If a particular feature has higher contribution then it should always have higher Shapley value/reward. From a Python perspective, the ‚Äúshap‚Äù package is used to calculate the Shapley values. Furthermore, it provides 3 key plots: 1. Summary Plot: This plot helps to understand a. Relative importance of each feature: For example, in the summary plot below, Feature 5 is the most important, followed by other features in the decreasing order. b. Directionality: Whether the high/low value of the feature, on an average, increases/decreases the model output. For example, for Feature 1 in the plot below, the blue/low values would generally mean negative SHAP value of model output and vice versa for the red/high values. 2. Dependency Plot: This provides a view into the kind of relationship between the SHAP and the value of the variable i.e. whether the relationship is monotonic, U-shape etc. In addition to this, it also provides the variable which is the most correlated with the given feature. In the plot below Feature 15 has the monotonic relation and Feature 1 is strongly correlated with Feature 15. 3. Force Plot: This is very similar to the commonly used waterfall diagram wherein the contribution of each of the features is highlighted which led to change in SHAP value from its base value to the output value. Having covered SHAP in decent detail, let‚Äôs now address the elephant in the room. Credit Risk Analytics (Regulatory), as a domain, has been pretty constrained around the analytics methodologies use. The primary reason being the ‚Äúexplainability of models‚Äù mandated by the financial risk regulations. The idea behind these regulations has been to ensure that the models and the features are clearly understood by the management and no risk drivers are missed from the model features. Understandably, this has been a big impediment to the usage of core AI/ML (black-box) models. If we were to break down, the explainability here would mean: 1. Understanding the features/variables in the model. 2. Understanding the ‚Äúdirectionality‚Äù of the model variables. 3. Understanding the ‚Äúcontribution‚Äù of each of the model variables. 4. Knowing the parameters of the variables (including the activation function). This basically means knowing the equation. For a long period, the researchers could just get the names of the features in the AI/ML models (#1 ‚Äî list above) but what‚Äôs happening with the features inside these black-box models remained veiled. Though Shapley‚Äôs theorem has been there for quite some time, it is only in the last few years wherein Shapley theorem has been used to understand the ‚Äúdirectionality‚Äù (#2 ‚Äî list above) and ‚Äúcontribution‚Äù (#3 ‚Äî list above) of each of the features. Given this, the only part left is not knowing the equation (#4 ‚Äî list above). Though the explainability problem is still not completely solved, from credit risk analytics perspective, leveraging this Shapley theorem and derived Shapley Values could open new avenues: 1. Reliable challenger models (Regulatory Models): Though some banks have been using the AI/ML sophisticated models as challenger models. However, both the modelers and the management couldn‚Äôt sufficiently rely on these models. This is primarily because there has been no surety, given the portfolio, whether, in the model, the features are given right weightages and the signage/directionality of the features are intuitive or not. With the introduction of Shapley Values now the modelers and management could rely more on the models knowing that the right features are given appropriate importance and the directionality of the features are also correct. 2. Mining the right features (Regulatory Models): There are a lot of credit portfolios where feature engineering, to get a reliable model, is difficult and time consuming. In cases like these, modelers can now leverage the AI/ML complex models together with Shapley Values to understand the main drivers of the dependent variable. Once the modeler has better understanding of the main features, their contribution level and their directionality, the same features can be used in non-AI/ML methodology to get more meaningful, reliable and sensitive models. 3. Impact on Non-Regulatory models: Another area in credit risk analytics, where AI/ML is finding its space is around application/behavioral/collection scorecards. Though the financial industry had been using the AI/ML models for these cases for quite some time, however, key concerns around explanability has remained. For example, around application scorecard if the loan is rejected ‚Äî on one end the customer wants to know on what basis the loan is rejected and on the other hand management wants to understand the financial risk around rejection of customers. Since Shapley Values helps to understand the contribution and directionality of each feature ‚Äî it has become much easier for the modelers to rationalize the decision produced by the model. Albeit, there is still more work required to make these black box models completely explainable per regulator‚Äôs expectations; however, Shapley values have surely helped in making significant strides in improving the acceptability of the AI/ML models in risk analytics space. John Nash once said ‚ÄúI can observe the game theory is applied very much in economics. Generally, it would be wise to get into the mathematics as much as seems reasonable because the economists who use more mathematics are somehow more respected than those who use less. That‚Äôs the trend.‚Äù We as risk analytics practitioners, are glad and grateful that the theory found its way to mathematics and consequently created attribute like Shapley values which has potential to create a paradigm shift in the risk analytics space. There are still miles to cover but it doesn‚Äôt hurt to be hopeful!! Disclaimer: The views expressed in this article are opinions of the authors in their personal capacity and not of their respective employers.",69,0,7,Towards Data Science,2021-01-04,2021
https://towardsdatascience.com/normal-distribution-160a93939248,Comprehensive Guide to the Normal Distribution,Drop in for some tips on how this fundamental‚Ä¶,7,23,"['Comprehensive Guide to the Normal Distribution', 'Overview', 'Skewness', 'Kurtosis', 'A caveat about the Normal Distribution', 'Summary', 'More Articles You Might Enjoy']","The distribution of data refers to the way the data is spread out. In this article, we‚Äôll discuss the essential concepts related to the normal distribution: Data distribution is of great importance in statistics because we are pretty much always sampling from a population where the full distribution is unknown. The distribution of our sample may put limitations on the statistical techniques available to us. The normal distribution is a frequently observed continuous probability distribution. When a dataset conforms to the normal distribution, it is possible to utilize many handy techniques to explore the data: In some cases, it‚Äôs beneficial to transform a skewed dataset so that it conforms to the normal distribution, thereby unlocking the use of this set of statistical techniques. This is more likely to be relevant when your data is almost normally distributed except for some distortion. More on this in a moment. Normal distributions have the following features: Here are some terms you should be familiar with relevant to a general overview of the normal distribution: If your dataset does not conform to the normal distribution, here are some suggestions: In the sections that follow, we‚Äôll explore some measures of normality and how you would use them in a Data Science project. Skewness is a measure of asymmetry relative to the mean. Here‚Äôs a graph of a left skewed distribution. üí° I‚Äôve always found this to be a bit counterintuitive, so it‚Äôs worth paying close attention here. This graph has negative skewness. This means that the tail of the distribution is longer on the left. The counterintuitive bit (to me at least) is that most of the data points are clustered to the right. Do not be tempted to confuse with right or positive skewness, which would be represented by this graph‚Äôs mirror image. Understanding skewness is important because it is a key factor in model performance. To measure skewness, use skew from the scipy.stats module. The skewness measure can clue us in to potential deviation in model performance across the feature values. A positively skewed feature, like the second array above, will enable better performance on lower values, given that we‚Äôre providing more data in that range (opposed to higher value outliers). From Greek kurtos, meaning curved, kurtosis is a measure of the tailedness of the distribution. Kurtosis is typically measured relative to 0, the kurtosis value of the normal distribution using Fisher‚Äôs definition. A positive kurtosis value indicates ‚Äúfatter‚Äù tails (i.e., a slimmer bell curve with more outliers). Understanding kurtosis provides a lens to the presence of outliers in a dataset. To measure kurtosis, use kurtosis from the scipy.stats module. A negative kurtosis value indicates data that is more tightly grouped around the mean with fewer outliers. You may have heard that many naturally occurring datasets conform to the normal distribution. This claim has been made for everything from IQ to human heights. While it‚Äôs true that the normal distribution is drawn from observations of nature and does occur frequently, we risk oversimplification by applying this assumption too liberally. The normal model often doesn‚Äôt fit well in the extremes. It often underestimates the probability of rare events. The Black Swan by Nassim Nicholas Taleb gives numerous examples of rare events that were not as rare as a normal distribution would predict. www.johndcook.com In this brief article on the normal distribution, we covered some fundamental concepts, how it is measured, and how it is used. Be careful not to overapply the normal distribution or you risk discounting the likelihood of outliers. Hope this article provided some insight on this commonly observed and highly useful statistical concept. towardsdatascience.com towardsdatascience.com towardsdatascience.com",100,2,5,Towards Data Science,2021-01-04,2021
https://towardsdatascience.com/can-online-courses-make-you-a-data-scientist-4a34ca85fa6f,Online Course won‚Äôt Make You a Data Scientist,There is no shortcut to becoming a data scientist,1,13,['Can Online Courses Make You a Data Scientist?'],"Data Scientist, we all probably already know this one profession. The profession was once dubbed ‚ÄúThe Sexiest Job of the 21st Century‚Äù as reported by the Harvard Business Review. Even though the trend of data scientist tends to decline from previous years, however, a data scientist is still a profession that is in great demand. Currently, there are many curricula in universities, courses, or other places that provide curricula to become a data scientist. One of the tools that are widely used to learn data science is by taking online courses (Udemy, Coursera, Datacamp, etc.). The reason is that the time is relatively short compared to if we have to attend lectures, and we can access it wherever and whenever we want. ‚ÄúCan online courses make you a data scientist?‚Äù Before we answer that question, we must first know what it takes to become a data scientist. Basically, there are two main things needed to become a data scientist, namely hard skills and soft skills. Generally, hard-skills include math, statistics, and programming skills. Of course, this capability is an obligation that a data scientist must-have. However, this is not enough. Besides having to master hard-skills, we also need to have soft-skills. One of the reasons for this is due to the very rapid development of technology where in this day and age there are many tools that can be used by data scientists to process data, do modeling, etc. In addition, this profession also has an important position to determine the business model of a company. The following are soft skills that are generally used to become a data scientist. As I said earlier, to become a data scientist we need 2 main skills, namely hard skills and soft skills. In general, most of the online courses currently available focus more on hard skills, such as how to do data cleaning, classify, cluster, visualize, etc. However, we rarely find other skills (soft-skills) in online courses. Because in online courses we tend to only follow what the instructor explains. Besides that, we can‚Äôt get these skills instantly. That is, to master these skills we have to practice a lot and gain experience to hone our abilities. To hone these skills, it is not enough for us to rely on online courses. We also need to practice by doing real projects. Why? By working on a real project we will practice how to solve a problem. You will realize that the data in the real world is not as beautiful as the data that we process when we take the online course. If you don‚Äôt feel confident or are still having trouble working on a real project, you can ask someone to be a mentor for you or take part in a Bootcamp. That way, someone will give you feedback regarding what you are doing. As information too, if one day you apply for a job as a data scientist, you most likely won‚Äôt be asked about how many courses you have taken or how many certificates you have, but what projects have you worked on and how did you handle the project. Online courses do not make you a data scientist right away. However, online courses can help you get started especially if you don‚Äôt have a background in data. By taking the online course, you can find out what data scientists are doing. However, if you want to become a professional data scientist, you also have to train your soft skills and these cannot be obtained just by taking online courses. You have to train it by working on a real data science project.",15,0,4,Towards Data Science,2021-01-04,2021
https://towardsdatascience.com/using-machine-learning-to-predict-credit-losses-why-current-methods-are-falling-short-e18ac0a191b3,Using Machine Learning to Predict Credit Losses: Why Current Methods are Falling¬†Short,,1,29,['Using Machine Learning to Predict Credit Losses: Why Current Methods are Falling Short'],"‚ÄúAll models are wrong, but some are useful‚Äù The above quote from George Box‚Äôs 1976 paper still rings true 45 years later (Happy New Year!) in many ways. The advances in machine learning over the past decade have brought a lot of promise (and a lot of hype) to helping solve the toughest challenge in business: knowing what is going to happen before it happens. We propose that even if all models are wrong, some are less wrong than others. To provide some basic background on how machine learning works for classification problems, the visual representation below shows how a dataset made up of cases with known outcomes (i.e., each row in the dataset represents a unique loan, with attributes about that loan in each column, and a final column representing whether that loan defaulted or not) can be fed through an algorithm to train (i.e., create) a model: Once the model has been trained on the known historical cases and outcomes (default vs. no default), it can be fed new cases where the outcome is not known; the model makes a prediction about what it expects the outcome to be for each of these new cases, based upon what it learned from the training dataset: Now we have the basics of how machine learning works. It sounds great in theory, but let‚Äôs explore why models like the one we trained above fall short in practice for many applications, including most credit loss modeling. In our data warehouse, we likely store data that represents transactions. For example, we probably have multiple rows of data for each loan: a new row representing the outstanding balance at each month if the loan is paid back monthly. This is in contrast to the structure of the dataset we need to train a model; each row in our training dataset should be a unique case of what we are going to later ask it to make predictions on. If we are building a model to predict whether a banana is ripe or rotten, each row in our training dataset must represent information about a unique banana, and not daily observations of redundant bananas. First, recall that our training dataset was comprised of cases which either defaulted or did not default. Thinking about a loan portfolio, we have some historical data in our data warehouse containing loans that either defaulted or repaid in full on schedule (let‚Äôs ignore the idea of prepayment for now ‚Äî we can discuss that another day). However, a large portion of our data probably represents loans that are still on the books and have not defaulted but also have not reached the end of their loan term yet (their outcomes are unknown). This raises a couple of important questions: If we are not including these ‚Äúactive‚Äù cases in our training dataset, aren‚Äôt we leaving out a lot of important information that we should have told our model? After all, if they are still on the books, they haven‚Äôt defaulted, yet‚Ä¶ Furthermore, knowing that each row in our training dataset has to represent a unique loan: If one of the cases in our training dataset has a known outcome of ‚Äúdefault‚Äù, isn‚Äôt it important to know whether that loan defaulted after 1 month or after 1 year? Surely we would want our model to assess the customer who paid back their 30-year mortgage on time for 28 years and then defaulted differently than the customer who went belly-up after the first few months. Consider a different problem, like whether or not a visitor will click on an ad on our webpage. In this example, there probably isn‚Äôt much of a ‚Äúlifespan‚Äù to consider. They went to our website and in the course of a few seconds, either clicked on the ad or moved on ‚Äî there isn‚Äôt much more to it than that. We can quickly create a training dataset that can be fed into an algorithm and produce a model, as described previously in our Background section. Conversely, a customer defaulting on a loan isn‚Äôt a process that happens in a few seconds; it is almost certainly the result of a series of events that took place over the span of months or years leading up to the default. We can boil down the two questions called out earlier into one general question that encompasses the heart of what we are trying to get at: How do we incorporate more information about the ‚Äúlifespan‚Äù of a case (a loan) into our model? Let‚Äôs look at three different methodologies we can use accomplish this: Survival Analysis is a regression methodology that aims to predict the time until an event will occur. It also allows us to introduce cases where the ‚Äúevent‚Äù hasn‚Äôt happened yet. When training a Survival model, each row in our training dataset would is a unique loan and the columns are specified as follows: The output of a Survival model is different than that of a traditional machine learning model. Instead of outputting it‚Äôs guess at which class (default or non-default) a new loan case belongs to, the Survival model outputs a probability of that new loan case ‚Äúsurviving‚Äù (not defaulting) past a particular number of months. More specifically, when fed a new case, the model returns a probability of surviving past each unique time value found in the ‚ÄòTime (months)‚Äô column in the training dataset. In our mock dataset above ‚Äî ignoring Attribute 1 & 2 ‚Äî our Survival model would tell us that a new loan has a 67% probability of surviving past 13 months (since 2/3 of the loans were observed past 13 months), and a 33% probability of surviving past 27 months (since 1 of the 3 loans ‚Äúsurvived‚Äù past 27 months). Of course, the more complex model would take into account our additional independent variables (such as ‚ÄòAttribute 1‚Äô and ‚ÄòAttribute 2‚Äô) and alter those probabilities. Survival Analysis is an improvement over traditional machine learning because it allows us to incorporate into our model all of that information embedded within cases where we don‚Äôt yet know the outcome. It also allows us to answer very powerful questions like, ‚ÄúWhich loans in this segment of our portfolio have a greater than 50% probability of defaulting before 12 months? How about 24 months?‚Äù. However, Survival models assume that the event (in our case, default) will eventually happen at some point in the future. This doesn‚Äôt entirely line up with our business problem, since many loans will not experience a default (they will be paid in full). Perhaps there is a way to merge some of the methodology from Survival Analysis into traditional machine learning models‚Ä¶ In Survival Analysis, cases with unknown outcomes are referred to as being ‚Äúcensored‚Äù. In the 2016 paper authored by Vock, et al, titled, ‚ÄúAdapting machine learning techniques to censored time-to-event health record data: A general-purpose approach using inverse probability of censoring weighting‚Äù, the authors discuss an approach to introducing Survival methods into any machine learning classification model. The approach they propose calls for using only cases with known outcomes in the training dataset, but weighting each of those cases individually using a technique called Inverse Probability of Censoring Weighting (IPCW). These weights are calculated based on the entire dataset (cases with known outcomes and cases with unknown outcomes). Instead of deriving the probability that an event will occur by time t, IPCW weights are calculated by taking the inverse of that derivation, or the probability that the outcome will still be unknown past time t. Using an IPCW-weighted training dataset to develop a model allows us to utilize all of the latest and greatest machine learning algorithms, while also encoding information from all of the data we have on loans with unknown outcomes. In particular, it helps our model to understand the difference between loans that defaulted shortly into their life and loans that defaulted but were nearly paid off. If you have made it this far, you should see that we are slightly altering the business problem of estimating credit loss in different ways to line up with appropriate modeling approaches. The last methodology we believe lines up well with modeling credit loss is Bayesian Inference. For those not familiar, these models ask for a prior distribution of possible outcomes and associated probabilities, then adds the data (‚Äòevidence‚Äô in the chart below) that has been collected, to create a final posterior distribution. It works a lot like our brain works ‚Äî we typically have some prior belief about the range of possible outcomes and their likelihoods, and we gather additional evidence over time which updates and refines our understanding. Bayesian Inference provides us with two unique advantages in the context of modeling credit loss: Using Bayesian approaches to model credit losses is perhaps best outlined in Kwon‚Äôs 2012 paper, ‚ÄúThree Essays on Credit Risk Models and Their Bayesian Estimation‚Äù. Although the promise around using machine learning to help improve credit loss prediction is justified, doing so requires ensuring that you are setting up your data and model appropriately. It can be tempting to plug your data directly into a machine learning algorithm too quickly, so consider using some methods from Survival Analysis ‚Äî including IPCW ‚Äî or Bayesian Inference to ensure you are building the best model with the information you have. If you want to learn more about how to improve your credit loss estimation models, don‚Äôt hesitate to get in touch with us at Ketchbrook Analytics.",6,0,8,Towards Data Science,2021-01-04,2021
https://towardsdatascience.com/creating-apa-style-plots-in-python-e8adddc3421,Creating APA style plots in Python with minimal¬†code,The final step in a fully Python-based psychology‚Ä¶,1,25,['Creating APA style plots in Python with minimal code'],"APA is the magic word, and mostly likely the reason you‚Äôre reading this article. Like many psychology students, I learned how to collect, analyze, and interpret data. But that‚Äôs not all. You probably feel me if you‚Äôve gone through one of these tutorials, where psychologists-to-be learn to master the art of citing research papers, placing Oxford commas, and everything else that comprises a good psychology paper. (For more on general APA format: APA.org.) An education in psychology (and other experimental sciences) equips students with the capability to interpret and present data in the best (read: clearest) way possible. Here‚Äôs a counterexample: By omitting the axis values and subtly manipulating the axis starting point, the unsuspecting reader can be tricked into thinking that the competitor is failing. While this seems like a highly contrived example, real cases abound in media. When you‚Äôve spent years learning how to design unbiased experiments, collect appropriate data, and conduct unbiased analyses, the last thing you‚Äôd want is to present the results in a way that distracts ‚Äî or worse, deceives ‚Äî your reader. Data visualization in research is not just about having pretty plots, nor is it meant to sway opinion. Hence, the APA guidelines are in place to promote unbiased presentation of data that delivers the true message. Given the emphasis on style and format, it‚Äôs not surprising that basic data visualization (think auto-generated graphs in Excel) is frowned upon. The clarity and the aesthetics ‚Äî the style ‚Äî have to be commensurate with the hard-earned data. That‚Äôs where Python and R come into the picture. If you can use R and the ggplot2 package, it shouldn‚Äôt be a problem to design a figure with whatever aesthetic customization you desire. But what if you could do the same with Python? After all, more and more experiments are being designed and run in Python (see e.g., psychopy and pygame), instead of using proprietary experiment software. More statistical analyses are provided by open source packages. Imagine having a full pipeline where you can collect, analyze, and visualize data, all within a single tool. Amazing, isn‚Äôt it? Plus, it opens up new career opportunities ;) A few advantages of using Python and R code, over software that let you create figures using a simple graphic user interface, include: Beautiful plots that can communicate and highlight your results, done within few lines of code. After you create a custom ‚ÄúAPA style‚Äù style sheet, the styling will be done in the background. Here‚Äôs an example: Comparing to a plot made with the same code, but without using a custom style sheet: There are quite a few style sheets that come with matplotlib. You can take a look at them all here or test them out yourself: While the out-of-the-bag styles are pretty indeed, most of these make use of colors that aren‚Äôt quite accessible, with the exceptions of ‚Äúgrayscale‚Äù, ‚Äúseaborn-colorblind‚Äù, and ‚Äútableau-colorblind10‚Äù. But we can do even better. It is recommended to ‚Äúuse a pattern in combination with color so that the differentiation of elements does not rely on color alone.‚Äù It also doesn‚Äôt hurt to add your personal touch while making the plots more publishable. Here‚Äôs how it might look, with slightly more specification: For messier plots in neuropsychology: All the data and labels have been obfuscated on purpose. The matplotlib documentation explains it all. Here are the basic steps: Some ideas: Another useful tool is the cycler , which allows full control over the line type and color, so you can have a solid black line, a dashed black line, etc., until the cycle runs out: I‚Äôve placed my final apa.mplstyle file here, which is by no means perfect or complete (I made it on the fly when I had to make dozens of paper figures over the holidays), but it could be a good start (and please let me know about your improvement ideas!) Hopefully this helps make it a little easier to incorporate Python into your research pipeline, and I wish you‚Äôll enjoy working with data as much as I do.",73,2,6,Towards Data Science,2021-01-04,2021
https://towardsdatascience.com/minimal-sql-for-a-2021-data-scientist-fc3c17f1434c,Minimal SQL for a 2021 Data Scientist,Basic must-know SQL for aspiring Data Scientists or¬†Analysts,12,42,"['Minimal SQL for a 2021 Data Scientist', '1) Where to type and execute the SQL queries?', '2) Creating a database and table and storing data in it', '3) Accessing/querying data from a table', '4) Adding a new column in the query output', '5) Filtering the data', '6) Counting the number of rows', '7) Grouping the data', '8) CASE statement', '9) Joining tables', '10) Nested queries', 'Conclusion']","Every now and then, I see a new Medium article saying ‚Äú7 must-have skills‚Äù, ‚Äú10 important skills‚Äù, ‚Äútop 3 skills‚Äù, etc. for a data scientist. All such posts acknowledge SQL as a must-know skillset for data scientists. Coming from a computational physics background, SQL was foreign to me until I moved to the industry. In academics, you don‚Äôt use it much. However, for most companies, the data is often stored in relational databases. SQL is a query language (Structured Query Language) used to communicate (fetch, store, manipulate, etc.) with a database. You can think of SQL as the lingua franca of data scientists to communicate with data. The fact is that it is equally easy to learn as it is essential as a skill. This post tells you the bare essential SQL you need to know to get started as a data analyst/scientist. NOTE: This post is not an exhaustive, comprehensive course on SQL. Of course, to be proficient at your work, you will learn and improve your SQL skills day-by-day. Often, the realistic databases (one database has several tables) are huge; a single table might contain even over 1 billion rows of data. However, to work with such colossal data, you just need to know SQL building blocks. So, I will start by creating a simple database that has a very small table containing just 6 rows. The same concepts can be used on your real dataset with millions or billions of entries. You can practice all the queries/commands you will read here on your PC in two steps: 1) installing the open-source MySQL community server from here, 2) having installed the server, install the MySQL workbench (editor) to run the SQL queries from here. Just keep following the installation instructions. The data is stored in a database that can have several tables. Imagine a city that has several schools, where each school represents a database, and the different classes/grades in it represent the different tables in a database (school). In simple terms, a company is like a city, having different databases. Let‚Äôs create a simple database (synonymous to using Schema in MySQL) called SAMPLE. The following piece of code first checks if a database named SAMPLE exists or not. If it does, it will be dropped (deleted). Then you create the database. You can also write DROP SCHEMA instead of DROP DATABASE. Next, we create a table named Class in the SAMPLE database, and therefore, you have to specify ‚ÄúSAMPLE.class‚Äù while creating the table. Before creating the table, you again check if the table exists or not. A table can have several columns of either the same or different data types. While creating the table, you have to specify the column names and their respective data types. The table is now created. Now you can insert entries (your data in the form of rows) in it. The VALUES that represent your data should be in the same order as the column order specified while creating the table. NOTE: SQL is case insensitive. It is customary to write keywords in BLOCK letters but it is not necessary. Congrats! You have created your very first database and a table. All the individual lines of code you saw above are called queries. You are querying the database and the table for certain data/information. You can display the contents of the table using the ‚ÄúSELECT *‚Äù command. The asterisk (*) here represents all the columns. The following image shows the table contents created above. If you want to query (select/display) only certain columns, you can specify the required column names separated by a comma as shown below. Note that the data is printed (queried) in the same order as you inserted it. To print it in a certain order, specify the fields, you want to order with, after the ORDER BY keyword. You can specify multiple column names to order by more than one field. For e.g., the following query shows the data printed in the ascending order by the student name (Student) and then their grades. By default, the data is sorted in ascending order. To sort in descending order, specify the keyword desc after the field, you want to sort. For example, ORDER BY Student desc, Grade desc. If you want to print additional column/s based on the existing ones, specify them after the asterisk (*) separated by a comma. For e.g., the following code adds a column dividing grades by 100. If you don‚Äôt specify the output column name (by usingAS 'Grade_new'), it will be left blank. You can filter the data that satisfies certain conditions using the WHERE keyword. For e.g., the following query fetches the information of students who scored more than 85 marks. You can also filter query results using multiple fields via theAND keyword. For e.g., try usingWHERE Grade > 85 AND Course = 'Math'. Note that you can also order the filtered result by using the ORDER BY keyword at the end. You cannot specify the ORDER BY before the WHERE keyword due to its precedence. To get the total number of rows in the table, you can use SELECT count(*) FROM SAMPLE.class. If you want to count the rows where the grade is more than 85, you can add the WHERE keyword. Try specifying multiple conditions using the AND keyword as shown previously. Sometimes, for some statistical analysis, you need to group the data based on some fields. This is done using the GROUP BY keyword. It is one of the most frequently used SQL keywords. Of course, when you are grouping, you are interested in knowing some aggregated value, e.g. average, minimum, maximum, standard deviation, etc. of a group. The following query returns the student names and their average marks. Suppose you only want to show or query the students whose average grades (calculated above) are more than 81. The HAVING keyword comes in handy here. The below query shows how to filter results using the average grades. You can also combine multiple filters (conditions) in HAVING keyword. For e.g., try using HAVING AVG(Grade) > 80 and AVG(GRADE) < 88. The CASE keyword is a very helpful one. It is used in if-else situations where you have to assign one value if a condition is true and something else if the condition is false (you can also have multiple else situations). The following is the syntax of the CASE statement inside the SELECT statement. The following illustrates the use of the CASE keyword for the Class table. The following shows the usage of CASE along with the GROUP BY keyword. This is perhaps the most widely used operation in real-world SQL applications. The idea is to join a table with one or more tables to get additional information from the latter. There are several ways of joining two tables. For e.g., you can have all the merged data from both tables, the common data in both tables, the data that is in one table and not in the other, or vice versa. This page explains all the joins using simple examples. I will show two such join operations named LEFT JOIN and INNER JOIN. So far, we have one table named Class in our database. We create a second table Address, containing students‚Äô names, cities, and states in the database. Let us look at the contents of the Address table. In my experience, the left join is the most often used operation. The figure below explains the gist of LEFT JOIN. This operation returns all rows from the left table and the corresponding matching entries from the right table. Suppose we want to query the cities and states of our students in addition to their course and grades. Now, our left table (main table) is Class. To fetch the cities and states from the right table (Address), we need a common column to make a one-to-one match. In both our tables, Student is the common column. The following code shows how to use LEFT JOIN in order to join the Class table to the Address table ON the Student column. As you can notice, how the City and State fields for Dave are NULL. This is because the right table (Address), does not contain data for Dave, and hence, the left join could not find any matching entry. We use aliases for the table names, using the ‚Äúas‚Äù keyword, to refer to the columns (l for the left and r for the right here). You can also use filtering while left joining using the WHERE keyword. Compare the below output with the above without the WHERE keyword. Above, you saw that the left join query produced rows for Dave too although there was no address for Dave in the right (Address) table. If you want to exclude such NULL (not available) entries, you can use an INNER JOIN, explained in the following sketch. This operation returns entries (rows) that have matching values in both tables. The syntax is the same as the left join except that now you use the keyword INNER JOIN. As you can notice, the inner join does not result in Dave‚Äôs data because it is not common to both tables. I refer readers to visit this post for a review on all SQL joins. The simple examples so far allowed you to filter results using WHERE and HAVING keywords together with GROUP BY. In the above output of the inner join (or the left join), note that the cities and states are repeated because each student has multiple entries in the table. Suppose that you want their average grades along with their addresses (city + state). You need to 1) compute the average using GROUP BY, and 2) get the address using LEFT JOIN. The following code contains an inner query that calculates the average. The output of the inner query serves as a table for the LEFT JOIN. The inner query output is aliased as l. Altogether, the two queries form a nested query. The SQL basics presented in this post form the bare minimum basics one needs to start working with data in their roles as data scientists or analysts or in ETL-related positions. The real tables might contain even several tens or hundreds of columns and millions or billions of rows, with the SELECT queries spanning a large number of rows. However, the knowledge gained here will help you to understand even the complicated SQL queries. Since python programming language is another most-sought skillset for data scientists, you might also refer to my following posts on python: towardsdatascience.com towardsdatascience.com",134,1,9,Towards Data Science,2021-01-04,2021
https://towardsdatascience.com/data-science-needs-refactoring-a-tutorial-case-study-tips-tricks-8fff8f38acbb,"Data Science Needs Refactoring, A Tutorial, Case Study, Tips &¬†Tricks",,9,55,"['Data Science Needs Refactoring, A Tutorial, Case Study, Tips & Tricks', 'What is Refactoring', 'Introduction', 'Limits', 'The Ball Game', 'Break the Problem Down', 'Refactoring Tips & Tricks', 'Further Blogs', 'Smarter People Than Me & Extra Resources']","If you introduce poorly written code and don‚Äôt refactor your work, whether you realise it or not you are invisibly slowing yourself down and making your life harder. Refactoring is fun, it makes your code faster, it makes you faster at writing code, you write more maintainable code, and it helps you write better code in the first place once you‚Äôve had to clean up after yourself. Tech debt is invisible glue and as you add more of it to your code base with each unchecked commit, your progress will eventually grind to a sticky halt. It happens every single time, no matter how much you try to outrun it. Learning to refactor starts with learning how to re-write existing code without changing the outputs but improving its legibility, performance, simplicity, maintainability or any combination or all of the above. The difference between ‚Äúpoorly written once and never again touched code‚Äù and even code that is just refactored once changes your software from about a 2 to a 7 in quality and ease of extension. Everyone (including yourself in a few days when you encounter your own previously written code) will love you for it. If you plan on working on a piece of software past a couple of days, refactoring needs to be part of your story. If you‚Äôre working on a script for a couple of hours and will throw it away after, that‚Äôs a caveat to this and feel emboldened to spaghetti code and live freely. It might be a fun exercise to look at a small script you‚Äôve previously written and take some of these refactoring tips for a spin! Lets get started‚Ä¶ People smarter than myself have already defined this for us, see Martin Fowler‚Äôs wise words below. ‚ÄúA disciplined technique for restructuring an existing body of code, altering its internal structure without changing its external behavior.‚Äù Refactoring is an unloved habit that pays dividends forever and I wish there were more data science investors who incorporated it into their programming portfolio of habits. It‚Äôs a staple of any good software engineer‚Äôs habits but it doesn‚Äôt pop up all that often in the data practitioner world and I want to change that, starting with this blog. The other week at work, by doing some simple refactoring which I‚Äôll walk you through below, there was a 113x performance improvement which changed a data pipeline processing time from ~84 hours down to 45 minutes. This isn‚Äôt a claim on how much faster your code will become, it was merely the experience I had with little effort that I feel is a story worth sharing. This was also done with simple work that took an hour or so. There is still has room for improvement, there‚Äôs no telling how much faster it could be done. The impact on your code from refactoring might be many more orders of magnitude an improvement for your code or you might not have a performance problem, only a headache problem when reading what you wrote last week. Refactoring generally helps both. I want to note that this is a Frankenstein blog of a poor mans version of Raymond Hettingers talk (Link at the bottom), a case study, a tutorial and my rambling thoughts. Raymond‚Äôs talk changed my experience of refactoring and problem solving from a chore to a love. I have noted his talk down in my ‚ÄúPeople Much Smarter Than Me‚Äù section at the end of this page. Please go watch that and follow him on twitter, he‚Äôs part of the core python developer team and he puts out lots of wisdom and love in this planet and he needs to be protected at all costs. My claim of 113x improvement in speed is related to an experience I had rewriting a function for a colleague at work the other week and of course has no bearing on how amazing refactoring will be for your situation. I‚Äôm also writing everything in python and so my styling, behaviors and recommendations are biased to python and pandas. Drawing inspiration from Jeremy Howard and Rachel Thomas and their teaching style, instead of starting from foundations and building up, its important to see the end result first so that there‚Äôs an obvious outcome to work towards. Show me the whole match of AFL before teaching me how to kick a football so to speak. We can then break down the thinking, steps, and work in between to get there and why this is important along the way. So our situation involves a sample of two data frames: So for each observation of each feature in the main dataset, there will be a set of entries in the reference dataset containing bins to put those main dataset values into and a corresponding special value for that bin. Our goal is to connect each main dataset entry to a bin which is then connected to our special value and then have that all available back in the main dataset. Note that we‚Äôve only shown the solution for a single variable but solving for 1 feature solves for n features by wrapping our solution into a loop etc. Below is the end result we‚Äôre looking for Now that we have our stated end goal, lets now look at the code‚Ä¶ See the below 3 gists, the first being the setup dummy dataframes, the second being the old code that took ~85 hours to run on the full dataset and the third gist being a refactored rewrite that produces the same output. The second gist was used with approximately 3,000 features and a few million rows took ~85 hours. This is in place of the sample we‚Äôre working with. We will be working with a mock sample but the changes are still the same. I‚Äôve also included some comments as to the sins I don‚Äôt like and reasons why I think the refactored lines are nicer to manage. We‚Äôre going to solve the problem for the simplest situation (one feature) of which could then be looped/extended for as many features as exist in the main dataset. I find it effective to solve the simplest and smallest directional leap you can when working through a problem and then slowly extending and iterating to your result. The above is fairly self explanatory, you can see the reference DataFrame as ref_df and the main DataFrame as main_df which we had screenshot examples of above. This is our scary original method that I worked through the other week. Take a moment and think if you can understand it or how the authors have tried to achieve our stated goal. Don‚Äôt worry if it feels awkward or if it's hard to understand, it is and it is. We will rewrite this problem from scratch as well as talk about ways we could modify this code in place if we were stuck with the method in its current form. You‚Äôll notice I‚Äôve written sins in as comments to the right of the code. This is the final 10 liner that achieves the same stated goal. I‚Äôve included some of the behaviors that I prefer when comparing this gist to the old code gist as comments on the right of the lines above. Also, check out this colab notebook if you‚Äôd like to see the thinking behind these 10 lines. See below, I‚Äôve also included it at my resources at the end. colab.research.google.com So how do we go from the second gist to the third?! This seems like a big leap at first glance and it is without the story in between but I promise there‚Äôs simplicity in the magic. All programs and problems can generally be broken down into inputs, the processing of those inputs and some outputs. Setting the stage at the beginning of your problem solving and refactoring can be a fantastic way to clear your mind and distill the problem at hand. Lets quickly do that in our case study as this will reveal the steps we take in our few lines of code. Once you break a problem down into the simple inputs, outputs and steps in between, you can progressively work through each step and arrive at your destination. Trying to do everything at once is too hard and you don‚Äôt have the mental registers to do it anyway (hint: check out Raymond‚Äôs wonderful talk, he talks about mental registers and your feeble human limits of thought) This feels a lot more manageable than trying to eat up the original method presented to us. I can definitely set out the problem in my head much clearer with the above to reference rather than trying to interpret the original method and implementation. If you‚Äôre feeling game, get started unit testing and again distilling your code to further improve what you‚Äôve re-written. Doing this over and over to distill and refine and distill your code is refactoring in essence. As long as your improving the ease of reading, the performance, the simplicity or ease of use then you‚Äôre on the right path. Code is read more often than it's written, so optimize for the reader and not you, the writer. Be empathetic as often it‚Äôll be you reading it in a week but you‚Äôve forgotten that you‚Äôve written it in the first place. The best place to start is to make small but directional improvements to the code as is. This could be for any benefit such as legibility, simplicity, brevity, using libraries instead of handwriting a process, etc. Lets look at a small example below. Look at lines 1‚Äì5 and 7‚Äì11 which are the original and refactored versions of the same snippet from the original large method. I‚Äôll quickly list the changes below: These are nice for the following respective reasons: So I know the above is a bit of a spot the difference but there are some subtle improvements. Making small adjustments like this may not feel magical or powerful but when you iterate and iterate and iterate over the same piece and distill it down to something concentrated, its often quite nice code! We‚Äôre not there yet with the example above but I wanted to show a small slice. This is an example of a complete rewrite where non of the old code has been taken into account and a totally fresh solution has been built by breaking the problem down and going back to basics. Sometimes if your working with a problem you are confident in solving, its quicker to demolish what was there and start fresh. Be careful as this often isn‚Äôt the best path, usually someone has thought long and hard about what they‚Äôve written and its worth taking time to understand who has come before you and their lines of thinking. However in this scenario there are too many garden paths to go down and too much complexity for a problem that when you take a step back, is actually quite simple and would benefit from a rewrite. Go look at the colab notebook for how I‚Äôve worked through this solution and produced the second gist. Continue reading if you‚Äôd like to see how I‚Äôd modify the original method in place if I wasn‚Äôt game enough to write it from scratch and was trying to make directional improvement in the code quality. Referencing the original method, lets look at the small sins that when summed together, makes the code difficult to understand, slow and hard to change or maintain. I‚Äôll also post what to do with each of these problems. I‚Äôve popped in a screenshot followed by the few sins I spot. I‚Äôd love feedback and comments on what was enjoyable or not useful in this blog as to help drive further writing. I‚Äôm thinking of doing a small tutorial on unit testing which makes refactoring even more enjoyable. You have a test harness to cover your butt and you have provable evidence that given the same inputs, you get the same outputs. You can edit and refactor to your hearts content with much higher confidence that you haven‚Äôt introduced strange behavior. I‚Äôm open to any line of thinking that may have been brought up from this piece, any comments are welcome and encouraged. Know that this is only one way of solving the problem, feel free to write your own solution to this problem or re-write a solution to another problem you have. My solution even has room to be refactored and reduced. Refactoring is never finished and there‚Äôs always ways to simplify, distill, improve, and refine your work! Special Thanks to ColJung for encouraging me to start writing. I always appreciate his guidance, encouragement, questions and comradery. I highly recommend his writing, it‚Äôs wonderful stuff and is in large part the inspiration for me starting a blog. Here‚Äôs a full running notebook you can reference if you‚Äôd like to see the above gists running with some extra commentary: colab.research.google.com This is Collin, the planet brained colleague I love working with who I will likely forever pester for knowledge and company col-jung.medium.com This is Raymond‚Äôs Talk that made me love refactoring & made me think differently about solving problems. This is Joel, he‚Äôs a fantastic writer and his top blogs such as the one on encoding are brilliant. www.joelonsoftware.com Martin Fowler is another great writer that I thoroughly enjoy who‚Äôs written about refactoring extensively martinfowler.com Here is his page on refactoring! refactoring.com Jeremy Howard & Rachel Thomas are my two heroes, Rachel in the fast.ai deep learning for coders course encourages everyone to start a blog and begin writing so I also partly attribute the start of this blog to her. I want to share them to anyone and everyone as they are brilliant. They made fast.ai among other things and its fantastic. For all things ethical and practical neural net related, go there and gorge on the brilliant content they‚Äôve produced. www.fast.ai",,0,16,Towards Data Science,2021-01-04,2021
https://towardsdatascience.com/creative-report-designed-only-with-matplotlib-without-office-software-9d8b5af4f9c2,"Creative report designed only with Matplotlib, without office¬†software",,4,14,"['Creative report designed only with Matplotlib, without office software', '1. Short introduction of data and charts', '2. Design layout and basic setting with Matplotlib', '3. Summary']","In this article, our aim is to create an intuitive report with Matplotlib. Attention: during the work, none of the office software will be demanded. What we only use, is Python. A series of meaningful data plots works usually as ingredients in a report. As spoken, a good chart is better than a speech. If we have obtained these precious ingredients in advance, the last step is how to place them in a proper way. Of course, we can use Microsoft Word, Powerpoint, etc. But not necessary. Here I present my recipe with Matplotlib. Based on the excel file, we have gotten some representative charts (bar plot and donut plot). If you don't know how to handle this, please check my previous stories. The links are given, please check them one after one if you feel the tasks not easy or new for you. Therewith, we have gathered some charts consisting of one bar plot and nine donut plots. If you enter ‚Äúplt.show()‚Äù now, the figure looks like below Next, we should add content (title, charts, etc.) to the blank paper. Now, the first page of the report is finished. Similarly, we continue the style on the second page, which should be filled with detailed donut plots for each year. To make the content clear, we have to arrange the plots neatly. Therewith, the second page is well decorated with nine donut plots. Additionally, what we can improve is to draw a conclusion of these plots, which I didn't have time to write within the report. For example, due to the covid-19 pandemic, the total sales in 2020 are dramatically falling. In this short article, the approach to generate a report is explained in detail, which doesn‚Äôt need Word or Powerpoint. Our report created by Matplotlib presents relevant information directly obtained by data processing and analysis, which means if you are used to processing data in Python, there is no need to switch apps or platforms. The code is submitted in Github, which you can download and review.",89,0,4,Towards Data Science,2021-01-04,2021
https://towardsdatascience.com/the-six-traps-in-data-analysis-and-how-to-escape-them-ea9d2e3bad46,The six traps in data analysis and how to escape¬†them,,4,63,"['The six traps in data analysis and how to escape them', 'The trap of each step!', 'The Steps', 'Conclusion']","Anyone who works with data analysis knows the importance of following a process to conduct their work. This way, we guarantee we do not skip important steps, we can have a clear notion of the timeline ‚Äî where we started, where we want to go ‚Äî and what were the challenges found along the way. A defined process also helps to report the current status of the analysis, and it helps us organize ourselves to deliver a better quality analysis. We have several references in literature we can use. The Women In Data page published a pyramid that shows several interesting points in a process of analysis. There are six steps: Frame the Problem, Collect the raw needed for your problem, Process the data for analysis, Explore the data, Perform the in-depth analysis, and Communicate results of the analysis. Each step has its purpose and plays a key role in generating valuable information. In this article, we‚Äôll cover the steps that data analysis typically follows, as well as the traps that come along and how to escape them‚Ä¶ In the image below we can have an initial idea of this relationship between steps and traps: As we can see, not all steps contain traps and some have more than one. Now that you have an overview of the data analysis process, it‚Äôs time to go deeper into each step and understand each one! Step 01: Frame the Problem Here is the key to all the questions that will come after starting the analysis. When I start an analysis, I like to think that to facilitate the understanding of the work we must first list two points: 1- What questions do we want to answer? At the end of our analysis, what questions do we want to have answers to? This directs the next steps very well. Based on this, we will know better which strategy to adopt for our analysis. At this moment it is important to talk and align with those involved, what were the questions that resulted in this request for analysis, how the demand was born, etc. Besides, it makes us wonder if our time spent will generate value. 2- What are the actionable insights with the conclusion of the analysis? Depending on the answer to the questions we want to answer, what do you want to do with that information? What decision making do you want? Without these answers, you can fall into the trap of ‚ÄúI don‚Äôt know which problem to solve‚Äù and start analyzing without direction. Also, given the risks of bad communication, the analysis conclusion can differ from what was expected‚Ä¶ These answers will also greatly guide the choice of data, assumptions that will be used, which sampling method to choose, and also question the problem: Are we working on the right problem? Are there other perspectives to consider? Therefore, spend the necessary time in this step, because the way we identify the problem will determine the context, objective, significance, and scope of the analysis. The important thing is to keep in mind the answers to the questions mentioned, and when new doubts arise, we will be able to summarize the objective of the analysis and reinforce the decisions you have taken. Also, bear in mind that it‚Äôs possible that new questions might come across and that these questions will complement the scope and strengthen the premises and methodologies adopted. Step 02: Collect the raw needed for your problem. In this step, we need to collect data from the sources to answer the questions raised. A good exercise that I do before extracting data or creating a database is to draw the final output I believe will answer these questions, whether on paper or even excel. This way, we can visualize repeated or unnecessary data that can hinder their analysis, either in the extraction performance or confusing the public. And here the trap ‚ÄúBring data to solve all problems in the universe‚Äù can come across. Sometimes, when we start extracting data, it is common to keep excess data with thoughts like: ‚ÄúWhat if they ask for that data? Better to leave it here in the database .. ‚Äù, and suddenly we have a lot of data that doesn‚Äôt help to answer the question and it give us more work because one thing is a fact for those working with data analysis is: If you are going to work with that data, it is your responsibility to ensure quality. Why are we going to waste time doing quality analysis of data that won‚Äôt help? Step 03: Process the data for analysis In this step, we will understand the extracted, do data quality, and clean up the data. It seems like a simple step, but the data is hardly consistent. On the contrary, a lot of data has different formatting than what you need: fields are null or different than expected, different categories from what the team thought they would have, and so on. It is part of our job to raise these errors and, more than that, share them with those responsible to ensure that it is visible to the company. Thus, it is easier to prioritize the delivery of this correction, other areas are also aware and the company is more confident that it is not using the wrong data. Step 04: Explore the data With the data extracted and validated it is time to enter the world of exploration. At this stage, we will most likely want to understand the relationship between two variables, analyze their behavior in different data clusters, compare different scenarios, etc. When you start to share the first insights into your analysis, many other questions arise: - ‚ÄúWhy don‚Äôt you analyze this variable too? ‚Äú - ‚ÄúWhy don‚Äôt you simulate a result with this premise? - ‚ÄúWhy don‚Äôt you change the date range? ‚Äú And so on ‚Ä¶ and in those moments we can enter the so-called ‚ÄúDown the Rabbit Hole‚Äù, inspired by the novel ‚ÄúAlice in Wonderland‚Äù. In the novel, Alice falls into a rabbit hole that transports her to a fantastic place populated by peculiar creatures, revealing a logic of the absurd, in the end when she is attacked by the Queen‚Äôs soldiers, Alice wakes up, discovering that the whole trip was a dream‚Ä¶ What do you mean? That sometimes we enter a cycle of analysis and more analysis, which lasts a long time without realizing it. Naturally, curiosities arise with the results, but we must always remember the question: Will this additional analysis help us answer the main questions? If so, spend some time, but give yourself considerable time to not get stuck. If not, write it down on your to-do list. Another common trap: ‚ÄúBad Moment‚Äù. It‚Äôs about suffering from these various questions, start to doubt the objective of the analysis, if you are following the right path ‚Ä¶ This is quite common, and you often end up suffering alone. At these times, it is very important to remain calm and share these pains and insights with the team. They will help you to validate, raise new hypotheses, and reinforce the focus of the analysis. Take advantage of all available channels of communication for this: meetings, e-mails, etc. One more trap: ‚ÄúI have no results‚Äù. Usually, a data analysis task takes a long time, especially when we are dealing with data with errors, or different business rules, and it often hits a feeling that we are not delivering. However, it is necessary to remember that identifying errors in data or even doubts regarding graphics are considered deliveries as well. We don‚Äôt need to conclude the analysis just to have this feeling of closing. Step 05: Perform in-depth analysis It is important to go into detail in the analysis, both to validate the data and to support the results and conclusion. This avoids biasing your analysis.You need to understand what are the variables that reflect different behaviors in the data and analyze the information at that level. For example, in an analysis of real estate sales, the value of a property can vary a lot by state, city, and neighborhood level, so it‚Äôs essential to factor each of these in the analysis. Step 06: Communicate results of the analysis No analysis is completed if there is no communication of the results about the questions and the insights. This gives you a feeling of closure, and here‚Äôs the last trap: ‚ÄúDon‚Äôt believe in yourself‚Äù. Don‚Äôt be ashamed of presenting results. If you have gone a long way in your analysis and discussed the methodologies and strategy adopted, you will certainly be prepared for any questions that may appear. If there are any questions for which you do not know the answer, it is okay to say that you do not know and you will research. Recognizing that you don‚Äôt have all the answers is part of your maturity as a professional. Everything I‚Äôve said so far was based on my experience, the lessons I learned from each analysis I did, and how I deal with each trap today. I believe that whatever your job is, should be encouraging, something that always motivates you, so you must overcome these moments that can hold you or even make you give up. There will always be challenges to overcome, and it‚Äôs okay! Stop and reflect: How did you overcome? Next time you will already know how to escape. Did you have any questions, do you remember any other trap or do you have any suggestions? Post in the comments! Thanks to Manu and Madu!",44,0,7,Towards Data Science,2021-01-04,2021
https://towardsdatascience.com/sport-analytics-nba-2k-ratings-prediction-b7b72e2e72eb,Sport Analytics‚Ää‚Äî‚ÄäNBA 2K ratings prediction,"As a Data Scientist passionate about sport, and especially‚Ä¶",1,32,['Sport Analytics ‚Äî NBA 2K ratings prediction'],"As a Data Scientist passionate about sport, especially basketball, I am writing a series of articles related to Sport Analytics. It will go from classic regression or classification problems to more advanced computer vision applications. NBA 2K is a series of basketball sport simulation video games developed since 1999 with annual release. The latest edition is 2K21, issued on September 4, 2020. The video games are now published by 2K Sports. In each release, all active players in the NBA and some legends are individually rated on a 99-point scale. Those ratings always lead to discussion, debate, reactions‚Ä¶ even from the players themselves. The objective of this post is to build a model to predict this 99-point scale rating for each player, using features related to the player itself and its game statistics from the previous year. As we try to predict a number, it is a regression problem. When it comes to NBA data, a large variety of open data exists, the statistics are widely used by the league itself. Data is part of the NBA culture. NBA 2K ratings are available at https://hoopshype.com/nba2k/ from 2K14 to 2K21. These 8 years would be our project scope. We observe that the distribution of the ratings are pretty similar since 2015 with a median in the range [74.5 ; 76.9] and rather constant quartiles. In 2K14, the scores were in a broader space with a lower median (71.4). This evolution may be due to a change in the rating strategy by 2K Sports. It is an important information to note as it could introduce bias and worsen model performance. Regarding NBA player statistics, the data can be retrieved on https://www.basketball-reference.com/leagues. We will only scrap the seasons 2013‚Äì2014 to 2020‚Äì2021 in order to be aligned with 2K data period. Many views are available on the website, here we focus only on Totals and Advanced tabs. Type of features in the dataset: To enrich our dataset, we will perform feature engineering by combining variables especially with normalization by minutes and games played as well as position and team one-hot encoding. The dataset is split in training/test/validation with time dependance to avoid leakage in the model. Note that we removed 2014 as the distribution of the rating was too different from following years. Many algorithms can be used to perform regression: from basic Linear Regression to advanced tree methods. Comparing different algorithm performances, we will choose XGBoost and do hyper-parameters tuning with Grid Search. On the validation set (that corresponds to 2K21 ratings), we achieve a MSE of 1.75 and a MAE of 1.33 with the following residuals: The color of the dot represents the number of game played. We observe that the main outliers are players that did not play enough game during the previous season (e.g Stephen Curry in season 2019/20). In that case, the statistics may be hardly representative of the real player value, this bias is corrected by 2K. Now that we have a model with a reasonable performance, we can open the black box and move to interpretable Machine Learning. The objective is to analyze what is behind the model and which are the features that have the higher impact on the rating prediction. In other words, we can highlight which player attributes will affect the most (either in a positive or negative way) their 2K rating. The goal of SHAP (SHapley Additive exPlanations) is to explain the prediction of an instance x by computing the contribution of each feature to the prediction. Interpretable Machine Learning Using SHAP library, we can plot the top 10 most important features (left) and the combination of feature importance with feature effects (right). The 2 top features reflects a player importance within his teams with his average play time and the percentage of games in the starting five. We also notice that the general team performance (% of win share) have a positive influence on the individual player ratings. Any position (guard, center‚Ä¶) gives an advantage for higher ratings and it doesn‚Äôt seem either that the model favors a specific teams (the first franchise that appears in the feature importance plot is Golden State Warriors in 27th position). Finally, we can explore SHAP explanation force plots for two specifics player to visualize which features had a positive impact (= increase the rating in red) and a negative impact (= decrease the rating in blue). Conclusion Using player statistics, I built a XGBoost model that predicts 2K ratings with a great performance. This shows that 2K notation is globally impartial and honestly represent the reality on the court. Some exceptions can be found, when a player has been injured during the season for example. Manipulating basketball data was a lot of fun and the model interpretation is really interesting to have a proper understanding of each prediction and to deep dive in player attributes. 2K rating prediction is a very specific use case but Sport Analytics is an infinite playground with a lot of under-exploited data that I will continue to explore in my Data Science journey. All the code is available in Github. [1] NBA 2K, Wikipedia (2020) [2] Christoph Molnar, Interpretable Machine Learning (2021)",18,0,5,Towards Data Science,2021-01-04,2021
https://towardsdatascience.com/top-3-challenges-with-starting-out-as-a-data-scientist-705757a6fc09,Top 3 Challenges with Starting out as a Data Scientist,Problems I Faced as a Young Data Scientist and‚Ä¶,2,20,"['Top 3 Challenges with Starting Out as a Data Scientist', 'Final Thoughts']","When I started my career, I was the only woman engineer on my team and in the company where I was working. I was studying for my Master‚Äôs in Data Science while working in DevOps on servers and developing dashboards. The room I sat in consisted of almost 30 people; all men expect me. As I learned, that environment was not one I could thrive and grow in. It was an environment that would keep me hidden, overworked, and pause my career growth. The best decision I made was to recognize the problems and find ways to resolve them as I continued to traverse my career and work towards becoming a data scientist. It happened almost daily that my teammates forgot me in meetings, open conversations, or in passing. I didn‚Äôt notice it at first, but then I began to realize that some of the men I worked with overlooked my presence or outright ignored me. As I saw it, I started to find ways to stand out in meetings and not be left on the sidelines. I no longer would sit ideally in meetings. I may stay quiet for most of the meeting, but I am thinking through the discussions, gathering my thoughts, and formulating responses. The more I worked and the more experiences I began to have, I realized that I needed to stand up and stand out for myself because no one else will. I couldn‚Äôt let myself fade into the background on the sidelines in meetings. If there are meetings I am interested in but am not invited to, I ask if I can be invited to attend them. Early in my journey, I found it easy to be forgotten and pushed aside if you let it happen. Staying quiet and not participating much in group conversations help facilitate that, so I like to be actively engaged in discussions with the team and attend different types of meetings to know what work is happening. Advice: Don‚Äôt let yourself be forgotten in a meeting or on the team. You should feel that you can speak up and speak out about what is going on. Whether it is to share your opinion, ask a question, or discuss a problem, you should not feel that you cannot express yourself. I would often share my opinion on a topic of data science, and my team did not take it seriously. What was the issue? I wanted the team to recognize the amount of data they had at their fingertips and begin to leverage it to provide better insights to themselves and their customers. No matter how I presented my case, and no matter to whom, I always received the same feedback. They told me I did not have enough experience, did not have enough schooling, or didn‚Äôt know what I was talking about. They saw me as too junior to share my opinions. After trying for so many months, I recognized that the company was not looking nor interested in changing their processes to leverage their data in a better way. That was when I left to find a company that would respect my opinion, appreciate my experiences and education, and works with me to find a path forward on projects that would improve their processes. When I started working as a full-time data scientist, I learned how to work with data, provide it to other teams, and run analyses. I began to share my insights with others both inside and outside of my team. I developed their trust as a team member, and they began to respect my thoughts and decisions. Advice: You should never feel you are too junior in your position to share your opinions or experiences. You should be able to express your thoughts in a conversation with anyone. If you can‚Äôt get your opinion heard where you are, find a team you can build trust within to listen to you and respect your views. Not every environment will be a healthy one to work into progress yourself in your career. Sometimes you need to walk away to find a team that you can learn and grow with. The first big project I had was to update some servers. I had to update the operating system, all dependencies, and make sure our software worked properly on the machines. I spent my first three months learning about the system architecture, working on this project, and testing my changes. Then I walked into my 90-day review. I expected to walk through my accomplishments with my manager and for him to provide me feedback. Instead, he laughed. He gave me a project he did not expect me to complete by that timeframe and laughed when I told him I finished the work and needed to test it on the servers. It was an unsettling first review of my work. If he genuinely felt I could not do the project, why did he set me up to fail? Though not a data science project, I believe this story still provides some valuable lessons. The first thing I learned as I left this job for another was not letting myself get blindsided. Instead, I work to gather the expectations of my manager during all stages of the project. I do this to understand what they are looking for from a project, the timeline, and the expected deliverables. It also gives me opportunities to talk with my manager about why things may be taking longer than expected and examine any roadblocks that I have encountered. The next lesson learned from this project was understanding the importance of testing and reviews. Having someone test or review your code can bring out issues that you may not be aware of. After my 90-day review, I worked with my manager to test my code on the local servers for the next few months. These tests showed small problems that I was able to identify, fix, and then retest. For this project, the reviews focused on software/server testing and code reviews. But, you can apply the concept to data science work as well. No matter the job you are doing, test that your project does as expected, and get your work reviewed by at least one individual who can give feedback. Getting feedback, especially in data science, can provide interesting insights that you may not have considered, such as adding a new feature or including additional datasets you may not have been aware of. Advice: Don‚Äôt let one bad experience stop you from learning and becoming better. Since my first big project, I have taken away many things that have helped me in my current position: My first few jobs were not ideal, and they were not what I wanted to be doing, but they taught me many lessons that I still use today. These jobs taught me how to communicate better to be heard, build trust within a team to gain support for my decisions, and persevere when everything appears to be set up for failure. What lessons have you learned from your past jobs? How have you used those lessons in your current position? If you would like to read more, check out some of my other articles below! towardsdatascience.com towardsdatascience.com towardsdatascience.com",56,0,7,Towards Data Science,2021-01-04,2021
https://towardsdatascience.com/trends-analysis-explorations-and-further-research-proposal-for-the-most-recent-world-happiness-9728cf0ac343,"Trends, analysis, explorations and further research proposal for the most recent World Happiness",,10,31,"['Trends, analysis, explorations & further research proposal of the World Happiness Report: United Nations Sustainable Development Solutions Network', 'Introduction', 'Correlation', 'Health, Happiness & Prosperity', 'Key Tendencies', 'More Questions!', 'Heat Map', 'Top & Bottom 5 Analysis', 'Further Research Proposal', 'Research Proposal Outline']","If we could boil down the 7.5+ billion people down to few key tendencies that most contribute to happiness, what would they look like? What would this tell us about who we are as a collective? What would it tell us about what we most value and what, ultimately, correlates most with increased feelings of happiness within a country‚Äôs population? I had the impulse to want to choose certain countries for this project, but I thought that looking at all the countries represented in my data at once could lead us to some interesting and intuitive conclusions about what most of us seek, in varying phases and degrees, throughout our life journey: happiness. Further research will need to be conducted to try and find causality, but since we‚Äôre at the beginning of our discovery we can begin with a few basics. We‚Äôre looking at 159 countries across the world with metrics in the following categories: Ladder: Measure of life satisfaction. Positive affect: Measure of positive emotion. Negative affect: Measure of negative emotion. Social support: The extent to which Social support contributed to the calculation of the Happiness Score. Freedom: The extent to which Freedom contributed to the calculation of the Happiness Score. Corruption: The extent to which Perception of Corruption contributes to Happiness Score. Generosity: The extent to which Generosity contributed to the calculation of the Happiness Score. Log of GDP per capita: The extent to which GDP contributes to the calculation of the Happiness Score. Healthy life expectancy: The extent to which Life expectancy contributed to the calculation of the Happiness Score. Taking a birds eye view of this data all at once is a bit daunting, but if you take a quick glance below, you see that there are few concentrated areas of strong correlation between some of the above variables. The four most notable pairs include: ‚ÄúLog of GDP per capita‚Äù & ‚ÄúHealthy life expectancy‚Äù, Social support & ‚ÄúLog of GDP per capita‚Äù, ‚ÄúHealthy life expectancy‚Äù & Social support, and ‚ÄúFreedom‚Äù and ‚ÄúPositive Affect‚Äù were the closest clustered pairings with the clearest, most linear relationship contributing to the factors that bring people greatest happiness. How many times have we seen the classic ‚ÄúHealth, Happiness & Prosperity!‚Äù on a greeting card? When we want to express sincere well wishes, the best we can come up with is a wealth of health, happiness and joy (presumably spent with those nearest and dearest) and prosperity and a feeling of abundance, of having just a bit more than you need for your wants. What about freedom? Indeed without one of these, we‚Äôre off. But why is that exactly? Taking a look at a few of these greater insights can show us a bit about our relationship to happiness and its‚Äô intrinsic components. Coming back to the four most notable pairs mentioned above: ‚ÄúLog of GDP per capita‚Äù & ‚ÄúHealthy life expectancy‚Äù, Social support & ‚ÄúLog of GDP per capita‚Äù, ‚ÄúHealthy life expectancy‚Äù & Social support, and ‚ÄúFreedom‚Äù and ‚ÄúPositive Affect‚Äù, we see a strong connection between these sets more so than any other. From the following four groupings we can make the following observations of factors that contribute most to happiness: These tendencies leave us asking more questions. Which comes first, GDP growth or high life expectancy? Are people living longer and thus contributing further to GDP or are they living longer because the economy is healthier and is improving quality of life? Are countries with greater social support more successful, or does the success of the nation create more social cohesion? Do people feel more positive because of freedom in society or do negative societies breed more authoritarianism? Does life expectancy grow due to increased social bonds or do we create more social bonds because we live longer? Perhaps taking another look at how the tendencies relate to each other could be best served with a heat map. Below, we see areas of high correlation and areas of low correlation: Among the lowest areas of impact we will find Corruption. Could this be because given the other areas of measuring quality of life and happiness in a country, corruption has the lowest felt impact contributing to happiness? Among the warmest areas of the map, we yet again see close relationships between social support, GDP and healthy life expectancy as we observed in the previous graphs. A great area to explore hereafter would be to look at why corruption has such a weak impact on average for most countries. Is corruption expected and ignored? Or perhaps is it so out of sight and behind the scenes that it doesn‚Äôt impact people? We can use analysis of the top 5 (Finland, Denmark, Norway, Iceland and the Netherlands) and bottom 5 countries (Rwanda, Tanzania, Afghanistan, Central African Republic, South Sudan) in our list of 156 countries to examine a few telling trends from our list. We‚Äôve chosen the following categories to distinguish factors that contribute to the happiness of these two groups: Positive affect, social support, negative affect, healthy life expectancy, Curiously, some of these categories give us some surprising results. The bottom five countries derive happiness most from Generosity, Social support, Positive affect and Healthy life expectancy as compared with the top five countries. What does this tell us about these societies? Perhaps that they are societies built with interdependence as a shared value which in turn impacts how they view their overall quality of life (skews more positive) as well as the way they view their health and life expectancy. Of the categories we‚Äôve examined, the bottom five countries score higher in negative affect which might not be as surprising considering this had a higher impact on their happiness than the top five. In order to answer some of the questions posed throughout this report, we will need to uncover additional data that could help us explore some of these relationships and try and distinguish causality from correlation. Uncovering which aspects are drivers can tell us a lot about some of the social benefits of strengthening our economies and prioritizing health and well being the way New Zealand recently announced it would. Our comparison of the top and bottom five countries in our list also gives us some additional points to want to follow up on. The most central of those points being: what can the most vulnerable countries teach us about how human beings connect and rely on one another. Why are their contributors to happiness so much more different than the top five countries? What can they teach us about the shared experience of living in a country with high insecurity. Could it be true that the citizens of those countries prioritize each other more and don‚Äôt take the blessings they do have for granted. Could hardship in turn create more happiness due to a greater sense of purpose? Could the top countries tendencies show us that comfort and opportunity breed further individualism, loss of social dependence and complacency? The Problem: There is confusion over which of the aspects of world happiness which we looked at are causing certain relationships or if they have a more complex relationship. Understanding more about which of the studied metrics contribute more to the happiness of the countries included in our dataset would help us with perpetuating happiness for citizens in these countries in the future. We know that happier citizens in countries would do much to boost wellbeing and even productivity in ways that are innumerable, qualitative and often behind the scenes. While this is a nebulous undertaking, if we can get closer to understanding these relationships, we can get closer to quantifying some of the dynamics previously unseen. The potential solution: One potential way we can gauge causality is to administer a survey to those that originally provided answers to the UNDSC explicitly asking for causality so that we may gather enough data on the contributing factors to happiness specifically. Without more exploration into the driving factors, those most directly linked to an increase in happiness, we wouldn‚Äôt be able to determine causality so this could be a natural next step to determining causality. The method of testing the solution: Rather than understanding these relationships through the context of which factors are most active to their happiness, we can ask the surveyed population to rate the top five contributors to their happiness. This would allow us to quantify which factors specifically would produce an increase in happiness for those surveyed. Assigning a descending value of 5 to the top contributor, 4 to the next highest and so forth would allow us to give each factor of happiness a quantifiable value that we can use for further research. The first step in the design would be to craft the top five qualities. Given the key tendencies mentioned above, we can use the values in each set to try and gauge hierarchy. We would be asking those surveyed to rate the following factors in a scale from 1‚Äì5, 5 being the top factor: ‚ÄúLog of GDP per capita‚Äù, ‚ÄúHealthy life expectancy‚Äù, ‚ÄúSocial support‚Äù, ‚ÄúFreedom‚Äù and ‚ÄúPositive Affect‚Äù. To account for any inherent bias we are unaware of, we could administer the survey twice at six months apart. If there is more than a 5% difference, we could average the delta. Suspending disbelief and exaggerating the realm of possibility, we could also go as far as to make large scale changes to these factors themselves within the countries we‚Äôve seen indexed in the world happiness report 2019 and see if the participants exhibit any changes to their answers. Because we are working in a very subjective capacity, having iterations and steps would allow us to derive insights from any changes in recorded data. The participants in the first part of the experiment would likely not rate their answers accurately. In order to account for this, we would need to create a scenario where there would be a control group as the observed data could be too subjective. In order to simulate a randomized, controlled trial we could identify all the factors we want to rate, in our case the five mentioned above, and analyze the observed data from the initial survey in order to tease out how important one factor is relative to the others. From this second set of data that we would gather after there have been substantial changes, we could turn our attention to two pairs of countries that could have similar dynamics. In our case, we have selected two countries from the top five and bottom five list: Finland and Norway, as well as Rwanda and South Sudan. Our interest in selecting these two pairs would be to analyze their similarities and differences in efforts of finding some composite tendencies between the two pairs.",5,0,9,Towards Data Science,2021-01-04,2021
https://towardsdatascience.com/can-i-eat-that-mushroom-fc84e5b6b2c4,Can I Eat that Mushroom?,Neural Networks can¬†Help,6,35,"['Can I Eat that Mushroom?', 'The Method', 'The Results', 'More Applications', 'Key Takeaways', 'Further reading']","Imagine you‚Äôre in the woods for a night, and all you can only eat what you find in the surrounding area. You stumble upon a patch of mushrooms. Problem solved! Not quite. For the naked eye, it‚Äôs almost impossible to tell if a shroom is safe to eat. However, with machine learning, we can create an AI to predict whether or not given mushrooms are edible. Recently, I used Tensorflow and Keras, a Python neural network (NN) library and it‚Äôs API, respectively, to develop such an algorithm. Let‚Äôs take a look at precisely how it worked and how well it ended up performing its task. To start off the task, we need to find a large data set that contains thousands of mushrooms in it, a number of their characteristics, and if they are a edible. I found such a data set here. The file mushroom_weka_dataset.csv (csv is the file extension for lists of comma separated values), which I renamed data.csv in my Python editor, contains information about 8417 mushrooms, and tells us whether or not each one is edible in addition to 22 properties our network may use, such as color, the shape of a given shroom‚Äôs cap, and the smoothness of that cap. Using this knowledge, we can teach our soon to be born neural network what properties of mushrooms indicate their safety. On a very high level, our network does this by going through the data and first making a random guess as to how well each characteristic determines edibility (Yes, that is a word, for those of you wondering), and then gradually adjusting that guess depending on how it performs. Ultimately, all of these adjustments are geared towards minimizing a loss function, a measure of how badly the network is performing. In this case, the loss function would output how close the network was to calling an edible mushroom poisonous, or vice versa. (For an in depth explanation of how NN‚Äôs work, read this article) Just finding a CSV file with what we want does not suffice though. A big part of data science is processing data, and this project is no exception. We have to translate the file into a language that our network can understand. Namely, that language is math, and the words have to become numbers. Right now, data.csv is just a collection of words. It tells us that the cap‚Äôs surface is ‚Äúsmooth,‚Äù but our code has no idea how to interpret that unless it takes on some numerical value. To perform this transformation, I first transcribed the file into a Python list called mushrooms. This also made it easier to ultimately feed the data into our network. This code skips the first line of the file because that contains descriptions of the properties. It then simply takes each line of the CSV file, automatically converts it to a list, and adds that list on to the larger master list that is mushrooms. After that, the actual translation could begin. To accomplish the task, I created a Python dictionary called replacements and assigned each word that appeared in the file a corresponding number. Here‚Äôs the actual code: As the comments in the code explain, the above software creates a new entry in replacements for every unique word it encounters while looping through every entry in data.csv that maps the word to a number. It then uses that mapping to turn every word into the number given to it by replacements. To illustrate, as a result of this process the word ‚Äòedible‚Äô became 16 and ‚Äòpoisonous‚Äô became 44. Thus, the network will ‚Äòsee‚Äô 16 every time the word edible would have shown up, and it can use this to train itself to make accurate predictions based on these numerical representations. Below is a full, translated row: number 5015, the same one that is pictured above, from the new and improved version of our data. Now that we‚Äôve turned the data from machine-gibberish into machine-readable, we have to split it up along two axes: input/output and training/testing. The first step here is to split it into input and output data, because once we‚Äôve done that Python has a nice module that will do the train-test split for us. Separating input and output data just means putting our predictors we‚Äôre using into one group and the thing we‚Äôre trying to predict in another group. In the case of mushrooms, it means we need to create one list that has the lists of their characteristics (recall that these are color, shape, etc.) and another that says whether or not they are poisonous. Here‚Äôs what that looks like in Python: This code creates empty lists for the characteristics of mushrooms and their edibleness (also a word). Since everything in our data file except for the last column is supposed to move us towards the goal of figuring out if the last column says ‚Äòedible‚Äô or ‚Äòpoisonous‚Äô (or for our network 44 or 16), all the data for each mushroom except the last row goes into the input list, and the last number goes into the output list. The code accomplishes this by looping through mushrooms and doling out the data into input_data and ouptut_data according to that condition. Crucially, the first row of input_data matches up with the first row of ouptut_data and so on, thus our network is able to use the two lists to see how well it‚Äôs doing at predicting the safety of a mushroom and gradually adjust itself. You‚Äôll notice that for the list of outputs, instead of 16 and 44 I use 1 and 0, respectively; this is to take advantage of Tensorflow‚Äôs ability to model binary functions whose output is specifically either one or zero. The last piece of cake cutting we have to do before we can build our network is a train-test split. This simply means that we set aside a certain portion of our data to test our network on after we‚Äôve used the other portion to train it. This split helps catch and avoid overfitting, a pitfall where our network gets so good at evaluating on one set of data that it no longer predicts the general case very well. The good news: this split isn‚Äôt just easy, it‚Äôs two lines of code easy with sklearn‚Äôs train_test_split function. It works like this: The function returns four lists, which I‚Äôve named input_training, input_testing, output_training, and output_testing. I‚Äôve chosen to set the parameter test_size to 0.2, which means that the parts returned that concern training contain 80% of the data and the parts concerning testing contain 20%. Now that we‚Äôre finally done with all the boring data manipulation, we can get to creating a network. I‚Äôve decided to use a very simple model for our network, called the sequential model. It can hold as many layers as we want, and in this model each layer gets its data from the last one and feeds into the next one. Also, I‚Äôve used dense layers, which means that every node is connected (via a weight) to every node in the layer before and after it if a layer like that exists (the first and last layers don‚Äôt have nodes before and after them, respectively). With that said, we‚Äôve got a couple decisions that the data has already made for us with our layers and some that we need to make for ourselves. We know that our input layer has to have 22 nodes in it as we have 22 different potential predictors of edibleness. We also know that our output layer only has to have one node (which I will refer to as the output node) because our only possible outputs are one, meaning edible, and zero, meaning poisonous. Of course, the output node won‚Äôt always have the value of one or zero; rather, it will be some value that can be interpreted as one or zero. For example, our network may figure out that if the value of the output node is above 0.5 the mushroom is edible and otherwise it is poisonous. We will help our network do this a little bit later with the sigmoid activation function for the output node. What we can decide on is the number of hidden layers we have and how many nodes are in those layers. There‚Äôs an intrinsic tradeoff here: the more hidden layers and nodes we have, the more complex a function our network can model, but it will also be slower as it has to figure out more weights and consequently do more burdensome math. The correct balance will vary for each data set. For our mushrooms example, I found that two hidden layers, each with four nodes, was sufficiently efficient and accurate. So, to sum up: we need a network with four layers. The first one, the input layer, has 22 nodes, the second and third, both hidden layers, have four nodes each, and the final, output layer has one node. The final decision point here is what activation function to use for each layer. Activation functions just tell our network how to evaluate each node beyond the input layer (nodes in the input layer are just the values we give our network so we can tell it how to read those without an activation function). For this network, I used ReLu, or rectified linear unit, to activate the hidden layers. ReLu simply takes the value of each node in the hidden layer and sets it to zero if it‚Äôs negative. Otherwise, it returns the value itself. ReLu is useful because it allows us to process less data, increasing efficiency, and create a less linear model for our neural network to better model curves. Moreover, the function that models for ReLu, f(x) = max (0, x) is very easy for Python to compute and takes little processing power, making it very efficient. I used the Sigmoid activation function for the output layer, which takes its value and normalizes it down to somewhere between zero and one. This allows our network to set a threshold for whether or not a mushroom is safe that it can be reasonably sure that all the outputs will conform to. If that‚Äôs a bit abstract, think about it this way: without Sigmoid, we could have outputs all over the place and it would be impossible to make a statement like ‚Äúall outputs below 0.5 (or any other number) constitute poisonous mushrooms‚Äù because 0.5 would be such an arbitrary threshold when outputs could range from -789 to 1234. However, when outputs can only be between zero and one, to number will take our network by surprise so a threshold is easy to create. Note that Sigmoid uses a fractional exponential function to do it‚Äôs modeling, so it is rather computationally taxing. Fortunately, we only have to use it for one node in our network. Let‚Äôs take a look at how all of this materializes as code with Tensorflow and Keras. Here I‚Äôve imported them and renamed them tf_k for simplicity. With that, we‚Äôre able to construct the network described above. Using network.add we add all the required dense layers to our sequential model, and we‚Äôre able to set the number of nodes with the first parameter (or hyper-parameter because that‚Äôs a value that the coder sets rather than the network learning it). I‚Äôve also described the activation function as a parameter of each layer, and named each layer for future reference, perhaps to visualize or debug You may have noticed that we did not add an input layer. We don‚Äôt need one, as we simply give the list input_training to tensorflow and it feeds that directly into our first hidden layer. The last step before our network becomes operational is training it. To do that, we have to put it together and fit it to our data with the following two lines: The first of these lines brings all of our layers together into a unified model that will use Tensorflow‚Äôs built in optimizer adam, which uses gradient descent, a function to update our network‚Äôs weights. It also tells our network that we‚Äôre shooting for high accuracy by making that the metric we evaluate it under. Lastly, the network learns that it should use the binary_crossentropy loss function, which simply figures out how well our network is doing by telling us how close it‚Äôs getting to outputting the wrong binary variable. As indicated by its name, binary_crossentropy is used specifically for binary predictions like the difference between poisonous and edible mushrooms, perfectly suiting our purposes. Like I explained previously, the network ‚Äòlearns‚Äô by attempting to minimize this loss function. The second line actually trains the network by fitting its weights to minimize the loss function when looking at the data sets input_training and output_training. The parameter epochs allows us to dictate how many times the network goes through the data and refines it‚Äôs weights. Again here, we have to choose between efficiency and accuracy. I found that 20 times over created a good balance for this particular network. Verbose simply gives a number of different options as to how the network shows you its learning. I like verbose = 2‚Äôs method of visualization, though something else could work for a different individual. The final line of code I needed was one that used the testing data to make sure the network could work for the general case. This is a cakewalk compared to everything else. All we have to do is tell the network which data to use to evaluate itself, hence the parameters input_testing and output_testing, and how we want to see the results. Here again, I use verbose = 2 as my preferred method of visualization. All right. We‚Äôve put it the hard work and built our neural network. Time to take it out for a spin. After 20 epochs of training and a look at the testing data, we get the following results from our program. I‚Äôve displayed results from just epochs 1, 10, and 20 and the testing phase to avoid a tedious amount of programming output in this article. Cleary, for each epoch that goes by the network gets better and better. At first, the network does only a little better than a random guess and certainly worse than a skilled human, reaching the correct conclusion about the mushroom only 69% of the time. However, we must trust the process, for as it wears on, the loss function returns smaller and smaller values and the network‚Äôs accuracy approaches 100%. Now, we can‚Äôt get too excited yet, as such high accuracy may be indicative of overfitting. If the network never gets anything wrong on the training data it may have simply mastered that to the point where it only works on that data. We have no idea if that mastery will translate to any other mushrooms. But fortunately, the network seems to be really good at the general case as well, because it also got over 99% accuracy on the testing data. Interestingly, it seems that there was a slight bit of overfitting as the network tested a little bit worse than it did on the final training epoch. Though, it actually had less loss on the testing set so perhaps this was merely a coincidence and not a result of the common pitfall. Now, if you think machine learning is really cool but think you‚Äôll never encounter wild mushrooms, fear not! The technology outlined in this article can be applied to almost any field where there is a cause-effect relationship. For example one could input a movies reviews and how many tickets it sold to try and figure out if it will break even. All that needs to happen is changing the data set that is fed in, and perhaps tweaking the hyper-parameters. More thought provoking prospects include predicting credit card fraud, to medical disease diagnosis, to a new cracking the mystery of protein folding just last month, the clich√© that the possibilities are limitless rings true when it comes to neural networks. The source code for this article is in this Github repo. If you want to learn more, there are loads of resources online. I would specifically recommend edx‚Äôs CS50: Introduction to AI course. If you want to follow up with me, have a conversation, or have further questions, here is my contact info: Email | Linkedin |Github And please subscribe to my monthly newsletter!",72,0,14,Towards Data Science,2021-01-04,2021
https://towardsdatascience.com/the-cryptographic-states-of-america-ebe94659cb6,The Cryptographic States of¬†America:,A Look at Bitcoin ATMs Across the¬†U.S.,1,39,['The Cryptographic States of America:'],"For this Data Storytelling project, I was interested in: My research question- Did the BitLicense legislation that New York state passed in 2015 have a negative impact on Bitcoin ATM availability in the state? Background Bitcoin is a transparent and decentralized network and was created in 2009 by an individual under the pseudonym Satoshi Nakamoto. Bitcoin proponents frequently promote the idea that bitcoin allows everyone to ‚Äúbe their own bank‚Äù. Due to its‚Äô decentralized nature, the Bitcoin network allows users to transact peer-to-peer with no need for the intermediaries we are used to: traditional banks. There is one deterrent to that idea, though; individuals need an easy way to purchase Bitcoin. Centralized exchanges such as Coinbase allow individuals to purchase bitcoin but doing so requires a bank account. This results in a circular situation: to ‚Äúbe your own bank‚Äù, you first need a bank account to kick-start the process. In 2018, the Federal Reserve estimated that there are as many as 55 million unbanked adult Americans.¬π It‚Äôs more difficult for unbanked individuals to buy bitcoin; people with bank accounts have many more options. Bitcoin appears to be transitioning into a store-of-value more so than an everyday currency; it has seen an unprecedented increase in USD value during a global pandemic. According to SEC Chairman Jay Clayton, ‚ÄúWe determined that bitcoin was not a security, it was much more payment mechanism and store of value.¬≤‚Äù Paypal has recently allowed users to buy bitcoin directly, but currently, there is no way for customers to withdraw what they purchased. If unbanked individuals want to buy Bitcoin, they have few options to benefit from its store of value, and freestanding Bitcoin ATMs can fill that need for unbanked and banked users alike. Despite the fees and rates associated with their use, Bitcoin ATMs are very convenient. Bitcoin ATMs that allow users to walk up to the machine, buy bitcoin using cash, and have it immediately sent to a bitcoin address of their choice represent a valuable and convenient method to buy bitcoin. The CoinATMRadar dataset that I evaluated for this project contains Bitcoin ATMs that fit that criteria. (The alternative to freestanding Bitcoin ATMs require users log on to their website to redeem their purchased bitcoin.) Dataset Description For this Data Storytelling project, I evaluated a dataset detailing the locations and number of Bitcoin ATMs across the U.S. I received the main dataset directly from CoinATMRadar, and it includes freestanding Bitcoin ATMs in the United States as of November 16, 2020. You can view the corresponding code/IPython notebook for this project at the link at the bottom of this blog post. CoinATMRadar is an online directory that lists Bitcoin ATM locations, and they have strict requirements for listings. Their directory also includes ATMs that buy and sell other cryptocurrencies including Ethererum, Monero, and Dash, among others. I did not evaluate alternative cryptocurrencies for this project; Bitcoin is the exclusive focus. There is a self-reporting mechanism to listing a Bitcoin ATM, and so there is probably a small gap of time before any potential errors are detected. The specific limitations to this dataset: I also downloaded a dataset with estimated 2020 population for all U.S. states from census.gov. and merged the two datasets so that I could evaluate the number of Bitcoin ATMs in relationship to each state‚Äôs population. In 2015, New York state passed legislation that required any company who wanted to operate an exchange in the state or sell cryptocurrency to apply for a BitLicense. Many people have speculated that this has had a negative effect on the industry, and I wanted to quantify and prove that effect. Data Wrangling and Feature Engineering My data wrangling process included cleaning the column that listed city locations. There were some misspellings that needed to be corrected. Also, the city column included punctuation and state abbreviations in some cases. All of those needed to be removed. Using Python‚Äôs split() method allowed me to easily remove the commas in many rows and any two-letter abbreviation that appeared after it. I also used Python‚Äôs RegEx module. In total, it took me 16 lines of code to clean a dataset that contained 9,550 rows. In future units, I look forward to expanding my data wrangling skills and being able to accomplish the process in fewer lines of code and employing the least-compute intensive process possible. I also added a new feature to the dataset including a column that calculated the per capita ATMs per 100k residents. Here is a visualization showing those calculations. To create the above visualization, I needed to define a data dictionary and map it to each state, resulting in a new column in the dataset that contained the two-letter abbreviations for each state. For those who prefer to see the states by name, here is another visualization that indicates the top-ten states for Bitcoin ATMs per 100,000 residents. My statistical methods included creating a scatter plot and overlaying a regression line, which showed an apparent positive linear correlation between population and number of Bitcoin ATMs, with an outlier at 1e7. I then calculated a correlation coefficient to measure the strength and direction of this relationship. The result was 0.93 which represents a strong correlation. New York state is our nation‚Äôs 4th most populated state, yet only has 113 Bitcoin ATMs. This much lower than the mean of 198, which we wouldn‚Äôt expect from a state that size. Next, I wanted to investigate my initial idea that the restrictive BitLicense legislation that New York enacted in 2015 has had a negative impact on the cryptocurrency industry as evidenced by limiting the number of Bitcoin ATMs in New York state beyond what we would typically expect from a state that populated. So, I created a linear regression model and used it to calculate three separate predictions. The linear regression model showed that 86% of the variability in number of Bitcoin ATMs can be accounted for by population, so I was unsure if this project would be able to quantify my initial idea. However, the resulting predictions came as a pleasant surprise! Results and Conclusion I first calculated a prediction for the number of Bitcoin ATMs for a state with a population equal to the size of New York and also a prediction for both New Jersey and Connecticut. The predicted number of Bitcoin ATMs for a state whose population is equal to New York state‚Äôs population is 634.69, which is 521.73 more than the mere 113 Bitcoin ATMs that New York has! New Jersey has 286 more Bitcoin ATMs compared to neighboring New York state, even though New Jersey is less than half as populated. The linear regression model predicts that a state the size of New Jersey would have 270 Bitcoin ATMs and Connecticut would have 67.3. According to our dataset, New Jersey has 399 Bitcoin ATMs and Connecticut has 154. It‚Äôs certainly possible that businesses that would have opened Bitcoin ATMs in New York went to neighboring New Jersey and Connecticut and opened Bitcoin ATMs there to avoid the high cost of applying for a BitLicense. This could account for New Jersey having roughly 129 more Bitcoin ATMs than our linear regression model predicted and 286 more Bitcoin ATMs than New York state, even though NJ is less than half as populated. It could also account for Connecticut‚Äôs actual number of Bitcoin ATMs being more than double the predicted number. Compared to New York, Connecticut has 41 more ATMS even though Connecticut is less than one-fifth as populated! Further areas of research include periodic similar evaluations on a more recent dataset. Also, cross-referencing Bitcoin ATM locations from multiple sources before comparison could further refine the results. Legislation involving Bitcoin and the cryptocurrency industry as a whole is evolving, and future research and analysis can take into account these changes as they develop. Also, I did a lot of feature engineering to explore the possibility of quantifying the term ‚Äúbitcoin-friendly state‚Äù. This exploration didn‚Äôt make it into the final IPython notebook, but it could be a sizeable and separate area of research by itself! Here is a link to the IPython notebook corresponding to this blog post: https://github.com/cryptobellum/DS-Unit-1-Build/blob/main/The_Cryptographic_States_of_America.ipynb Works Cited (1) https://www.bloomberg.com/news/articles/2019-06-04/why-cleveland-wants-to-bring-back-postal-banking (2) https://news.bitcoin.com/us-cryptocurrency-regulation-sec-chairman-jay-clayton-bitcoin/",8,0,7,Towards Data Science,2021-01-04,2021
https://towardsdatascience.com/p-value-in-a-nutshell-what-does-it-actually-mean-d180388bb499,P-value in a Nutshell: What Does it Actually¬†Mean?,"Understand, visualizing, and calculating p-value",11,41,"['P-value in a Nutshell: What Does it Actually Mean?', 'Introduction', 'Hypothesis Testing Refresher', 'Diving Deeper', 'Let‚Äôs Calculate our Observed Statistic', 'Time to Simulate!', 'Infer Fundamentals', 'Null Sampling Distribution', 'Calculating P-value', 'Final Notes on P-value Interpretation', 'Conclusion']","Welcome to this lesson on calculating p-values. Before we jump into how to calculate a p-value, it‚Äôs important to think about what the p-value is really for. Without going into too much detail for this post, when establishing a hypothesis test, you will determine a null hypothesis. Your null hypothesis represents the world in which the two variables your assessing don‚Äôt have any given relationship. Conversely the alternative hypothesis represents the world where there is a statistically significant relationship such that you‚Äôre able to reject the null hypothesis in favor of the alternative hypothesis. Before we move on from the idea of hypothesis testing‚Ä¶ think about what we just said. You effectively need to prove that with little room for error, what we‚Äôre seeing in the real world could not be taking place in a world where these variables are not related or in a world where the relationship is independent. Sometimes when learning concepts in statistics, you hear the definition, but take little time to conceptualize. There is often a lot of memorization of rule sets‚Ä¶ I find that understanding the intuitive foundation of these principles will serve you far better when finding their practical applications. Continuing on this vein of thought. If you want to compare your real world stat with the fake world, that‚Äôs exactly what you should do. As you‚Äôd guess we can calculate our observed statistic by creating a linear regression model where we explain our response variable as a function of our explanatory variable. Once we‚Äôve done this we can quantify the relationship between these two variables using the slope or coefficient identified through our ols regression. But now we need to come up with a this idea of the null world‚Ä¶ or the world where these variables are independent. This is something we don‚Äôt have, so we‚Äôll need to simulate it. For our convenience, we‚Äôre going to leverage the infer package. First things first, let‚Äôs get our observed statistic! The dataset we‚Äôre working with is a Seattle home prices dataset. I‚Äôve used this dataset many times before and find it particularly flexible for demonstration. The record level of the dataset is by home and details price, square footage, # of beds, # of baths, and so forth. Through the course of this post, we‚Äôll be trying to explain price through a function of square footage. Let‚Äôs create our regression model As you can see in the output above, the statistic we‚Äôre after is the Estimate for our explanatory variable, sqft_living_log. A very clean way to do this is to tidy our results such that rather than a linear model, we get a tibble. Tibbles, tables, or data frames are going to make it a lot easier for us to systematically interact with. We‚Äôll then want to filter down to the sqft_living_log term and we'll wrap it up by using the pull function to return the estimate itself. This will return the slope as a number, which will make things easier to compare with our null distribution later on. Take a look! To kick things off, you should know there are various types of simulation. The one we‚Äôll be using here is what‚Äôs called permutation. Permutation is particularly helpful when it comes to showing a world where variables are independent of one another. While we won‚Äôt be going into the specifics of how a permutation sample is created under the hood; it‚Äôs worth noting that the sample will be normal and center around 0 for the observed statistic. In this case, the slope would center around 0 as we‚Äôre operating under the premise that there is no relationship between our explanatory and response variables. A few things for you to know: Same distribution with 1000 reps Ok we‚Äôve done it! We‚Äôve created what is known as the null sampling distribution. What we‚Äôre seeing above is a distribution of 1000 slopes each modeled after 1000 simulations of independent data. This gives us just what we needed. A simulated world against which we can compare reality. Taking the visual we just made, let‚Äôs use a density plot and add a vertical line for our observed slope, marked in red. Visually, you can see that this is happening far beyond the occurrences of random chance. As you can guess from visually looking at this the p-value here is going to be 0. As to say, in 0% of the null sampling distribution is greater than or equal to our observed statistic. If in fact we were seeing cases where our permuted data was greater than or equal to our observed statistic, we would know that it was just random. The reiterate the message here, the purpose of p-value is to give you an idea of how feasible it is that we saw such a slope randomly versus a statistically significant relationship. While we know what our p-value will be here, let‚Äôs get you set up with the calculation for p-value. To re-prime this idea; p-value is the portion of replicates that were (randomly) greater than or equal to our observed slope. You‚Äôll see in our summarise function that we're checking to see whether our stat or slope is greater than or equal to the observed slope. Each record will be assigned TRUE or FALSE accordingly.. When you wrap that in a mean function, TRUE will represent 1 and FALSE 0, resulting in a proportion of the cases stat was greater than or equal to our observed slope. For the sake of identifying the case of a weaker relationship in which we would not have sufficient evidence to reject the null hypothesis, let‚Äôs look at price explained as a function of the year it was built. Using the same calculation as above, this results in a p-value of 12%; which according to a standard confidence level of 95%, is not sufficient evidence to reject the null hypothesis. One final thing I want to highlight just one more time‚Ä¶. The meaning of 12%. We saw that when we randomly generated an independent sample‚Ä¶ a whole 12% of the time, our randomly generated slope was as or more extreme‚Ä¶ You might see such a result as much as 12% just due to random chance That‚Äôs it! You‚Äôre a master of the calculating & understanding p-value. In a few short minutes we have learned a lot: It‚Äôs easy to get lost when dissecting statistics concepts like p-value. My hope is that having a strong foundational understanding of the need and corresponding execution allows you to understand and correctly apply this to any variety of problems. If this was helpful, feel free to check out my other posts at https://medium.com/@datasciencelessons. Happy Data Science-ing!",73,0,6,Towards Data Science,2021-01-04,2021
https://towardsdatascience.com/when-ai-gets-it-wrong-2d561e7d7d73,When AI gets it¬†wrong,What ethical framework is needed¬†to‚Ä¶,8,40,"['When AI gets it wrong', 'Challenges raised by Artificial Intelligence', 'What is Responsible AI?', 'When it comes to decision making', 'So what should we do?', 'Under-specification', 'The Z-inspection process', 'References']","In early December 2020, Timnit Gebru was ousted from Google after submitting a paper outlining the ethical tensions raised by the use of AI-based language models. This event made a lot of noise in the technology industry and highlighted the ethical issues faced by big tech companies and their struggle to manage them internally. As an article from the MIT technology review highlighted [1], the formation of these models is very CO2-consuming. The formation of a transformer model (213M params) with neural architecture research consumes more than 300 times the amount of CO2 equivalent of a NYC-San Francisco round trip. Another problem with these huge training datasets is that it is impossible to carefully examine the composition of the dataset and identify biases or unethical content. This could easily lead to a racist or non-inclusive model. This is a recent example of the risk associated with the increasing capabilities and scalability of artificial intelligence models. Without careful thought, they can easily endanger our society in many ways. In order for these new products to gain the trust and recognition of the users of the system, a clear ethical framework will have to be defined and, as showed above, internal ethical control is probably not the solution. The use of Artificial Intelligence raises a number of questions: ‚ÄúDeep neural networks can fail to generalize to out of-distribution inputs, including natural, nonadversarial ones, which are common in real-time settings‚Äù. (2) As models get more complex, we run into issues of privacy, interpretability, fairness, and discrimination. Responsible AI is making sure that we align with societal values. We want the product we build to be inclusive and harmless. Responsible AI best practices require the greatest possible transparency when presenting data to users: for example why users see a recommendation. This will help build trust. In addition, there needs to be a good internal workflow, with people evaluating the results of generated models and verifying that the models are doing what they are supposed to do. Finally, we also want to get fair (equitable) results, and not replicate or reinforce the biases you see in your data. Many people take models off the shelf when you need to understand the limitations of the model, what it can do, and what it can‚Äôt do. In addition, the way you build the product must also be ethical: Do you have all the necessary permissions to use the data? Do you treat people who label the data fairly? Today, in some cases, AI makes the decision for us. When it is not, it influences our decisions. AI can cause harm on a greater scale. This is the case with language models that reinforce stereotypes. Even if your gender is not in your resume, it can be inferred by the model and used as a feature [3]. These models are currently used for recruitment, credit scoring, or by the justice system and they can lead to the loss of opportunities for an entire category of the population since the same heuristic can be applied a million times. That is why we need to take a step back and look not just at performance metrics, but at how it actually changes behavior. Not many companies in the world are looking at the ethical metrics of their model. That need to be assessed when developing the model, and then when maintaining it. If you are building an AI product, where is the best place to start your ethical journey? We usually care about data quality and data volume. Now we also need to look at the representation: does our data have a fair representation of all the categories? Do we have sensitive data that we need to watch out for, like gender, race, or nationality? Removing these characteristics from the dataset is usually not enough. It is probably possible to reconstruct the race data, for example, using highly correlated other features (where you live, what are your hobbies‚Ä¶). AI teams need to be educated to not only pursue the accuracy of the model but also the responsibilities behind it. Good diversity in your data means that you don‚Äôt just reinforce existing trends that are already obvious. For example, an item recommendation system based solely on the number of purchases would highlight existing items that are well known and hide new items from emerging designers who are not yet known but could offer better original suggestions. In the end, the model will not be very useful. Are certain groups receiving more favorable outcomes? This effort requires the inclusion of the voices of people from very different places, so as not to repeat the mistakes of the past. [2] ‚ÄúThose of us working in AI ethics will do more harm than good if we allow the field‚Äôs lack of geographic diversity to define our own efforts.‚Äù Abhishek Gupta [2] Under-specification is when you learn the correct mapping on your training data but without reproducing the natural causal relationship. This usually occurs when the observed behavior is the result of multiple causes [4]. This results in a model that will not generalize well. Forming a model on a fixed data set can produce several models that work similarly. The differences between them will not be directly visible if there is no explanation of how the models make predictions. It is only by testing on specific examples in the real world that we will see the distinction. So, to avoid unpleasant surprises in production, we should add tests and specifications that cover a large number of scenarios, even improbable ones. ‚ÄúWe need to get better at specifying exactly what our requirements are for our models. [‚Ä¶] Because often what ends up happening is that we discover these requirements only after the model has failed out in the world.‚Äù Alex D‚ÄôAmour When you have all the stakeholders in the room, ask who is going to be affected by your product. Make sure you are not reinforcing any bias. Think about the uses of your product that is different from the original intent. Finally, if it goes wrong, what is the worst-case scenario, and is there a way to change it? We must be able to give the why. Why do I have this terrible credit limit? Why do I have to hire 7 more people? Why do you recommend that I watch this show? It has to do with being able to explain the model predictions. ‚ÄúYes, excessive automation at Tesla was a mistake. To be precise, my mistake. Humans are underrated.‚Äù Elon Musk With multiple and diverse collaborators from Europe and Califonia and with the leadership of Professor Zicari, we started a few months ago the ethical inspection of two AI products. The former is a Machine Learning system used as a supportive tool to recognize cardiac arrest in emergency calls [4], the latter is a Deep Learning based skin lesion classifier [5]. The purpose of the inspection is to ensure that the product under examination does not exhibit inappropriate behavior and to resolve the ethical tensions surrounding it. This inspection begins with the creation of a multidisciplinary team of experts who will be the external actor in charge of examining the ethical implications of AI products. The process begins with meetings with the product‚Äôs creators to understand the product‚Äôs specifications, objectives, and limitations. By going through multiple iterations, we have created a backlog of information and tensions to consider. This is used to create socio-technical scenarios that put the product in interaction with actors, users, and systems. This, in turn, highlights tensions and dilemmas that will be analyzed and resolved through recommendations and ethical maintenance if necessary. More information on the process is available on the website [6]. I will describe in more detail the story of this ethical inspection in a future article. [1] Karen Hao, We read the paper that forced Timnit Gebru out of Google. Here‚Äôs what it says. MIT Technology Review. Link [2] Abhishek Gupta & Victoria Heatharchive, AI ethics groups are repeating one of society‚Äôs classic mistakes. MIT Technology Review. Link [3] Gupta, A., Lutz, M.J., Lakhmani, A. and Wang, K., 2019. Implicit Gender Bias within Resume-Ranking Tools. [3] Will Douglas Heavenarchive, The way we train AI is fundamentally flawed. MIT Technology Review. Link [4] Blomberg SN, Folke F, Ersb√∏ll AK, Christensen HC, Torp-Pedersen C, Sayre MR, Counts CR, Lippert FK. Machine learning as a supportive tool to recognize cardiac arrest in emergency calls. Resuscitation. 2019 May;138:322‚Äì329. doi: 10.1016/j.resuscitation.2019.01.015. Epub 2019 Jan 18. PMID: 30664917. [5] Lucieri, A., Bajwa, M.N., Braun, S., Malik, M., Dengel, A., & Ahmed, S. (2020). On Interpretability of Deep Learning based Skin Lesion Classifiers using Concept Activation Vectors. 2020 International Joint Conference on Neural Networks (IJCNN), 1‚Äì10. [6] http://z-inspection.org/the-process-in-a-nutshell/",,0,8,Towards Data Science,2021-01-04,2021
https://towardsdatascience.com/how-to-scale-the-training-of-several-models-64180480ca3d,How to scale the training of several¬†models?,This describes a simple architecture in python and¬†with‚Ä¶,6,23,"['How to scale the training of several models', 'Motivation', 'Overview of the solution', 'Implementation', 'Other possibilities', 'Conclusion']","Let‚Äôs consider the following task: predicting the demand for something in several cities. This is a time series forecasting problem. Forecasting differentiates between local models and global models. With local models, you train a model for each city. With global models, you train one model for all the cities. All to say: it‚Äôs sometimes necessary to train multiple similar models. The naive solution consists in training the models sequentially: one model after another. Imagine you need 10 minutes to train the model of a city, and you have 100 cities. You need 1 000 minutes to train all the models (16 hours). This is a productivity-killer. And as fitting a model is part of the deployment, it would violate the principles of continuous integration and continuous development. To finish the training within the same duration, independently of the number of models, the solution is to distribute the training among n computers (also known as workers). This article will describe a solution in python. The solution is based on the famous library celery. A prototype is available on github. This problem is commonly solved with a task queue manager. In python, the most famous task queue manager is celery. We will have: For training models, there are 2 more important requirements: The implementation is available on github. Step 1 ‚Äî first celery usage & prototype skeleton Prerequisites: install the celery package; install and start redis; and understand the celery first steps. First, define a celery task called‚Äúfit‚Äù. It will be executed by the workers. It‚Äôs the code that is executed to fit a model, and it returns the accuracy. Then, the master delegates the computation to the worker. Add this code, for example in bin/master.py. Finally, execute the prototype like: N.B. On github, a file (bin/worker.sh) has been created for starting a worker. Step 2 ‚Äî Waiting for the tasks to be processed The master might have to wait that the tasks are processed. This happens likely in a deployment pipeline. This is the responsibility of the following code: Step 3 ‚Äî Getting the average accuracy of each training To better understand and track the accuracy, the master can print the average accuracy across each city. The following code demonstrates how the master can read the results of its workers. Step 4 ‚Äî Stopping the worker Define a task ‚Äúshutdown‚Äù in src/tasks.py. A single execution by one worker will shut down all the workers. But the workers will first finish the execution of their current task.cal The master delegates the task ‚Äúshutdown‚Äù to the workers (in bin/master.py): Step 5 ‚Äî Run everything! Starting 3 processes (1 master and 2 workers) in one tab: This article shows that celery is a simple solution for distributing the training of several models. Celery is the most famous python library for task processing. The knowledge and the skills that you will win with celery, are transferable to other problems. Out of the box, Celery comes with great features like ‚Äúshut down all the workers‚Äù or ‚Äúreturn a value from the worker to the master‚Äù. Nonetheless, other task queue managers can solve the same problem. You can develop a solution directly using a database (e.g. Redis) or a AMQP message broker (e.g. RabbitMQ). Even if this looks like writing your own task queue manager, it can be a more cost-efficient solution. Let me know with a comment if you are interested in more details!",17,0,4,Towards Data Science,2021-01-04,2021
https://towardsdatascience.com/building-a-deployable-jira-bug-similarity-engine-using-word-embedding-and-cosine-similarity-1c78eeb23a8d,Building a Deployable Jira Bug Similarity Engine using Word Embedding and Cosine Similarity,,8,52,"['Building a Deployable Jira Bug Similarity Engine using Word Embedding and Cosine Similarity', 'Introduction', 'What is a word embedding?', 'But we‚Äôre not dealing with single words', 'Algorithm Exploration', 'Deployment', 'Running the engine', 'References']","In a previous article, I detailed how Amazon‚Äôs BlazingText could be used to build a Jira bug classification engine. In there, I also referenced the corresponding Gitlab repository. In the README.md there, I very briefly mentioned about word2vec, word embeddings and the vectors.txt file that was generated as output to the word2vec algorithm. In this article, we are going to be using this same vectors.txt word embedding to build a text similarity engine to find similar bugs in a given corpus of bugs. I‚Äôll probably not be able to do sufficient justice in explaining what a word embedding is but macheads101 on YouTube has a pretty good video [1] that explains it well. In a nutshell, in a word embedding, we map each word in the corpus into a vector space based on their context, which in this case is the proximity with other words in the corpus. Some of you may find this fun: macheads101 used the word2vec algorithm to create a word embedding using Twitter tweets he collected over a period of time and then created a nice little web app to demonstrate how you can use the embedding to find similar words in the corpus. In the case of vectors.txt which I described above, I also plotted a -SNE plot of the vector space just to get a feel of the data: (Essentially a t-SNE is a dimensional reduction algorithm where we map an n-dimensional space to one of lower dimension (in this case 2-d for plotting). Another popular dimensionality reduction algorithm is PCA) As you can see, there are clusters of words that are deemed to be ‚Äúsimilar‚Äù based on their context. Notice those long streaks in the plot? Those correspond to code fragments in the Jira tickets. Notice they all cluster together? The nice thing about this way of representing words as vectors is that then the ‚Äúdistance‚Äù between words then approximates the idea of similarity or rather affinity or ‚Äúthey belong together‚Äù. There are several ways of measuring that and [2] has a nice list. For simplicity sake, we‚Äôll just be using Cosine Similarity (and it turns out not to be a bad idea, as we‚Äôll see later). Now, remember the original objective: how do we estimate the similarity of Jira bugs? Can we extend this idea of word similarity? In [3], Yatin Vij provides a few options: Again, to keep things simple, we‚Äôre going to to use (3) ie. perform vector addition of all the words in the bug (in this case only the bug summary and description). But in [3] the approach calls for averaging not summation. This will be a problem if one bug has more words than another correct? Well, not if we use Cosine Similarity because unlike Euclidean distance, Cosine Similarity uses the angle between 2 points on a vector space to determine similarity. This means that it could be possible to use the vector sum instead of the average. This summarizes the main idea. Next comes trying it out to see if there‚Äôs some hope it will work. In the following sections, I will be referring to code found here: https://gitlab.com/foohm71/cuttlefish (Why ‚Äúcuttlefish‚Äù? Well my last project was called ‚Äúoctopus‚Äù so I just went with the flow) In this section, I‚Äôm walking through TextSimilarityExplorations.ipynb found in the cuttlefishrepository. This Python notebook was designed to be run on Google Colab and I do recommend it being run there. Note: as described in this article, the corpus of Jira bug information is obtained from an open source repository of Jira tickets from several open source projects. Also there were 2 datasets used: JIRA_OPEN_DATA_LARGESET.csv was used to obtain the vectors.txt and I will be using JIRA_OPEN_DATA_ZOOKEEPER.csv to try out the approach. The 2 data sets should be orthogonal as the ‚ÄúLargeset‚Äù does not contain any Zookeeper bugs. I wanted to do this to see how robust the word embedding approach is. Basic setup stuff: The first part ie. drive.mount('/content/drive') basically mounts your gdrive so that you are able to read in data from gdrive to Colab. First read in the vectors.txt: Note: you‚Äôll have to modify the path accordingly based on where you have uploaded the vectors.txt file to (if you plan to run the notebook). Next, read in the Zookeeper data: Next, we only need the id, title and description fields and we also combine the title and description into a field called features features is essentially the ‚Äúdocument‚Äù we will be comparing similarity on. Next we remove not needed text in the features ie. email addresses, URLs as they don‚Äôt provide a whole lot of information: The next part is the heart of the matter. I‚Äôm also skipping some part of the code as they are exploratory code for me. First are some useful functions: The first sentance2vector basically converts a sentence (yes, it‚Äôs a spelling error) into a 100-d vector to match the word embedding in vectors.txt. Also note I had to skip words not already in the vocabulary. This could potentially be bad if all the words in the features are not in the vocabulary. My assumption was that it would not be that case. Next is cosineSimilarity function which is self-explanatory. I then create another column in the dataframe to store the score ie. the similarity estimate and initialize it to zeros: Next is the main calculation: Note: vector is the vector version of the features column of the first row in df. Essentially what this does it calculate the similarity of the first row of df against all the others. So is the similarity estimation any good? Well to find out I needed to sort out the wheat from the chaff. In other words, only select the bugs with high similarity (I chose a threshold of 0.87). Then I looked at those with high scores and compare them with the ‚Äútarget‚Äù bug: Some of the higher scoring ones were: Which is not bad. The first deals with a data structure issue and so does the target. The second has to do with a process issue on some data handling and so does the target. So here is the problem: in traditional ML, you have a tagged dataset that you split into training/test subsets and use the test subset to evaluate how well your algorithm is performing. In this case, unless you are very familiar with the bugs, it‚Äôs really very challenging to perform an evaluation. One simple way to perform an evaluation is to deploy it, use it and have a way to score the algorithm (the idea is similar to an A/B test). One simple way to make this algorithm deployable is to build it as a Docker container and publish it on a Docker registry. In a previous article (see section ‚ÄúBuilding the Deployable Model‚Äù), I described the steps to do so for the octopus2 project. For cuttlefish, the steps are pretty much the same except that here, instead of building the app.py on its own, I have used an approach that I‚Äôve developed to first prototype the app.py as a Jupyter notebook and then covert it into the app.py for the container. I‚Äôll start first with the high level build process. To do so we should look at the .gitlab-ci.yml file: There are essentially 2 stages to the build: (a) building the app.py from Cuttlefish.ipynb and (b) building and deploying the Docker container. (b) is pretty boiler-plate and is described in the earlier article as well. (a) basically just uses nbconvert to covert the notebook into app.py and uses remove_lines.py to remove unwanted code (to be explained in a bit). If you take a good look at the notebook, you will notice it is pretty much a cleaned up version of TextSimilarityExplorations.ipynb. There is also additional standard Flask code to make it into an API and that‚Äôs pretty much it. Notice all the # START REMOVE and # END REMOVE. Those are so that remove_lines.py knows what code blocks to remove. There are 3 blocks of code: (a) the pip install section (b) the app.run() section. (b) is good for testing out the code as a notebook but it‚Äôll not work for deploying as a Docker container. That will require: Finally we get to the fun part! The first part is to do a docker run as such: Next we give you a choice: make the call to the API using a notebook ( testing.ipynb) or run a script ( test.sh). Both a pretty straight-forward. test.sh is just a curl using the payload in test.json and testing.ipynb just uses requests to make that same call to the API. The README.md on the repo also describes the payload format: And that‚Äôs it. I hope you have found this useful! (For those who are looking to deploying this on GCP or AWS, the README.md has some tips on how to do that) [1] Word Embeddings, machead101, July 2017, https://youtu.be/5PL0TmQhItY [2] Text Similarities : Estimate the degree of similarity between two texts, Adrien Sieg, July 2018, https://medium.com/@adriensieg/text-similarities-da019229c894 [3] Combining Word Embeddings to form Document Embeddings, Yatin Vij, Sept 2019, https://www.analyticsvidhya.com/blog/2020/08/top-4-sentence-embedding-techniques-using-python/",55,0,8,Towards Data Science,2021-01-04,2021
https://towardsdatascience.com/market-intelligence-powered-by-deep-learning-bb63dae49481,"<strong class=""markup--strong markup--h3-strong"">Market intelligence powered by deep¬†learning</strong>","<strong class=""markup--strong markup--h4-strong"">Can tensorflow increase a freelancer‚Äôs chances of winning projects?</strong>",1,18,['Market intelligence powered by deep learning'],"Based on a recent report there are 59M freelancers in US. The freelance workforce is global and very competitive; the talent supply is higher than the jobs demand. You can read more about this changing global marketplace and the covid impact in this financial results presentation. Freelancers face multiple challenges two of them being: Freelancer.com has an excellent API that can be used to extract project details. It is a rich data set with a respectable history (more than 5 years); it contains both contests and projects, although I will only cover the latter. The above mentioned reports contain a lot of good information (freelancer profiles, average bids per project, gross market volume trends, etc.). In this post I will try to answer the following question: can a project probability to be awarded be determined accurately? Given the extensive schema I have been focusing on 2 main components: Admittedly, there is a lot of hype around deep learning which is seen as the magic bullet on a lot of current problems. There is also a tendency (of which I am guilty of as well) to focus on tools. In this post I will focus on the value that deep learning can add; specifically, can deep learning accurately help a freelancer focus only on those projects that will be awarded? In the end, this will not only save a freelancer time, but also money as bids are not free; and bids wasted on projects that are not awarded are not refunded. At this point you might be asking yourself: how many projects are awarded from total projects posted (I call this project award rate). Based on my calculation (and consider this within the context of this analysis) the project award rate is around 30%. This might seem promising, but this marketplace is very competitive given that on each project you have on average 45 bids. Therefore focusing on the 30% projects that will be awarded will save time and energy. Of course a deep learning model will provide you with a probability which none the less is a powerful insight. So I built a tensorflow model that predicts if a project will be awarded or not. I have only extracted the project and user details on 16 jobs/skills; some examples: Here are some high level steps on how to develop the model: In addition to the currency column detailed above I have the following variables as input (do note that I include numeric, categorical and text): The above is a small subset and the Freelancer API provides a lot more details. The training data set is not huge (only have projects with closed status). It is around 90K records. We answer the question by training the model for 1 epoch: 75% accuracy. Below you have a few active project examples along with the predicted probability. How can the model 75% accuracy be improved? A few areas: Bottom line, the value add of this model could be: 2. include these type of insights in the premium membership plans Let me know what you think.",27,1,4,Towards Data Science,2021-01-04,2021
https://towardsdatascience.com/one-line-of-code-for-a-common-text-pre-processing-step-in-pandas-e3c4f5d60639,One Line of Code for a Common Text Pre-Processing Step in¬†Pandas,,5,19,"['One Line of Code for a Common Text Pre-Processing Step in Pandas', 'Basic splits', 'Specifying the number of splits', 'Controlling the delimiter and expanding splits into a DataFrame', 'Splitting with regular expressions']","Working with text data often means that you‚Äôll get unstructured data that you need to parse to get relevant information. Even if you do get clean data, sometimes you‚Äôll want to do some processing to create new variables out of your existing data. This can be as simple as splitting up a ‚Äúname‚Äù column into ‚Äúfirst name‚Äù and ‚Äúlast name‚Äù. Whatever the case may be, Pandas will allow you to effortlessly work with text data through a variety of in-built methods. In this piece, we‚Äôll go specifically into parsing text columns for the exact information you need either for further data analysis or for use in a machine learning model. If you‚Äôd like to follow along, go ahead and download the ‚Äòtrain‚Äô dataset here. Once you‚Äôve done that, make sure it‚Äôs saved to the same directory as your notebook and then run the code below to read it in: Let‚Äôs get to it! Throughout this piece, we‚Äôll be using just the ‚ÄúName‚Äù column in the table we‚Äôve read above. Our goal will be to use the Pandas split method to try and separate the components of the ‚ÄúName‚Äù column‚Äôs First, let‚Äôs just take a look at what split does when we don‚Äôt pass in any arguments to the optional parameters: By default, split will separate elements in each column value on whitespaces. The above line of code returns a Series, as split will return an object which has the type of its caller (in this case we called split on a Series). So far, the returned columns have lists that can potentially contain different numbers of elements. To control how many splits can be present in the output, you simply need to specify an integer in n: Now, the output is still a Series, but in this case, there is only one split in each column value. This means that the (starting from the left) everything before the first whitespace is one element and everything after the whitespace constitutes the other element. As a result, above you can see that in the first row ‚ÄúMr. Owen Harris‚Äù is no longer separated into different elements. Next, let‚Äôs again try to limit the number of splits, but we‚Äôve noticed that the first element includes the comma present in the original column. To get rid of that let‚Äôs specify the delimiter (character to split on) by specifying it in pat. Let‚Äôs also take a look at the output when we pass True to expand. Instead of having the returned value‚Äôs type match its caller, passing True to expand ensures that split returns a DataFrame with the split values. In our case, we‚Äôve split on the first comma from the left, so everything before the first comma makes up the values in the ‚Äú0‚Äù column, and everything after fills the ‚Äú1‚Äù column values. This is already quite useful as we could potentially rename the columns and use them in further analysis. If need be, you could also add them to the original DataFrame if you wanted to use them for a machine learning model. However, we could still do a bit more, so let‚Äôs see what else split can do. Sometimes we‚Äôd like to be able to customize what character (or characters) to split on. In these cases, we can pass a regular expression into pat. Since we‚Äôre splitting the Name column, you‚Äôll notice that we could potentially split the column values based on the comma, the spaces, the period, and the parentheses. But why not split the column for any of those characters? Let‚Äôs see what that would look like below: The \W part of the expression is used to select any character that is not a letter, digit, or underscore. The + character specifically is used to match one or more of the \W characters. The output gives us all the potential splits in every row. However, we end up with many columns that have only ‚ÄúNone‚Äù values. Because of this, it would actually be better for us to limit the number of splits in our output. If we take a cursory look at the output, we‚Äôll see that there are five components to the name for the most part. Thus, we can get cleaner output by specifying a value for n like so: Our output looks much cleaner now and we could potentially rename the columns to ‚ÄúLast Name‚Äù, ‚ÄúTitle‚Äù, ‚ÄúFirst Name‚Äù, and so on. Then, we could join this to the original DataFrame for use in further analysis or as inputs to a machine learning model. And that‚Äôs all! I hope you found these five tips helpful for when you get started working with a new dataset. I find that running these lines of code before doing anything else helps me figure out how to move forward with my data analysis. Knowing how the data is structured and its basic descriptive statistics will help you determine what steps are necessary for further preprocessing. Good luck with your future Pandas adventures!",20,0,5,Towards Data Science,2021-01-04,2021
https://towardsdatascience.com/back-to-basics-nearest-neighbours-676d6f11a31,Back to Basics: Nearest Neighbours,"Deep learning is over-hyped, it‚Äôs time to go back to the¬†basics.",6,30,"['Back to Basics: Nearest Neighbours', 'Introduction', 'Nearest Neighbour in the context of density estimation', 'Advantages and Disadvantages', 'Nearest Neighbour in the context of Recommenders', 'References']","Given the recent hyped progress in deep learning based approaches in AI research especially in NLP and CV, it is time to go back to the fundamentals. Many of the fundamental methods, such as neighbourhood based methods are surprisingly effective in many areas, even today. We take a brief look at nearest neighbour method and its applications in density estimation and recommender systems. User-based and item-based collaborative filtering are discussed. In the end, the cold start and data sparsity issue are also touched upon. Let‚Äôs start with the problem of density estimation. In plain words, density estimation is the construction of an estimate, based on observed data, of an unobservable underlying probability density function. It can be used to gain insights into your data¬†at¬†hand, even better, it can also be utilized to generate synthetic data. One most common method of density estimation is simply histogram, in which we count the number of points within a bin, and use the tally to visualize the underlying distribution. The main disadvantage with histogram is that it is very sensitive to bin selection both in terms of bin size and bin position. Readers can refer to scikit-learn documentation for better illustrations. A more sophisticated method is to use a kernel (e.g. Gaussian kernel) in place of a simple tally count, and a Parzen window to replace a naive bin. A Parzen window can be intuitively understood as a defined region surrounding a data point. The detailed mathematics is left out intentionally to focus on intuitions. The main idea remains similar to histogram, we still try to estimate the probability density using neighbour points in a region (instead of a bin) using a kernel (instead of simple tally) to smooth the distribution. For more information on kernel density estimation, please refer to PRML by Bishop in Chapter 2 or a more practical guide. In kernel density estimation methods, the region is fixed, while the number of neighbour points can vary depending on local data sparcity. Another approch is to fix the number of neighbour points K, and expand the region until K points are included. This approach is called K-Nearest Neighbours (KNN). KNN methods (also kernel density estimation) are a form of non-parametric methods in that it doesn‚Äôt assume an underlying distribution on training data. Instead, it ‚Äúremembers‚Äù the whole training data, hence it is also called ‚Äúmemory based methods‚Äù. When KNN is applied to classification, it finds the nearest neighbours, or in other words, most similar training data points by predefined similarity measure (more on this later), and use the labels of neighbour points to determine the prediction results by majority vote, for example. Some also tag KNN as ‚Äúinstance-based learning‚Äù or ‚Äúlazy learning‚Äù, because it does not need any training process to fit the training data, the heavy-lifting calculation is delayed until prediction, and prediction is made based on a few instances in the neighbourhood of prediction sample. As we will see later, in real world implementation, some engineering optimization can be made to alleviate this issue. It is always about trade-offs in the world of algorithms. Advantages: Disadvantages: There also exist many remedies to tackle above disadvantages. One method is to use an offline phase to pre-calculate similarity score and store intermediate results, which can be fetched efficiently later during online phase. For a dataset of size N and dimension D, the computation cost to calculate similarity of all pairs of samples are O(DN¬≤). Using the K-D tree, the complexity is reduced to O(DNlogN), which is significant improvement when N is large. Actually, many libraries out there, scikit-learn being the most popular example, already does this by building an efficient data structure during the training phase. Once the fit method is called, depending on the algorithm selection (sklearn offers kdtree, balltree, bruto). If you are looking even faster implementations of such index, check out this blog post. For those interested, the best paper from 2019 RecSys, a top conference of recommender systems shed some lights on some of the issues with deep learning research in this area [1]. And it turns out, neighbourhood based approaches are still forming a strong baseline. KNN methods are used in collaborative filtering (CF) algorithms in recommender systems. At the core of CF is how to find similar users or items to form a peer group or neighbourhood. There are two symmetrical methods of neighbourhood based CF in recommenders, namely, user-based CF and item-based CF. As shown below, in a fruit store where users‚Äô consumer patterns are logged, the goal is to recommend fruit to Bob. In the user-based approach on the left, Alice is considered similar to our target user (in reality, we normally identify a group of similar users), because they both like watermelon and strawberry in their consumption behaviour. Therefore, based on the assumption that similar users share similar fruit taste, we recommend grapes and orange to Bob because those are bought by him peers, but hasn‚Äôt been bought by him. In the item-based approach on the right, we look for similar items to the item that has been bought by the target user. In this case, Bob has bought watermelon, and watermelon is most similar to grapes because they are bought by many overlapping users (both Alice and Eve). Therefore, grapes are recommended. As we have seen, the most vital part of both item-based and user-based CF methods is to choose the right similarity metric and form neighbourhoods. Some of the similarity metrics are: Apart from the problems mentioned previously about KNN methods in general regarding space and time complexity, other issues arise in the application area of recommender systems. Two main issues are: There are many ways to alleviate the issues above: [1] ‚ÄúAre We Really Making Much Progress? A Worrying Analysis of Recent Neural Recommendation Approaches‚Äù, https://arxiv.org/abs/1907.06902 [2] ‚ÄúRecommender Systems: the Textbook‚Äù by Aggarwal [3] Tables made with https://www.tablesgenerator.com/markdown_tables [4] ‚ÄúResearch Commentary on Recommendations with Side Information: A Survey and Research Directions‚Äù, https://arxiv.org/abs/1909.12807 [5] blog: https://albertauyeung.github.io/2017/04/23/python-matrix-factorization.html and code: https://github.com/albertauyeung/matrix-factorization-in-python",154,0,6,Towards Data Science,2021-01-04,2021
https://towardsdatascience.com/how-can-nlp-impact-the-future-of-minority-languages-555b0fc80bd0,How can NLP impact the future of minority languages?,The field of NLP has seen major advancements in‚Ä¶,1,19,['How can NLP impact the future of minority languages?'],"OpenAI‚Äôs GPT-3 paper was introduced May 28th, 2020, becoming a hot topic in the field of AI Natural Language Processing. Its 175 billion parameters transformer architecture made it very popular in both specialized and general news, thanks to the vast landscape of applications that some developers quickly showcased, some of which I listed on my introductory article: towardsdatascience.com Let‚Äôs take GPT-3 as an example. According to the GPT-3 paper, it was pre-trained on massive datasets, including the Common Crawl dataset, which contains petabytes of data collected since 2008 and weights 60% of the total training mix for GPT-3, which also includes other datasets like WebText2, Books1, Books2, etc.. According to the Common Crawl github information, this is the distribution of languages across documents in the dataset: commoncrawl.github.io English is clearly the dominant language in the corpus, representing more than the 40% of total documents, while the second most represented language is Russian, with less than a 10% of the total documents. Some top 10 spoken languages in terms of native speakers are clearly underrepresented. Spanish for example, top 2 language with 460 million native speakers (more than native English-speakers) only accounts for 4% of the total documents in the corpus). In the link below you can find another statistic based on the total number of characters per language on the GPT-3 training dataset: github.com So, how well does GPT-3 manages languages other than English? In an interesting reddit post, a user seemed confused about how it managed to learn coding languages like Python, and received an interesting response from a user by the name of gwern, pointing out another question which is ‚Äúhow GPT‚Äôs architecture & data processing affects what it learns for various things‚Äù. The same reddit user wrote an excellent article on GPT-3 pointing out how the use of BPE (Byte Pair Encoding) can affect different kind of languages (analytic or synthetic, for example) in a different way, therefore introducing implicit bias. You can find the complete article below: As a summary, while the vast amount of training data makes language models like GPT-3 good in several languages even if they are optimized for English and/ or they use a small % of non-English documents, minority languages (even non-minority ones) are still in big disadvantage. Even if language models like GPT-3 don‚Äôt know what they are talking about, our relationships are becoming more and more digital and automated through AI. How well NLP models perform in a specific language can become crucial for their future, with the risk of being substituted more and more in online services by those who are more generally used and therefore optimized for in newer machine learning solutions. That is why some language academies like RAE (Real Academia de la Lengua) in Spain or local governments like Generalitat de Catalunya are launching specific initiatives to tackle this problem: www.rae.es politiquesdigitals.gencat.cat While these initiatives show awareness and are definitely good steps towards improving the use of AI in non-English languages, it is clear that major research is done in English. As an example, one of the latest diverse text datasets on the date this article was written, named The Pile, is an 815Gb English corpus. pile.eleuther.ai What kind of challenges does NLP pose for minority languages? The article below provides an excellent summary, focusing on three aspects: towardsdatascience.com History has shown us that technology has been crucial to keep the memory of languages alive. Maybe the biggest example of that technology is the book, in its different forms. Clay tablets, papyrus, and later books as we know it have been key to preserve not only ancient languages, but also our memory. In another maybe not so well-know story, China faced a huge problem in the 1980s when they realized its 70.000 plus characters could not fit into a keyboard. If they had not solved that issue with the Wubi method, would China still be a technology superpower 40 years later? What will happen in 40 years to languages that do not get enough NLP support? If you enjoyed reading this piece, please consider a membership to get full access to every story on while supporting me and other writers on Medium.",6,0,4,Towards Data Science,2021-01-04,2021
https://towardsdatascience.com/loglet-analysis-revisiting-covid19-projections-5e9d14a46f2,Loglet Analysis-Revisiting Covid-19 Projections,In the last article we showed how to make a forecast‚Ä¶,1,37,['Loglet Analysis-Revisiting Covid-19 Projections'],"Note from Towards Data Science‚Äôs editors: While we allow independent authors to publish articles in accordance with our rules and guidelines, we do not endorse each author‚Äôs contribution. You should not rely on an author‚Äôs works without seeking professional advice. See our Reader Terms for details. In the last article, we showed how to make a forecast for the next 30 days using covid data from the Johns Hopkins Institute with KNIME, Jupyter, and Tableau. The projections were optimized for a logistic growth model. We will show that the decomposition of growth into S-shaped logistic components also known as Loglet analysis, is more accurate as it takes into account the evolution of multiple covid waves. The term ‚ÄúLoglet‚Äù, coined at The Rockefeller University in 1994 joins ‚Äúlogistic‚Äù and ‚Äúwavelet‚Äù. The Loglet analysis is interesting because it can handle multi-peak profiles which is a common challenge for curve fitting techniques. The growth of replicating organisms, such as a virus, is basically characterized by three growth phases: In the first phase, growth is exponential. In mathematical terminology, the growth rate of a population P(t) is proportional to the population. The growth rate at time t is defined as the derivative dP(t)/dt . The exponential growth model is written as a differential equation is: This equation can be solved by introducing e (the base of the natural logarithm, approximately 2.71 . . .). The familiar solution is: where ‚Äúa‚Äù is the growth rate constant and ‚Äúb‚Äù is the initial population P(0). ‚Äúa‚Äù is often expressed in percent. An ‚Äúa‚Äù with a value of 0.02 is equivalent to the statement ‚Äúthe population was growing continuously at 2% per year‚Äù for example. Although many populations grow exponentially for a time, no finite system can sustain exponential growth indefinitely unless the parameters or limits of the system are changed. Let‚Äôs try to visualize this growth in Jupyter-Notebooks: 2. Logistic growthSince few systems sustain exponential growth over time, the exponential growth equation must be modified with a limit or carrying capacity that gives it the more realistic sigmoidal shape.The most widely used modification of the exponential growth model is the logistic one. It was introduced by Verhulst in 1838, but popularized in mathematical biology by Lotka in the 1920s. The logistic equation begins with the exponential model but adds a ‚Äúnegative feedback‚Äù term that slows the growth rate of a population as the limit K is approached: Notice that the feedback term (1‚àíP (t))/K is close to 1 when P(t) << K and approaches zero as P(t) -> K. Thus, the growth rate begins exponentially but then decreases to zero as the population P(t) approaches the limit K, producing an S-shaped (sigmoidal) growth trajectory. There are a number of closed form solutions to the logistic differential equation. We will use for our further analysis the Fisher-Pry equation: Equation above produces the familiar S-shaped curve. The three parameters are needed to fully specify the curve, ‚Äúa‚Äù, ‚Äúb‚Äù, and K. The growth rate parameter ‚Äúa‚Äù specifies the ‚Äúwidth‚Äù of the sigmoidal curve. Let‚Äôs again to try to visualize this growth in Jupyter: Further it is often better to replace ‚ÄûŒ±‚Äú with a variable that specifies the time required for the curve to grow from 10% to 90% of the limit K, a period which we call characteristic duration Œît. To achieve this, we have to perform the following algebraic transformations. First, we calculate the time needed to reach 90% of the total growth. Then the time to reach 10% of the total growth which we subtract from the value for 90%. The characteristic duration is related to ‚Äûa‚Äú by ln(81) / Œît. The parameter Œît is usually more useful than ‚Äûa‚Äú for the analysis of historical time-series data because the units are easier to appreciate. The three parameters K, ‚àÜt, and ‚Äûb‚Äú define the parameterization of the logistic model used as the basic building block for our Loglet analysis. 3. Multi-logistic growthIn the last article, we showed that the logistic growth model can be a good approximation for the evolution of a covid19 wave. But since this spread is not limited to a single wave, we need an extended approach to describe the entire process. Many growth processes consist of several subprocesses. First, let us model a system that experiences growth in two discrete phases. Then, we will extend this model to an arbitrary number of phases. In the so called Bi-Logistic model, growth is the sum of two ‚Äúwavelets‚Äù, each of which is logistic: where The implementation of the Bi-Logistic model requires 6 parameters and is visualized in Jupyter as follows: The next figure shows the decompositions of the Bi-Logistic wave. The blue pulse is superimposed by the orange pulse that follows. In this way, the seemingly complex behavior is reduced to the sum of the two logistic wavelets. The next step is a generalization of the Bi-Logistic model to a Multi-logistic model, where growth is the sum of n simple logistics: where The following Jupyter code which is also available on my github-site, predicts for Switzerland the expected covid-19 cases for the next 30 days and requires the combination of 4 logistic wavelets. For other countries a different number of wavelets are necessary. We will come to this point below.Fitting of the models was performed using the curve_fit function of the scipy package in Jupyter. For every country, the model fits several logistic pulses and calculates the parameters which optimally describe the spread of the virus. How much training data is necessary?We have now created a model that can approximate multiple consecutive and overlapping covid waves. But now the question rises: how many waves do we have to consider? This is a parameter that has not been calculated yet. ‚ÄúStudy the past if you would define the future.‚Äù‚Äï Confucius In the last article, we decided to consider only the cases of the last 90 days to fit our logistic model. With our new Multi-logistic model, this limitation is no longer necessary because we can use data from the first day of the pandemic outbreak. But now we have a new parameter to choose for the number of wavelets. This is an open point an we decide to take the following strategy:It‚Äôs now Jan 2021. For every country we start to fit the model with 4 wavelets. If the model doesn‚Äôt converge, we reduce it to 3 wavelets and so on until we found the optimal approximation. Putting it all togetherWith the code above it‚Äôs already possible to make forecasts for every country, but we want to implement it in our existing KNIME-Workflow process for better data transformation and subsequent visualization in Tableau.As you remember from the last article we use KNIME to load the covid19 data from the Johns Hopkins institute, transform it and calculate the forecasts with Jupyter. Finally we export the data in Tableau to visualize it. The KNIME-workflow is available on my KNIME-hub site and the Tableau Dashboard is published with the actual data nearly every day on my Tableau-Public site here This extended Loglet-prediction method is much more flexible than the simple logistic analysis because it takes into account the evolution of continuous waves. New waves or mutations of the virus can thus be tracked and extrapolated more quickly. And that is becoming more and more important as this virus has already mutated into a more contagious version in the United Kingdom and in South Africa. medium.com Follow me on Medium, Linkedin or Twitterand follow my Facebook Group ‚ÄúData Science with Yodime‚Äù Material for this project:Jupyter-Code: githubknime-workflow: knime-hubTableau-Dashboard: Tableau-Public References- Covid 19-Projections with Knime, Jupyter and Tableau- A primer on logistic growth and substitution: The mathematics of the Loglet Lab software- Scipy-documentation As first published in KDnuggets.",15,0,9,Towards Data Science,2021-01-04,2021
https://towardsdatascience.com/the-best-data-science-is-permissionless-8c0d139a7369,The best data science is permissionless,Moving beyond ‚ÄúThe Box‚Äù and other advice for aspiring¬†data‚Ä¶,1,29,['Stop waiting for someone to ‚Äúgive you a chance‚Äù to be a data scientist'],"For many years, it has been a commonly accepted mantra in tech circles that the best innovation is ‚Äúpermissionless‚Äù ‚Äî that is, it is driven by outsiders working on the fringes of existing power structures and is more likely to take place in people‚Äôs garages rather in universities or large corporations. The blockchain community has vigorously embraced the concept of permissionlessness as an all-encompassing ethos governing its core technologies as well as the type of society those technologies are intended to usher in. As a data scientist who spent nearly a decade learning from Nick Spanos, who founded Bitcoin Center NYC, the world‚Äôs first physical bitcoin exchange, in 2013, it recently occurred to me that data science is one of those remarkable fields that is truly permissionless ‚Äî and more so today than ever before. Unlike rocket science or civil engineering, which are not only highly regulated fields but require large amounts of resources in the real world, data science today just requires a laptop, learning core statistics and computer science principles, and learning a statistical programming language such as R or Python ‚Äî all of which can easily be self-taught. The other ingredient is access to data, which is literally all around us, and can easily be obtained in unimaginable quantities from open data sets or scraped from the internet. (Of course, the truly secret ingredient is finding a good question to ask, but this can only be learned through relentless repetition and trial and error.) All of this occurred to me a few weeks ago when I heard a heart warming story from a young man in an interview. (Stories like this used to very common but I‚Äôm not sure what happened.) He had been working in analytics for a few years and then lost his job in a layoff. He tried very hard to find a job but he couldn‚Äôt ‚Äî so he started his own company! He hacked on it for 9 months, built an actual product, got clients, and even built up an entire offshore engineering team to sustain and iterate on the product. (Literally nothing on his resume would have led one to believe he had any of that in him ‚Äî before he demonstrated that he had it in him all along.) He also asked me hesitantly how a big company would view his entrepreneurial journey and I assured him that big companies are the ones in greatest need of entrepreneurial thinking because they realize that without innovating there is no way they can keep up with the competition ‚Äî which is coming fast and furious from all directions. I only wish many of the aspiring data scientists whose resumes I routinely come across would realize this ‚Äî what matters is not just what skills and abilities you have, but what you do with them is what matters most. For example, anyone who has reviewed data science resumes will be very familiar with what I call ‚ÄúThe Box‚Äù ‚Äî almost every resume has one. The Box is an impressive and intimidating ‚Äúeverything but the kitchen sink‚Äù list of tools and technologies that the applicant claims to possess (see below for some examples from actual resumes). The problem is only a tiny percentage of these resumes provide any meaningful examples of what the applicant has built in the real world using these tools. The tools and technologies you know are only a means to an end ‚Äî and without the end, they are an indictment of your unreached potential. (It‚Äôs also ironic given that the vast majority of day-to-day data science and analytics in large tech companies requires just three tools: SQL, R, and Python.) This is why the most important question I ever ask in a data science interview is ‚Äúwhat have you built with data?‚Äù or alternatively ‚Äúhow have you used your data skills to drive impact?‚Äù Every aspiring data scientist should make sure they have solid experiences and answers to point to in response to this question, the more interesting the better ‚Äî there is no other way to stand out amongst the thousands of graduates every year who are trying to get into ‚Äúthe sexiest job of the 21st century.‚Äù We live in a world in which data is ubiquitous and widespread and almost entirely free. We literally live in a world that is bursting at the seams with data. Not only that, the tools necessary to analyze this data are widely accessible and free. (This is no longer the 1990s when you needed an expensive SAS or SPSS or Minitab subscription ‚Äî R or Python are completely free and far more useful than all three of those combined.) This means that the barrier to entry to data science and data analytics in some sense has never been lower, but ironically because of that reason, it is now actually quite high! By this, I mean, the job market has been flooded by candidates with solely technical skills and a desire to climb the Silicon Valley ladder but no drive or intuition or innate curiosity or ability to think outside the box. So many of them are still complaining about how ‚Äúevery position requires X years of experience‚Äù without realizing that, unlike in rocket science or bridge building, there is literally nothing at all preventing them from gaining experience in data science or data analytics. Data is everywhere and the tools are all free ‚Äî what else do you need?! Just like the young man I interviewed who saw no reason to wait for some company to ‚Äúgive him the chance to manage a product‚Äù, instead he built his own product and now he is on the pathway to interacting with those same companies on an equal playing field, not as someone supplicating for a chance to prove themself, but as a peer technologist who has also created something of value in the real world. One reason, I believe, that we see less of this now than we used to back in the day is because technology, and data science in particular, has become a stable field that naturally attracts more career-oriented people rather than creation-oriented people. These career-oriented people can only imagine coloring within the lines and only coloring when they‚Äôre given permission to color. Even when the skillset they have enables them to answer an infinitely vast array of questions, they remain focused on the most boring questions, lacking even the faintest attempt at creativity. For them, their skillset is solely a means to an end, and that end is a 9-to-5 job, so without that job, how can they even think of using their skillset to add value to the world? Meanwhile, for the entrepreneurial free thinkers who first entered these fields before they became stable careers, the career, and even the job, was always incidental to doing what they loved to do ‚Äî which was creating, building, and using technology to understand the world. My advice to new and aspiring data scientists ‚Äî whether you are a recent graduate or are working at a startup or even at a big company ‚Äî is the same. You don‚Äôt need to wait for anyone to ‚Äúgive you a chance‚Äù to become a data scientist. The best ‚Äî and only ‚Äî way to become a data scientist is by doing data science. It‚Äôs by searching deeply within yourself and finding the intersection between your unique skills and abilities and your passions and purpose and figuring out what is the unique contribution that only you can make in the world. (I alluded to this in my TEDx talk at NYU in 2017.) For most people, it‚Äôs not about entering Kaggle competitions and trying to improve on someone else‚Äôs model by 0.1%. Rather, it‚Äôs about asking novel questions, turning raw, messy data into curated datasets, and finding insights and patterns in that data that are valuable to someone ‚Äî even if that‚Äôs just one other person. Data scientists are explorers ‚Äî so let‚Äôs go out and explore. HAMDAN AZHAR is a data science, analytics, and research leader with over 10 years of experience discovering meaningful insights in complex datasets and using storytelling to drive business, product, and social impact. The founder of PRISMOJI, he was formerly a director at Blockchain Technologies Corporation, and prior to that, was a data scientist at Facebook, at GraphScience, and on Ron Paul‚Äôs 2012 presidential campaign.",236,0,6,Towards Data Science,2021-01-04,2021
https://towardsdatascience.com/data-science-i-wonder-if-its-still-sexy-in-2021-b68006c78f71,Data Science: I Wonder If It‚Äôs Still Sexy In¬†2021,Understanding Whether Data Science is still Sexy¬†and‚Ä¶,1,47,['Data Science: I Wonder If It‚Äôs Still Exciting In 2021'],"The increase of data being produced each year was the provocative gesture that enlightened businesses to take more action and use the data at their disposal. Although data-driven decisions were around before 2012, a reputable write-up published in Havard Business Review attracted the attention of many. ‚ÄúTheir sudden appearance on the Business scene reflects the fact that companies are now wrestling with information that comes in varieties and volumes never encountered before‚Äù ‚Äî Havard Business Review, 2012. With a title such as Data Scientists: The Sexiest Job of the 21st Century, It‚Äôs not difficult to understand why this piece may have been an influential factor in why many people want to become Data Scientists today. However, as we rapidly approach a decade since the original piece was published, I was curious to determine whether Data Science may have lost its sexiness over the years and whether it is a good career to pursue. Depending on who you ask, you may be given an extremely different answer to this question. For instance, Nate Silver an American statistician believes Data Science is simply another name for Statistics whereas, Andrew Gelman, also an American Statistician (and professor of statistics at Columbia University) has described statistics as a non-essential part of Data Science. Despite the confusion, I believe we could all agree that Data Science is focused on extracting knowledge and insights from datasets using scientific methods, processes, algorithms, and systems, thereby encompassing analysis, ensuring data is in a good enough state to be analyzed, and then providing insights to inform high-level decisions. If we want to know whether Data Science would be a good career, it is important we define the things that make up a good career. Many often would often imagine that finding the right career consists of finding one's passion through a flash of insight or determining the easiest job that pays the most money. However, according to a 2017 career guide titled, We reviewed over 60 studies about what makes for a dream job. Here‚Äôs what we found, there is no evidence to back up these claims. Instead, from their research, they were able to come up with 5 key ingredients (It was actually 6, but I felt as though the 6th one was a bit repetitive) that go into making the dream job of which I will be using to determine whether Data Science is still a good career to pursue in 2021. Contrary to popular belief, salary, status, and the type of company one may work for do not contribute as much as one may think to whether work is engaging. In fact, what determines whether work is engaging is the work that is done on a Day-to-Day basis. ‚ÄúEngaging work is work that draws you in, holds your attention, and gives you a sense of flow.‚Äù ‚Äî Benjamin Todd, Author at 80,000 hours Come to think about it, this makes a lot of sense ‚Äî think about how time whizzes past when we are playing a fun game. We find Games engaging since we are provided with: To bring this back into context, a Data Scientists' daily task would generally revolve around data ‚Äî no surprise there. It‚Äôs often quoted that much of Data Science work is gathering, analyzing, or cleaning data to put it into a suitable format to do further work. However, the full scope of the job goes beyond this and there are other activities that are just as important as the technical task listed above, such as communicating with non-technical stakeholders and keeping up with changes in the field. Many would agree that Data Science is pretty much an art as much ‚Äî if not more ‚Äî as it is a science hence providing practitioners with the freedom to perform the task at hand. The issue is that all the freedom in the world doesn‚Äôt mean that value is being created as we‚Äôve seen in the past. There was a popular article in 2017 ‚Äî Big Data Strategies Disappoint with 85% Failure Rate ‚Äî that made a major claim that data projects failed to deliver the expected transformations that companies were expecting. It would be criminal of me to point my finger to say that is the absolute reason for Data projects high delivery rate, yet, for the sake of this post emphasis will be put on the Prolonged turn around time, and massive effort without visibility into the potential value since this covers a keys points of engagement ‚Äî feedback. Data Science projects require lots of effort, despite there being a lack of visibility into the eventual outcome of the project and its business value. Traditional projects can take a very long time to complete until the outcome can be evaluated and the delayed feedback can be demoralizing especially if after all the effort and finances invested, the project is a failure. Additionally, if you are working for a smaller company you are less likely to be able to work on a variety of projects since they may not be as willing to put so much investment towards something that may or may not work. My Rating (Engaging Work): 3/5 Gaming may provide a high level of engagement, but where it lacks is in its ability to help others‚Äî although this is beginning to shift as many are finding a great deal of entertainment in watching others game which can be considered as a form of help. Growing evidence has emerged that helping others plays a major role in our life satisfaction and if that‚Äôs the case, here is where Data Science thrives. ‚Äî For more on this you may read 10 benefits of helping others which was published by University College London regarding more charitable support, although I am under the conviction that people who use business to support others will also reap the same benefits. Data is generated by people and if Data Science is about using that data to discover knowledge and insights to make informed decisions to better others then I believe the inherent nature of Data Science is all about helping others. Of course, when the projects do not fail. I believe this is most eminent on platforms such as Youtube, Amazon, and Netflix which use recommendation systems to aid user efficiency and to provide resources that one may of never explicitly searched for but is still extremely valuable. My Rating (Work That Helps Others): 4/5 Absolutely nobody enjoys constantly getting spanked at anything. Including their job. When you are good at what you do it provides you with a sense of achievement which is also considered a vital substance of life satisfaction. When you are good at what you do, it serves as leverage to: The dilemma we have with this ingredient is that absolutely nobody begins by being good at their job. In fact, it is almost certain that your first few attempts of anything in a new role would be shabby. Therefore, to properly evaluate this category we have to assume that the person interested in being a Data Scientist is someone that is eager to improve and we can then begin to consider the number of options open to them for improvement. Where Data Science adopts a fairly new role in business, many of the early adopters had to rely on textbooks to begin their journey in Data Science. As time has gone on, thousands of Data Science MOOCs have been developed and accessible books, talkless of the great open-source community. It also helps that a key aspect of the Data Science workflow, which is the programming language being used, is narrowed down to a dispute between R and Python ‚Äî Unlike Software Engineering which has many programming languages with their own Idiosychrosies, pros, and cons. In that respect, I‚Äôd say if you want to become good at Data Science then there are more than enough resources out there to develop yourself and take your skills to the next level. The sole factor of whether you become a good Data Scientist is yourself. On that note, I‚Äôd give being Good at your job a 5/5. My Rating (Work You‚Äôre good at): 5/5 If you have colleagues you completely dislike and your boss acts like he was employed by Satan to torment you, then it‚Äôs highly unlikely you‚Äôd be satisfied with your job. Good relationships are a major factor when we discuss the topic of a fulfilling life, hence it's vital that we are able to establish friendships with a handful of people where we work. This doesn‚Äôt mean everyone must become friends, instead what matters is that you are able to receive assistance when you become stuck on a task. Supportive colleagues do not essentially come in the form of the big softy that is scared to tell you that the work you‚Äôve produced is a bunch of trash. Adam Grant, an American Psychologist, would tell you that the best employees are not in fact the agreeable ones. ‚ÄúDisagreeable givers are the people who, on the surface, are rough and tough, but ultimately have others‚Äô best interest at heart. [‚Ä¶]. They are the people who are willing to give you critical feedback, that you don‚Äôt want to hear but you have to hear‚Äù ‚Äî Adam Grant Whether you have supportive colleagues or not is totally dependent on the organization you are at so it‚Äôs not a good way to measure this ingredient. Instead, I will base my evaluation on the Data Science community as a whole of which generally, you would hear is one of the best in tech due to how willing people are to share their research. However, where I feel the field is lacking is in the diversity department. You often find that the most disagreeable people are usually the ones that are very different from ourselves, and provided they have our best interest at heart, their feedback is often very valuable. Being apart of the underrepresented ethnicities in tech, I feel lots of big transformations are happening in the world as a result of big data, and the people that are being affected most have the least input. My Rating (Work You‚Äôre good at): 4/5 For work to be truly satisfying, the job should lack the things that make work unpleasant, such as: Commute time is subjective to the individual, although, in the midst of the pandemic, I do believe that we are in a transition phase where remote work is going to become more prominent (especially for technical roles) hence making this criterion potentially obsolete. I couldn't find much data regarding how long actual Data Scientist work on average, but I did find a pretty interesting subreddit that discussed that 25 hours of the week is spent waiting for work and 10‚Äì15 hours is the amount of time spent doing actual work ‚Äî This makes it around 40 hours a week. I do believe the latency between being set a task and actually receiving the data is pretty common in most organizations. Data Science salaries are range quite a bit in my opinion. Since there has been no clear distinction agreed upon in business, oftentimes somebody could be paid less to do significantly more work than someone else at another organization. Job Insecurity is another tricky one. From my own personal experience, I‚Äôve realized that Data Science is very different from Software Engineering in the sense that Data Science requires a lot more freeflow and it could be difficult to distinguish a definitive end, whereas Software engineering has a clear goal that needs to be reached. As a result, it could sometimes seem like you aren‚Äôt doing really doing anything as a Data Scientist, especially when you don‚Äôt have much to show for what you‚Äôve done ‚Äî If we are going off this then I‚Äôd say job security isn't one of Data Science most glamourous appeals. My Rating (Lack Of Major Negatives): 3/5 The total sexiness score I give Data Science is about 19/25 which is quite high. I would say much of the hype that was driving business is slowly dying down and more products of value are being developed. I‚Äôd definitely recommend a career in Data Science to anyone that has aspirations to be one. But I‚Äôd advise anyone that is considering this career to be ready to commit to developing themselves and learning something new constantly as the field evolves quickly. It‚Äôs important to note that all of the ratings are based on my opinion and not definite. If you‚Äôd rate something I‚Äôve rated differently then leave a response to share how you would rate it as I‚Äôd love to know. Connect with me on LinkedIn and/or Twitter towardsdatascience.com towardsdatascience.com towardsdatascience.com",49,0,10,Towards Data Science,2021-01-04,2021
https://towardsdatascience.com/well-architected-framework-security-aspects-of-the-cloud-1119417f7cb8,Well-Architected Framework‚Ää‚Äî‚ÄäSecurity Aspects of the¬†Cloud,Assume breach is obvious and use the¬†,9,28,"['Well-Architected Framework ‚Äî Security Aspects of the Cloud', 'Introduction', 'Defense in Depth', 'Identity Management', 'Encryption', 'Network Security', 'Infrastructure Security', 'Conclusion', 'References']","In this second post on Cloud Well-Architected Framework, I will discuss in detail the Security pillar of the Cloud Well-Architected framework. I will elaborate on the concept of defense-in-depth for secure architecture, examples of tools and processes offered by the Public Could providers to secure the infrastructure, application, and data. I will also briefly touch upon secure identity management, network security, and infrastructure security. This series of posts will cover Cloud Well-Architected Framework in general, the reference URL to other posts are:[1] Well-Architected Framework ‚Äî Performance Efficiency on the Cloud It encompasses the holistic approach to identify key concepts of security components, technologies and approaches to support a defense-in-depth strategy at each layer. This strategy empowers the organization to employ a series of processes and governance mechanisms to slow the advance of an attack that‚Äôs aimed at acquiring unauthorized access to information. If an attacker breaches the protection by one layer then there is a subsequent layer to prevent further exposure. Therefore, the objective of defense in depth is to protect data and prevent it from unauthorized access. The baseline for security policy is to ensure‚Äî Let's help you visualize defense in depth through layers of security implementation to protect the data information. This approach removes dependencies on any single layer for protection, acts as a shield to slow down an attack, and provide alerts telemetry that can be acted upon, either automatically or manually Here is the summary of layered security implementation: Identities are an integral part of today‚Äôs business and social transactions whether on-premise or online. Cloud provides various robust capabilities to an application: SSO to application users, enforce MFA (Multi-Factor Authentication) for all sign-ins outside the company‚Äôs network. With SSO users just need to remember only one ID and password. Access across the applications for the user is tied to a single identity, thus, it's simple to manage the access modification; e.g, if the user leaves the organization; disable its account. Implementing Active Directory (AD) based SSO allows an organization to use a centralized identity provider, enable centralized the security controls, reporting, alerting, and administration of identity infrastructure. Multifactor authentication provides additional security for your identities by requiring two or more elements for full authentication. These elements fall into three categories: Even if the attacker has the authentication information for a single factor, it may not be sufficient for identity verification, therefore, the attacker would be able to use those credentials to authenticate. Encryption is the most crucial weapon to secure the data, it is the process of making data unreadable and unusable. To use or read the data requires the use of a secret key. You should protect the data by encryption at rest and encryption in transit. The data is stored on a physical medium which can be: disk, database, or tape, so irrespective of any storage mechanism the encryption at rest ensures that data is always in unreadable format for any unauthenticated user without having a secret key. Data that is moving from one location to another, e.g, client-to-server, and vice-versa across the internet or any other untrusted n/w require to be encrypted while at transit. An organization can handle secure transfer by encrypting the data before sending it over a network or setting up a secure channel to transmit unencrypted data between two systems. Data classification solves the critical purpose of identification of critical v/s non-critical data for an organization. It enables to establish the process and policies for data governance for critical, sensitive, non-critical, and insensitive data objects. Accordingly, the data can be classified as: restricted, private, or public. Various factors influence the data classification for governance, such as business requirements and legal compliances (PCI, HIPAA, GDPR, etc). The regulatory requirements that an organization must adhere to often drive a large part of the data encryption requirements. All major public cloud providers, Azure, GCP, or AWS, support data encryption at various levels. They all provide tools and mechanism to: The network is the front-door for entering your organization‚Äôs data castle. Therefore, securing the organization‚Äôs network from attacks and unauthorized access is important for any architecture design. Network security can be looked at from various dimensions, such as: Limiting and eliminating attacks from the internet, few approaches that can help to achieve it are ‚Äî evaluate and inbound and outbound communication through the secured firewall, or using a layer-7 WAF also protect the perimeter to prevent XSS or SQL injection or prevention from other OWASP 3.0 vulnerabilities. Limit communication between resources to only what‚Äôs required by enabling security and protection inside a Virtual Network. Layer-3 protection can be enforced among communicating VMs by using Network Security Groups by enabling policies for allowing and restricting the communication among the applications running on VMs or PaaS services. Secure network integration between on-premise and cloud Virtual network. Virtual private network (VPN) connections are a common way of establishing secure communication channels between networks. Having a dedicated, private connection between your on-premise network and public cloud extends your on-premises networks into the cloud over a private connection facilitated by a connectivity provider. Overall infrastructure security on the Cloud is critical for the security posture of the organization. All public cloud providers have enabled mechanism to prevent unauthorized privilege access to the Cloud resources, such as restricting access to VMs and databases using IAM (Identity and Access Management) services. Role-based Access control (RBAC) mechanism helps in defining Roles as collections of access permissions, thereafter these roles are assigned to the Security Principals and later mapped to the Users & Groups. The users and groups are entities defined within AD (Active Directory). It is a way to provide identities to the services and the Cloud resources. The service principal is an identity that a service or application uses and like any other identities, it can be assigned roles. In this post, we discussed some of the key principles of the Security pillar of the Cloud Well-Architected Framework. I tried to highlight the importance of security in your architecture through the defense-in-depth by implementing the layered approach and address security at each layer. I also touched upon identity management and the importance of implementing secure applications by using SSO & MFA. Also, I tried to explain how encryption is often the last layer of defense against access to your data. Lastly, We looked at ways to secure traffic flow between applications and the internet. In a subsequent post in this series, I will highlight some other pillar of Cloud Well-Architected Framework. Connect with me on LinkedIn to discuss further [1] Microsoft Azure Well-Architected Framework ‚Äî Security[2] AWS Well-Architected ‚Äî Security[3]Google Cloud‚Äôs Architecture Framework ‚Äî Security",36,1,6,Towards Data Science,2021-01-04,2021
https://towardsdatascience.com/codir-train-smaller-faster-nlp-models-bac6318a9950,"CODIR: Train smaller, faster NLP¬†models",A unique approach to knowledge distillation,1,10,"['CODIR: Train smaller, faster NLP models']","The field of natural language processing has been revolutionized with the advent of large pretrained models like BERT and GPT-3. These models have been able to capture incredible amounts of information from the vast amounts of text they‚Äôve been trained on and use this information to reach state of the art performance and continually improve on a wide variety of tasks like classification, summarization and entailment. One reason for these models‚Äô great performance is their size. BERT-base has 110 million parameters, BERT-large with 340 million parameters, GPT-2 with 1.5 billion parameters and GPT-3 has a whopping 175 billion parameters. As these models grow in size, so does the time and resources required to pretrain them. To make these models cost and resource efficient, a stream of research focused on reducing model size has been burgeoning. Several techniques have been adopted to do this: Generally, knowledge distillation consists of 2 parts: The researchers at Microsoft Dynamics 365 AI Research try to approach the distillation problem through a different lens. Instead of trying to minimize the distance for all examples between the teacher and the student, why not try to maximize the distance between false examples and minimize the distance between the true examples? This led them to propose CODIR, a technique that uses contrastive learning to distil the teacher‚Äôs hidden states to the student. CODIR can be applied to both the pretraining and finetuning stage. Unlike traditional knowledge distillation techniques, CODIR has 3 components: Applying CODIR to the finetuning stage is very straightforward. Since the training examples have labels, negative examples are just those which have a different label than the positive example. In the pretraining stage, there are no labels so applying the contrasting learning objective becomes a little more challenging. The researchers test CODIR on the RoBERTa model, which uses masked language modeling as the pretraining objective. In this case, negative examples would be masked sentences or text from the same article so that they‚Äôre semantically similar to the positive examples. This way, the contrastive learning objective doesn‚Äôt become too easy to solve. The researchers use CODIR to reduce a Roberta model to 6 layers and verify its effectiveness on the GLUE dataset. This model achieves almost the same accuracy as the original BERT model with half the inference time. Here‚Äôs a link to the paper if you want learn more about CODIR, a link to the code for you to distil your own models and click here to see more of our publications and other work. References",26,0,4,Towards Data Science,2021-01-04,2021
https://towardsdatascience.com/ml-apps-using-dstack-ai-953d20a489b,ML Apps using dstack.ai,,2,29,"['ML Apps using dstack.ai', 'Deploying your app on dstack.cloud']","In this article, we are going to see how to use dstack.ai for creating ML apps. I have taken the loan prediction dataset. This particular loan prediction app helps predict whether the customer should be given a loan or not based on different parameters. It‚Äôs a classification based problem and gives the output as Yes/No(1/0). dstack.ai is a python framework to create AI and ML apps without writing any frontend code. Before we get started I am assuming that you have installed dstack server on your system and it is running. You can start the server by the following command. If you wish to change the port you can do that also. Let‚Äôs get started now. We are going to use the loan prediction dataset and deploy it on dstack. Importing some libraries along with dstack and dstack controls Functions to get the train and test data. The ds.cache() is used to quickly retrieve the data and not load it everytime we run the program We are going to make 3 ds.apps viz Predicted, Scatter plot and bar plot and then put them on a frame as tabs for user to access(see below image). First, we look at the plot and then later on the ML part. Let's create 2 functions scatter_handler and bar_handler. These two functions return the scatter and bar plot respectively from plotly.express library Now, we need to add these as tabs on a frame. To do this, we create a frame first. You can see the frame on the dashboard image above. Once this is done, we add the functions as apps on the frame. We pass function created to the ds.app function as a parameter and mention it as a tab. Then we push the frame to the dstack application. When you run your application you can view your dstack app now. Let‚Äôs look at the ML part with dstack We get the training and testing data and separate the output from the training data. Then we do encoding to convert our categorical into numerical values. This is a simple pre-processing of the data, make sure you do all the necessary steps before using any ML model on the data. We are training a simple random forest algorithm from the sklearn. Once the model is trained, you will have to push to the dstack application. It will look like this. This is where all your models will be stored and you can use it. To use the model you will have to pull it. Once you have pulled it now you can use it to predict on your test data. We are going to create dropdown called as Combobox in dstack and it will contain one value called as ‚ÄòPredict‚Äô. Create a function to predict the model on the test data. We have to now create an app and add the function to the frame as a tab. That‚Äôs it. When you run the application now it will show you the Predicted tab. When you click on Apply it will show you the predicted values with the loan status. You can also deploy your app on dstack.cloud. The following steps will help you to do so (You must have an account on dstack.cloud). Load Prediction dstack app Here is the documentation link on dstack.cloud You can find the code on my Github: https://github.com/aniketwattamwar/Create-ML-apps-using-dstack.ai Hope this helps. Peace.",154,0,4,Towards Data Science,2021-01-04,2021
https://towardsdatascience.com/making-movies-a68fc0f0852e,Making Movies,Providing business recommendations to help executives of a new movie studio maximise‚Ä¶,8,21,"['Making Movies', 'Problem Statement', 'Business value', 'Data', 'Methodology', 'Challenges', 'Findings and recommendations', 'Future Work']","In this blog post, I will describe the project I completed to a non-technical audience. For the code, I would encourage you to look at my GitHub repository. Executives of a new movie studio are after actionable insights to maximise their return on investment and ensure successful movies are produced. ‚ÄúAvengers: Endgame‚Äôs $1.2 billion opening weekend is the biggest in movie history‚Äù ‚Äî Vox, April 2019. ‚ÄúBox office cats-tastrophe: Cats projected to lose $70m‚Äù ‚Äî The Guardian, December 2019. From these two contrasting headlines, we see that entering the movie industry can be viewed as a high risk/ high reward venture for our stakeholders. There is potential but need to ensure the ‚Äúright‚Äù movie is made. Through data analysis we will seek to provide recommendations to maximise the chance of success. The main data used for this project came from two sources. Data from IMDB consisted of 146,144 entries with start year, runtime and genres as key features. Data from the-numbers consisted of 5,782 entries with release_date, production_budget, domestic_gross and worldwide_gross as key features. We also scrapped data from Wikipedia relating to Netflix Original Movies. The first stage focussed on data preparation including:- Importing libraries- Reading and cleaning provided data- Dealing with missing values- Joining datasets- Scraping additional data and cleaning it The second stage focussed on visualisations and insights including:- Conducting feature engineering where applicable- Creating visualisations- Drawing conclusions- Providing recommendations The first challenge was to decide what is meant by successful movie. I looked at two metrics profit (worldwide gross revenue less budget) and profit margin (profit over revenue). I saw that the most profitable movies were the action/adventure blockbusters you‚Äôd expect. However the movies with the highest profit margin were mostly horror films with a small budget. We noted that the movie industry is notorious for using opaque accounting methods and it is unlikely that we‚Äôd get an accurate picture of the financials using such such metrics. We have not taken into consideration other forms of revenue including merchandise and DVD sales nor other costs including deferments to be paid to talent. Due to the ambiguity we focussed on worldwide gross as a metric of success. The second challenge was combining the data from two different sources. The IMDB data had relevant details such as runtime, genre and release date whereas the data from the-numbers had the worldwide gross and budget figures. However there was no unique movie identifier across the two sources and as such the only option was to merge the DataFrames using the movie name, i.e. a string. This was not ideal as strings are sensitive to spacing, punctuation. Moreover, there was a risk of erroneous matching where movies had the same name. With more time, we would have explored alternative methods. There was no clear correlation between production budget and worldwide gross, with the top 100 grossing movies post 2010 having varying budgets ranging from $35m to $410m. As discussed earlier, the financial information publicly available is limited and further analysis would be required to help determine the optimal budget. The average runtime for a movie based on our data was 100 minutes. However when looking specifically at the top 100 grossing movies, the runtime was longer, at 120 minutes. We would recommend aiming for a 2 hour movie as that appears to be the right duration for cinema goers. We next investigated genre. Each movie was assigned up to three genres. In the top 100 grossing movies post 2010, we saw that more than 80 were described as adventure movies and 60 were considered to be action. As such aiming for an action/adventure movie is advisable. However we saw that these type of movies cost on average $45m more to make. Other genres worth considering are sci-fi, comedy or animation. Finally, we looked at when movies were released. The release months associated with top grossing films are May/June, the start of the Summer blockbuster season and November/December coinciding with the holidays and optimised for the Awards in the new year. The movie studio should aim to release their movie during one of these two periods. As a final consideration, whilst not explicitly requested by our stakeholders, it might be worth investigating the online distribution of movies, with the rise of platforms such as Netflix. We briefly examined Netflix Original Movies and saw that comedy/drama stood out (as opposed to action/adventure movies). This makes sense as action/adventure movies are more suited to the box office benefiting from the large screen and more immersive experience. We also saw that on average Netflix Original Movies had a runtime of 99 minutes. As next steps we would be keen to perform the following: I hope you found this project review interesting. If you‚Äôd like to see more, please visit my GitHub repository, which features the Jupyter Notebook as well as a non-technical presentation.",122,0,5,Towards Data Science,2021-01-04,2021
https://towardsdatascience.com/latest-picks-meta-policy-gradients-a-survey-9fc11033006a,Latest picks: Meta-Policy Gradients: A¬†Survey,Your daily dose of data¬†science,0,2,[],Latest picks: In case you missed them:,24,0,1,Towards Data Science,2021-01-04,2021
https://towardsdatascience.com/coding-linear-regression-from-scratch-c42ec079902,Coding Linear Regression from¬†Scratch,Implementing Linear Regression from absolute scratch using¬†python,10,53,"['Coding Linear Regression from Scratch', 'Data', 'Hypothesis', 'Cost Function', 'Gradient Descent', 'Putting it all together', 'Test', 'The Complete Code', 'That‚Äôs it!', 'References']","This post follows the linear regression post in the ‚ÄòBasics and Beyond‚Äô series so if you are just getting started with machine learning I would recommend going through that post first and then starting with this tutorial. If you already have an idea about what linear regression is then lets get started! medium.com In this post we will be coding the entire linear regression algorithm from absolute scratch using python so we will really be getting our hands dirty today! Let's go! The first step for any machine learning problem is getting the data. There is no machine ‚Äúlearning‚Äù if there is nothing to ‚Äúlearn‚Äù from. So for this tutorial we will be using a very common dataset for linear regression i.e. the house-price prediction dataset. The dataset can be found here. This is a simple dataset containing housing prices in Portland, Oregon. The first column is the size of the house (in square feet), the second column is the number of bedrooms, and the third column is the price of the house. You might have noticed that we have more than one feature in our dataset (i.e. the house_size(in sqft) and the number of rooms) hence we will be looking at multivariate linear regression and the label (y) will be the house price as that is what we are going to be predicting. Lets define the function for loading the dataset: We will be calling the above function later to load the dataset. This function returns x and y (note x is made up of the first 2 columns of the dataset whereas y is the last column of the dataset as that is the price column hence in order to return x and y we are returning data[:,:2] and data[:,-1] respectively from the function). The above code not only loads the data but also normalizes it and plots the data points. We will look at the plot of the data in a bit but first lets understand what the normalize(data) line is doing above. If you look at the raw dataset you will notice that the values in the second column (i.e. the number of rooms) are much smaller than the those in the first (i.e house size). Our model does not evaluate this data as number of rooms or size of house. For the model its all just numbers. This can create an unwanted bias in your machine learning model towards the columns (or features) that have higher numerical values than the others. It can also create imbalance in the variance and mathematical mean. For these reasons and also to make the job easier it is always advised to scale or normalize your features so that they all lie within the same range ( e.g. [-1 to 1] or [0 to 1] ). This makes training much easier. So for our purpose here we will be using feature normalization which in the mathematical sense means: Z = (x ‚Äî Œº) / œÉ Œº : mean œÉ : standard deviation In the above formula z is our normalized feature and x is the non-normalized feature. Don‚Äôt worry if you are not very familiar with these mathematical concepts. A quick review should get you going. Alright so now that we have our normalization formula lets make a function for normalization: This code does exactly what we have discussed. It goes through each column and normalizes all data elements of that column using the mean and standard deviation of those elements. Now before we jump to coding our linear regression model one thing we need to ask is WHY? Why are we solving this problem using linear regression? This is a very valid question and before actually jumping to any concrete code you should be very clear about what algorithm you want to use and if that really is the best option given the dataset and the problem you are trying to solve. One way we can prove why using linear regression will work for our current dataset is by plotting it. For that purpose we have called the plot_data function in load_data above. Lets define the plot_data function now: This function on being called generates the following plot: You can see that it is possible to roughly fit a line through the above plot. This means a linear approximation will actually allow us to make pretty accurate predictions and hence we go for linear regression. Well now that we have the data ready lets move on to the fun part. Coding the algorithm! First of all we need to define what our hypothesis function looks like because we will be using this hypothesis for calculating the cost later on. We know for linear regression our hypothesis is: hŒ∏(x) = Œ∏0 + Œ∏1x1 + Œ∏2x2 + Œ∏3x3 +‚Ä¶..+ Œ∏nxn Our dataset however has only 2 features, so for our current problem the hypothesis is: hŒ∏(x) = Œ∏0 + Œ∏1x1 + Œ∏2x2 where x1 and x2 are the two features (i.e. size of house and number of rooms). Lets put this in a simple python function which returns the hypothesis: Woah what‚Äôs with the matrix multiplication?! Don‚Äôt worry it still gives us the same hypothesis equation and we will take a deeper look into why this is mathematically correct later in this post. Okay so now we have the hypothesis function, the next important thing is the cost function. To evaluate the quality of our model we make use of the cost function. Again this post is the exact ‚Äúcode version‚Äù of: medium.com So you can go through it if anything here doesn‚Äôt make sense or just follow along both the posts. Alright so the equation for the cost function is: and the code for our cost function is: On a closer look you will probably notice that all the python functions we have defined so far are exactly the same as the mathematics we had defined earlier for linear regression. Now that we have the cost we must minimize it and for that we use‚Ä¶ yes gradient descent indeed! Gradient descent in our context is an optimization algorithm that aims to adjust the parameters in order to minimize the cost function . The main update step for gradient descent is: So we multiply the derivative of the cost function with the learning rate(Œ±) and subtract it from the present value of the parameters(Œ∏) to get the new updated parameters(Œ∏). The gradient_descent function returns theta and J_all. theta is obviously our parameter vector which contains the values of Œ∏s for the hypothesis and J_all is a list containing the cost function after each epoch. The J_all variable isn‚Äôt exactly essential but it helps to analyze the model better as you will see later in the post. Now all that's left to do is call our functions in the correct order: We first call the load_data function to load the x and y values. x contains the training examples and y contains the labels (the house prices in our case). You might have noticed that in the code throughout we have been using matrix multiplication to achieve the expressions we want. For example in order to get the hypothesis, we had to multiply each parameter(Œ∏) with each feature vector(x) we could use for loops for this and loop over each example and perform the multiplication each time however this would not be the most efficient method if we were to have say 10 million training examples. A more efficient approach here would be to use matrix multiplication. If you aren't very familiar with matrix multiplication I would suggest you go over it once, it‚Äôs fairly simple. For our dataset we have two features (i.e. the house size and the number of rooms) so we will have (2+1) 3 parameters. The extra parameter Œ∏0 can be accounted for by considering that the hypothesis is nothing but a line in the graphical sense. So the extra Œ∏0 accounts for this line to be as required. Okay so we have 3 parameters and 2 features. This means our Œ∏ or parameter vector (1-D matrix) will have the dimensions (3,1) but our feature vector will have the dimensions (46,2) {according to our dataset}. You probably have noticed by now that its not mathematically possible to multiply these two matrices. Lets take a look at our hypothesis once again: hŒ∏(x) = Œ∏0 + Œ∏1x1 + Œ∏2x2 If you look closely it is actually quite intuitive that if we add an extra column of ones in the beginning of our feature vector(x){ making it have the dimensions (46, 3)} and if we perform matrix multiplication on x and theta we in fact will arrive at the above equation for hŒ∏(x). If it still isn't obvious then just try working out an example on a piece of paper. Remember when we actually run our code for implementing this function we won‚Äôt be returning the expression like for hŒ∏(x) instead we are returning the mathematical value that this expression evaluates to. In the above code the line x = np.hstack((np.ones((x.shape[0],1)), x)) adds an extra column of ones to the beginning of x in order to allow matrix multiplication as required. After this we initialize our theta vector with zeros. You can also initialize it with some small random values. We also specify the learning rate and the number of epochs (an epoch is the number of times the algorithm will go through the entire dataset) we want to train for . Once we have all our hyper-parameters defined, we call the gradient descent function which returns a history of all the cost functions and the final vector of parameters theta. This theta vector is essentially what defines our final hypothesis. You may observe that the shape of the theta vector that is returned by the gradient descent function has the dimensions (3,1). Remember our hypothesis function? hŒ∏(x) = Œ∏0 + Œ∏1x1 + Œ∏2x2 Well we needed 3 Œ∏s and our theta vector has the dimensions (3,1) hence each of theta[0], theta[1]and theta[2]is in fact Œ∏0, Œ∏1 andŒ∏2 respectively. The J_all variable is nothing but the history of all the cost functions. You can print the J_all array to see how the cost function progressively decreases for each epoch of gradient descent. This graph can be plotted by defining and calling a plot_cost function like so: Now we can use these parameters to find the label i.e. the price of any house (in Portland, Oregon) given the house size and number of rooms. You may now test your code calling a test function that will take as input the size of the house, the number of rooms and the final theta vector that was returned by our linear regression model and will give us the price of the house. Believe it or not that‚Äôs actually all there is to coding linear regression. Congratulations! You have now successfully coded a linear regression model from absolute scratch. Being able to understand and code the entire algorithm is not easy so you can pat yourself on the back for getting through. Linear regression is usually the first algorithm we usually start machine learning with so if you understood what we did here I would suggest you pick up another dataset (for linear regression) and try to apply linear regression on your own. Happy Coding :) For your reference you can find the entire code and dataset here: github.com",340,5,9,Towards Data Science,2021-01-05,2021
https://towardsdatascience.com/predicting-the-outcome-of-nba-games-with-machine-learning-a810bb768f20,Predicting the outcome of NBA games with Machine¬†Learning,How we used (and you can too)¬†machine‚Ä¶,8,57,"['Predicting the outcome of NBA games with Machine Learning', 'Scraping our Data', 'Cleaning the Data', 'Feature Engineering', 'Data Analysis', 'Predicting the Outcome of Games Based on Team Statistics and Elo Ratings', 'Predicting the Outcome of Games Based on Individual Player Statistics and Scoring', 'Conclusions and Future Considerations']","When deciding on a final project for our Big Data Analytics class, my partners Jack Rosener, Jackson Joffe and I looked to combine an interest in sports with the principles learned throughout the semester. After a few days of discussion, we settled on a project that would aim to predict the outcome of NBA games. While implementing our goal, we found it helpful to distill the project into the following steps with the following questions: Before we get into the nitty-gritty of our workflow, let‚Äôs take a moment to review and acknowledge the other work done on this exact topic. First, in 2013 Renato Torres from the University of Wisconsin-Madison set out to accomplish a similar goal as we did and predict specific season outcomes of NBA data using different machine learning models. He used several techniques featured in our project, primarily feature reduction to eliminate multicollinearity from the available data, and also explored different models to explore those with the highest accuracies. Like our project, his selected features included points scored, but unlike our project had a particular focus on win-loss percentage at home and away. (We will explore our feature analysis later.)A lot of other fantastic work on this has been done before and can be read here: Cheng, Ge & Zhang, Zhenyu & Kyebambe, Moses & Nasser, Kimbugwe. (2016). Predicting the Outcome of NBA Playoffs Based on the Maximum Entropy Principle.Jones, Eri. (2016) Predicting the Outcomes of NBA Games. North Dakota State University.Fayad, Alexander. Building My First Machine Learning Model | NBA Prediction Algorithm. Towards Data Science.The Complete History of the NBA. FiveThirtyEight. Through our research, we found that the best published model had a prediction accuracy of 74.1% (for playoff outcomes), with most others achieving an upper bound between 66‚Äì72% accuracy. Most published research, too, focused on predicting playoff scores ‚Äî which may lend towards biased data: playoff teams are more consistent in a number of stats throughout the regular season, and playoff game expected outcomes likely experience less variance as a result. Critically, note that the upset rate across the entire season in the NBA averages 32.1%. In the playoffs, the upset rate ‚Äî as defined by teams with a lower regular season record winning‚Äî drops to 22% (which actually means most NBA-playoff prediction models underperform). Because our project looked to predict the outcome of any NBA game and is playoff agnostic, we were looking to develop a model that could reach and hopefully beat a 67.9% accuracy ‚Äî and in doing so predict some upsets. Feel free to follow along here or view our files on GitHub. We scraped our data from available information at Synergy Sports ‚Äî which has extremely detailed team and player data for each game played since the 2008‚Äì2009 season. Scraping took several days due to rate-limiting (as we had to query the results of each game over 12 seasons), and was initially compiled into a JSON format and finally saved to csv files. Author‚Äôs Note: We accessed data from Synergy Sports with funding from the University of Pennsylvania. As an unfortunate consequence of this, we are not at liberty to provide public access to our dataset. However, we have compiled a list of alternative datasets through which one can replicate and improve upon our data:https://www.basketball-reference.com/leagueshttps://www.kaggle.com/datasets/wyattowalsh/basketballhttps://www.kaggle.com/datasets/nathanlauga/nba-gameshttps://www.kaggle.com/datasets/drgilermo/nba-players-stats We now had both player stats and team stats for each NBA season saved as seperate csv files. Our next step was to read in all this data and combine it into two large dataframes: one will containing the player stats for the past 12 seasons, and the other containing the team stats. Once created, we would clean the dataframes to remove invalid statistics (negative minutes) and columns that served little purpose to us (charges taken/committed, for example). We then saved these new dataframes to the following csv files, which allowed us (and you) to skip the laborious and lengthy steps of both scraping and cleaning the data when restarting our notebook‚Äôs runtime: This is where the fun stuff begins. Our primary goal was to make all of the available data understandable: game-by-game rebounds for an entire team don‚Äôt help us much unless we can use that data in a higher-level analysis that leads us to our ultimate goal ‚Äì predicting wins and losses. To that end, we sought to create five different features which we would use in understanding how our teams progressed and regressed throughout each season: This is perhaps the best existing method to relativize NBA team strength and performance over many seasons. The way Elo Ratings are calculated is simple: all teams start at a median score of 1500 and are either given or subtracted points based on the final score of each game and where it was played with weights being given to point difference, upsets, and location. In essence, it‚Äôs a more sophisticated win-loss record. Most NBA-prediction models don‚Äôt look at Elo Ratings but instead amalgamate a simple win-loss record with several other stats. We wanted to use Elo to appropriately weight quality wins (and losses), while also recognizing that not all teams are created equal.The exact formula is as follows:If ùëÖ_ùëñ is the current Elo rating of a team, its Elo rating after its played its next game is defined as follows: Here, S_team is a state variable: 1 if the team wins, 0 if the team loses. E_team represents the expected win probability of the team, which is represented as: k is a moving constant, dependent on both the margin of victory and difference in Elo ratings: It‚Äôs also important to note that Elo ratings carry over from season-to-season (as all teams are not created equal, good teams tend to stay good or at least gradually decline ‚Äî very rarely do teams drop onto or off of the map). If R represents the final Elo of a team in one season, it‚Äôs Elo Rating at the beginning of the next season is approximately: We can actually take a look at this metric over time, randomly selecting three teams to view, and immediately see that we can get key insights about the strength of teams throughout seasons: 2. Recent Team Performance (Avg. stats over 10 most recent games) These are pretty self-explanatory, we are simply looking to average the stats for each team over their last 10 games. To do this we wrote a simple function that would calculate a sliding average for a given team‚Äôs stats and a window of n games: After saving this data into a new dataframe, we sought to separate each game (which contains stats for both home and away teams) into its own rows by team, which allows us to group-by and aggregate team stats much more easily and simplifies existing features. Finally, we added a win state-variable column to include the most critical measurement to our project: wins and losses. 3. Recent Player Performance (Avg. stats over 10 most recent games) We create our player_recent_performance dataframe using similar methods to the above section, this time with individual players as opposed to teams. This created a dataframe of each player‚Äôs performance over the past 10 games. 4. Player Season Performance We also sought to include the average player stats over the entire season: unlike teams, players themselves get injured or fall in and out of the rotation and it‚Äôs perhaps more critical for us to understand how player performances in individual games track with their averages. We will use this later in our models to see if it will allow for accurate predictions on the team-level. 5. Player Efficiency Ratings (PER) Critically, just as we had done with teams via Elo Ratings, we wanted to be able to relativize player performance using a metric that combines seemingly unrelated statistics. Our hope was that we could use Hollinger‚Äôs Player Efficiency Ratings to compare and predict team performance by the aggregated PER scores of their players. In the NBA, it is easy for players to experience wildly inflated or deflated statistics (such as points per minute) simply by virtue of the amount of playing time they get, against bench players or versus starters, number of games played, or even from outlier performances. We did not want to rely solely on player averages simply because of their ability to skew. PER solves that problem by weighting certain in-game statistics by the inverse of number of minutes played, which creates a metric that defines player performance relative to the number of minutes played.Thus for each player, we added a column for PER in a given game according to the following formula: PER = (FGM x 85.910 + Steals x 53.897 + 3PTM x 51.757 + FTM x 46.845 + Blocks x 39.190 + Offensive_Reb x 39.190 + Assists x 34.677 + Defensive_Reb x 14.707 ‚Äî Foul x 17.174 ‚Äî FT_Miss x 20.091 ‚Äî FG_Miss x 39.190 ‚Äî TO x 53.897) x (1 / Minutes) Our data analysis was centered around the use of Elo Ratings as our test metric. Essentially, could we be confident that Elo correlates with and correctly aggregates other statistics? Furthermore, would it be more appropriate for us to predict game outcomes using team stats (Elo Ratings) or averaged player stats (PER Ratings)? First, let‚Äôs explore the density of Elo Ratings across the NBA on a per-season basis. This tells us a little about the level of parity across the league: if we can see Elo Ratings approach a normal distribution that would suggest the league‚Äôs teams are relatively well-matched. Otherwise, we see large disparities and the development of ‚Äúsuper-teams‚Äù. Moving away from an understanding of Elo Ratings from a league-perspective, we endeavored to see how Elo Ratings tracked against an individual team‚Äôs performance in other statistics. First, we looked to plot the distribution of Elo for a random team against the average number of points scored in recent games: We can actually see from this that there is some correlation between the average number of points a team scores versus its Elo Rating ‚Äî the higher the average points scored across a window of games the higher the Elo Rating seems to climb. However, we can also see that the Elo may also exhibit a high variance across similar scoring figures. So, to better understand how Elo Ratings track with points scored, we examine how the average points scored compares to season averages across the league ‚Äì from there we can determine if points scored improves ELO, provided that high scoring is relative to the rest of the league. To do this, let‚Äôs look at that same team for the same seasons and plot the distribution of points scored against its opponents. This confirms our suspicions, as we can see that when the distribution of average points is greater than those of its opponents, or is more concentrated at an equal or higher level, the Elo is higher for those seasons. When the groupings approach an even or lesser value, those seasons‚Äô Elo ratings for the given team are lower. Therefore, average points scored is a alone a solid determinant of predicting game outcomes, but better when relativized. This demonstrated for us that Elo would be a much better determinant in predicting wins for us than points, as it is by design a relative statistic. Shifting away from team statistics, we sought to understand if Elo tracks better with player performance than it does team performance. To do this, we took a similar approach to how we plotted Elo Ratings with average points scored for the same random team, this time with PER. From the plotted data, we can see that aggregated PER as compared to opponents doesn‚Äôt show much of any correlation with the strength of a team as determined by Elo Rating. Instead, points scored translates better ‚Äî which makes some sense as a player‚Äôs efficiency isn‚Äôt necessarily tied to scoring the most points ‚Äî and points scored against opponents is the determinant of winning a game and therefore impacting Elo. We can see this further by mapping the Orlando Magic‚Äôs mean and median PER ratings against its opponents for the same given seasons, and find that there is almost no relation between team-PER averages or medians and team strength. From all of our analysis of relativized team versus aggregated player statistics, it looks clear to us that our Elo Rating and its determinants would be better features to train our models on when it comes to predicting the outcome of NBA games. Our first step here was to split our data into features and columns. Reading from our dataset, once split, we then used sklearn to randomly split our data into train and test sets with an 80:20 ratio. The first model we aimed to use to predict the outcome of an NBA game was a Logistic Regression model. Unlike a Linear Regression model which predicts outcomes on a range of values between (and sometimes outside) 0 and 1, Logistic Regression models aim to group predictions into binary outcomes. Since we are predicting wins and losses, this type of classification suits us perfectly. To begin, we used a simple non-parameterized LR model with our team stats and Elo Ratings as parameters using sklearn: After playing around with some hyperparameter tuning, we found that using max_iter=131 and verbose=2 slightly improved our initial testing accuracy to 66.95%. Definitely not bad for a non-parameterized model and very close to our desired prediction accuracy. However, we sought to see if we could better tune our hyperparameters to improve our overall accuracy. Essentially, we would try out many combinations of possible hyperparameters on our data to give us the absolute best weights for our LR model. We accomplished this using cross-validation: because we only have a vague idea of the parameters we might want to use, our best approach is to narrow our search is and evaluate a wide range of values for each hyperparameter. Using RandomizedSearchCV, we searched among 2 * 4 * 5 * 11 * 3 * 3 * 5 * 3 = 59,400 possible settings ‚Äì and so the most efficient way to do this would be to take a random sample of the values. Running our model with the best parameter values of the random samples actually decreased the accuracy of our model to 66.27%, which showed us that while random sampling helped us narrow down our hyper parameter tuning within a distribution, we would have to explicitly check all combinations with GridSearchCV. In this case, implementing GridSearch only marginally increased our accuracy with our LR model. The second model we looked to implement was a RandomForestClassifier, which can be efficiently used for both regressions and classifications. In this case, we will see if the Classifier can build a proper decision tree to determine wins from the given team-stats. Immediately, we get that the RandomForestClassifier reaches an initial accuracy of 66.95%, which again is pretty good. Like with the LR model, we attempted to tune the hyperparameters to give us more accurate results ‚Äî first using RandomizedSearchCV. Unlike with the LR model, we find that RandomizedSearch improves our hyperparameter tuning, giving us a better accuracy of 67.15%. Running GridSearchCV in a similar manner to what we did above, we also sought to explicitly test 2 * 1 * 6 * 2 * 3 * 3 * 5 = 1080 combinations of settings instead of randomly sampling a distribution of settings. GridSearch also gave us an improvement from the base RandomForestClassifier, with an accuracy of 67.11%. Overall, when running both a LinearRegression and RandomForestClassifier on the team stats and Elo Ratings, we achieved a win-prediction accuracy of 66.95%‚Äì67.15%. For basketball games, which as we established earlier are quite variable in their actual versus predicted results, this is a significant result. We then took a different approach to predicting the outcome of a game to see if we can achieve any better perfomance. Using the larger dataset of individual player statistics that we‚Äôve collected, we will train a model to predict how many points a player will score in a given game. We will predict this based on a players average season stats up until the game we are trying to predict as well as their average performance over the past 10 games. We already created this data in the feature engineering section above. We will also make use of Elo ratings in our prediction as well, as presumably the higher rating of the opposing team the less points a player will score. Once we have this model we can predict how many points a team will score in a game by summing the predicted number of points of each individual player will score. With this information we will be able to predict which team will score more points and thus win the game. Before we run our models, we need to clean our data slightly. For some games in this dataset, we have the statistics for one teams‚Äô players, but not for the other team ‚Äî generally only for the first game that other team plays in the season. Thus, we will remove all these games from the dataset. Unlike with the above games, we can‚Äôt randomly split our data into train and test sets. We are looking to use individual player statistics to predict the final score of a team, thus we must keep all players playing in the same game together. To do this, we will split up our train and test sets by game so players playing in the same game stay together. About 80% of the games will be in the train set and 20% will be in the test set: Instead of using a Logistic Regression model, for player scoring we will use a Linear Regression model as we are looking to predict a range of possible values (points scored) instead of simply predicting a win or a loss. Our RMSE (Root Mean Squared Error) for all players was 5.56, or the equivalent of each player making or missing around 2‚Äì3 baskets game around their averages. On the test set, we grouped each team‚Äôs predicted scoring for each game and compared it with their actual scoring numbers. Computing the numbers of games won versus the winner based on predicted scoring gave us a ratio of 1483/2528, or an accuracy of 58.66%. Clearly, and as we realized earlier when looking at PER distributions of teams versus their opponents, aggregated player performance is too variable of a determinant to accurately predict the outcome of games ‚Äî especially when compared to team performance which tends to be more consistent across games. As avid NBA fans, we felt that creating a model to predict the outcome of NBA games would be an interesting project and taught us a lot about building classifiers for professional sports game outcomes. We were able to utilize many of the concepts learned in our Big Data Analytics class for this project ‚Äî including scraping, data cleaning, feature analysis, building models and hyperparameter tuning ‚Äî and want to thank Professor Ives for his fantastic work in teaching throughout the semester. Our Random Forest Regression model, with parameters optimized through RandomSearchCV, gave us the highest testing accuracy of 67.15%. It is slightly higher than the Logistic Regression model, and it is much higher than the Linear Regression model based on individual player statistics. Optimizing parameters using GridSearchCV and RandomizedSearchCV was time consuming and computationally costly, and it resulted in only marginal changes in testing accuracy. If we had more time, we‚Äôd likely spend less time optimizing parameters and more time selecting a model. The best NBA game prediction models only accurately predict the winner about 70% of the time, so our logistic regression model and random forest classifier are both very close to the upper bound of predictions that currently exist. If we had more time, we would explore other models and see just how high of a test accuracy we could get. Some of those candidates include an SGD Classifier, linear discriminant analysis, convolutional network, or a na√Øve Bayes classifier. Hopefully, you enjoyed reading about our work as much as we enjoyed making it ‚Äî and learned something from it too.",146,3,15,Towards Data Science,2021-01-05,2021
https://towardsdatascience.com/predictions-and-hopes-for-graph-ml-in-2021-6af2121c3e3d,Predictions and hopes for Graph ML in¬†2021,Leading researchers in Graph ML summarise the progress¬†in‚Ä¶,9,112,"['What does 2021 hold for Graph ML?', 'Beyond message passing', 'Algorithmic reasoning', 'Relational structure discovery', 'Expressive power', 'Scalability', 'Dynamic graphs', 'New hardware', 'Applications in the industry, physics, medicine, and beyond']","Will Hamilton, Assistant Professor at McGill University and CIFAR Chair at Mila, author of GraphSAGE. ‚Äú2020 saw the field of Graph ML come to terms with the fundamental limitations of the message-passing paradigm. These limitations include the so-called ‚Äúbottleneck‚Äù issue [1], problems with over-smoothing [2], and theoretical limits in terms of representational capacity [3,4]. Looking forward, I expect that in 2021 we will be searching for the next big paradigm for Graph ML. I am not sure what exactly the next generation of Graph ML algorithms will look like, but I am confident that progress will require breaking away from the message-passing schemes that dominated the field in 2020 and before. I am also hopeful that 2021 will also see Graph ML move into more impactful and challenging application domains. Too much recent research focuses on simple, homophilous node-classification tasks. I also hope to see methodological advancements towards tasks that require more complex algorithmic reasoning, such as tasks involving knowledge graphs, reinforcement learning, and combinatorial optimisation.‚Äù Petar Veliƒçkoviƒá, Senior Researcher at DeepMind, author of Graph Attention Networks. ‚Äú2020 has definitively and irreversibly turned graph representation learning into a first-class citizen in ML.‚Äù The great advances made this year are far too many to enumerate briefly, but I am personally most excited about neural algorithmic reasoning. Neural networks are traditionally very powerful in the interpolation regime, but are known to be terrible extrapolators ‚Äî and hence inadequate reasoners; as one of the main traits of reasoning is being able to function out-of-distribution. Reasoning tasks are likely to be ideal for further development of GNNs, not only because they are known to align very well with such tasks [5], but also because many real-world graph tasks exhibit homophily, meaning that the most impactful and scalable approaches typically will be much simpler forms of GNNs [6,7]. Building on the historical successes of previous neural executors such as the Neural Turing Machine [8] and the Differentiable Neural Computer [9], and reinforced by the now-omnipresent graph machine learning toolbox, several works published in 2020 explored the theoretical limits of neural executors [5,10,11], derived novel and stronger reasoning architectures based on GNNs [12‚Äì15], and enabled perfect strong generalisation on neural reasoning tasks [16]. While such architectures could naturally translate into wins for combinatorial optimisation [17] in 2021, I am personally most thrilled about how pre-trained algorithmic executors can allow us to apply classical algorithms to inputs that are too raw or otherwise unsuitable for the algorithm. As one example, our XLVIN agent [18] used exactly these concepts to allow a GNN to execute value iteration style algorithms within the reinforcement learning pipeline, even though the specifics of the underlying MDP were not known. I believe 2021 will be ripe with GNN applications to reinforcement learning in general.‚Äù Thomas Kipf, Research Scientist at Google Brain, author of Graph Convolutional Networks. ‚ÄúOne particularly noteworthy trend in the Graph ML community since the recent widespread adoption of GNN-based models is the separation of computation structure from the data structure. In a recent ICML workshop talk, I termed this trend relational structure discovery. Typically, we design graph neural networks to pass messages on a fixed (or temporally evolving) structure provided by the dataset, i.e. the nodes and edges of the dataset are taken as the gold standard for the computation structure or message passing structure of our model. In 2020, we have seen rising interest in models that are able to adapt the computation structure, i.e., which components they use as nodes and over which pairs of nodes they perform message passing, on the fly ‚Äî while going beyond simple attention-based models. Influential examples in 2020 include Amortised Causal Discovery [19‚Äì20], which makes use of Neural Relational Inference to infer (and reason with) causal graphs from time-series data, GNNs with learnable pointer [21,15] and relation mechanisms [22‚Äì23], learning mesh-based physical simulators with adaptive computation graphs [24], and models that learn to infer abstract nodes over which to perform computations [25‚Äì26]. This development has widespread implications, as it allows us to effectively utilise symmetries (e.g. node permutation equivariance) and inductive biases (e.g. modeling of pairwise interaction functions) afforded by GNN architectures in other domains, such as text or video processing. Going forward, I expect that we will see many developments in how one can learn the optimal computational graph structure (both in terms of nodes and relations) given some data and tasks without relying on explicit supervision. Inspection of such learned structures will likely be valuable in deriving better explanations and interpretations of the computations that learned models perform to solve a task, and will likely allow us to draw further analogies to causal reasoning.‚Äù Haggai Maron, Research Scientist at Nvidia, author of provably expressive high-dimensional graph neural networks. ‚ÄúThe expressive power of graph neural networks was one of the central topics in Graph ML in 2020. There were many excellent papers discussing the expressive power of various GNN architectures [27] and showing fundamental expressivity limits of GNNs when their depth and width is restricted [28], describing what kind of structures can be detected and counted using GNNs [29], showing that using a fixed number of GNNs does not make sense for many graph tasks and suggesting an iterative GNN that learns to terminate the message passing process adaptively [14]. In 2021, I would be happy to see advancements in principled approaches for generative models for graphs, connections between graph matching with GNNs and the expressive power of GNNs, learning graphs of structured data like images and audio, and developing stronger connections between the GNN community and the computer vision community working on scene graphs.‚Äù Matthias Fey, PhD student at TU Dortmund, developer of PyTorch Geometric and Open Graph Benchmark. ‚ÄúOne of the most trending topics in Graph ML research in 2020 was tackling the scalability issues of GNNs. Several approaches relied on simplifying the underlying computation by decoupling prediction from propagation. We have seen numerous papers that simply combine a non-trainable propagation scheme with a graph-agnostic module, either as a pre- [30,7] or post-processing [6] step. This leads to superb runtime and, remarkably, mostly on par performance on homophily graphs. With access to increasingly bigger datasets, I am eager to see how to advance from here and how to make use of trainable and expressive propagation in a scalable fashion.‚Äù Emanuele Rossi, ML Researcher at Twitter and PhD student at Imperial College London, author of Temporal Graph Networks. ‚ÄúMany interesting Graph ML applications are inherently dynamic, where both the graph topology and the attributes evolve over time. This is the case in social networks, financial transaction networks, or user-item interaction networks. Until recently, the vast majority of research on Graph ML has focused on static graphs. The few works attempting to deal with dynamic graphs mainly considered discrete-time dynamic graphs, a series of graph snapshots at regular intervals. In 2020, we saw an emerging set of works [31‚Äì34] on a more general category of continuous-time dynamic graphs, that can be thought of as an asynchronous stream of timed events. Moreover, the first interesting successful applications of models for dynamic graphs are also starting to emerge: we saw fake account detection [35], fraud detection [36], and controlling the spreading of an epidemic [37]. I think that we are only scratching the surface of this exciting direction and many interesting questions remain unanswered. Among important open problems are scalability, better theoretical understanding of dynamic models, and combining spatial and temporal diffusion of information in a single framework. We also need more reliable and challenging benchmarks to make sure progress can be better evaluated and tracked. Finally, I hope to see more successful applications of dynamic graph neural architectures, especially in the industry.‚Äù Mark Saroufim, ML Engineer at Graphcore. ‚ÄúI cannot think of a single customer I have worked with who has not either deployed a Graph Neural Network in production or is planning to. Part of this trend is that the natural graph structure in applications such as NLP, protein design, or molecule property prediction has been traditionally ignored, and instead the data was treated as sequences amenable for existing and well-established ML models such as Transformers. We know, however, that Transformers are nothing but GNNs where attention is used as the neighbourhood aggregation function. In computing, the phenomenon when certain algorithms win not because they are ideally suited to solve a certain problem, but because they run well on existing hardware is called Hardware Lottery [38] ‚Äî and this is the case with Transformers running on GPUs. At Graphcore, we have built a new MIMD architecture with 1472 cores that can run a total of 8832 programs in parallel, which we call the Intelligence Processing Unit (IPU). This architecture is ideally suited for accelerating GNNs. Our Poplar software stack takes advantage of sparsity to allocate different nodes of a computational graph to different cores. For models that can fit into the IPU‚Äôs 900MB on-chip memory, our architecture offers substantial improvement of the throughput over GPUs; otherwise, with just a few lines of code, it is possible to distribute the model over thousands of IPUs. I am excited to see our customers building a large body of research taking advantage of our architecture, including applications such as bundle adjustment for SLAM, training deep networks using local updates, or speeding up a variety of problems in particle physics. I hope to see more researchers taking advantage of our advanced ML hardware in 2021.‚Äù Sergey Ivanov, Research Scientist at Criteo, editor of the Graph Machine Learning newsletter. ‚ÄúIt was an astounding year for Graph ML research. All major ML conferences had about 10‚Äì20% of all papers dedicated to this field and at this scale, everyone can find an interesting graph topic to their taste. The Google Graph Mining team was prominently present at NeurIPS. Looking at the 312-page presentation, one can say that Google has advanced in utilising graphs in production more than anyone else. The applications they address using Graph ML include modeling COVID-19 with spatio-temporal GNNs, fraud detection, privacy preservation, and more. Furthermore, DeepMind rolled out GNNs in production for travel time predictions globally in Google Maps. An interesting detail of their method is the integration of an RL model to select similar sampled subgraphs into a single batch for training parameters of GNNs. This and advanced hyperparameter tuning brought up to +50% improvement in the accuracy of real-time time-of-arrival estimation. Another notable application of GNNs was done at Magic Leap, which specialises in 3D computer-generated graphics. Their SuperGlue architecture [39] applies GNNs to feature matching in images ‚Äî an important subject for 3D reconstruction, place recognition, localisation, and mapping. This end-to-end feature representation paired with optimal transport optimisation triumphed on real-time indoor and outdoor pose estimation. These results just scratch the surface of what has been achieved in 2020. Next year, I believe we will see further use of Graph ML developments in industrial settings. This would include production pipelines and frameworks, new open-source graph datasets, and deployment of GNNs at scale for e-commerce, engineering design, and the pharmaceutical industry.‚Äù Kyle Cranmer, Professor of Physics at NYU, one of the discoverers of the Higgs boson. ‚ÄúIt has been amazing to see how in the last two years Graph ML has become very popular in the field of physics. Early work with deep learning in particle physics often forced the data into an image representation to work with CNNs, which was not natural as our data are not natively grid-like and the image representation is very sparse. Graphs are a much more natural representation of our data [40,41]. Researchers on the Large Hadron Collider are now working to integrate Graph ML into the real-time data processing systems that process billions of collisions per second. There is an effort to achieve this by deploying inference servers to integrate Graph ML with the real-time data acquisition systems [42] and efforts to implement these algorithms on FPGAs and other special hardware [43]. Another highlight from Graph ML in 2020 is the demonstration that its inductive bias can pair with symbolic approaches. For example, we used a GNN to learn how to predict various dynamical systems, and then we ran symbolic regression on the messages being sent along the edges [44]. Not only were we able to recover the ground-truth force laws for those dynamical systems, but we were also able to extract equations in situations where we don‚Äôt have ground truth. Amazingly, the symbolic equations that were extracted could then be re-introduced into the GNN, replacing the original learned components, and we obtained even better generalisation to out of distribution data.‚Äù Anees Kazi, PhD student at TUM, author of multiple papers on Graph ML in medical imaging. ‚ÄúIn the medical domain, Graph ML transformed the way of analyzing multimodal data in a way that closely resembles how experts look at the patient‚Äôs condition from all the available dimensions in clinical routines. There has recently been a huge spike in the research related to Graph ML in medical imaging and healthcare applications [45], including brain segmentation [46], brain structure analysis using MRI/fMRI data targeted towards disease prediction [47], and drug effect analysis [48]. Among topics in Graph ML, several stood out in the medical domain in 2020. First, latent graph learning [22,49,50], as empirically defining a graph for the given data was till then a bottleneck for optimal outcomes, now has been solved by methods which learn the latent graph structure automatically. Second, data imputation [51], as missing data is one standing problem in a lot of datasets in the medical domain, graph-based methods have helped in the imputation of data depending on relations coming from graph neighbourhood. Third, the interpretability for Graph ML models [52], since it is important for clinical and technical experts to focus on reasoning the outcomes of Graph ML models for their reliable incorporation into a CADx system. Another important highlight of 2020 in the medical domain was of course the coronavirus pandemic, and Graph ML methods were used for detection of Covid-19 [53]. In 2021, Graph ML could be used to further the interpretability of ML models for better decision making. Secondly, it has been observed that Graph ML methods are still sensitive to the graph structure, hence robustness to graph perturbations and adversarial attacks is an important topic. Finally, it would be interesting to see the integration of self-supervised learning with Graph ML applied to the medical domain.‚Äù Bruno Correia, Assistant Professor at EPFL, head of the Protein Design and Immunoengineering Laboratory, one of the developers of MaSIF. ‚ÄúIn 2020, exciting progress has been made in protein structure prediction, a key problem in bioinformatics. Yet, ultimately the chemical and geometric pattern displayed at the surface of these molecules are critical for protein function. Surface-based representations of molecules have been used for decades but they pose challenges for machine learning methods. Approaches from the realm of Geometric Deep Learning have brought impressive capabilities to the field of protein modeling given their ability to deal with irregular data, which are particularly well-suited for protein representations. In MaSIF [1], we used geometric deep learning on mesh-based molecular surface representations to learn patterns that allow us to predict interactions of proteins with other molecules (proteins and metabolites) and speed up docking calculations by several orders of magnitude. This, in turn, could facilitate a much larger scale of prediction of protein-protein interaction networks. In a further development of the MaSIF framework [2], we managed to generate our surface and chemical feature on the fly avoiding all precomputation stages. I anticipate that such advances will be transformative for protein and small molecule design, and in the long term could help faster development of biological drugs.‚Äù Marinka Zitnik, Assistant Professor of Biomedical Informatics at Harvard Medical School, author of Decagon. ‚ÄúIt was exciting to see how Graph ML entered the fields of life sciences in 2020. We have seen how graph neural networks not only outperform earlier methods on carefully designed benchmark datasets but can open up avenues for developing new medicines to help people and understanding nature at the fundamental level. Highlights include advances in single-cell biology [56], protein and structural biology [54,57], and drug discovery[58] and repositioning [59]. For centuries, the scientific method ‚Äî the fundamental practice of science that scientists use to systematically and logically explain the natural world ‚Äî has remained largely the same. I hope that in 2021, we will make substantial progress on using Graph ML to change that. To do that, I think we need to design methods that can optimize and manipulate networked systems and predict their behavior, such as how genomics ‚Äî Nature‚Äôs experiments on people ‚Äî influences human traits in the context of disease. Such methods need to work with perturbational and interventional data (not only ingest observational measurements of our world). Also, I hope we will develop more methods for learning actionable representations that readily lend themselves to actionable hypotheses in science. Such methods can enable decision making in high-stakes settings (e.g., chemistry tests, particle physics, human clinical trials) where we need precise, robust predictions that can be interpreted meaningfully.‚Äù [1] U. Alon and E. Yahav, On the bottleneck of graph neural networks and its practical implications (2020) arXiv:2006.05205. [2] Q. Li, Z. Han, X.-M. Wu, Deeper insights into graph convolutional networks for semi-supervised learning (2019) Proc. AAAI. [3] K. Xu et al. How powerful are graph neural networks? (2019) Proc. ICLR. [4] C. Morris et al. Weisfeiler and Leman go neural: Higher-order graph neural networks (2019) Proc. AAAI. [5] K. Xu et al. What can neural networks reason about? (2019) arXiv:1905.13211. [6] Q. Huang et al. Combining label propagation and simple models out-performs graph neural networks (2020) arXiv:2010.13993. [7] F. Frasca et al. SIGN: Scalable Inception Graph Neural Networks (2020) arXiv:2004.11198. [8] A. Graves, G. Wayne, and I. Danihelka, Neural Turing Machines (2014) arXiv:1410.5401. [9] A. Graves et al. Hybrid computing using a neural network with dynamic external memory (2016). Nature 538:471‚Äì476. [10] G. Yehuda, M. Gabel, and A. Schuster. It‚Äôs not what machines can learn, it‚Äôs what we cannot teach (2020) arXiv:2002.09398. [11] K. Xu et al. How neural networks extrapolate: From feedforward to graph neural networks (2020) arXiv:2009.11848. [12] P. Veliƒçkoviƒá et al., Neural execution of graph algorithms (2019) arXiv:1910.10593. [13] O. Richter and R. Wattenhofer, Normalized attention without probability cage (2020) arXiv:2005.09561. [14] H. Tang et al., Towards scale-invariant graph-related problem solving by iterative homogeneous graph neural networks (2020) arXiv:2010.13547. [15] P. Veliƒçkoviƒá et al. Pointer Graph Networks (2020) Proc. NeurIPS. [16] Y. Yan et al. Neural execution engines: Learning to execute subroutines (2020) Proc. ICLR. [17] C. K. Joshi et al. Learning TSP requires rethinking generalization (2020) arXiv:2006.07054. [18] A. Deac et al. XLVIN: eXecuted Latent Value Iteration Nets (2020) arXiv:2010.13146. [19] S. L√∂we et al., Amortized Causal Discovery: Learning to infer causal graphs from time-series data (2020) arXiv:2006.10833. [20] Y. Li et al., Causal discovery in physical systems from videos (2020) Proc. NeurIPS. [21] D. Bieber et al., Learning to execute programs with instruction pointer attention graph neural networks (2020) Proc. NeurIPS. [22] A. Kazi et al., Differentiable Graph Module (DGM) for graph convolutional networks (2020) arXiv:2002.04999 [23] D. D. Johnson, H. Larochelle, and D. Tarlow., Learning graph structure with a finite-state automaton layer (2020). arXiv:2007.04929. [24] T. Pfaff et al., Learning mesh-based simulation with graph networks (2020) arXiv:2010.03409. [25] T. Kipf et al., Contrastive learning of structured world models (2020) Proc. ICLR [26] F. Locatello et al., Object-centric learning with slot attention (2020) Proc. NeurIPS. [27] W. Azizian and M. Lelarge, Characterizing the expressive power of invariant and equivariant graph neural networks (2020) arXiv:2006.15646. [28] A. Loukas, What graph neural networks cannot learn: depth vs width (2020) Proc. ICLR. [29] Z. Chen et al., Can graph neural networks count substructures? (2020) Proc. NeurIPS. [30] A. Bojchevski et al., Scaling graph neural networks with approximate PageRank (2020) Proc. KDD. [31] E. Rossi et al., Temporal Graph Networks for deep learning on dynamic graphs (2020) arXiv:2006.10637. [32] S. Kumar, X. Zhang, and J. Leskovec, Predicting dynamic embedding trajectory in temporal interaction networks (2019) Proc. KDD. [33] R. Trivedi et al., DyRep: Learning representations over dynamic graphs (2019) Proc. ICLR. [34] D. Xu et al., Inductive representation learning on temporal graphs (2019) Proc. ICLR. [35] M. Noorshams, S. Verma, and A. Hofleitner, TIES: Temporal Interaction Embeddings for enhancing social media integrity at Facebook (2020) arXiv:2002.07917. [36] X. Wang et al., APAN: Asynchronous Propagation Attention Network for real-time temporal graph embedding (2020) arXiv:2011.11545. [37] E. A. Meirom et al., How to stop epidemics: Controlling graph dynamics with reinforcement learning and graph neural networks (2020) arXiv:2010.05313. [38] S. Hooker, Hardware lottery (2020), arXiv:2009.06489. [39] P. E. Sarlin et al., SuperGlue: Learning feature matching with graph neural networks (2020). Proc. CVPR. [40] S. Ruhk et al., Learning representations of irregular particle-detector geometry with distance-weighted graph networks (2019) arXiv:1902.07987. [41] J. Shlomi, P. Battaglia, J.-R. Vlimant, Graph Neural Networks in particle physics (2020) arXiv:2007.13681. [42] J. Krupa et al., GPU coprocessors as a service for deep learning inference in high energy physics (2020) arXiv:2007.10359. [43] A. Heintz et al., Accelerated charged particle tracking with graph neural networks on FPGAs (2020) arXiv:2012.01563. [44] M. Cranmer et al., Discovering symbolic models from deep learning with inductive biases (2020) arXiv:2006.11287. Miles Cranmer is unrelated to Kyle Cranmer, though both are co-authors of the paper. See also the video presentation of the paper. [45] Q. Cai et al., A survey on multimodal data-driven smart healthcare systems: Approaches and applications (2020) IEEE Access 7:133583‚Äì133599 [46] K. Gopinath, C. Desrosiers, and H. Lombaert, Graph domain adaptation for alignment-invariant brain surface segmentation (2020) arXiv:2004.00074 [47] J. Liu et al., Identification of early mild cognitive impairment using multi-modal data and graph convolutional networks (2020) BMC Bioinformatics 21(6):1‚Äì12 [48] H. E. Manoochehri and M. Nourani, Drug-target interaction prediction using semi-bipartite graph model and deep learning (2020). BMC Bioinformatics 21(4):1‚Äì16 [49] Y. Huang and A. C. Chung, Edge-variational graph convolutional networks for uncertainty-aware disease prediction (2020) Proc. MICCAI [50] L. Cosmo et al., Latent-graph learning for disease prediction (2020) Proc. MICCAI [51] G. Vivar et al., Simultaneous imputation and disease classification in incomplete medical datasets using Multigraph Geometric Matrix Completion (2020) arXiv:2005.06935. [52] X. Li and J. Duncan, BrainGNN: Interpretable brain graph neural network for fMRI analysis (2020) bioRxiv:2020.05.16.100057 [53] X. Yu et al., ResGNet-C: A graph convolutional neural network for detection of COVID-19 (2020) Neurocomputing. [54] P. Gainza et al., Deciphering interaction fingerprints from protein molecular surfaces using geometric deep learning (2020) Nature Methods 17(2):184‚Äì192. [55] F. Sverrisson et al., Fast end-to-end learning on protein surfaces (2020) bioRxiv:2020.12.28.424589. [56] A. Klimovskaia et al., Poincar√© maps for analyzing complex hierarchies in single-cell data (2020) Nature Communications 11. [57] J. Jumper et al., High accuracy protein structure prediction using deep learning (2020) a.k.a. AlphaFold 2.0 (paper not yet available). [58] J. M. Stokes et al., A deep learning approach to antibiotic discovery (2020) Cell 180(4):688‚Äì702. [59] D. Morselli Gysi et al., Network medicine framework for identifying drug repurposing opportunities for COVID-19 (2020) arXiv:2004.07229. I am grateful to Bruno Correia, Kyle Cranmer, Matthias Fey, Will Hamilton, Sergey Ivanov, Anees Kazi, Thomas Kipf, Haggai Maron, Emanuele Rossi, Mark Saroufim, Petar Veliƒçkoviƒá, and Marinka Zitnik for their inspiring comments and predictions. This is my first experiment with a new format of ‚Äúscientific journalism‚Äù and I appreciate suggestions for improvement. Needless to say, all the credit goes to the aforementioned people, whereas any criticism should be my sole responsibility. A Chinese translation of this post is available courtesy of Zhiqiang Zhong. I also discuss some of these predictions in the TWIML Podcast with Sam Charrington. Interested in graph ML and Geometric Deep Learning? See my blog on Towards Data Science, subscribe to my posts, get Medium membership, or follow me on Twitter.",863,5,17,Towards Data Science,2021-01-05,2021
https://towardsdatascience.com/clean-architecture-for-ai-ml-applications-using-dash-and-plotly-with-docker-42a3eeba6233,Clean Architecture for AI/ML Applications using Dash and Plotly with¬†Docker,"<em class=""markup--em markup--h4-em"">Create enterprise-level</em>",10,74,"['Clean Architecture for AI/ML Applications using Dash and Plotly with Docker', 'Why Dash', 'Clean Architecture for Dash', 'MVC in Dash', 'Routing', 'Dockerize our Dash Application', 'Conclusions', 'Next Steps', 'The Code', 'Final Words']","Almost every Data Science project requires some kind of visualization, like visualizing the input data, exploratory data analysis using histograms or scatter plots, finding outliers or plotting statistics using box and whisker plots, visualizing the relationship between nodes using network diagrams, checking the relationships between variables using correlation matrices, visualization techniques to help understand relationships within high-dimensional datasets, visualizing the performance of the models, or the train history, etc. Furthermore, data visualization may become a valuable addition to any presentation and the quickest path to understanding your data. As you can see, data visualization is a crucial part of any Data Science project, but creating a dashboard is not a trivial task. There are lots of libraries available to generate beautiful diagrams, but if you are working in python Dash is the best choice in my opinion. As you can see in the official documentation, Dash is a productive Python framework for building web analytic applications. Dash is written on top of the most popular frameworks and libraries, like Flask, React, and Plotly and it is ideal for building data visualization apps. It is easy to learn because Dash abstracts away all of the technologies and protocols that are required to build an interactive web-based application. Using Dash you can work purely in Python, the backend, and also the frontend can be written using Python. Every framework or library has also disadvantages, but in the case of Dash, there are more advantages than disadvantages, and the disadvantages can be resolved using some workarounds. Overall, if you want to create an enterprise-level interactive dashboard for your Data Science model or algorithm, with little boilerplate, written fully in Python (so you can reuse code between backend and frontend) Dash is the best choice. Note: We will work in a virtual environment. To create and activate a virtual environment see Python Virtual Environments. The first step in obtaining a clean architecture is to define the File Structure of the project. Any serious web application has multiple components and multiple pages. In order to make it easy to add functionality without impacting other layouts and introduce a clear separation of concerns, I recommend the following File Structure: If you want to automatically include images, Stylesheets, or Javascript files in your project, Dash requires you to add them to the Assets folder. Dash will automatically serve all of the files that are included in this folder. See Dash external resources. This folder will contain the reusable components, like a button that is used on multiple pages, a table to visualize the data, a header that is used on multiple pages, etc. For example, it can contain a function to construct a table based on a dataframe: The web application will have a common layout that can always be seen. Usually, this is composed of a Sidebar and/or a Menu and the Content Page. In our example, we have a collapsible Sidebar and the content page. As we can see in the image above, as a common layout we have a Sidebar (highlighted with green), which can always be seen (unless it is collapsed), and the Content, which will change, base on the page selection from the Sidebar. For the Sidebar we have two files, one for defining the view (sidebar.py) and one for defining the behavior, to declare the callbacks (sidebar_callbacks.py). The content is a simple Div and the final common Layout is composed of the Sidebar, the Content div and it also includes the Location component of Dash, which represents the location or address bar in your web browser. Through its href, pathname, search, and hash properties you can access different portions of the URL that the app is loaded on, which is very helpful for routing and navigation. An enterprise-level web application usually is composed of multiple pages and for each page, we will create a new folder. Each page will be composed of three different files, this way we can apply the MVC architectural pattern also for Dash. We will describe how to use MVC in Dash later in this tutorial. This folder will contain generic and reusable functions that are not related to specific pages or components but can be used in general. For example, here we can add functions like currency conversion, decimal digit formatting, or other mathematical functions. It will also contain constants used globally or links to external assets (links to stylesheets, icons, fonts, etc.) Because different environments (dev, testing, staging, pre-prod, prod, or others) will have different configuration files with different environment variables, we need a way to choose between these configuration files. This folder will contain the different configuration files and the mechanism to select between these files base on the environment. In our tutorial, we will have two environments, development and production. For these two environments, we will have two different .env files. In order to read the key-value pair from the .env files and add them to the environment variables, we use the dotenv library. For development, we will use the .env.development config file. For production, we will use the .env file. The host must be 0.0.0.0 because later in this tutorial we will create a Docker container for our example application and Docker requires host=0.0.0.0. The logic to read different configurations based on the environment can be found in the settings.py file. In line 5 we setup the path to the correct configuration file based on the value of the ENVIRONMENT_FILE environment variable. For development, this will have as value ‚Äú.env.development‚Äù and on the production environment we will set this variable to ‚Äú.env‚Äù. This way we will have different configurations based on different environments. To set the value of the ENVIRONMENT_FILE environment variable, in VS Code, we can use the following launch.json file: Our production environment will be Docker, so in the Dockerfile we will set this environment variable using ENV ENVIRONMENT_FILE=‚Äù.env‚Äù command. This folder is autogenerated by the so-called Memoization capabilities of Dash. Since Dash‚Äôs callbacks are functional in nature (they don‚Äôt contain any state), it‚Äôs easy to add memoization caching. Memoization stores the results of a function after it is called and reuses the result if the function is called with the same arguments. In order to setup this Memoization capability, we have to define the cache type. Instead of the local file system, we can also use a Redis cache. Setting up Redis is out-of-scope for this tutorial, so we will use the file system in order to cache and reuse data between calls. In the case of the MVC (Model-View-Controller) architectural design pattern, each interconnected component is built to take on a specific task in the development process. Using this pattern has multiple benefits: One of the most important disadvantages of the MVC pattern is that usually, we need to know multiple technologies because the View and the Controller + Model is implemented using different technologies (like React/Angular/Vue/etc. for the View and .Net/Java/Python/Node/Go/etc. for the backend). If you are working in Python and Dash, then this issues is resolved, you can use Python for both the View and the Controller+Model. As you can see in the picture above, in our project, for each page we will have three files and we have a common layout as we‚Äôve presented earlier in this article. Now let's see an example, how do we create a page to use the MVC pattern. We will create an example page in which we will plot a diagram about the life expectancy for different countries based on the GDP. Example for the Model In the code above we used the Memoization feature of Dash because we don‚Äôt want to read the same data multiple times, so we cache the data, this way increasing the performance of our application. The Model does NOT depend on the View or on the Controller, it is decoupled and its responsibility is to handle the data, it does not have to deal with the user interface or data processing. Example for the View The only responsibility of the view is to show the data to the user. As you can see, it uses the Model (dataframe) and defines the Html/Css/Js/other markup language elements to present the data to the user. As you can see in the code above, our example uses only the Model and creates the presentation elements. It does NOT define logic for handling user events and it does NOT contain business logic. Example for the Controller The Controller interprets the mouse and keyboard inputs from the user, informing the Model and the View to change as appropriate. As you can see in the example above, the update_figure callback gets as Input the value that the user chooses on the Slider (so it handles user events) and as Output, it updates the figure from the view, based on the data read from the Model. So as the MVC pattern defines, the Controller is the part in which we process the data after we get a request from View and before updating anything in our database with our Model. In order to navigate between the pages, we need to define the rules for routing. For this, in the root folder, we‚Äôve created a file called routes.py. As we can see in the code, we define a callback that accepts as Input the active path (this method will be triggered, when the route changes) and the Output will be rendered in the Content of the main Layout (page-content). To render the appropriate page, we check the active path and based on the active path we return a page. The first step is to define the Dockerfile. To reduce the size of the image, we used the slim version and we also removed the cached files(line 8). The app will be served by Gunicorn, so we will have to define the configuration for it (see gunicorn_config.py). Inside the docker image, the host must be 0.0.0.0 otherwise it won‚Äôt be accessible. The Dash instance is defined in a separate app.py, while the entry point for running the app is index.py. This separation is required to avoid circular imports: the files containing the callback definitions require access to the Dash app instance however if this were imported from index.py, the initial loading of index.py would ultimately require itself to be already imported, which cannot be satisfied. For Gunicorn it is important to explicitly define the server as a Falsk app (line 12). Since we‚Äôre adding callbacks to elements that don‚Äôt exist in the app.layout, Dash will raise an exception to warn us that we might be doing something wrong. We can ignore the exception (it is normal on multi-page apps) by setting suppress_callback_exceptions=True. Note that we read the values for host, port, debug, and dev_tools_props_check from the environment.settings this way we can have different values based on the active environment. For Gunicorn we also need to define the Procfile. As you can see, we run the server (which is the Falsk app as we defined in the app.py) from the index file (index:server). To install all the required dependencies we define the requirements.txt file. We also define a docker-compose.yml file, this way it will be easy to build and run our Docker image. To set the $VERSION and the $TARGET variables, we define a .env file next to the docker-compose.yml (in the same folder) To build and run our application we run docker-compose build and then docker-compose up -d commands (in the root folder). In this tutorial we described a possible way of organizing your Dash application, we showed how to apply architectural design patterns to introduce separation of concerns and write clean and maintainable code. We also presented how to use multiple pages, how to use the Memoization of Dash to cache and reuse data, how to use different configurations for different environments and we described a clean way to setup and create Docker container for your Dash application. I really like coffee, because it gives me the energy to write more articles. If you liked this article, then you can show your appreciation and support by buying me a coffee! Become a writer on Medium: https://czakozoltan08.medium.com/membership In our next tutorial, we will present more advanced concepts of Dash, like sharing data between callbacks, persistence, memoization using Redis, creating callbacks dynamically, and using pattern matching callbacks. The code for this tutorial can be found in this git repository. (Please leave a ‚ú© if you like my code) Thank you for reading this long article! If you want some ‚ÄúStupid Simple‚Äù explanations, please follow me on Medium! There is an ongoing ‚ÄúStupid Simple AI‚Äù series. The first two articles can be found here: SVM and Kernel SVM and KNN in Python. If you want some ‚ÄúStupid Simple‚Äù explanations and tutorials about Kubernetes, you can check my Stupid Simple Kubernetes series. Thank you!",427,7,11,Towards Data Science,2021-01-05,2021
https://towardsdatascience.com/top-10-cnn-architectures-every-machine-learning-engineer-should-know-68e2b0e07201,Top 10 CNN Architectures Every Machine Learning Engineer Should¬†Know,,5,25,"['Top 10 CNN Architectures Every Machine Learning Engineer Should Know', 'What is CNN?', 'Top 10 CNN architectures', 'Conculusion', 'References']","A convolutional neural network (CNN or ConvNet) is a specific kind of deep learning architecture. At the moment, there are many tech companies have developed active research groups for exploring new architectures of CNN such as Google, Microsoft, and Facebook and they demonstrated that CNNs are one of the best learning algorithms for understanding and analyzing image content that has shown high performance in image segmentation, classification, detection, and retrieval related tasks. CNNs were designed for image recognition tasks were originally applied to the challenge of handwritten digit recognition¬π ¬≤. The basic design goal of CNNs was to create a network where the neurons in the early layer of the network would extract local visual features, and neurons in later layers would combine these features to form higher-order features. There are three main types of layers that you will see in almost every CNNs which are convolutional layer, pooling layer, and fully connected layer. Over the years, there are many variants of CNN architectures have been developed to solve real-world problems. LeNet is the first successful application of CNNs and was developed by Yann Lecun in the 1990s that was used to read zip codes, digits, etc. The latest work is called LeNet-5 which a 5-layer CNN that reaches 99.2 % accuracy on insolated character recognition. In this article, we will discuss the top 10 CNN architectures every machine learning engineer should know that have provided that boost to the field of deep learning over the world. In 2012, Alex Krizhevsky, Ilya Sutskever, and Geoff Hinton won the ImageNet Large Scale Visual Recognition Challenge with a test accuracy of 84.6%¬≥. The model significantly outperformed the second runner-up (top-5 error of 16% compared to runner-up with 26% error). Krizhevsky used GPUs to train the AlexNet, which enabled faster training of CNNs models and started a wave of interest and new works based on CNNs. The network consists of 5 convolutional layers and 3 fully connected layers. The model achieves 92.7% top-5 test accuracy in ImageNet, which is a dataset of over 14 million images belonging to 1000 classes. It was proposed by Karen Simonyan and Andrew Zisserman of the Visual Geometry Group Lab of Oxford University in 2014‚Å¥. The 16 in VGG16 refers to it has a total of 16 layers that have weights. VGG-19 is a convolutional neural network that is 19 layers deep and can classify images into 1000 object categories such as a keyboard, mouse, and many animals. The model trained on more than a million images from the Imagenet database with an accuracy of 92%. GoogLeNet (or Inception v1) has 22 layers deep‚Å¥. With the accuracy of 93.3% this model won the 2014 ImageNet competition in both classification an detection task. The network has been created and acquainted by Microsoft. With 96.4% accuracy this model won the 2016 ImageNet competition. It is well-known due to its depth (to 152 layers) and the introduction of residual blocks. It consists of 18 deep layers and can classify images into 1000 object categories, such as keyboard, mouse, pencil, and many animals. For the same accuracy as AlexNet, SqueezeNet can be three times faster and 500 times smaller. The name ‚ÄúDenseNet‚Äù refers to Densely Connected Convolutional Networks‚Å∑ developed by Gao Huang, Zhuang Liu, and their team in 2017 at the CVPR Conference. It received the best paper award, and has accrued over 2000 citations. With traditional convolutional networks with n layers have n connections but DensetNet has n(n+1)/2 connections in total becaues of feed-forward fashion. It is an extremely efficient CNN architecture with 173 deep layers, designed for mobile devices with the computing power of 10‚Äì150 MFLOPs‚Å∂. It manages to obtain lower top-1 error (absolute 7.8%) than the Mobile Net system on Image Net classification. ‚ÄãEfficient Neural Network‚Å∏ gives the ability to perform pixel-wise semantic segmentation in real-time. ENet is up to 18x faster, requires 75x fewer FLOPs , has 79x fewer parameters, and provides similar or better accuracy to existing models. Enet is the fastest model in semantic segmentation. We can see that the accuracy has been gradually increasing in most of the cases with time. [1] Fukushima, K. Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position. Biol. Cybernetics 36, 193‚Äì202 (1980). https://doi.org/10.1007/BF00344251 [2] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 1998. [3] Krizhevsky, Alex & Sutskever, Ilya & Hinton, Geoffrey. (2012). ImageNet Classification with Deep Convolutional Neural Networks. Neural Information Processing Systems. 25. 10.1145/3065386. [4] C. Szegedy et al., ‚ÄúGoing deeper with convolutions,‚Äù 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Boston, MA, 2015, pp. 1‚Äì9, doi: 10.1109/CVPR.2015.7298594. [5] Simonyan, Karen & Zisserman, Andrew. (2014). Very Deep Convolutional Networks for Large-Scale Image Recognition. arXiv 1409.1556. [6] X. Zhang, X. Zhou, M. Lin and J. Sun, ‚ÄúShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices,‚Äù 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, Salt Lake City, UT, 2018, pp. 6848‚Äì6856, doi: 10.1109/CVPR.2018.00716. [7] G. Huang, Z. Liu, L. Van Der Maaten and K. Q. Weinberger, ‚ÄúDensely Connected Convolutional Networks,‚Äù 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Honolulu, HI, 2017, pp. 2261‚Äì2269, doi: 10.1109/CVPR.2017.243. [8] Paszke, Adam & Chaurasia, Abhishek & Kim, Sangpil & Culurciello, Eugenio. (2016). ENet: A Deep Neural Network Architecture for Real-Time Semantic Segmentation.",296,1,5,Towards Data Science,2021-01-05,2021
https://towardsdatascience.com/efficient-time-series-using-pythons-pmdarima-library-f6825407b7f0,Efficient Time-Series Using Python‚Äôs Pmdarima¬†Library,Demonstrating the efficiency of pmdarima‚Äôs‚Ä¶,1,24,['Efficient Time-Series Analysis Using Python‚Äôs Pmdarima Library'],"One of the key concepts in data science is time-series analysis which involves the process of using a statistical model to predict future values of a time series (i.e. financial prices, weather, COVID-19 positive cases/deaths) based on past results. Some components that might be seen in a time-series analysis are: When conducting time-series analysis, there are either Univariate Time-Series analysis or Multivariate Time-Series analysis. Univariate is utilized when only one variable is being observed against time, whereas Multivariate is utilized if there are two or more variables being observed against time. ARIMA is an acronym which stands for Auto Regressive Integrated Moving Average and is a way of modeling time-series data for forecasting and is specified by three order parameters (p,d,q): There are three types of ARIMA models, ARIMA, SARIMA, and SARIMAX which differ depending on seasonality and/or use of exogenous variables. Pmdarima‚Äòs auto_arima function is extremely useful when building an ARIMA model as it helps us identify the most optimal p,d,q parameters and return a fitted ARIMA model. As a newcomer to data science, when conducting time-series analysis, I took the ‚Äúlong‚Äù way before coming across pmdarima‚Äôs auto_arima function to build a high performance time-series model. For this article, I will focus on the Univariate Time-Series analysis to forecast the number of airline passengers (from Kaggle) and discuss through the traditional ARIMA implementation versus the more efficient, auto_arima way. The general steps to implement an ARIMA model: First, I loaded and prepared the data by changing the date to a datetime object, setting the date to index using the set_index method, and checking for null values. I then took a preliminary look at the average monthly number of airline passengers, which revealed that the data was not stationary. This was further confirmed by conducting a Dickey-Fuller test which is a unit root test for stationarity, as shown in the image below: After differencing our data twice, our p-value was less than our alpha (0.05), so we were able to reject the null hypothesis and accept the alternative hypothesis that the data is stationary. We then modeled our time-series data by setting the d parameter to 2. Next, I looked at our ACF/PACF plots using the differenced data to visualize the lags that will likely be influential when modeling the number of passengers. From our visualizations, I determined that our p parameter is 0 and q parameter is 2 ‚Äî our p,d,q parameters will be (0,2,2) for the ARIMA model. After splitting the data into training and testing groups and fitting the ARIMA model on the training set to predict the test set, we obtained a r¬≤ value of -1.52 ‚Äî telling us that the model did not follow the trend of data at all. I most likely calculated the p,d,q values incorrectly which caused the r¬≤ value to be negative, but in the mean time let‚Äôs try to build another ARIMA model using pmdarima. In the previous method, checking for stationarity, making data stationary if necessary, and determining the values of p and q using the ACF/PACF plots can be time-consuming and less efficient. Using pmdarima‚Äôs auto_arima() function makes this task easier for us by eliminating steps 2 and 3 for implementing an ARIMA model. Let‚Äôs try it with the current dataset. After loading and preparing the data, we can use pmdarima‚Äôs ADFTest() function to conduct a Dickey-Fuller test. This result indicates that the data is not stationary, so we need to use the ‚ÄúIntegrated (I)‚Äù concept (d parameter) to make the data stationary while building the Auto ARIMA model. Next, I split the dataset into training and test (80%/20%) sets to build the Auto ARIMA model on the training set and forecast using the test dataset Then, we build the Auto ARIMA model by using pmdarima‚Äôs auto_arima() function. Using the auto_arima() function calls for small p,d,q values which represent non-seasonal components and uppercase P,D,Q values which represent seasonal components. Auto_arima() is similar to other hyperparameter tuning methods, and is determined to find the optimal values for p,d,q using different combinations. The final p,d,q values are determined with lower AIC and BIC parameters taken into consideration. We can view the model summary: Next, we can using the trained model to forecast the number of airline passengers on the test set and create a visualization. The Auto ARIMA model gave us a r¬≤ value of 0.65 ‚Äî this model did a much better job at capturing the trend in the data compared to my first implementation of the ARIMA model. In this article, I demonstrated the traditional implementation of an ARIMA model compared to the Auto ARIMA model using auto_arima(). While the traditional ARIMA implementation requires one to perform differencing and plotting ACF and PACF plots, the Auto ARIMA model using pmdarima‚Äôs auto_arima() function is more efficient in determining the optimal p,d,q values. For more information about pmdarima‚Äôs auto_arima() function, please see the following documentation alkaline-ml.com Thank you for reading! All code is available on my GitHub :)",341,0,6,Towards Data Science,2021-01-05,2021
https://towardsdatascience.com/making-network-graphs-interactive-with-python-and-pyvis-b754c22c270,Making network graphs interactive with Python and¬†Pyvis.,The easy way to plot superb¬†graphs.,6,28,"['Making network graphs interactive with Python and Pyvis.', '1. Simple graph example.', '2. Molecules as graphs.', '3. Now, some karate moves.', '4. Deploy on streamlit.', 'What to do now..']","For a while, I and others in the Streamlit community [1] have been seeking a tool to render interactive graphs, but until now this has been something only a few can achieve. This can be due to the fact that a level of expertise with javascript is required, something that many Streamlit users may want to avoid, as this is the promise of the project, deploy beautiful web apps from python, no javascript required!! (of course, mastering javascript is a big plus for making awesome streamlit apps). Recently I found Pyvis after a few weeks looking for an alternative to other javascript based libraries used to make graphs interactive. I liked pyvis a lot and wanted to share in this post some tips I‚Äôve learned. But if you want to see by yourself, check the documentation page [2]: pyvis.readthedocs.io The rest of this post is as follows: First, I will show a super simple example to do a small 3 nodes net. Second, I show how to fix the positions of nodes and how to preserve the shape of the graph of a molecule. The third case is devoted to building a network with Networkx. Finally, I show a Streamlit app working with Pyvis. I will start with a simple example, creating a Network object and adding 3 nodes (method add_node) labeled 1, 2, and 3 with two edges (method add_edge) [1‚Äì2] and [2‚Äì3]. In order to deploy the graph (for example in a Jupyter or Colab environment) pyvis ‚Äútranslates‚Äù the python code to html+javascript with the show method. Calling g.show(‚Äòexample.html‚Äô) writes to disk the html file which can be rendered with display: and here is the result, And that‚Äôs all easy, isn‚Äôt it? This basic recipe will be used in the following. I did a small function show_graph() to do the display in a single step, as you could see in the snippets below. You can find this and all the code in a Colab notebook in the github repo at the bottom üôå. The next case is something I figured out that could be achieved, combining pyvis with the chemoinformatics library RDKit ü§ì [3]. I assume this is not of general interest, so I will avoid the process to build up the molecule, and instead, I will present just the resulting nodes and connectivity information of the atoms. This is shown here for caffeine molecule ‚òïÔ∏è. As I want to preserve its shape, first I need the coordinates of the atoms, but also some very specific physics options. Let me explain in a simple way how this is accomplished: as there are 14 atoms, I did a loop running on each atom number, assigning nodes with the add_node method. For each node, I gave the atom symbol, and Cartesian coordinates from three separated lists (which I got from RDKit preprocessing) named respectively ids, xs and ys: (xs and ys are arbitrarily multiplied by 100 just to make the graph more manageable) The full snippet is hosted with ‚ù§Ô∏è :) where you can see also the physics options: And this is what results, pretty much acceptable I think: This can be used chiefly for presentations to catch eye attention or to design chemistry-oriented web pages. But other uses could be envisioned as well. If you are familiar with Networkx [4], you should know that it is very popular because this library is easy to use and well documented, but as far as I know, it lacks this feature of making the graph interactive. Fortunately, Pyvis accepts graph objects from Networkx beautifully. Networkx has prebuilt the Zachary‚Äôs Karate Club graph [5], where you have 34 members of the club, being 0 and 33 the President and the Sensei which have a conflict, and they separate the club in two groups based on influence among members. Below is shown the piece of code needed for this graph in pyvis. First, we got G with the graph from Networkx. Then we build the pyvis object g4, and method from_nx is used to get the graph into pyvis‚Ä¶ as simple as that: Here I must highlight the show_buttons method in line 8, which is used to deploy the physics options, allowing the user to change interactively the way the nodes interact with each other as if they were connected by springs and having other physical interactions. That makes the dynamics of the graph look more ‚Äúnatural‚Äù and visually is a joy. Of course, there are other ways to make graphs interactive, using advanced libraries, but in general, they are more complex. The simplicity offered by pyvis is something that is appreciated. Finally, I show how these dynamic graphs look in a Streamlit web app. I am not going to discuss how to do the app, I let that as homework to the reader. hint: you can see the github repo below üòè https://share.streamlit.io/napoles-uach/streamlit_network/main/app.py https://github.com/napoles-uach/streamlit_network https://github.com/napoles-uach/streamlit_network/blob/main/pyvis_sample.ipynb jnapoles.medium.com Links cited: discuss.streamlit.io pyvis.readthedocs.io www.rdkit.org networkx.org en.wikipedia.org",375,3,5,Towards Data Science,2021-01-05,2021
https://towardsdatascience.com/zero-and-few-shot-learning-c08e145dc4ed,Zero and Few Shot¬†Learning,Examples on low resource Indonesian language using FlairNLP¬†and‚Ä¶,6,27,"['Zero and Few Shot Learning', 'Introduction', 'Intuition behind zero and few shot learning', 'FlairNLP and Huggingface to the rescue!', 'Examples', 'Conclusion']","The field of NLP is getting more and more exciting each day. Until a few years ago, we were not able to fully leverage the vast sources of data available online. With the amazing success of unsupervised learning methods and transfer learning, the NLP community has built models which serve as a knowledge base for multiple NLP tasks. However, we‚Äôre still dependent on annotated data for fine-tuning on a downstream task. More often than not, getting labeled data is not handy and is a relatively expensive and time taking exercise. What can we do if we don‚Äôt have any labeled data or have very less of it? The answer to this problem is zero-shot and few shot learning. There is no single definition of zero and few shot methods. Rather, one can say that its definition is task dependent. Zero shot classification means that we train a model on some classes and predict for a new class, which the model has never seen before. Obviously, the class name needs to exist in the list of classes, but there are no training samples for this class. In the more broader sense, zero shot setting refers to using a model to do something it wasn‚Äôt trained to do. Let‚Äôs assume we train a language model on a large text corpus (or use a pre-trained one like GPT-2). Our task is to predict whether a given article is about sports, entertainment or technology. Normally, we would formulate this as a fine tuning task with many labeled examples, and add a linear layer for classification on top of the language model. But with zero shot, we use the language model directly (without any explicit fine-tuning). We can give the article along with explanatory label names and get a prediction. Does it still sound confusing? Fret not! and dive into the intuition behind these concepts to make things clearer. Also, the example in the end will ensure a deeper understanding of its usability. We as humans store a huge amount of information that we learn from every resource, be it books, news, courses, or just experience. If we are asked to do the following task:‚ÄúTranslate from english to french‚Äù: How are you? -> ? From the task‚Äôs description, it is quite clear to us what is to be done here. We have used our knowledge base to infer what translation means. Another task can be as follows:‚ÄúI loved the movie!‚Äù -> happy-or-sad Reading the self explanatory task explanation (happy-or-sad), we understand that it is a classification task. Our knowledge base also helps us understand the sentence and infer that it is happy! This is exactly how zero shot classification works. We have a pre trained model (eg. a language model) which serves as the knowledge base since it has been trained on a huge amount of text from many websites. For any type of task, we give relevant class descriptors and let the model infer what the task actually is. Needless to say, the more labeled data we provide, the better the results would be. And many times, zero shot doesn‚Äôt work very well. If we have a few samples of labeled data but not enough for fine tuning, few shot is the way to go. As used in GPT-3, ‚ÄúLanguage Models are Few Shot Learners‚Äù, the authors prove that very large language models can perform competitively on downstream tasks with much lesser labeled data as compared to what smaller models would require. Few Shot is simply an extension of zero shot, but with a few examples to further train the model. Both FlairNLP and Huggingface have zero shot classification pipelines for english (since they use bert as the model). Even though flairNLP uses bert-base-uncased for english as its base model, it works surprisingly well with simple indonesian text. In the below example, I‚Äôll walk you through the steps of zero and few shot learning using the TARS model in flairNLP on indonesian text. The zero-shot classification pipeline implemented by huggingface has some excellent articles and demos. Check out this excellent blog and this live demo on zero shot classification by HuggingFace. Since I am using Bahasa Indonesia, I will not use the huggingface zero-shot pipeline, but use the GPT-2 model (trained on Bahasa Indonesia by cahya and available on huggingface) and implement zero-shot classification. Use the zero shot classifier (TARS) model to classify whether the sentence is about sports (olahraga) or politics (politik) Output The english model works surprisingly well on Bahasa Indonesia! Although it predicts the second example incorrectly, the sentence ‚ÄúThe cabinet ministers are playing a nasty game‚Äù has been categorized as sports and not politics. Probably because the word playing confusing the model! Let us make it learn with 4 examples (2 from each class) and re-evaluate on the same example using few shot classification. Train the model on the new data Output Output Zero shot and few shot learning methods are reducing the reliance on annotated data. The GPT-2 and GPT-3 models have shown remarkable results to prove this. However, for low resource languages like Bahasa Indonesia, it is still an area of active research. However, thanks to valuable resources like cahya, we have a GPT-2 model trained specifically for Indonesian. However, since the data for indonesian is not humongous, few shot is preferred over zero-shot learning. Nonetheless, it is absolutely remarkable what we can achieve with only a few examples, and how libraries like flairNLP and huggingface have made life so much easier to implement cutting edge models! Cheers! Eram",251,1,6,Towards Data Science,2021-01-05,2021
https://towardsdatascience.com/learning-by-implementing-gaussian-naive-bayes-3f0e3d2c01b2,Learning by Implementing: Gaussian Naive¬†Bayes,Learn how Gaussian Naive Bayes works and implement it‚Ä¶,4,59,"['Understanding by Implementing: Gaussian Naive Bayes', 'The Theory', 'The complete Implementation', 'Conclusion']","I think this is a classic at the beginning of each data science career: the Naive Bayes Classifier. Or I should rather say the family of naive Bayes classifiers, as they come in many flavors. For example, there is a multinomial naive Bayes, a Bernoulli naive Bayes, and also a Gaussian naive Bayes classifier, each different in only one small detail, as we will find out. The naive Bayes algorithms are quite simple in design but proved useful in many complex real-world situations. In this article, you can learn You can find the code on my Github. It might help a bit to check out my primer on Bayesian statistics A gentle Introduction to Bayesian Inference to get used to the Bayes formula. As we will implement the classifier in a scikit learn-conform way, it‚Äôs also worthwhile to check out my article Build your own custom scikit-learn Regression. However, the scikit-learn overhead is quite small and you should be able to follow along anyway. We will start exploring the astonishingly simple theory of naive Bayes classification and then turn to the implementation. What are we really interested in when classifying? What are we actually doing, what is the input and the output? The answer is simple: Given a data point x, what is the probability of x belonging to some class c? That‚Äôs all we want to answer with any classification. You can directly model this statement as a conditional probability: p(c|x). For example, if there are the result of a classifier could be something like p(c‚ÇÅ|x‚ÇÅ, x‚ÇÇ)=0.3, p(c‚ÇÇ|x‚ÇÅ, x‚ÇÇ)=0.5 and p(c‚ÇÉ|x‚ÇÅ, x‚ÇÇ)=0.2. If we care for a single label as the output, we would choose the one with the highest probability, i.e. c‚ÇÇ with a probability of 50% here. The naive Bayes classifier tries to compute these probabilities directly. Ok, so given a data point x, we want to compute p(c|x) for all classes c and then output the c with the highest probability. In formulas you often see this as Note: max p(c|x) returns the maximum probability while argmax p(c|x) returns the c with this highest probability. But before we can optimize p(c|x), we have to be able to compute it. For this, we use Bayes‚Äô theorem: This is the Bayes part of naive Bayes. But now, we have the following problem: What are p(x|c) and p(c)? This is what the training of a naive Bayes classifier is all about. To illustrate everything, let us use a toy dataset with two real features x‚ÇÅ, x‚ÇÇ, and three classes c‚ÇÅ, c‚ÇÇ, c‚ÇÉ in the following. You can create this exact dataset via Let us start with the class probability p(c), the probability that some class c is observed in the labeled dataset. The simplest way to estimate this is to just compute the relative frequencies of the classes and use them as the probabilities. We can use our dataset to see what this means exactly. There are 7 out of 20 points labeled class c‚ÇÅ (blue) in the dataset, therefore we say p(c‚ÇÅ)=7/20. We have 7 points for class c‚ÇÇ (red) as well, therefore we set p(c‚ÇÇ)=7/20. The last class c‚ÇÉ (yellow) has only 6 points, hence p(c‚ÇÉ)=6/20. This simple calculation of the class probabilities resembles a maximum likelihood approach. You can, however, also use another prior distribution, if you like. For example, if you know that this dataset is not representative of the true population because class c‚ÇÉ should appear in 50% of the cases, then you set p(c‚ÇÅ)=0.25, p(c‚ÇÇ)=0.25 and p(c‚ÇÉ)=0.5. Whatever helps you improving the performance on the test set. We now turn to the likelihood p(x|c)=p(x‚ÇÅ, x‚ÇÇ|c). One approach to calculate this likelihood is to filter the dataset for samples with label c and then try to find a distribution (e.g. a 2-dimensional Gaussian) that captures the features x‚ÇÅ, x‚ÇÇ. Unfortunately, usually, we don‚Äôt have enough samples per class to do a proper estimation of the likelihood. To be able to build a more robust model, we make the naive assumption that the features x‚ÇÅ, x‚ÇÇ are stochastically independent, given c. This is just a fancy way of making the math easier via for every class c. This is where the naive part of naive Bayes comes from because this equation does not hold in general. Still, even then the naive Bayes yields good, sometimes outstanding results in practice. Especially for NLP problems with bag-of-words features, the multinomial naive Bayes shines. The arguments given above are the same for any naive Bayes classifier you can find. Now it just depends on how you model p(x‚ÇÅ|c‚ÇÅ), p(x‚ÇÇ|c‚ÇÅ), p(x‚ÇÅ|c‚ÇÇ), p(x‚ÇÇ|c‚ÇÇ), p(x‚ÇÅ|c‚ÇÉ) and p(x‚ÇÇ|c‚ÇÉ). If your features are 0 and 1 only, you could use a Bernoulli distribution. If they are integers, a Multinomial distribution. However, we have real feature values and decide for a Gaussian distribution, hence the name Gaussian naive Bayes. We assume the following form where Œº·µ¢,‚±º is the mean and œÉ·µ¢,‚±º is the standard deviation that we have to estimate from the data. This means that we get one mean for each feature i coupled with a class c‚±º, in our case 2*3=6 means. The same goes for the standard deviations. This calls for an example. Let us try to estimate Œº‚ÇÇ,‚ÇÅ and œÉ‚ÇÇ,‚ÇÅ. Because j=1, we are only interested in class c‚ÇÅ, let us only keep samples with this label. The following samples remain: Now, because of i=2 we only have to consider the second column. Œº‚ÇÇ,‚ÇÅ is the mean and œÉ‚ÇÇ,‚ÇÅ the standard deviation for this column, i.e. Œº‚ÇÇ,‚ÇÅ = 0.49985176 and œÉ‚ÇÇ,‚ÇÅ = 0.9789976. These numbers make sense if you look at the scatter plot from above again. The features x‚ÇÇ of the samples from class c‚ÇÅ are around 0.5, as you can see from the picture. We compute this now for the other five combinations and we are done! üòÉ In Python, you can do it like this: We receive This is the result of the training of a Gaussian naive Bayes classifier. The complete prediction formula is Let‚Äôs assume a new data point x*=(-2, 5) comes in. To see which class it belongs to, let us compute p(c|x*) for all classes. From the picture, it should belong to class c‚ÇÉ = 2, but let‚Äôs see. Let us ignore the denominator p(x) for a second. Using the following loop computed the nominators for j = 1, 2, 3. We receive Of course, these probabilities (we shouldn‚Äôt call them that way) don‚Äôt add up to one since we ignored the denominator. However, this is no problem since we can just take these unnormalized probabilities and divide them by their sum, then they will add up to one. So, dividing these three values by their sum of about 0.00032569, we get A clear winner, as we expected. Now, let us implement it! This implementation is by far not efficient, not numerically stable, it only serves an educational purpose. We have discussed most of the things, so it should be easy to follow along now. You can ignore all the check functions, or read my article Build your own custom scikit-learn if you are interested in what they exactly do. Just note that I implemented a predict_proba method first to compute probabilities. The method predict just calls this method and returns the index (=class) with the highest probability using an argmax function (there it is again!). The class awaits classes from 0 to k-1, where k is the number of classes. While the code is quite short it is still too long to be completely sure that we didn‚Äôt do any mistakes. So, let us check how it fares against the scikit-learn GaussianNB classifier. outputs The predictions using the predict method are Now, let us use scikit-learn. Throwing in some code yields The numbers look kind of similar to the ones of our classifier, but they are a little bit off in the last few displayed digits. Did we do anything wrong? No. The scikit-learn version just merely uses another hyperparameter var_smoothing=1e-09 . If we set this one to zero, we get exactly our numbers. Perfect! Have a look at the decision regions of our classifier. I also marked the three points we used for testing. That one point close to the border has only a 56.9% chance to belong to the red class, as you can see from the predict_proba outputs. The other two points are classified with much higher confidence. In this article, we have learned how the Gaussian naive Bayes classifier works and gave an intuition on why it was designed that way ‚Äî it is a direct approach to model the probability of interest. Compare this with Logistic regression: there, the probability is modeled using a linear function with a sigmoid function applied on top of it. It‚Äôs still an easy model, but it does not feel as natural as a naive Bayes classifier. We continued by calculating a few examples and collecting some useful pieces of code on the way. Finally, we have implemented a complete Gaussian naive Bayes classifier in a way that works well with scikit-learn. That means you can use it in pipelines or grid search, for example. In the end, we did a small sanity check by importing scikit-learns own Gaussian naive Bayes classifier and testing if both, our and scikit-learn‚Äôs classifier yield the same result. This test was successful. üòé I hope that you learned something new, interesting, and useful today. Thanks for reading! As the last point, if you why not do it via this link? This would help me a lot! üòä To be transparent, the price for you does not change, but about half of the subscription fees go directly to me. Thanks a lot, if you consider supporting me! If you have any questions, write me on LinkedIn!",105,1,10,Towards Data Science,2021-01-05,2021
https://towardsdatascience.com/reinforcement-learning-and-q-learning-an-example-of-the-taxi-problem-in-python-d8fd258d6d45,Reinforcement Learning and Q learning ‚ÄîAn example of the ‚Äòtaxi problem‚Äô in¬†Python,Introduction of‚Ä¶,11,48,"['Reinforcement Learning and Q learning ‚ÄîAn example of the ‚Äòtaxi problem‚Äô in Python', 'Beginner‚Äôs guide for beginners', 'Some Notes:', 'What is Reinforcement Learning?', 'Q Learning', 'Learning Process', 'The taxi Problem', 'Problem specification:', 'Here‚Äôs the code!', 'Introducing Enlightment:', 'Follow us / connect on:']","A typical path for data scientists is to go from exploratory data analysis to machine learning, to deep learning and then to reinforcement learning, just like the structure of machine learning courses like the famous Andrew Ng ‚ÄòMachine Learning‚Äô course. Going from machine learning to deep learning is often more intuitive, however, when proceeding to reinforcement learning problems it could be more confusing. Concepts such as Markov Decision Process, Reward Functions, Bellman Equation seems to be in a different universe. Going through this learning process myself, I found that going through an example and practical implementation of reinforcement learning model can help a lot in clearing up the concepts. Playing with models to tune the parameters, enjoy the visualizations, could all help you to boost your understanding of reinforcement learning. In this post I will walk you through a clear and simple introduction to reinforcement learning and Q-learning, and then share an example of using the technique of Q-learning to solve an reinforcement learning problem ‚Äî ‚Äúthe taxi problem ‚Äù in python. You could download the code and follow through, I hope this will give you some basic ideas of what is reinforcement learning and how to implement it in simple problem settings. In the simplest term, Reinforcement Learning differs from a typical ‚Äúinput x, output y‚Äù supervised learning problems as it involves an agent interacting with its surrounding environment to determine what is the best action to take. The environment could be uncertain, complex, and the agent‚Äôs behavior can also be probabilistic, not deterministic. This make the problem seems extremely complex, so we need to first define the following settings of typical Reinforcement Learning problems: This defines all possible states that the agent could be in, note that when we‚Äôre defining states we have to be extremely careful so that the definition contains all information we need to know to determine the next state. For instance, if we‚Äôre solving a car driving problem we might need to know the (x,y) coordination of the position, the x-direction velocity, y-direction velocity, or even the x and y direction acceleration. The state space could be discrete (finite) or continuous (infinite). 2. Actions Space This is often more straightforward than defining the state space. The action space is all possible actions the agent could take. For example, a simple robot walking in a grid have an action space of walking Left, Right, Forward, Backward. Thus, typically, the action space is finite and have much less elements comparing to the state space. 3. Transition Probability The transition Probability is the probability of getting to the next state ‚Äî ( St+1) given that the agent is at current state (St) and executed an action (At). This is the component indicating that the agent‚Äôs behavior is probabilistic, there‚Äôs always a possibility that the robot doesn‚Äôt act as you‚Äôve instructed it to do! This concept is used in Markov Decision Process but is a necessary component in Q-Learning, so we will not elaborate too much on it. 4. Reward The reward is a a function of state and action R(s,a) , which we can think of as an incentive for the agent to reach its desired goal quickly and efficiently. We often place a positive reward value for the state that we want the agent to get to and place a small penalty (negative reward) for each extra step taken by the agent. In many practical situations the reward is not well defined, so we need to first run a large sample of random actions executed by the agent and gather information to estimate the rewards. 5. Policy The policy maps a state into an action, so it takes the current state as an input and output an action for the agent to take, in MDP problems we want to find the optimal policy, however in Q learning problems it is also not critical. In a RL problem, the agent would want to maximize its total rewards. It constantly interact with the environment and explore to gather information about what reward it gets from executing an action as specific states, then search for the optimal action to take at each state. Q Learning is a type of Value-based learning algorithms. The agent‚Äôs objective is to optimize a ‚ÄúValue function‚Äù suited to the problem it faces. We have previously defined a reward function R(s,a), in Q learning we have a value function which is similar to the reward function, but it assess a particular action in a particular state for a given policy. It takes into account of all future rewards in resulting from taking that particular action, not just a current reward. In a Q learning process we have a Q-table that stores the Q value for each state and each possible action, the agent explores the environment and make update to the Q values iteratively. step 1: Initialization Initialize all Q values in the Q- table to 0, the agent has no knowledge about the environment it is in. Step 2: Explore the space The agent keep exploring the environment by executing actions at the states it is in. Here we have a problem of exploration v.s. exploitation: although if we define the action to be such that the agent keeps executing the action that returns the highest value function, the problem is guaranteed to converge to a global optimum, but the process could be slow and painful. What we would like in settings with large state action space is for the agent to occasionally choose its action in random, and there is a chance that it will find the optimal value faster. You can think about a scenario where Amazon keeps recommending products that are similar to the ones you‚Äôve purchased, but occasionally it might show you something at random, and there‚Äôs always a chance that the product actually matches with the customer‚Äôs preference. This gives the recommender system new information about the customer that would otherwise take longer time to become clear. The way we could implement exploration strategies is t use the epsilon greedy strategy, where there is a probability of epsilon that we take a random action rather than the action that maximizes the value function, this will become clearer when we get to the code section. Step 3: observe the reward When exploring, the agent would observe what reward it gets from executing a particular action (at) in state (st) to go to next state (st+1). Step 4: Update the value function After observing the reward, the agent then update the value function for the particular state and action pair using the following formula, this returns a updated Q-table. We have the learning rate and discount rate as two hyperparameters. You can think of the discount rate as a tuning parameter to incentivize the agent to achieve the objective faster, as it ‚Äúdiscounts‚Äù the future value in state (st+1). The learning rate controls how much weighting we give to the current value comparing to the new value. The taxi Problem is build by contributors of the OpenAI Gym ‚Äî an open- source library that you could install in python to access different sets of environments for you to explore, develop, test and practice with reinforcement learning algorithms. I highly recommend you to install it and follow along, after going through my example you could also explore other standard environments, such as ‚ÄúGymFC‚Äù: A flight control tuning and training framework‚Äù, ‚ÄúGymGo‚Äù: The Board Game Go, they will all be great resources for you to practice and ‚Äúreinforce‚Äù your understanding of RL. github.com First we import the open source library ‚Äî gym and start working with the taxi environment. Here we used version 3 but new versions keeps getting released, so if it doesn‚Äôt work then try v2 or v4. We can use streets.render() to output a visualization of our environment, with our cute little taxi. We will set a initial state for our problem, the agent start at row index 2 and column index 3, we need to pick up customer from Yellow spot and drop him off at Red spot. A initialization of the q table is generated as a 2D Numpy array that represents every possible state and action pair in our virtual space. The parameters are specified , 10000 epochs means we would repeatedly let the taxi explore for 10000 times. The epsilon greedy strategy comes into play when we generate a random value from the uniform distribution between 0 and 1 and compare it to our epsilon ( exploration rate ), if the random value s smaller we take a random action from our action space and if not we look at our current Q-table and take the action that maximize the value function. We then perform the action, observe the reward and update our Q value and update our state(st) to be the ‚Äúnext state‚Äù(st+1). Now let‚Äôs look at the result! If we‚Äôre at the initial state, the highest value is -2.3639511, which corresponds to taking a step to the left, and if we look back to our original visualization, we can see that if the taxi wants to pick up the passenger from Yellow spot, moving to the left is obviously the best choice! You can play with this and check for different states, it is a great sanity check to see if your model converged. Using the Q-table we have computed we could instruct the taxi driver to do his job now. This piece of code randomly initialize a state and ask the taxi driver to perform the task according to the Q-table generated. We‚Äôve restricted the number of steps the taxi could take to a maximum of 25 steps in case we have a taxi driver that has really bad luck and we repeat this for 10 times, calculate the average number of steps it takes for the taxi driver to complete the task to evaluate our model. We can see that the average number of step is 13.9, not bad! In order to improve upon our existing model, we need to tune our hyperparameters. This step is extremely important if we want our model to achieve better result, in this case, we want the taxi driver to be able to carry out the task with less step. So first I‚Äôve re-defined the Q- learning process and calculation of average trip length functions to test different learning rate, discount factor and exploration rates (epsilon). One thing to note is that because we the initial states are randomly generated when we test for average number of steps, each time the result could be different, so we take the average of 10 trails for each average of 10 trails! Sounds confusing, but if you look closely at the code above it would become clearer. The best discount factor turns out to be 0.9, so we fix 0.9 and try different learning rates, and do the same for exploration rates. Finally, combining the optimal outcomes for each parameter, we‚Äôve achieved a much better result, the average number of steps reduced from 13.9 to 11.8! You could download this code from Jovian and play with it, have fun! jovian.ai visit www.enlightmentblog.com for more insightful, quality articles Enlightment is a blogging and networking platform that is built around the theme of share, inspire and connect. It aims to facilitate the sharing of ideas among ambitious university students around the globe. The platform continues to deliver high quality contents created by aspiring students could be distributed to inspire others, helping university students to develop an analytical and innovative mindset. We also aspires to build communities, bridging students with similar interest, aspirations, or from the same university. Join us today and enjoy ad free, subscription free, seamless reading experience. Facebook:https://www.facebook.com/EnlightmentblogInstagram: @enlightmentblog2021Linkedin : https://www.linkedin.com/company/enlightment-blog",214,0,10,Towards Data Science,2021-01-05,2021
https://towardsdatascience.com/retention-and-daily-active-users-explained-79c5dab1bf84,Retention and Daily Active Users Explained.,Complete Data Studio guide and BigQuery tutorial¬†for‚Ä¶,2,88,"['Retention and Daily Active Users Explained.', 'Calendar Charts']","Have you ever wondered how to reduce user churn and save money spent on user acquisition? This article is about how to count those users who stay in your App in order to understand what makes them stay. If you like the template you can download it from here. It‚Äôs free. All datasets are included in this template. Read how to copy it in the end of this article. I will be using BigQuery (BQ) and Data Studio (DS) to work with data. All data we need will be included in SQL below. So it‚Äôs a really small dataset and it won‚Äôt cost anything to run it. SQL can be easily adapted to any other datawarehouse but it‚Äôs just easier to run it in BQ and visualize in DS. So don‚Äôt hesitate to try it. Let‚Äôs take a look at some user activity example (just copy paste it and run it in BQ): This will give us a mocked up user activity table: If you use Firebase or Google Analytics you can extract the data and upload it in BigQuery for further analysis. I wrote about it in one of my previous articles here: towardsdatascience.com Now when we have the data we need a view to see if a user was active during each day after registration and then count those who were active. To count active users per day in BigQuery you can use this SQL below. A bit ugly but something like this should work: For many marketers the metric that represents the most interest would be 30 day retention because if a user was active on day 30 that means it has been using an App for 30 days. If a user was active at least once during the last 30 days it makes that user a monthly active one (MAU). However, it‚Äôs not necessarily true that this user is an active one. We need to distinguish between measurable user activity (e.g. K-means clustering of your user base) and simply being active as MAU. For example, if a user opened your App and then uninstalled it ‚Äî is it an active user? Yes, it is. In this article I will be talking about counting user logons and producing DAU/MAU metrics. New active users are the users that, in a specified period, interact with your app for the first time. (eg. created an account) and then start using it daily, weekly or monthly. These users are often heavily influenced by the user onboarding process they experience. Returning active users are the users that keep coming back to your app after the period of inactivity and are closely related to the retention and recurring revenue metrics of your business. We don‚Äôt really need to use aggregation in SQL and materialize it because our dataset is relatively small and Data Studio can do aggregation for us. Datasource: You can see that I created a new field called user_pseudo_id_percent . We will use it to show the percentage of retained users later. Now let‚Äôs add a new column week_ which we will use to calculate retention per week. Try running this bit of SQL and see what happens: For example, in Firebase we have a similar thing called Weekly retention cohorts. Let‚Äôs add this new field to our datasource: To accurately report on retention, your analytics tool needs to be able to recognize the same user even if they log in six months later using a different browser or device. This might become a real challenge because ‚Ä¶ in real life it becomes way more complicated when users have different accounts and multiple devices. Let‚Äôs assume every client Application will log in several times a day when running. Each logon event can be recorded to your Data lake/Data warehouse along with the user id and the device id. This is pretty much how Firebase logs users. You will have user_pseudo_id by default and user_id can be enabled like so: firebase.google.com There is a lot of things you can do to listen in on those events and even pre-process them to just the data you need before sending them to the Data warehouse. Things like Kinesis or Kafka can easily stream event data from your microservices. ‚Äî So what is an active user? ‚Äî It depends on the business logic you want to apply to your datasets and reports. Active users are often reported as Monthly active users (MAU), Weekly (WAU), and Daily (DAU). Let‚Äôs illustrate active users a little bit more with a visual example: In the illustration above, you can see user logons each day after registration. Each coloured circle implies a user being active that day. User 'J' (‚ÄòJ‚Äô is for Johnas) was active during each week 1‚Äì6. However, it still counts as 1 MAU (Monthly active user). So we can see 2 Active Users for that month, because all of the users were active at least once during that month. You probably noticed that user id ‚ÄòE01‚Äô (‚ÄòE‚Äô is for Emma) has 2 devices (user_pseudo_id 2001 and 2000). We calculated users and that‚Äôs why it is 2 (not 3) MAU. So it totally depends on methodology as you can see. In this case active user numbers may easily double or treble. Just imagine Johnas creating multiple accounts from his one device. We will take a better look into how to tackle this issue later. If we narrow it down to weeks (WAU) we can see 2 active users during week 1‚Äì3 when during the week 4 there is only 1. I saw people often questioning the user activity numbers when analyzed it on different time frames (monthly, weekly, daily) and thinking there is something wrong with it when it is not. If a user visits your app every day of the month, that‚Äôs a single user. Not 30 different users. If you like the template above you can simply copy it from here. Dataset: We might want to answer the following questions: How many of all new users were active on day 1,2,‚Ä¶,n? This is Daily retention cohorts. It‚Äôs day number after user registration (not date). Some accounts may have been registered on different dates. How many of all new users were active on week 1,2,‚Ä¶,n? This is Weekly retention cohorts (week numbers after the registration date). How many of all new users were active on month 1,2,‚Ä¶,n? Monthly retention cohorts (month numbers after the registration date). Let‚Äôs create a dataset for our retention cohorts. To answer the first question How many of all new users were active on day 1,2,‚Ä¶,n? I will use Data Studio with the dataset above and default aggregation set to MAX. I will use day_number, week_diff and month_diff as dimensions, rows and columns in my pivot table. I will use user_pseudo_id (device_id) to count unique users as this is pretty much the standard in Firebase for example. Result: How many of all new users were active on week 1,2,‚Ä¶,n? Remember that Firebase retention view that looks familiar? It simply goes 5 weeks backwards and adds first_seen_week to rows: We can achieve this in Data Studio by adding first_seen to our dataset: Alternatively we can add first_seen_week: EXTRACT(WEEK FROM first_seen) AS first_seen_week I‚Äôve added another device_id which will be first seen on November the 1st and the new dataset will be (remember now we need to use first_seen in partitions for row groupings): How about a better visualization for retention? What if we could display our retention rates in calendar style? We usually use pivot tables to visualize data over retention cohorts, but it can often be insightful to see that same data overlaid on a calendar. In the next example I will make an assumption that each 30 day retention period equals one month. So it‚Äôs not a calendar month really but nothing stops you from displaying retention rates overlaid on calendar months instead of retention cohorts. Just remember it will have a completely different meaning. Indeed, in many cases you would probably want to display retention simplified like so: It makes sense to use activity date instead of daily, weekly or monthly cohorts as it is much more intuitive and easier to comprehend. Remember 30 day retention and why it is so important for marketers? This brings me to the end of this post. Using this nifty template you can easily mock up user activity data and unit test your retention reports. We have just used 5 different ways to display user retention in BigQuery and Data Studio and there is so much more you could do with Firebase, Google Analytics data and this little report. With just a few simple tweaks in Data Studio this template can be turned into a beautiful real-time analytics dashboard and used to understand you user behaviour better. Retention is a very important metric because being able to see why, when and where users leave might help you to understand the reasons why they do that. Retention analysis is a very powerful tool and aims to explain user churn. Using this knowledge marketers can increase retention rates by fixing application bugs, improving App features and building more successful marketing funnels. It is a well known fact that it costs more to acquire new users than it is to retain existing ones. Because churned users can translate into a direct loss of revenue, predicting possible users who can churn beforehand can help the company save this loss. And that is why different views on Retention, ways to interpret retention numbers and explanation of reasons why users churn become increasingly important in building Machine learning models (e.g. Chur Prediction). Thanks for reading! 2. Click Copy report with datasets: 3. Some widgets will appear broken but click one and then click Edit 4. Select your billing project and click Reconnect: Success! There is a lot more templates and tutorials on datastudioguides.com datastudioguides.com towardsdatascience.com firebase.google.com support.google.com stackoverflow.com cloud.google.com firebase.google.com",386,2,11,Towards Data Science,2021-01-05,2021
https://towardsdatascience.com/clinical-natural-language-processing-5c7b3d17e137,Clinical Natural Language Processing,Transfer Learning and Weak Supervision,1,14,['Clinical Natural Language Processing'],"Every day across the country, doctors are seeing patients and carefully documenting their conditions, social determinants of health, medical histories and more into electronic health records (EHRs). These documentation-heavy workflows produce rich data stores with the potential to radically improve patient care. The bulk of this data is not in discrete fields, but rather free text clinical notes. Traditional healthcare analytics depends predominantly on discrete data fields and occasionally regular expressions for free text data, missing a wealth of clinical data. Syndromic information about COVID-19 (i.e., fever, cough, shortness of breath) was valuable early in the pandemic to track spread before widespread testing was established. It continues to be valuable to better understand the progression of the disease and identify patients likely to experience worse outcomes. Syndromic data is not captured robustly in discrete data fields. Clinical progress notes, especially in the outpatient setting, provide early evidence of COVID-19 infections, enabling forecasting of upcoming hospital surges. In this article, we‚Äôll examine how NLP enables these insights through transfer learning and weak supervision. Natural language processing (NLP) can extract coded data from clinical text, making previously-‚Äúdark data‚Äù available for analytics and modelling. With the recent algorithm improvements and simplified tooling, NLP is more powerful and accessible than ever before, however, it‚Äôs not without some logistical hurdles. Useful NLP engines require a great deal of labelled data to ‚Äúlearn‚Äù a data domain well. The specialized nature of clinical text precludes crowd source labelling, it requires expertise and the clinicians with that expertise are in high demand for much more pressing affairs ‚Äî especially during a pandemic. So how can health systems make use of their troves of free text data while respecting clinician time? A very practical approach is with transfer learning and weak supervision. Modern NLP models no longer need to be trained from scratch. Many state-of-the-art language models are already pretrained on clinical text datasets. For COVID-19 Syndromic data, we started with Bio_Discharge_Summary_BERT available in a pytorch framework called huggingface. As described in the ClinicalBERT paper, the model is trained on MIMIC III dataset of discharge summaries. We used the transformer word embeddings from Bio_Discharge_Summary_BERT as a transfer learning base and fine-tuned a sequence tagging layer to classify entities as with our specific symptom labels. For example, we were interested in ‚ÄúShortness of Breath‚Äù, clinically there are a lot of symptoms that can be classified under this umbrella (e.g., ‚Äúdyspnea‚Äù, ‚Äúwinded‚Äù, ‚Äútachypneic‚Äù). Our classification problem was limited to approximately 20 symptom labels, yielding higher performance results than a generalized Clinical NER problem. To train this sequence tagging layer, however, we came back to the data problem. Both MIMIC III and our internal clinical text datasets were unlabeled. The few publicly available, labelled clinical text datasets (e.g., N2C2 2010) were labeled with a different use case in mind. How could we get enough data labeled for our targeted use case that is sampled responsibly to prevent bias in the model? Our strategy had 3 steps: selective sampling for annotation, weak supervision, and responsible AI fairness techniques We used selective sampling to leverage our clinicians‚Äô time more efficiently. For Covid-19 symptoms, that meant only serving up notes to annotators that were likely to have symptom information in them. A prenatal appointment note or a behavioral health note are very unlikely to be discussing fever, cough, runny nose, or shortness of breath. Strategically limiting the note pool we sent to annotators increased the labels per annotation hour spent by our clinicians. For annotation we provided our clinicians with a tool called prodigy. The user interface was easy for them to use and it is flexible for different annotation strategies. One of the main decision points when setting up an annotation strategy is determining what granularity you want your annotators to label at. Choosing too high of a granularity like ‚Äúsymptom‚Äù would not give us the data we need for our use case but getting too specific like ‚Äúunproductive cough‚Äù versus ‚Äúproductive cough‚Äù would be a heavy burden for annotators with no additional benefit for us. For any annotation strategy, it is important to balance burden on annotators with reusability of the labelled dataset. The less we have to go back to the well the better, but if it takes a clinician 2 hours to annotate a single clinical note, we have not succeeded either. For our project, the first pass of annotation was for NER only. We did a later pass for sentiment of the NER (ie. Present, Absent, Hypothetical). Prodigy allows for targeted strategies using custom recipe scripts. After gathering the Prodigy annotations from our clinicians, we created rules-based labelling patterns to use in SpaCy for weak supervision. Prodigy and SpaCy are made by the same development group, making integration straightforward. Weak supervision is another annotation strategy, however, instead of ‚Äúgold standard‚Äù annotation from clinical subject matter experts, it uses an algorithm to annotate a much larger volume of text. Ideally, the decreased accuracy from using an algorithm is offset by the large number of documents that can be processed. Using an algorithm based on the labelling patterns below we were able to generate a very large training dataset. Since our selective sampling biased what notes we surfaced to our annotators, we needed to safeguard against bias in our weakly supervised dataset that would ultimately train the model. Machine learning in the clinical domain requires a higher degree of diligence to prevent bias in models. Responsible AI techniques are becoming mandatory in all industries, but as equality and justice are fundamental tenets of biomedical ethics, we took care to develop an unbiased note sampling approach for weak supervision. For each dataset, clinical notes were sampled in equal numbers across race and ethnicity, geographic location, gender, and age. The labelling patterns were then applied to the notes through SpaCy. The result was an annotated dataset in IOB format for 100,000 clinical notes. At this point we were ready to train our sequence tagging layer. We used a framework called Flair to create a corpus from our IOB labeled dataset. The corpus was then split into dev, train, and validation sets and Flair took it from there. The results were very promising. Given that we trained a transformer language model on a weakly-supervised, rules-based dataset, one might reasonably ask, ‚Äúwhy not just use the rules-based approach in production?‚Äù However, given that transformer language models (like BERT) use sub-word tokens and context-specific vectors, our trained model can identify symptoms not specified in the rules-based patterns file and can also correctly identify misspelled versions of our entities of interest (e.g., it correctly identifies ‚Äúcuogh‚Äù as a [COUGH]). With the rich data available in clinical free text notes and the logistical challenges of clinical note annotation, a very practical approach to healthcare NLP is with transfer learning and weak supervision.",181,1,7,Towards Data Science,2021-01-05,2021
https://towardsdatascience.com/functional-time-series-83b717cca12,Functional Time¬†Series,"When we measure data more frequently, how¬†can‚Ä¶",6,53,"['Functional Time Series', 'Introduction', 'Functional Time Series ‚Äî Basics', 'Tests on Stationarity and Independence', 'Implementation in Python', 'Conclusion']","As memory space grows, storing data becomes cheaper and cheaper, which in turn means that more and more data is stored. In case of time series, this means that data is collected more frequently. It is not clear, however, how to model time series that are recorded with a (very) high frequency, especially when (multiple) seasonality is involved. Shall we consider the data as a univariate time series or as a multivariate time series with high dimension? In many cases it is best to view the observations as functions of time and analyse this functional time series itself. For example, we can divide intraday prices of stocks into daily observations, where we observe for every day the price as function of time of day. (Sounds technical, but we‚Äôll see an example later!) This latter approach is particularly useful when the functional data is continuous because it implies more structure than vectors in higher dimensions (if x is close to y, f(x) is close to f(y) for a continuous function; x(i) does not need to be close to x(i+1) for a vector (x(1), ‚Ä¶, x(d))). Functional data arises naturally in different settings such as medicine (EEG data), finance (stock prices) or meteorology (temperature). In this blog post, we go through examples and try to develop an intuitive understanding of functional time series. Additionally, hypothesis tests for the assumptions of independence and stationarity of functional time series are introduced. Note: Throughout it is assumed that you are familiar with basic concepts of time series analysis. If you want to refresh your knowledge or get started with the topic, check my previous blog post. Let‚Äôs start with an example. Imagine that we measured the temperature in a specific location over time and have collected one observation per day, so for year t and day i we have observation X(t,i), where i ‚àà {1, ‚Ä¶, d}, t ‚àà {1, ‚Ä¶, n} and d = 365 (number of days per year), as in Figure 1. Now we have different options how to approach the data. First, we could regard it as a univariate time series and simply concatenate the data in chronological order, so technically Y(t) = X(j,i) where t = (j-1) d + i. In this case, we have seasonality, which makes the analysis more difficult. We could also model the time series as a multivariate time series with as many dimensions as observations per year, such that every observation of the time series corresponds to the data collected during the entire year: Y(t) = ( X(t,1), ‚Ä¶, X(t,d) ). Now we don‚Äôt have to take seasonality into account, but the dimension is very high (365 dimensions to be precise). Of course we can reduce the dimension by reducing the frequency of the observations. However, in this case, we lose information and it is not clear how to chose the frequency (weekly or monthly?). A final approach is to consider the data as a functional time series Y(t,x), where we have a function Y(.,x) for every year t with Y(t,i/d) = X(t,i). In this case, the yearly temperature is viewed as a function in time and every observation corresponds to a function, which describes the yearly temperature. In Figure 2 is the data from Figure 1 viewed as functional time series. A functional perspective on the data has important benefits. In the example, the mean of the temperature throughout the year is clearly not constant. However, the mean temperature in summer is above the mean temperature in winter. So, a non-stationary univariate time series might be stationary when modelled as functional time series, because we compare data that is reasonable to compare (e.g. comparing temperature in January 2016 with the temperature in January 2017 instead of July 2016). Further, modelling the data as a functional time series is often more natural than using a high-dimensional time series because it adds an additional structure. For example, if we observe continuous functions, such as temperature or stock prices, the values for two close time instants are similar, whereas such a structure is not given for arbitrary multivariate time series. This is similar in spirit to convolutional neural networks. Due to their specific architecture, the use of CNNs is more restricted to a specific set of problems, yet for this set of problems, they work very well. Functional data analysis (FDA) is an active area of research and can be used in various applications. In the following, we focus on a specific type of functional data, namely functional time series. Note however, that many classic results from statistics were generalized to functional data, such as t-tests to compare expected values of different groups. Okay, so I mentioned functional time series, we saw an example, but what is a functional time series mathematically? And how is it different from a univariate time series? Mathematically, there is only a small difference. A univariate real time series is a collection of real data indexed by time (see here). So for time instants 1, 2, ‚Ä¶, n we observe real-valued data Y(1), Y(2), ‚Ä¶, Y(n), such as the temperature at a specific location or the price of a certain share. A functional time series is basically the same, but we observe functions instead of real-valued data. In this case Y(1), Y(2), ‚Ä¶, Y(n) are functions and might be written as functions in x, i.e. Y(1)(x), Y(2)(x), ‚Ä¶, Y(n)(x). For simplicity, we often assume that the functions are defined on the interval [0,1] and rescale the interval if necessary (if f(x) is defined on an interval [0, T], g(x)=f(xT) is defined on the interval [0, 1]. Technical Note: We work in a functional space rather than in the space of the real numbers and it is not clear how quantities such as the expected value or the covariance are defined for functional data. Luckily, the mean and covariance can be defined pointwise in most cases, such that the identities E[Y(i)](x) = E[Y(i)(x)] and Cov(Y(i)(x), Y(j)(z)) = Cov(Y(i),Y(j))(x,z) hold.Depending on the assumptions such as continuity of the observations or L¬≤-integrability, we might have additional structure (the space of continuous functions is a Banach space, the L¬≤-space is a Hilbert space) and in these cases the pointwise definition can be justified. As for univariate time series, the concepts of stationarity and independence drastically simplify any further analysis, so we want to know if they are reasonable assumptions. On a conceptual level, the ideas are the same as in the univariate case: Two functional observations are independent if the probability factorizes and a time series is stationary if its distribution remains constant over time (see this blog post for a rigorous definition). In this blog post, we have seen how to validate the two assumptions for univariate time series. For functional data, we can do the very same thing, namely use the CUSUM statistic to determine whether a time series is (weakly) stationary and a Portmanteau-type test to validate (or reject) the null hypothesis of independence. When working with time series, we want to know if they are stationary, because in this case we do not need to take temporal changes into account. However, stationarity is difficult to measure and we often use the time series‚Äô moments as proxy. Intuitively, if the moments do not change over time, we can neglect temporal changes of the underlying distribution. Thus, instead of testing for stationarity, we want to test whether the mean and (auto-)covariances of the given time series are time-invariant. For a functional time series Y(1)(x), Y(2)(x), ‚Ä¶, Y(n)(x), this translates into the hypotheses for some i ‚àà { 2, ‚Ä¶ , n } and for some i ‚àà { 2, ‚Ä¶ , n-h}. The time series Y(i) is weakly stationary if the null hypotheses are valid for any positive integer h. Note that the first- and second-order moments of Y(i) are functions themselves. So equality of two functions depends on the function space. In the space of continuous functions, for example, two functions f and g are equal if they are equal in any point x, so if f(x)=g(x). Contrarily, two functions are equal in the space of square-integrable functions L¬≤ if they coincide in almost every point (w.r.t. the Lebesgue measure). Instead of testing the latter hypotheses for all lags h, we often restrict our attention to the first H lags (so for all h with 1 ‚â§ h ‚â§ H), as those fundamentally determine the behavior of the distribution. In the following, we will only test for H‚ÇÄ, as we can test the null hypotheses concerning the second-order moments analogously. Further, we assume the data to be square-integrable, so it belongs to the space L¬≤([0,1]) with norm denoted by ||.||. In this case, the testing problem in (1) is equivalent to As in the univariate scenario, we can employ the CUSUM statistic, which basically compares the average of the first with the average of the remaining observations. The (functional) CUSUM statistic is defined as Under the null hypothesis (and weak assumptions), ‚àön C(u, x) converges weakly to a centered Gaussian process B(u, x) with unknown covariance function in the space L¬≤([0,1]¬≤) with norm ||.||‚ÇÇ. Contrarily, ‚àön C(u, x) deviates to +‚àû or -‚àû under the alternative. So if ‚àön C(u, x) deviates too much from its limit B(u, x), we can reject H‚ÇÄ. Unfortunately, we don‚Äôt know the distribution of B(u, x) as we don‚Äôt know the covariance and need to estimate it. There are different ways how to do so and one common approach in time series analysis is to use a block multiplier bootstrap approximation, which is basically a resampling scheme that takes temporal dependence into account. If q(Œ±) denotes the Œ± quantile of ||B||‚ÇÇ (obtained, for example, through a bootstrap procedure or direct estimation of the covariance), we can reject H‚ÇÄ whenever ‚àön ||C||‚ÇÇ > q(1-Œ±). This defines an asymptotic consistent level Œ±-test for H‚ÇÄ. Similarly to stationarity, stochastic independence is difficult to measure and we use the time series‚Äô (auto-)covariance structure as a proxy to assess its degree of dependence. For simplicity, we assume that the time series is stationary and centered, that is E[Y(i)]=0 (this hypothesis can be tested analogously to the presented procedure). Again, we are mainly interested in autocovariances with small lags h. In line with the classic Portmanteau test, we consider the hypotheses As before, the second order moments of a functional time series are functions themselves, so we formulate the hypotheses in terms of their norms. In order to avoid the multiple testing problem, we compare all moments simultaneously instead of testing them individually by considering their maximum. As test statistic, we can use (the maximum of) the empiric moments The estimator M‚Çô(h) converges in probability to the E[Y(1) Y(1+h)], the autocovariance at lag h. Thus, we can reject the null hypothesis, if M‚Çô(h) deviates significantly from its limit. Under the null hypothesis, it holds that ‚àön ||M‚Çô(h)||‚ÇÇ converges weakly to ||B(h)||‚ÇÇ for some centered Gaussian variable B(h), and it diverges to infinity under the alternative. Again, the covariance structure of B(h) is unknown and its distribution can be approximated in terms of a bootstrap procedure as in the case of stationarity testing. If q(Œ±) denotes the (approximated) Œ± quantile of max {||B(h)||‚ÇÇ: 1 ‚â§ h ‚â§ H}, we can reject H‚ÇÄ whenever which defines an asymptotic consistent level Œ±-test for H‚ÇÄ. For the implementation, we use climate data from Australia. More specifically, the daily minimum temperature in Sydney (station number 066062) from 1859 to 2017 provided by the Bureau of Meteorology of the Australian Government. First, we need to load the needed packages and prepare the data: In order to test for stationarity of the mean, we define three auxiliary functions to calculate the cusum statistic, the L¬≤-norm and bootstrap replicates to approximate the quantile. The output suggests that we can reject the null hypothesis of a constant mean function. Thus, it is unlikely that the temperature was stationary in Sydney from 1859 to 2017, which suggests a change in climate. For the Portmanteau-type test as introduced earlier, we define two auxiliary functions. The first function calculates products of the (functional) observations, which are used later to calculate empirical moments more efficiently. The second function generates bootstrap replicates to approximate the quantile. Note that the calculations can take a moment because the quantile approximation is computationally expensive. Note further that we assumed for simplicity that the time series is centered and stationary. Both assumptions are clearly not met in the given example. By subtracting an estimated (local) mean, we can generalize the methodology to non-centered time series, but it will be non-stationary as suggested by the previous test. We can use the data anyway to illustrate the methodology. The result suggests that the null hypothesis of uncorrelatedness can be rejected. As mentioned before, this result is not interpretable as it is neither reasonble to assume centeredness nor stationarity of the time series. Functional data analysis is more technical than the analysis of univariate data, but has some important advantages and can be used in many applications. On a conceptual level, both approaches are similar and we can use (almost) the same ideas and techniques. Beyond functional time series, FDA has many other important applications ‚Äî it can even be used for dimensionality reduction which seems counterintuitive at first ‚Äî and is a topic that we should be aware of as data scientists. If you are interested in an application of time series analysis to monitor machine learning models, this post might be for you: towardsdatascience.com If you are interested in univariate time series, you can check out these two introductory posts: towardsdatascience.com towardsdatascience.com",,0,11,Towards Data Science,2021-01-05,2021
https://towardsdatascience.com/whataphish-detecting-phishing-websites-e5e1f14ef1a9,WhatAPhish: Detecting Phishing¬†Websites,An ML based approach to detect phishing¬†websites,9,34,"['WhatAPhish: Detecting Phishing Websites', 'Motivation', 'State of the Art', 'Methodology', 'Results', 'Discussion', 'Scrapping the Features for a given URL', 'Conclusion', 'References']","Website Phishing costs internet users billions of dollars per year. Phishers steal personal information and financial account details such as usernames and passwords, leaving users vulnerable in the online space. What is a hacker‚Äôs weekend getaway? They go phishing! The COVID-19 pandemic has boosted the use of technology in every sector, resulting in shifting of activities like organising official meetings, attending classes, shopping, payments, etc. from physical to online space. This means more opportunities for phishers to carry out attacks impacting the victim financially, psychologically & professionally. In 2013, 450 thousand phishing attacks led to financial losses of more than 5.9 billion dollars¬π. As per CheckPoint Research Security Report 2018, 77% of IT professionals feel their security teams are unprepared for today‚Äôs cybersecurity challenge. The same report indicates that 64% of organizations have experienced a phishing attack in the past year. Detecting phishing websites is not easy because of the use of URL obfuscation to shorten the URL, link redirections and manipulating link in such a way that it looks trustable and the list goes on. This necessitated the need to switch from traditional programming methods to machine learning approach. Traditionally, the ad-hoc methods have been used to detect phishing attacks based on content, URL of the webpage, etc. There are primarily three modes of phishing detection¬≤: M E Pratiwi et al used neural network perceptron on data provided by UCI Machine Learning and were able to achieve an accuracy of 83.38% by using 18 features‚Åµ . A. Alswailem et al used random features extracted from URL and page content out of 36 available features and achieved a minimum accuracy of 53% and maximum 98% by Random Forest Classifier¬≤. We used the dataset provided by UCI Machine Learning repository‚Å¥ collated by Mohammad et al¬≥. The dataset has 11055 datapoints with 6157 legitimate URLs and 4898 phishing URLs. Each datapoint had 30 features subdivided into following three categories: Studying the way of extraction and relevance of features, we dropped 5 features out of 30, namely: Port Number, Abnormal URL, Pop-up Window, Google Index and Number of Links Pointing to a Page. Port Number was dropped due to feature drift. Rest were dropped due to unavailability of methods to extract them programmatically or absence of public APIs. To see separability of the two classes, we plotted the t-SNE curve. The curve implied that though the classes are separable, they are not clustered together, and either transformation of the features or non-linear model is required to obtain good results. We splitted the available data into training and testing data using 80:20 split. Post that, since we had only 7075 data points in the training data, we trained it using 5 fold cross validation. Hence, we achieved a train:val:test split of 64:16:20. We one-hot encoded the features to avoid any biases due to numerical values. We tried out various classification models with hyperparameter tuning: For testing the results obtained, we used 3 parameters: Accuracy, Recall and False Positive Rate (FPR). The models trained and their performances on the validation data are as follows: We aim to increase accuracy, increase recall and decrease false positive rate so that most of the points are classified correctly and the number of phishing websites labelled as legitimate is reduced. Random Forest Classifier and Support Vector Machine perform similar on the given dataset and have higher accuracy and lower FPR compared to other models. Apart from Logistic Regression and Categorical Naive Bayes, the performance of other models is also comparable. Logistic Regression assumes linearly separable classes. However, as can be seen from the t-SNE plot, classes are separable, but not in a linear fashion. Hence logistic classifier fails to perform well. Naive Bayes assumes independence of features which may not be entirely true.In this case features like pagerank and statistical report are closely associated, similarly, Request URL and URL of Anchor also seem to be tightly coupled. This coupling occurs because most phishing websites tend to follow similar patterns in their static features. Thus Naive Bayes could not capture the patterns well. Decision tree is able to classify non-linearly separable patterns, and hence performs well on the given data after pruning, but as Random Forest is an ensemble model using Decision Trees which combines the output of various Decision Trees in a random fashion, it provides an improvement over decision trees as reflected in the results. K-nearest neighbours works on similarity of features. A lot of the phishing websites have some common features such as less than six months of activity, less than one year of domain registration length, etc. Hence KNN can classify well based on these features. However, with ‚Äòeuclidean‚Äô distance, non-convex patterns may not be classified well, thus impacting performance slightly. Support Vector Machine works well for linearly separable data. The data is not linearly separable directly, but after applying ‚Äòrbf‚Äô kernel, the data becomes separable and SVM is able to learn well from the data. XGBoost is an ensemble model based on decision tree which uses gradient boosting to reduce errors. Since it uses techniques similar to gradient descent, it is not very well suited for categorical data and even after one-hot encoding does not perform well. So, we decided to use SVM as the final model. It was performing well with high accuracy, high recall and low FPR. The problem at hand is binary classification and SVM is a comparatively simpler model with tolerance to outliers. Additionally it offers explainability for the involved features. The metrics obtained for SVM over the complete training and testing data are indicated in table below: We deployed the trained model as a webapp to classify any input URL. To scrape the features for the input URL, following conditions and tools were used based on the rules described along with the dataset: drive.google.com In this project, we built WhatAPhish: a mechanism to detect phishing websites. Our methodology uses not just traditional URL based or content based rules but rather employs the machine learning technique to identify not so obvious patterns and relations in the data. We have used features from various domain spanning from URL to HTML tags of the webpage, from embedded URLs to favicon, and databases like WHOIS, Alexa, Pagerank, etc. to check the traffic and status of the website. We were able to obtain an accuracy of more than 96%, recall greater than 96% with a False Positive Rate of less than 5%, thus classifying most websites correctly and proving the effectiveness of the machine learning based technique to attack the problem of phishing websites. We provided the output as a user-friendly web platform whch can futher be extended to a browser extension to provide safe and healthy online space to the users. The complete code and reports can be found on https://github.com/himanshi18037/WhatAPhish. [1] Ankit Jain and B B Gupta. Phishing detection: Analysis of visual similarity based approaches. Security and Communication Networks, 2017:1‚Äì20, 01 2017. [2] A. Alswailem, B. Alabdullah, N. Alrumayh, and A. Alsedrani. Detecting phishing websites using machine learning. In 2019 2nd International Conference on Computer Applications Information Security (ICCAIS), pages 1‚Äì6, 2019. [3] R. M. Mohammad, F. Thabtah, and L. McCluskey. An assessment of features related to phishing websites using an automated technique. In 2012 International Conference for Internet Technology and Secured Transactions, pages 492‚Äì497, 2012. [4] R. M. Mohammad, F. Thabtah, and L. McCluskey. UCI machine learning repository, 2012 (https://archive.ics.uci.edu/ml/datasets/Phishing+Websites#) [5] M E Pratiwi, T A Lorosae, and F W Wibowo. Phishing site detection analysis using artificial neural network. Journal of Physics: Conference Series, 1140:012048, dec 2018. Authors: Vibhu Agrawal, Himanshi Mathur, Vanshika Goel (Indraprastha Institute of Information Technology, Delhi) Course Project for CSE343: Machine Learning, Indraprastha Institute of Information Technology, Delhi. Course Instructor: Dr Jainendra Shukla.",102,0,8,Towards Data Science,2021-01-05,2021
https://towardsdatascience.com/displaying-geographic-information-using-custom-map-tiles-c0e3344909a4,Displaying Geographic Information Using Custom Map¬†Tiles,,10,49,"['Displaying Geographic Information Using Custom Map Tiles', 'Tile Coordinates', 'Quadkeys Revisited', 'Map Overlays', 'Tile Generation', 'Tile Data Collection', 'Tile Generation and Serving', 'Using the Code', 'Conclusion', 'Resources']","Interactive maps are now a staple of our everyday digital life. We use them to learn of our whereabouts, plan the next trip, or review our past travels. In a professional setting, maps became priceless tools for all manner of businesses in planning, operations, and analytics. An interactive map displays a patchwork of square tiles, each containing a small part of the complete image. A cloud-based service provides these tiles, either by retrieving them from a cache or generating them on-the-fly. The map software manages the display as if showing a continuous bitmap but operates on a tile-basis in the background. Whenever needed, the map software requests new tiles from the service and discards them when they are no longer required. This situation occurs when the user changes the zoom level or pans the map in any direction. When zooming in or out, the map software stretches or compresses the current display while retrieving the new level‚Äôs tiles. It then overlays the new tiles on top of the old ones, providing a pleasant visual continuity. There is nothing better than a live map example to help build your intuition of what is going on behind the scenes. Please follow the previous link and select the ‚ÄúShow tile borders‚Äù option. Browse the map, see how the software juggles the tiles, how it stretches them, and how it replaces and refreshes the content. When hovering each tile with the cursor, the software displays the tile‚Äôs coordinates. We can conceive the whole tileset as a two-dimensional space where each tile receives a unique set of integer coordinates for any zoom level. The first zoom level encompasses the entire mappable surface of the Earth and puts it into a single tile. Each subsequent zoom level splits the previous level‚Äôs tiles into four, thus doubling the detail and the integer coordinates‚Äô range. At zoom level one, both x and y coordinates range between zero and one, while at level two, they vary between zero and three. This process repeats up to the maximum supported zoom level, usually between 18 and 23. We can say that the tuple (x, y, zoom) uniquely identifies a tile. Interactive map software like Leaflet and its Python wrapper Folium use this type of tile addressing when issuing server requests. The server converts these coordinates into the tile‚Äôs geospatial shape, a square in latitude and longitude space, and uses these to query the underlying data. With these data, the server renders the tile‚Äôs graphic content and sends it back to the map software client. We can also address each tile through its corresponding quadkey. A quadkey uniquely encodes a tile into a single string or number, convenient for cache or dictionary keys. Depending on the integer encoding scheme, we can include the zoom level or leave it out as contextual information. Quadkeys are very convenient because we can easily calculate them from tile coordinates and back. We will use them here to encode file names for the local tile cache, as database keys, and as the base for an algebra that will ease plenty of computations. We can encode quadkeys using either strings or sixty-four-bit integers. The string encoding uses one character, either zero, one, two, or three, per zoom level and has the advantage of keeping the zoom level as the string‚Äôs length. Its most significant disadvantage is the storage size required to store each quadkey. Fortunately, it is easy to compact the string encoding into something far more manageable, a sixty-four-bit integer. The conversion is relatively easy to do once we realize that the string representation of a quadkey is nothing more than a base-four number. The most straightforward conversion encodes the string into an integer but loses the zoom level information. We must somehow, implicitly or explicitly, store the zoom level information somewhere else. Alternatively, we can use the whole sixty-four bits to encode both the key and the zoom level information, but this solution limits us to twenty-three levels only. In this article, I use the former encoding as the zoom level is always available from context, and it supports encoding the required twenty-six zoom levels. Using the image above as an example, the string encoding of the bottom-right corner tile, ‚Äú333,‚Äù encodes the integer 63. But there‚Äôs more to quadkey encodings than meets the eye. Using either the string or integer encodings, we can immediately derive the enclosing zoom level's tile key. For the string encoding, we remove the rightmost character, while for the integer encoding, we perform an integer division by 4. This property has an exciting implication when handling 256x256 bitmaps representing tiles ‚Äî each tile pixel is another tile eight zoom levels down, which will work wonders for us. Interactive maps realize their usefulness with overlaid geographic information. Interactive map software usually allows the overlaying of two different types of data: vectors and bitmaps. Here, we focus on a specific kind of bitmap data, tile overlays. A tile overlay works in the same way as the base map tiles do, providing a geographic content tile for each base map tile. Each tile is a square bitmap with the same dimensions as the map tiles and uses alpha compositing to reveal the underlying map information. This way, the overlay creator can draw just the data to display in the correct geographical location, not caring about how the underlying map is displayed, the approach we follow in this article. To illustrate this technique, let‚Äôs pick a tile from the OpenStreetMap server that matches our area of study, the city of Ann Arbor in Michigan, USA. Using the base tile‚Äôs coordinates, we can now pull the corresponding geographic data and generate the overlay tile. In this case, we calculate a bivariate normal distribution for each sampled location and add it (literally) to the map. Areas with higher density will show up in a greener shade. By composing the two images above using alpha compositing, we get the resulting bitmap (see below). Please note that the map software automatically handles this process. Each dot you see in the overlay tile is actually a circle generated using a bivariate normal distribution. The distribution‚Äôs mean point is the location, and we consider a diagonal covariance matrix with a one-pixel standard deviation. Thus, a location expands to the following intensity matrix. The final image is the sum of all locations added together. To avoid the inherent infinite size of a bivariate normal distribution, I decided to cut the representation when each cell‚Äôs value drops below 0.00001. Empty cells reflect such a situation. Tile generation is a three-step process. The first step consists of data collection and transformation into a format suitable for fast query and retrieval. The server draws the tile bitmap using the prepared data and stores it in a file cache for reuse in the second step. This article illustrates a lazy version of the second step where the server software generates and caches the tiles on demand. Should the tile be present in the cache, the server immediately delivers it to the client. During this process, the server code can compare the cached tile generation date with the current date and determine whether it needs refreshing. This process would keep the tile data up-to-date. The final step of the process is the tile delivery to the client. Here, I illustrate this process using a simple Flask-based API. Before we start exploring the tile generation‚Äôs three-step process, we must know the end product. As you have seen above, the goal is to display traffic density information over a set of roads, so I convert each location to a Normal bivariate distribution and add them all to produce a color-coded density map. Our tiles‚Äô source data is a long sequence of geospatial locations encoded as latitude and longitude pairs. For this article, I use the Vehicle Energy Dataset data that I have been exploring for some time. As I previously stated, interactive map software requests the tiles one at a time and pastes them together to create the final map or overlay. The serving software needs to be fast when retrieving the tile‚Äôs data to improve the user experience. Depending on the zoom level, this may be a lot of information to collect, as lower zoom level tiles contain more information. My approach to solving this challenge is to pre-calculate all tiles and store the data for each supported zoom level. Quadkeys are a lifesaver here. As we have seen before, each tile is uniquely addressable by a quadkey code. Each tile consists of a 256x256-pixel bitmap, which means we can address each pixel as another quadkey code eight zoom levels deeper (256 = 2‚Å∏). This insight allows the efficient encoding of tile data for later retrieval during the generation phase. My solution to this problem was to use an SQLite database to put all the tile data with per-pixel aggregation, using one table per zoom level. Each table‚Äôs structure is simple, with just three columns. Below I show the SQL table creation script for the lowest zoom level. Note that by aggregating the geographic data at zoom level 26, we can draw tiles up to level 18 only. The first column contains the sixty-four-bit encoded quadkey code for a single pixel at zoom level 26. As such, we can very quickly convert this value to the pixel coordinates within the tile and draw the pixel according to the calculated intensity value, the table‚Äôs third column. The second column encodes the enclosing tile, also as a sixty-four-bit quadkey code. We create a non-unique index for this column to make the tile data retrieval very fast and calculate it as the pixel quadkey code divided by 256, or 2‚Å∏. When the whole level 26 computation is complete, we can immediately derive level 25 through a simple aggregation, as illustrated by the SQL script below. To calculate all zoom levels, we need to repeat this process to the topmost level, eight in our case. We can now perform the final step of data preparation, namely calculating the intensity range for each zoom level. This information is essential for the coloring process while drawing the tile bitmap, as the value range maps to a predefined color gradient. Now that the data is fully prepared, we can proceed to the tile generation and serving process description. For this article, I devised a straightforward Flask-based API to serve tile files. The API endpoint receives as parameters the tile coordinates and returns the corresponding tile PNG file. Here I am using a generic function to do all the heavy lifting. As parameters, the function accepts the tile coordinates, the path to the SQLite database containing the tile data, and the file cache folder's path. The procedure starts by limiting the zoom level to the accepted range, between one and eighteen. Beyond these limits, it merely returns the default empty (fully transparent) tile. For proper zoom levels, the function computes the target tile filename using the tile quadkey code, and if this file already exists in the cache, serves it immediately. For nonexistent files, the function must render the tile and save it before serving. The process of generating a new tile starts by establishing a connection to the database containing the zoom level data. It then queries the database for all the tile‚Äôs pixel intensities. Should the tile be empty, the function serves the default transparent tile. The code uses lists of tuples of the individual pixel coordinates, as quadkey codes, and their respective intensities to represent tiles with data. These lists must then convert into tile-based pixel coordinates, meaning that each tile's top-left-hand corner has the (0, 0) coordinate. Then, the function collects the intensity range information for the zoom level at hand. With all this information, we can now paint the tile. While researching on creating PNG files from Python code, I came across an elegant package: PyPNG. This package can convert NumPy arrays into PNG files, which seems like a great idea. Here is how you create a NumPy array that represents a 256x256 RGBA image, encodable as PNG: Painting a tile is a simple matter of setting the individual pixel values to the appropriate color. The next function uses the list of pixels, the color gradient, and the suitable zoom range to paint a tile. Each color value from the gradient list is a NumPy vector with four dimensions, one for each channel component, so setting a pixel is a simple assignment. The function that generates the gradient list also sets the alpha channel value to 50%. Saving the tile to a PNG formatted file is straightforward with the PyPNG package. The code below illustrates the process. Note the required array reshape before saving. Finally, the API can serve the tile file by creating a response object around it. To use the code, start by cloning the GitHub repository to your local machine. The first step is to execute the first two numbered Jupyter notebooks to create the supporting database. This will read the data from the distribution dataset and import it into a local SQLite database. Next, you must create and populate the supporting SQLite tile database. You do so by running the following script: python generate_densities.py Please note that this script may take a very long time to run on the VED data. Expect more than one hour of total runtime. Once finished, you can start the tile API using the following script: python tileapi.py This command starts a Flask server listening on port 2310. To see the tile server in action, please run the Jupyter notebook number ten. You will see a map centered in Ann Harbor, Michigan. If all went well, you should start seeing the tiles being rendered over the map. As you pan and zoom, the tile server will generate, cache, and serve the appropriate tiles. In this article, we have explored the concept of interactive map tiles and how to generate them to convey custom geographic information dynamically. Interactive mapping software uses square bitmap tiles to build the whole map. To make these maps useful, we can overlay vector or raster images to convey geographically referenced information. Overlayed map tiles are convenient and fast to display such information but usually require some lengthy preprocessing. This article took you through the paces to deliver a custom solution that you can further adapt. Tile layer example GitHub Repository Jo√£o Paulo Figueira works as a Data Scientist at tb.lx by Daimler Trucks and Buses in Lisbon, Portugal",,0,11,Towards Data Science,2021-01-05,2021
https://towardsdatascience.com/how-to-create-your-own-data-science-curriculum-with-coursera-in-2021-36feda936ead,How to Create Your Own Data Science Curriculum with Coursera in¬†2021,A beginner-friendly approach¬†to‚Ä¶,5,77,"['How to Create Your Own Data Science Curriculum with Coursera in 2021', 'My own path', 'Your own path', 'An example data science and machine learning curriculum', 'After the fundamentals']","When I worked at Apple, we had a system. If you had a device with an issue, you could walk-in, tell us the problem, then we‚Äôd either let you know how to fix it on the spot or take your name down so we could book you 1-on-1 time with a technician. I was a technician so once I‚Äôd finished with one customer, I‚Äôd look at the list of names and go to the next. One day a girl came in for help with her iPhone. I walked over to her and said hello and noticed her scrolling through a blue app. I asked her what the app was. Coursera, she said. What‚Äôs that? I asked. It‚Äôs an app where you can learn different things. Oh really? Are you studying at university? Not anymore, I just think it‚Äôs always good to be learning something. She doesn‚Äôt know but that one line changed my whole life. Up until then, I‚Äôd only learned when I was required: for a job, for school. Of course, there were times when I‚Äôd followed my own genuine curiosity (playing video games, learning to cook) but her words illuminated the fact if I really wanted to, I could learn anything I wanted. Anything. It‚Äôs funny how chance encounters have that effect. That night I went home and searched for the name of the blue app she told me about. And it was like I‚Äôd discovered a new hidden land. You could pick what you wanted to learn and there was a class starting‚Ä¶ tomorrow. What? Up until then, I‚Äôd been studying (and failing) at university. I‚Äôd considered what to learn but having to be at certain places at certain times didn‚Äôt suit me. I started college at 17-years-old. All I wanted to do was sit on the grass and look at girls. Alas, I never actually used Coursera until a couple of years after the girl at the Apple Store told me about it. I left Apple and decided I wanted to learn how to build the programs I was helping troubleshoot. Whilst learning to code for the fourth time (I gave up the first three times), I stumbled upon machine learning. In other words, writing code to use math to find patterns in data. I have to learn this, I thought. And since I decided I‚Äôd spent enough time at university for the previous decade, I put together a list of online courses to create my own AI Masters Degree. And guess what? Many of them were from that blue app the girl showed me. If there‚Äôs anything 2020 has made clear, it‚Äôs that every individual has to be in charge of their own health and education. Now it‚Äôs 2021 (or later if you‚Äôre reading this in future, hello btw) and physical gathering rules are still up in the air, many (perhaps you) are turning to online resources to learn rather than traditional sources. This piece is not an argument for university versus learning online. Do whichever suits you. This piece is a demonstration of how quickly you can piece together your own curriculum for data science and machine learning using Coursera. However, don‚Äôt mistake how quickly you can put together a curriculum as the job being over. Learning anything worthwhile and especially learning online is not for the faint of heart. If you don‚Äôt already have one, you will have to develop a sense of accountability. That‚Äôs what developing your own curriculum does for you. It puts you on the hook. People always ask me, why not use free resources? And I tell them, sure, you can use free resources. But I‚Äôve found they don‚Äôt work as well for me. When I first used Coursera, they charged monthly. I knew what was I paying straight away. Knowing I was paying for something made me take it more seriously. But didn‚Äôt you pay for university? Yes. But I didn‚Äôt see the money come out of my account. I thought it was free, I thought it was magic. I took 5-years to complete a 3-year degree and ended up $35,000 in debt (significant but minor compared to others). Now Coursera have a service called Coursera Plus. ~$537 AUD (~$399 USD) for a whole year of access to world-class learning materials. Ho, ho. If that existed when I started using Coursera, I would‚Äôve saved a bunch. Time to get specific. About a year ago, I wrote a post called 5 Beginner-Friendly Steps to Learn Machine Learning putting together resources across the internet to learn the fundamentals of machine learning. The post focused on getting the reader from knowing nothing about programming to writing machine learning code with the Python programming language as soon as possible. So to kick 2021 off, I thought I‚Äôd replicate that post with a handful of Coursera courses all available through Coursera Plus. But wait‚Ä¶ this is not just a simple replication. I‚Äôve also put together a Notion template you can use to keep track of your progress. If you‚Äôve never used Notion before, explore the template and click a few things (don‚Äôt worry, you can‚Äôt break it) before clicking the ‚Äúduplicate‚Äù button in the top right corner, then you‚Äôll be able to alter it to suit your needs. You can see a video walkthrough of the above Notion template and curriculum below on YouTube. Now, how about a little less interactive-ness with a list of steps. Note: If you‚Äôre reading this and thinking ‚Äúwow, this sounds like an ad for Coursera‚Äù, you‚Äôre right. Coursera reached out to me asking if I‚Äôd be interested in writing a post dedicated to helping students put together their own curriculums with Coursera and I said yes. That being said, take this message as a disclosure, if you click on a link in this post and pay for something, the amount will not change but I will get a portion of the amount you pay (funds I use to create resources like this). What it is: Discover and copy the best learning techniques from artists, mathematicians, musicians and more. Why it‚Äôs important: If you‚Äôre going to be self-driving your own learning journey, you might as well learn how to learn. Learning how to learn is the ultimate meta-skill. Because if you can learn how to learn, you can learn anything. I recommend everyone do this course regardless of whether they‚Äôre learning data science and machine learning or particle physics. You can check out my full review article on my blog. What it is: This Specialization will help you go from zero programming ability to being able to write Python code. Why it‚Äôs important: If you want to get into data science and machine learning, chances are, you‚Äôre going to be writing Python code. But it‚Äôll be hard to write Python machine learning code without ever using the language before. So use this Specialization to familiarise yourself with Python‚Äôs fundamental concepts. What it is: Now you‚Äôve got some foundational Python skills, it‚Äôs time to start tailoring them to be able to work with data. This Specialization will get you familiar with popular Python data science libraries such as pandas, matplotlib and scikit-learn. Why it‚Äôs important: Since Python is a general programming language, you can do almost anything in it. But the good news is, the fundamentals you learned in the Python for Everybody Specialization can be used here. More specifically, you‚Äôll start to learn how to use: * What it is: How do you diagnose a machine learning problem? Is it regression? Classification? Clustering? Information Retrieval? This Specialization will teach you how to answer those questions and build systems which are able to learn patterns within datasets. Why it‚Äôs important: The Applied Data Science with Python Specialization taught you how to manipulate and visualise data, now in the Machine Learning Specialization, you‚Äôll learn how to write machine learning code to find patterns in that data. What it is: If you‚Äôve been wondering how machine learning algorithms are able to learn patterns in data, there‚Äôs one answer: math. Mostly through a combination of Linear Algebra and Multivariate Calculus (plus some others). The Mathematics for Machine Learning Specialization will help you understand the underlying mathematics which powers many of the most effective machine learning algorithms. What it‚Äôs important: Data is a compressed form of nature, math can be used to find patterns in data and code can be used to execute math at scale. The steps above focus on getting you to write code as soon as possible. However, as you learn more, you‚Äôll probably want to discover how the code you‚Äôre writing works. This Specialization will take you deep into the inner workings of machine learning algorithms and further strengthen your understanding of the code you‚Äôre writing. Why these? There are 3000+ Specializations/courses available on Coursera Plus. Too many for you to ever go through. The ones I‚Äôve picked are from my own personal experience (I‚Äôve gone through them) or because their content is world-class and exactly what you need to get started with data science and machine learning. But that being said, if something sparks your interest, go for it. How long should going through all of this take? Go at your own speed. But if you added up all the recommended timelines, you‚Äôd probably end up somewhere between 12‚Äì18 months. However, never underestimate the power of speed. The recommended timelines are for the average student. Are you average? Or are you eager to learn? If you‚Äôre the latter, remember, there is no speed limit. Do I need Coursera Plus? No. You can pick and choose any of the courses you want individually. However, if you plan on going through all of the above, Coursera Plus offers a hefty discount. I tried it and don‚Äôt like it‚Ä¶ can I get a refund? Yes. You‚Äôve got 14 days after you sign up to Coursera Plus. But here‚Äôs another anecdote from my own path. I signed up for a course in the past and got scared I couldn‚Äôt do it. So I emailed the support team and asked what the refund policy was. They told me two weeks. Turns out I didn‚Äôt get a refund. I went through the course, struggled, missed deadlines but then came out the other side knowing far more than when I started. The main point of this post is the internet has made it so you can create your own learning journey. Coursera is a phenomenal resource and I highly recommend it but it is only one of many out there. Everything we‚Äôve talked about can be learned outside of Coursera. You could even take the Notion curriculum template and reproduce it with all free resources and track your progress. Let‚Äôs say you did, you created your own curriculum, Coursera-driven or not. What should you do next? That question deserves its own article. But as a bare minimum: share your work. Write about what you‚Äôve learned. Create a blog under your own name. Search ‚Äúhow to create a blog with GitHub Pages‚Äù or start writing on Medium. Do not worry if someone else has already written about what you want to write about. The practice of writing about what you‚Äôve learned will further help your learning. What would the you six months ago like to have known? Write that. For more on this, I‚Äôd recommend reading ‚ÄúHow does a beginner data scientist like me get experience?‚Äù. What about advanced skills? This post has covered the fundamentals. If you‚Äôre looking to further your knowledge, I‚Äôd recommend all of the deeplearning.ai courses and all of the fast.ai curriculum. After that, you‚Äôre done with courses. Start your own projects. Courses teach fundamentals, self-driven projects help you learn specifics (knowledge which can‚Äôt be taught).",319,1,9,Towards Data Science,2021-01-05,2021
https://towardsdatascience.com/the-components-of-a-neural-network-af6244493b5b,The Components of a Neural¬†Network,A summary of the key parts that build up one of the most commonly‚Ä¶,1,31,['The Components of a Neural Network'],"This article is a continuation of a series I am writing on key theoretical concepts to Machine Learning. In addition to an introduction to ML, I have written articles on Classification and Regression that can be accessed on my page. Neural Networks are the poster boy of Deep Learning, a section of Machine Learning characterised by its use of a large number of interwoven computations. The individual computations themselves are relatively straightforward, but it is the complexity in the connections that give them their advanced analytic ability. The building block of a neural network is the single neuron. The diagram below shows the structure of a neutron with one input. The input to the neuron is x, which has a weight w associated with it. The weight is the intrinsic parameter, the parameter the model has control over in order to get a better fit for the output. When we pass an input into a neuron, we multiply it by its weight, giving us x * w. The second element of the input is called the bias. The bias is determined solely by the value b, since the value of the node is 1. The bias adds an element of unpredictability to our model, which helps it generalise and gives our model the flexibility to adapt to different unseen inputs when using testing data. The combination of the bias and input produces our output y, giving us a formula of w*x + b = y. This should look familiar as a modification of the equation of a straight line, y = mx + c. Neural Networks are made up of tens, hundreds or many even thousands of interconnected neurons, each of which runs its own regression. It‚Äôs essentially a regression on steroids. Naturally, we will not be able to analyse most datasets we come across in the real world using a regression as simple as the diagram above. We will expect to see many more inputs that are combined to estimate the output. This is achieved in a similar way as the neuron with one input. The formula for the above equation will read x0 * w0 + x1 * w1 + x2 * w2 + b = y. Neural networks organise neurons into layers. A layer in which every neuron is connected to every other neuron in its next layer is called a dense layer. Through this increasing complexity, neural networks are able to transform data and infer relationships in a variety of complex ways. As we add more layers and nodes to our network, this complexity increases. Currently our model is only good for predicting linear relationships in our data. In the previous diagram, there‚Äôs no benefit to running this neural network as opposed to a series of regressions. Neural Networks provide a solution to this in two ways. The first is the ability to add more layers to our network between the input and output, known as hidden layers. Each of these hidden layers will have a predefined number of nodes and this added complexity starts to separate the neural network from its regression counterpart. The second way that Neural Networks add complexity is through the introduction of an activation function at every node that isn‚Äôt an input or output. If you‚Äôre unfamiliar with the term, I would definitely check out a previous article I wrote on Linear Classification which looks at activation functions in far more depth, but to summarise from there, an activation function is a function that transforms our input data using a non linear method. Sigmoid and ReLu are the most commonly used activation functions. The fact that both of these models are non linear means that we add another element of adaptability to our model, because it can now predict classes that do not have linear decision boundaries or approximate non linear functions. In the simplest of terms, without an activation function, neural networks can only learn linear relationships. The fitting of an object as simple as an x¬≤ curve would not be possible without the introduction of an activation function. So the role of a neuron in a hidden layer is to take the sum of the products of the inputs and their weights and pass this value into an activation function. This will then be the value passed as the input to the next neuron, be it another hidden neuron or the output. When a Neural Network is initialised, its weights are randomly assigned. The power of the neural network comes from its access to a huge amount of control over the data, through the adjusting of these weights. The network iteratively adjusts weights and measures performance, continuing this procedure until the predictions are sufficiently accurate or another stopping criterion is reached. The accuracy of our predictions are determined by a loss function. Also known as a cost function, this function will compare the model output with the actual outputs and determine how bad our model is in estimating our dataset. Essentially we provide the model a function that it aims to minimise and it does this through the incremental tweaking of weights. A common metric for a loss function is Mean Absolute Error, MAE. This measures the sum of the absolute vertical differences between the estimates and their actual values. The job of finding the best set of weights is conducted by the optimiser. In neural networks, the optimisation method used is stochastic gradient descent. Every time period, or epoch, the stochastic gradient descent algorithm will repeat a certain set of steps in order to find the best weights. Gradient Descent requires a differentiable algorithm, because when we come to finding the minimum value, we do this by calculating the gradient of our current position and then deciding which direction to move to get to our gradient of 0. We know that the point at which the gradient of our error function is equal to 0 is the minimum point on the curve, as the diagrams below show. The algorithm we iterate over, step 2 of our gradient descent algorithm, takes our current weight and subtracts from it the differentiated cost function multiplied by what is called a learning rate, the size of which determines how quickly we converge to or diverge from the minimum value. I have an explanation in greater detail on the process of gradient descent in my article on Linear Regression. Overfitting and Underfitting are two of the most important concepts of machine learning, because they can help give you an idea of whether your ML algorithm is capable of its true purpose, being unleashed to the world and encountering new unseen data. Mathematically, overfitting is defined as the situation where the accuracy on your training data is greater than the accuracy on your testing data. Underfitting is generally defined as poor performance on both the training and testing side. So what do these two actually tell us about our model? Well, in the case of overfitting, we can essentially infer that our model does not generalise well to unseen data. It has taken the training data and instead of finding these complex, sophisticated relationships we are looking for, it has built a rigid framework based on the observed behaviour, taking the training data as gospel. This model doesn‚Äôt have any predictive power, because it has attached itself too strongly to the initial data it was provided, instead of trying to generalise and adapt to slightly different datasets. In the case of underfitting, we find the opposite, that our model has not attached itself to the data at all. Similar to before, the model has been unable to find strong relationships, but in this case, it has generated loose rules to provide crude estimations of the data, rather than anything concrete. An underfit model will therefore also perform poorly on training data because of its lack of understanding of the relationships between the variables. Avoiding underfitting is generally more straightforward than its counterpart, because general belief is that an underfit model is one that isn‚Äôt complex enough. We can avoid underfitting by adding layers, neurons or features to our model or increasing the training time. Some of the methods used to avoid overfititng are simply the direct opposites of avoiding underfitting. We can remove some features, particularly those that are correlated with others already present in the dataset or that have very little correlation with our output. Stopping the model earlier also ensures that we capture a more general model, instead of allowing it to over-analyse our data. In some cases, overfitting may occur due to a model‚Äôs over-reliance on a certain set of weights, or path in our neural network. The model may have found, during training, that a certain set of weights in a section of our neural network provide a very strong correlation with the output, but this is more a coincidence than the discovery of an actual relationship. If this occurs, then when presented with testing data, the model will not be able to deliver the same level of accuracy. Our solution here is to introduce the concept of dropout. The concept behind dropout is to essentially to exclude a section of the network every step of our training process. This will help us generate weights that are more even across the entire network and ensure that our model is not too reliant on any one subsection. That‚Äôs your summary of the components of Neural Networks. I‚Äôm looking to go through a lot more concepts in more detail in further articles, so keep an eye out for those! Evaluating models is up next. If you‚Äôre interested in any of my previous articles, give my page a follow as well. Until then, ‚úåÔ∏è.",53,0,8,Towards Data Science,2021-01-05,2021
https://towardsdatascience.com/7-must-know-visualizations-for-better-data-analysis-1ed6a440f7fe,7 Must-Know Visualizations for Better Data¬†Analysis,A practical guide for ggplot2 package in¬†R,1,29,['7 Must-Know Visualizations for Better Data Analysis'],"Data visualization is a very important part of data science. It is quite useful in exploring and understanding the data. In some cases, visualizations are much better than plain numbers at conveying information. The relationships among variables, the distribution of variables, and underlying structure in data can easily be discovered using data visualization techniques. In this post, we will learn about the 7 most commonly used types of data visualizations. I will use the ggplot2 library in R programming language. I also wrote an article that contains same visualizations created with Seaborn, a statistical data visualization library for Python. We will use the data.table package for data manipulation and ggplot2 for visualizations. I prefer R-studio to use R and its packages. Let‚Äôs first load the packages: The next step is to create tables (i.e. data frame) using the csv files that contain the data. We will use the groceries and insurance datasets available on Kaggle to create the visualizations. Line plots visualize the relation between two variables. One of them is usually the time. Thus, we can see how a variable changes over time. In the groceries dataset, we can visualize the number of items purchased over time. First, we will calculate the number of items purchased in each day. We can now plot the count (N) over time. Scatter plot is also a relational plot. It is commonly used to visualize the values of two numerical variables. We can observe if there is a correlation between them. We will visualize the bmi and charges columns in the insurance data frame and use the smoker column as a separator for the data points. Let‚Äôs elaborate on the syntax. The ggplot function creates an empty graph. The data is passed to the ggplot function. The second step adds a new layer on the graph based on the given mappings and plot type. The geom_point function creates a scatter plot. The columns to be plotted are specified in the aes method. The color parameter provides an overview of how the bmi and changes based on the categories in smoker column. Histograms are usually used to visualize the distribution of a continuous variable. The range of values of a continuous variables are divided into discrete bins and the number of data points (or values) in each bin is visualized with bars. The first thing we notice is the people who smoke are charged more in general. Kde plots are also used to visualize distributions. Instead of using discrete bins like histograms, kde plots smooth the observations with a Gaussian kernel. As a result, a continuous density estimate is produced. We can produce the kde version of histogram in the previous example. We use the geom_density function and remove the bins parameter because it is specific to histograms. Box plot provides an overview of the distribution of a variable. It shows how values are spread out by means of quartiles and outliers. The following code creates a box plot of the charges column and use the smoker column as a separator. The height of the box for smokers is more which means the data points in this category are more spread out. We clearly see the the smoker people are likely to be charged more than non-smokers. The red dots indicate the outliers. There are some non-smoker people who charged much more than the other people in this category. The bar plot of ggplot2 provides an overview of the distribution of a discrete variable. It shows the number of data points for each discrete value. The following is a bar plot for the children column. We see that most people do not have any children. 2D histograms combine 2 different histograms on a grid (x-axis and y-axis). Thus, we are able to visualize the density of overlaps or concurrence. In other words, we visualize the distribution of a pair of variables. We can use the geom_bin2d function to create a two-dimensional histogram of the charges and bmi columns. The lighter colors indicates the more dense region in terms of the number of observations (i.e. rows) that fall into that region. We have covered 8 basic yet very functional visualization types using the ggplot2 library of R. These basic visualizations can be created with almost any visualization library. The important points are to know when to use them and understand what they tell us. Thank you for reading. Please let me know if you have any feedback.",126,0,5,Towards Data Science,2021-01-05,2021
https://towardsdatascience.com/machine-learning-mini-project-6f67e511ffd3,Machine Learning Mini-Project,Poisson Regression on the Seoul Bike Sharing Dataset with pycaret¬†and‚Ä¶,1,28,['Machine Learning Mini-Project 5'],"Data: The Seoul Bike Sharing Data Set from UCI Machine Learning Repository. The target is Rented Bike Count. That means this is count data, which is positive integer values. The predictors/features are mostly weather-related, e.g. sunshine, rain, wind, visibility, as well as temporal features such as the hour of the day, whether it‚Äôs a holiday or not, etc. You can view my entire colab notebook here. Question: Should this be a Poisson regression problem? Count data is often modeled assuming that it comes from a Poisson distribution. Rather than review the basics, I refer you to two excellent tutorials below, as well as notes from a Princeton class on generalized linear models (GLMs). Intro to Poisson Regression Illustrated Guide to Poisson Regression Poisson Models for Count Data Assuming we are only modeling counts on days where the system works ‚Äî this is the histogram and kernel density estimate, giving you a sense of what the pdf would look like. The answer to the question: it probably doesn‚Äôt matter that much. As the tutorials mention you can use least-squares methods (that assume a Gaussian response distribution to some (non)linear predictor based on the covariates. If you want integer predictions, you could always just round the continuous labels to integer values. What we can also do is use different loss functions to adapt our normal methods (e.g. XGBoost) to be better suited to count data. Preprocessing. I use pycaret for this. I found that using the quantile transform for the features helped, opposed to a power transform like box-cox or yeo-johnson. I think that feature_selection and feature_interaction help quite a bit as well, as you‚Äôll see in the feature importance analysis below. The data is split 80/20 with no shuffling, since it‚Äôs a time series, we want don‚Äôt want leakage from future into the past. Hour and Seasons are integer encoded variables which I decided would be considered ordinal (ordered categorical) variables. I dropped the date variable itself; I thought maybe one could use the month as a new feature, but I think Seasons tends to cover that ‚Äî and in essence, so do the other variables, which describe daily and seasonal weather conditions. Default Learners in pycaret So ExtraTreesRegressor() seems to be the clear winner here. Here are some of the more important features, including a few interaction terms: Alternative Loss Functions. But these learners are set at default, meaning that for a lot of them, you are using some MSE/RMSE type loss function. Whereas you might be more interested in modeling MAE, or at least reporting it, since MAE is the most ‚Äúintuitive‚Äù way of measuring goodness of fit when you are talking about things like bikes. However, there is good enough reason to explore different loss functions, such as MAE, Huber, Poisson, Logcosh, etc. It might also be worthwhile to think about goodness of fit in terms of the Poisson deviance. The reason why? Because as we know, MSE tends to be influenced by outliers, whereas MAE is not as much. The Huber is a little like a combination of MSE and MAE, which means that it tries to not to be unduly influenced by outliers, the Logcosh is like a smoothed version of MAE whose gradient levels out as you get to 0 (thus mitigating against exploding gradients if you are using SGD). The Poisson is a little more weird, but tends to penalize larger count values you find in the heavy-tails of Poisson distributions, leading to better calibrated models. Here‚Äôs a list of learners with various loss functions that we‚Äôll try out. It‚Äôs not exhaustive but should let us know if there any further avenues to pursue. Here are the results, through pycaret: In terms of Poisson deviance, I found that the third one in the list above ‚Äî the CatBoostRegressor() with Huber loss ‚Äî performed the best. It also would have been considered the best if R2 was your performance metric of choice. Transforming the Target. It is common to take the square root of the response variable as a variance stabilizing transformation. That would mean you are transforming both sides of the regression equation. I tried using yeo-johnson in pycaret. If you want to take the square root, do it yourself with the TransformTargetRegressor() function in sklearn, like this. I‚Äôve read that many people add something to the response before taking the square root, especially if there are 0s involved, but we don‚Äôt have any, and I‚Äôm not sure the results would change much. I clip the predictions so that anything under 0 gets mapped to .0001. The results are surprising. You don‚Äôt see it in this picture, where the MAE on this round was 219, but you can get down to an MAE of 210‚Äì215 quite readily. The Poisson deviance is also fairly low at 126, but the ExtraTreesRegressor() on the non-transformed target can get to 119 or so, so it might be technically better calibrated. Either way, ExtraTreesRegressor() is head and shoulders above the rest. Keras. Let‚Äôs switch gears and see how deep learning might do on this task. We can start with a regular deep network that has a the tf.exp (exponential) activation at the end. What this means is that you are treating this like a Poisson regression with a log-link. The advantage of using deep learning is that you can express the log(rate) of the Poisson as more expressive nonlinear function of the features/regressors. The other nice thing is that we can use various loss functions that are built in to keras/tensorflow. In other words you can use ‚Äúhuber‚Äù, ‚Äúpoisson‚Äù, ‚Äúlogcosh‚Äù, ‚Äúmae‚Äù, ‚Äúmse‚Äù, etc. when you are compiling the model. Remember though, you have to reset callbacks and adjust the learning rate ‚Äî with ‚Äúmse‚Äù, for example, the gradients go as x¬≤, so you should turn the learning rate down to something smaller like 1e-4. Also, I would use MinMaxScaler() on the data to get it in the [0,1] range. It makes a big difference. Here‚Äôs the code, after you do your scaling: Of course, the results are different each time. I would say it is more or less a tie between MAE/Logcosh and Poisson loss functions. You can expect a MAE in the range of 220‚Äì235. The weird thing is how this model misses those spikes you see below, which I think is some kind of ‚Äúrush hour‚Äù. The ExtraTreesRegressor() at least has some kind of ‚Äúspike‚Äù in the model predictions. Tensorflow Probability. Let‚Äôs make the last layer of our NN more probabilistic, by asking that it return a sample from a parametric distribution ‚Äî meaning, let‚Äôs use the neural network to find the rate/variance of the Poisson, and then let our yhat prediction be that sample. For that we need layers that pump out samples from pdfs. You can see that I commented out a line that has tfd.Normal in it. The reason for this is that a Poisson with a rate > 20 or so can be approximated with a Gaussian of the same mean and variance. That means that the Gaussian‚Äôs ‚Äúscale‚Äù ‚Äî the standard deviation ‚Äî is the square root of that, hence the K.sqrt(x) part. The loss function is the negative log-likelihood compared to the theoretical distribution. The results are pretty good: the MAE can get down to 223 or so. With the keras/Normal model I can get down to an MAE of 213 on a good trial. The Poisson deviance on this keras/Normal model is about 106 , the keras/Poisson can get to 101. By comparison, the ExtraTreesRegressor() has Poisson deviance in the 120 range. Maybe the keras/Poisson model could be considered as a viable alternative to the ExtraTreesRegressor(), because it can achieve a comparable MAE but an obviously better Poisson deviance. That‚Äôs it. Hope you enjoyed. Questions and comments are welcome and remember to check out my colab notebook (link at top). Here are some references that I found helpful in understanding the theories and ideas. stats.stackexchange.com scikit-learn.org blog.tensorflow.org matthewmcateer.me stats.stackexchange.com pycaret.readthedocs.io peltarion.com",140,0,8,Towards Data Science,2021-01-05,2021
https://towardsdatascience.com/4-scikit-learn-tools-every-data-scientist-should-use-4ee942958d9e,4 Scikit-Learn Tools Every Data Scientist Should¬†Use,Composite Estimators and Transformers,9,44,"['4 Scikit-Learn Tools Every Data Scientist Should Use', 'Warming-up', '1 - Pipelines', '2 - Function Transformer', '3 - Column Transformer', '4 - Feature Union', 'Alternative Syntax: make_[composite_estimator]', 'Bonus: Visualizing Your Pipeline', 'Final Thoughts']","Written By: Amal Hasni & Dhia Hmila Data Science projects tend to include multiple back and forth passages between preprocessing, feature engineering, feature selection, training, testing ‚Ä¶ Juggling all of these steps, while trying multiple options or even in production¬†environments, can get messy very fast. Fortunately, Scikit-Learn provides options that allow us to chain multiple estimators into one. In other words, a particular action like fit or predict needs only to be applied once on the whole sequence of estimators. In this article, We share with you four of these tools with examples of use cases through a concrete project. 1 - Pipelines2 - Function Transformer3 - Column Transformer4 - Feature UnionAlternative Syntax: make_[composite_estimator]Bonus: Visualizing Your Pipeline Before we start exploring scikit-learn‚Äôs tools, let‚Äôs start by getting a dataset we can play with. We should mention that this is just for the sake of example. So you don‚Äôt necessarily need to download it (unless you want to try the code yourself). We actually stumbled on a nice python package called datasets that allows you to easily download more than 500 datasets: We‚Äôre going to use Amazon Us Reviews. It contains numerical and textual features (e.g. reviews and the number of helpful votes it got) and the target feature is the number of stars attributed. Pipelines are tools made to encapsulate sequences of estimators into a single one for convenience purposes. Pipelines are convenient for multiple reasons: In practice, a pipeline is a bunch of transformers followed by an estimator. If you don‚Äôt know what a transformer is, it‚Äôs basically any object that implements a fit and transform methods. In our example, we‚Äôre going to transform the reviews from textual to numeric data using TfidfVectorizer and then attempt a prediction using RandomForestClassifier : As we mentioned in the previous section, a Transformer needs to include fit and transform methods. A FunctionTransformer is a stateless transformer constructed from a callable (aka function) you've created. In some cases, you need to perform a transformation that doesn‚Äôt make any parameter fitting. In this scenario, it would be useless to create a fit method. It's in those cases that FunctionTransformer would be the most useful. Our Dataset includes a feature under a date format. An example of a transformation we can do over dates is to extract the year. This is how to do this with a FunctionTransformer: Note: If a lambda function is used with FunctionTransformer, then the resulting transformer will not be pickleable. A nice thing to know is that you can work around this by using cloudpickle package. Depending on your Dataset, you might need to apply distinct transformations on different columns of an array or pandas DataFrame. Column Transformer allows applying separate transformations before concatenating the resulting features. This estimator is particularly useful for heterogeneous data. In this case, you need to customize feature extraction mechanisms or transformations to the data type of the column or the subset of columns. In our example, we have both textual and numerical data. Here‚Äôs how we use Column Transformer to apply separate transformations depending on the data type: As expected, the output Data of the ColumnTransformer has 102 columns: 100 from the TF-IDF transformer and 2 from the Normalizer. Similar to ColumnTransformer¬†, a FeatureUnion concatenates the results of multiple transformers. Except it is slightly different since each transformer gets the whole dataset as their input instead of a subset of columns as in the ColumnTransformer. The two are quite equivalent in terms of what you can do with them, but depending on the situation, one may be more appropriate to use and require fewer lines of codes. FeatureUnion allows you to combine different feature extraction transformers into one Transformer. Equivalently to the previous case, we can construct the FeatureUnion by using a pipeline composed of ColumnSelector and a given Transformer. We‚Äôre going to construct a FeatureUnion that performs the same transformations as the previously implemented ColumnTransformer . To do that, we‚Äôre going to encapsulate two pipelines into a FeatureUnion. Each pipeline chains a ColumnSelector and a given Transformer. There is no pre-implemented ColumnSelector in scikit-learn, so we're going to build our own, using FunctionTransformer : There are alternatives to the previously mentioned methods(except Function Transformer) that have a slightly different syntax. These methods are: These are shorthands for the previous constructors. The main difference is that they do not require, and do not permit, naming the estimators. Instead, the component names will automatically be set to the lowercase of their types. In most cases, it is simpler, cleaner, and more easily readable to use the shorthand versions. However, you might need to customize your estimators' names if you need to perform a grid search for example. In this case, assigning short distinguishable names can be useful for clarity and compactness. If you‚Äôre not familiar with parameter optimization using grid search, we will be writing an article about it soon. You can see the difference between the two versions applied to the Pipeline case below: Scikit-Learn has a neat way of visualizing the composite estimators you create using the following lines of code: This will effectively create an interactive HTML file representing your pipeline in a clear way. An alternative to this, if you‚Äôre using notebooks is to use this code instead: Pipelines and composite estimators are powerful tools for Data Science projects especially those meant to be put in a production environment. Their added value is not only about clarity and convenience but also about safety and data leakage prevention. If you want to see how we used the different tools mentioned in this article in a Hands-on project, don‚Äôt hesitate to check out our previous article: towardsdatascience.com Finally, if you want to see how to make use of these composite estimators to optimize hyperparameters over the whole pipeline or to compare different algorithm performances, stay tuned for our future article about grid searches. Thank you for sticking with us this far. We hope you liked the content. Stay safe and we will see you in our future article üòä",365,0,6,Towards Data Science,2021-01-05,2021
https://towardsdatascience.com/introducing-sayn-a-simple-yet-powerful-data-processing-framework-ce1c89af0e47,Introducing SAYN: A Simple Yet Powerful Data Processing Framework,Our New Open Source Framework That‚Ä¶,1,19,['Introducing SAYN: A Simple Yet Powerful Data Processing Framework'],"So, what is SAYN? In simple terms, SAYN is an open source data processing framework. We (the team at 173Tech) have built it to be the simplest framework whilst maintaining full flexibility. Users can select from multiple predefined task types and build their own ETL processes. SAYN is really unique and unlike anything you have seen before. Want to know more? Then read on! Before we speak more about SAYN, let‚Äôs start with a quick refresher to place things in context. Modern analytics infrastructures are usually organised around a data warehouse using five core layers as shown on the following graph: It is crucial to have an efficient and scalable data process in place that can easily support the creation and maintenance of hundreds and more tasks and their dependencies. There are two common ways to go about this: What if you want to maintain high flexibility in your analytics processes at scale, but also keep things simple? Well, as it turns out, we never found a data processing framework that efficiently addressed those concerns. So we built it! We believe simplicity to be crucial when maintaining pipelines at scale. However, we also believe that simplicity should not come at the expense of flexibility. This is why we have built our own open source data processing framework: SAYN. SAYN is designed to empower analytics teams by being simple, flexible and centralised. It democratises the contribution to data processes within an analytics team, enables full flexibility and helps save a lot of time through automation. SAYN is built around the concept of tasks and currently has the following task types pre-built for you: The following graph displays how we typically use SAYN in the modern analytics stack. The blue lines are orchestrated by SAYN: SAYN is designed around three core beliefs that a modern data processing framework should empower data engineers and analysts by being simple, flexible and centralised. This is how SAYN lives up to that promise: Simple Flexible Centralised The best way to see how great SAYN is is to actually try it! SAYN is distributed on PyPi and works using the command line. It is executed using the sayn run command. You can literally get started in 2 minutes with the following four lines: This will install the sayn package, create a SAYN project called test_sayn, move you into the project directory and then execute SAYN. You should see the following happening: As mentioned before, SAYN projects are organised around the concept of tasks: Here are some example use cases of SAYN: If you want to understand more about how SAYN works, go through our tutorials which are good starting points. We are actively developing SAYN and it is getting even better by the day! SAYN has made our lives so much easier at 173Tech and it really unleashes our analytics proficiency. Your team can benefit from it as well! In addition, we would love to get feedback that can help us make the framework even better so please do reach out, we‚Äôre friendly :) You can contact us for questions or suggestions regarding SAYN via sayn@173tech.com. Speak soon! ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî This story was originally published at: https://www.173tech.com/insights/introducing-sayn/ .",243,0,5,Towards Data Science,2021-01-05,2021
https://towardsdatascience.com/retrieving-similar-e-commerce-images-using-deep-learning-6d43ed05f46b,Retrieving Similar E-Commerce Images Using Deep¬†Learning,Abstract: Finding products that look¬†similar‚Ä¶,1,39,['Retrieving Similar E-Commerce Images Using Deep Learning'],"Table of Contents1. Introduction2. Business problem3. Mapping to Deep Learning problem4. Data Set Used5. Model Architecture6. Loss Function7. Implementation Overview8. Building data pipeline9. Defining model architecture10. Defining loss and accuracy functions11. Defining Step function12. Final flow13. Validating results14. Understanding Deployment15. Future Work & Conclusion16. References In a Siamese network we basically pass 2 pair of images, one pair belongs to similar images and other ones to dissimilar. The core idea is for the model to learn weights based on similarity and dissimilarity of images. If we can capture the fine grained visual details by generating embedding using such an architecture. We can actually use these embedding to recommend similar products. The major contributions of this work would be a Siamese network consisting of a multi-scale deep CNN that learns a 4096-dimensional embedding of the image to capture the notion of visual similarity. Also, we need to decide on the loss function, we‚Äôll using contrastive loss for our implementation. So, how we are going to approach this is given an image, first we will identify a similar image and then a dissimilar image. Once we have these set of images with us. We‚Äôll pass each of these images to our model to get embedding. Then these embedding will be used to compute loss. Based on the loss value we‚Äôll learn weights till we reach convergence. Model performance will be measured using accuracy. 4. Data Set Used: For this work we have used triplet data to train and validate. Triplet data consists of a query image, positive image (similar to query image) and a negative image (relatively dissimilar to query image as positive image). The query image can either be a Wild Image(where people wearing the cloth in everyday uncontrolled settings) or Catalog Image(model wearing cloth in controlled settings as shown in an eCommerce app). While the positive and negative images can be In-class(same product category as query image) or Out-of-class(other product category than query image). The data used in this case study is provided in the following link by the authors of the paper. They have pro-grammatically generated these triplets from 4 different data set. Sample data looks like: 5. Model Architecture: Following architecture was implemented: As already shown, the Siamese network consists of two convolutional neural networks with shared weights which are optimized during training by minimizing the loss function. How it works is, the network takes two images as input, but then we have 2 pair of images, first pair having similar images and second pair having dissimilar images. So, for any one query(anchor) images we‚Äôll have two more images associated with it. One would be a positive image(similar) and other will be negative image(dissimilar). Once these pair of images are identified, we map these input images by the network into an embedding space. If embedding for similar images are close and for dissimilar are far-away then the network has learned a good embedding. Understanding various components of the architecture: There are multiple parts to the model architecture. First part is the Deep CNN. Deep CNN easily learns to encode strong in-variance into their architecture during training, which makes them achieve a good performance for image classification. So for this part we are using an architecture similar to that of VGG19‚Äôs convolutional neural network. This CNN is used to encode strong in-variance and capture the semantics present in the image because it has 19 convolutional layers. Among the 19 layers, the top layers are good at encoding complex representation of image features.The VGG19 like CNN has a high en-tropic capacity because of its 4096-dimensional final layer which allows the network to effectively encode the information into the sub-spaces. The other 2 CNNs use a shallower network architecture to capture the down-sampled images. Due to the shallower architecture, these CNN‚Äôs have less in-variance and are used to capture simpler aspects like shapes, pattern, and color which makes the visual appearance of an image. Thus employing three different convolution neural networks instead of a single CNN and making them share lower level layers, makes each CNN independent of the other two. At last, the embedding from the three convolutional neural networks are normalized and combined with a 4096-dimensional linear embedding layer which encodes and represents an input image as a 4096-dimensional vector. In order to prevent over-fitting, L2 normalization was used. 6. Loss Function: Contrastive loss function is a distance-based Loss function as opposed to prediction error-based loss functions. Like any other distance-based loss function, it tries to ensure that semantically similar examples are embedded close together. When similar image pair (label Y = 0) is fed to the network, the right-hand additive section of below image nullifies and the loss becomes equal to the part containing the positive pair distance between the embedding of two similar images. Thus if two images are visually similar, the gradient descent reduces the distance between them which is learned by the network. On the other hand, when two dissimilar images(label Y = 1) are fed to the network, the left-hand additive section goes away and the remaining additive section of the equation basically works as a hinge loss function. If the image pair is completely dissimilar and the network outputs a pair of embedding whose proximity is greater than m, then the value of the loss function is maximized to zero else if the images are somewhat similar then we trigger the proximity minimization by optimizing the weights as there is an error. The value m is the margin of separation between negative and positive samples and is decided empirically. When m is large, it pushes dissimilar and similar images further apart thus acting as a margin. In this work, we have used m = 1. 7. Implementation Overview: Actual implementation of the research work can be found here. We are not going to replicate the same code, instead, we‚Äôll implement the work using TensorFlow 2, where we‚Äôll be creating input pipeline using tf.data and for training we‚Äôll create custom train loop. 8. Building Data Pipeline: For building our input pipeline we‚Äôll be using tf.data. The Dataset API allows you to build an asynchronous, highly optimized data pipeline to prevent your GPU from data starvation. It loads data from the disk (images or text), applies optimized transformations, creates batches and sends it to the GPU. Former data pipelines made the GPU wait for the CPU to load the data, leading to performance issues. Some official sources: API docs, Datasets Quick Start, Programmer‚Äôs guide, Official blog post, Slides from the creator of tf.data, Origin github issue, Stackoverflow tag for the Datasets API.Our first method for pipeline is to get image paths, we start by reading the input file(train/val/test), these files have our triplet info, append the actual path where the image is present and finally return a final list which contains the full path of the image for each triplet. Then we define 2 functions that will take care of full input pipeline. These functions are: Above function uses tf.data to define the input pipeline. In the above implementation we read the input file name from an array which contains details of input file path for each image. Then we shuffle the content read. Then we map read content using input_parser to get feature vector for each image and then that final tensor as dataset object is returned which will be used during modelling. 9. Defining model architecture: We have already understood what the architecture is, code implementation of the discussed architecture is: 10. Defining loss and accuracy functions: We have already discussed about the loss function, below code is the loss implementation of the contrastive loss function. This function calculates loss for a batch of images. For one batch it starts by calculating loss value for each of the pair. Once we have that value, that value is added to the final loss and then loss is normalized by diving it by the 2 time the batch_size. In the loss equation, label Y = 1 is assigned to dissimilar or negative image pairs whereas Y = 0 is allotted to similar or positive image pairs. For accuracy function calculates the accuracy for one batch. It takes the embedding tensors for each of the image, batch_size and iterator through the batch to give a value of 1 if neg_dist > pos_dist or 0 otherwise. It finally returns the final accuracy for a batch. 11. Defining Step Function: Till now we have seen the data pipeline functions, model architecture, loss definition, now it is time to define the step function that encapsulates the forward and backward pass of the network. For doing this, we‚Äôll be using Gradient Tape. tf.GradientTape allows us to track TensorFlow computations and calculate gradients w.r.t some given variables. GradientTape is a brand new function in TensorFlow 2.0 and that it can be used for automatic differentiation and writing custom training loops. GradientTape can be used to write custom training loops. In our implementation, we will be using 2 step function, one will be the train step and other will be validation step. Train step is where actual training is performed. In our case since we are dealing with triplets, for one batch we first find the embedding for each of the image in that batch such that we get embedding for query, positive and negative image. This embedding is generated using model architecture that was defined. Once we have this, we will then calculate loss and accuracy for that batch and then based on the loss we calculate gradients using the gradient tape. Once we have these gradients with us we use optimizer object to update weights. Code implementation: tf.function decorator is used to cause TensorFlow autograph working and accelerate execution for those operation inside it. When we call a @tf.function decorator the first time, TensorFlow will first convert it into a graph, then execute it, after that, when we call the function again, it‚Äôll just execute the graph. Validation step is responsible for giving us validation loss and accuracy on validation data per step for an epoch. This function uses the model(trained with updated weights) to generate embedding and based on those embedding we calculate the loss and accuracy. 12. Final Flow: Once all the helper functions are defined, next step is to define the final flow method, which will be responsible for creating custom training, there are multiple parts to it and we‚Äôll break down each of these parts.We start by first defining the path of train and validation csv files. Once this is done, we generate 2 lists, one containing path for train and other for validation set. This list is generated by calling get_image_path() function. Finally we measures for train loss and train accuracy using tf.metrics.Mean. Then we‚Äôll enable GPU usage and then start our flow for each epoch. Inside epoch loop, we first call the input_pipeline() function that returns a dataset object, then to this an iterator is linked. Finally we start step by step training with step size equals to number of datapoints // batch_size. Once this is done, we‚Äôll then call the step function to train, we keep on recording the scores we get during each step of training inside an epoch and finally when this is done, we use validation set to calculate the loss and accuracy and print the final one for both train and validation. Model weights are also saved after each epoch. We also write scores to tensorboard which can be done using following code: Full implementation of flow function: Once all our steps are defined, we can call overall flow using the following: After running this for 10 epochs, best results were seen for 4th epoch, the weights from this was then used to check model performance on validation and test set. Results: We got an accuracy of 94.19 on validation set. 13. Validation Results: We have defined a validation block on the validation set itself, it takes a batch of 128 images and return a score. It does it for 5 iteration and images are taken from validation set in random. Along with this, we also have taken 5 different set of triplets to see if model is able to correctly identify the images which are similar and the one that are distinct. Code implementation: This can be called using: Results of the above cell: 14. Understanding Deployment: This whole architecture can be deployed using the following flow: Base Model Training: This is the part what we have seen in the previous notebook, where we build a pipeline and do full training based on the train set available, once this is done, we save our best model weights to a storage bucket(ex: S3).Incremental Training: This step is required when we have new set of images with us that are still not seen by our train set, this can be either weekly, monthly or quarterly. After doing the incremental training the updated model weights are again stored on storage bucket.Generate Embedding: Once our training is done, we can generate embedding of all the images we have in our data repository, for this we need to first load all the images we have from data repository, and load the model weights and based on this generate final embedding.Cache DB: In real world scenario if a user clicks on an image we need to show recommendation with very low latency, for this it is good to store your embedding in cache DB(example Redis), time taken by these to get records are much lesser than conventional DB, hence, in most of the production environment where latency matters, we prefer using such DB.Recommendation: Once all of our above steps are in place, if we have a query image, whose embedding we already have, it is passed through a similarity function, which will use the loaded embedding from the cache DB to find the most similar products based on NN computed using some similarity(Ex: cosine similarity) and the user will see the recommended products along with the actual products. Following functions were used in colab to depict deployment: For any query images, this function is called to generate the embedding corresponding to that image‚Ä¶ This function is used by input pipeline function to map image paths and convert them into tensor representing an image This is our input pipeline function that actually reads all the image paths and is used to finally generate embedding for all the images already present in image repository In above cell we started by defining the path where the images are present and then generated 2 list, one for query and rest of the images. Once this is done, we are loading weights of best model. Then because of memory issue we iterate through the data set with a batch size of 128 images to generate tensors of size 28, 4096. Finally we concatenate all these tensors to get embedding for all the images that are part of our image repository. Above code is a simple implementation of cosine similarity, when given embedding for 2 images, this will return how similar those images are, higher the value more similar the images are. Above function is used to find the top n similar images for a query image. Given an images based on cosine similarity it‚Äôll give you the index of the image and the score it has with that query images, lastly based on the value of k it‚Äôll return the top k images similar to query image based on cosine similarity between image embedding. In above cell for all the images we are actually finding the top 10 NN and storing the results in a final list. Results seen on test sample: 15. Future Work & Conclusion: The above results are on some 16k images scrapped from amazon. The recommendations seems to be good and we can further improve performance by doing augmentation of our images. This whole implementation is divided into 3 parts. A simple EDA on the data that we have, training implementation and inference(deployment steps). You can find the full code here. 16. References: https://www.appliedaicourse.com/ https://arxiv.org/pdf/1901.03546v1.pdfhttps://medium.com/predict/face-recognition-from-scratch-using-siamese-networks-and-tensorflow-df03e32f8cd0https://cs230.stanford.edu/blog/datapipeline/https://www.srijan.net/blog/building-a-high-performance-data-pipeline-with-tensorflowhttps://github.com/gofynd/mildnethttps://www.tensorflow.org/api_docs/python/tf/GradientTapehttps://www.pyimagesearch.com/2020/03/23/using-tensorflow-and-gradienttape-to-train-a-keras-model/https://towardsdatascience.com/how-to-reduce-training-time-for-a-deep-learning-model-using-tf-data-43e1989d2961 You can also find and connect with me on Linkedin and Github. This concludes the work, Thanks for reading!",18,0,13,Towards Data Science,2021-01-05,2021
