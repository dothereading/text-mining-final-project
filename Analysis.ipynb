{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b785ab59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-16 12:05:25.973079: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-16 12:05:26.382526: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-02-16 12:05:26.382549: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-02-16 12:05:27.393534: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-16 12:05:27.393630: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-16 12:05:27.393639: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-02-16 12:05:28.371790: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-02-16 12:05:28.372082: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-02-16 12:05:28.372108: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (dominik-ThinkPad-T580): /proc/driver/nvidia/version does not exist\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import SnowballStemmer \n",
    "from nltk.stem import PorterStemmer\n",
    "#nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#!python -m spacy download en_core_web_sm\n",
    "import spacy\n",
    "sp = spacy.load('en_core_web_sm')\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "30867115",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "#getting a library of stopwords and defining a lemmatizer\n",
    "porter=SnowballStemmer(\"english\")\n",
    "lmtzr = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e39259b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"tds.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "95b05e60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>subtitle</th>\n",
       "      <th>n_sections</th>\n",
       "      <th>n_paragraphs</th>\n",
       "      <th>section_titles</th>\n",
       "      <th>story_text</th>\n",
       "      <th>claps</th>\n",
       "      <th>responses</th>\n",
       "      <th>reading_time</th>\n",
       "      <th>publication</th>\n",
       "      <th>date</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>https://towardsdatascience.com/difference-betw...</td>\n",
       "      <td>Difference between Permutation and Combination</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>['Difference between Permutation and Combinati...</td>\n",
       "      <td>Long story short The difference between Permut...</td>\n",
       "      <td>479.0</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>Towards Data Science</td>\n",
       "      <td>2017-05-31</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>https://towardsdatascience.com/building-a-real...</td>\n",
       "      <td>Building a realtime dashboard with Flink: The ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>['Building a realtime dashboard with Flink: Th...</td>\n",
       "      <td>With the demand for “realtime” low latency dat...</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Towards Data Science</td>\n",
       "      <td>2017-05-31</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>https://towardsdatascience.com/artificial-inte...</td>\n",
       "      <td>Artificial Intelligence is the Panacea to Toda...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12</td>\n",
       "      <td>27</td>\n",
       "      <td>['Artificial Intelligence is the Panacea to To...</td>\n",
       "      <td>My foray into understanding, and more importan...</td>\n",
       "      <td>33.0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>Towards Data Science</td>\n",
       "      <td>2017-05-31</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>https://towardsdatascience.com/opportunities-a...</td>\n",
       "      <td>&lt;span class=\"markup--anchor markup--h3-anchor\"...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>['Opportunities And Obstacles For Deep Learnin...</td>\n",
       "      <td>Target audience: general. 27 scientists collab...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Towards Data Science</td>\n",
       "      <td>2017-05-31</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>https://towardsdatascience.com/https-medium-co...</td>\n",
       "      <td>The Single Most Important Thing That Data Can ...</td>\n",
       "      <td>How to reduce the cognitive load that entrepre...</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>['The Single Most Important Thing That Data Ca...</td>\n",
       "      <td>I’ve worked with a number of startups and ther...</td>\n",
       "      <td>83.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>Towards Data Science</td>\n",
       "      <td>2017-05-31</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>813</th>\n",
       "      <td>813</td>\n",
       "      <td>814</td>\n",
       "      <td>https://towardsdatascience.com/a-low-down-on-m...</td>\n",
       "      <td>A Low Down on Machine Learning</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12</td>\n",
       "      <td>68</td>\n",
       "      <td>['A Low Down on Machine Learning', 'Part 1', '...</td>\n",
       "      <td>How machine learning and artificial intelligen...</td>\n",
       "      <td>79.0</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>Towards Data Science</td>\n",
       "      <td>2022-02-28</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>814</th>\n",
       "      <td>814</td>\n",
       "      <td>815</td>\n",
       "      <td>https://towardsdatascience.com/background-task...</td>\n",
       "      <td>Background tasks for NLP</td>\n",
       "      <td>Heavy lifting is better in the background rath...</td>\n",
       "      <td>3</td>\n",
       "      <td>33</td>\n",
       "      <td>['Background tasks for NLP', 'Recap', 'Heavy l...</td>\n",
       "      <td>Those people dancing in the photo are similar ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>Towards Data Science</td>\n",
       "      <td>2022-02-28</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>815</th>\n",
       "      <td>815</td>\n",
       "      <td>816</td>\n",
       "      <td>https://towardsdatascience.com/elasticsearch-w...</td>\n",
       "      <td>Elasticsearch Workshop #6 — Scripting Part 4</td>\n",
       "      <td>Regex and pattern matching</td>\n",
       "      <td>9</td>\n",
       "      <td>27</td>\n",
       "      <td>['Elasticsearch Workshop #6 — Scripting Part 4...</td>\n",
       "      <td>Welcome to part 6 of the workshop. As usual, t...</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>Towards Data Science</td>\n",
       "      <td>2022-02-28</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>816</th>\n",
       "      <td>816</td>\n",
       "      <td>817</td>\n",
       "      <td>https://towardsdatascience.com/data-science-tr...</td>\n",
       "      <td>Data Science training — run them effectively</td>\n",
       "      <td>A few leads and watch-outs as you prepare to t...</td>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>['Data Science training — run them effectively...</td>\n",
       "      <td>Data Science and Analytics is a very rapidly e...</td>\n",
       "      <td>55.0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>Towards Data Science</td>\n",
       "      <td>2022-02-28</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>817</th>\n",
       "      <td>817</td>\n",
       "      <td>818</td>\n",
       "      <td>https://towardsdatascience.com/4-ways-ai-can-i...</td>\n",
       "      <td>4 Ways AI Can Improve Your Productivity</td>\n",
       "      <td>A Collection of Tools helping you Increase You...</td>\n",
       "      <td>6</td>\n",
       "      <td>20</td>\n",
       "      <td>['4 Ways AI Can Improve Your Productivity', 'T...</td>\n",
       "      <td>Using Artificial Intelligence or Machine Learn...</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>Towards Data Science</td>\n",
       "      <td>2022-02-28</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>818 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0   id                                                url  \\\n",
       "0             0    1  https://towardsdatascience.com/difference-betw...   \n",
       "1             1    2  https://towardsdatascience.com/building-a-real...   \n",
       "2             2    3  https://towardsdatascience.com/artificial-inte...   \n",
       "3             3    4  https://towardsdatascience.com/opportunities-a...   \n",
       "4             4    5  https://towardsdatascience.com/https-medium-co...   \n",
       "..          ...  ...                                                ...   \n",
       "813         813  814  https://towardsdatascience.com/a-low-down-on-m...   \n",
       "814         814  815  https://towardsdatascience.com/background-task...   \n",
       "815         815  816  https://towardsdatascience.com/elasticsearch-w...   \n",
       "816         816  817  https://towardsdatascience.com/data-science-tr...   \n",
       "817         817  818  https://towardsdatascience.com/4-ways-ai-can-i...   \n",
       "\n",
       "                                                 title  \\\n",
       "0       Difference between Permutation and Combination   \n",
       "1    Building a realtime dashboard with Flink: The ...   \n",
       "2    Artificial Intelligence is the Panacea to Toda...   \n",
       "3    <span class=\"markup--anchor markup--h3-anchor\"...   \n",
       "4    The Single Most Important Thing That Data Can ...   \n",
       "..                                                 ...   \n",
       "813                     A Low Down on Machine Learning   \n",
       "814                           Background tasks for NLP   \n",
       "815       Elasticsearch Workshop #6 — Scripting Part 4   \n",
       "816       Data Science training — run them effectively   \n",
       "817            4 Ways AI Can Improve Your Productivity   \n",
       "\n",
       "                                              subtitle  n_sections  \\\n",
       "0                                                  NaN           1   \n",
       "1                                                  NaN           1   \n",
       "2                                                  NaN          12   \n",
       "3                                                  NaN           1   \n",
       "4    How to reduce the cognitive load that entrepre...           1   \n",
       "..                                                 ...         ...   \n",
       "813                                                NaN          12   \n",
       "814  Heavy lifting is better in the background rath...           3   \n",
       "815                         Regex and pattern matching           9   \n",
       "816  A few leads and watch-outs as you prepare to t...           2   \n",
       "817  A Collection of Tools helping you Increase You...           6   \n",
       "\n",
       "     n_paragraphs                                     section_titles  \\\n",
       "0              25  ['Difference between Permutation and Combinati...   \n",
       "1               9  ['Building a realtime dashboard with Flink: Th...   \n",
       "2              27  ['Artificial Intelligence is the Panacea to To...   \n",
       "3               6  ['Opportunities And Obstacles For Deep Learnin...   \n",
       "4              22  ['The Single Most Important Thing That Data Ca...   \n",
       "..            ...                                                ...   \n",
       "813            68  ['A Low Down on Machine Learning', 'Part 1', '...   \n",
       "814            33  ['Background tasks for NLP', 'Recap', 'Heavy l...   \n",
       "815            27  ['Elasticsearch Workshop #6 — Scripting Part 4...   \n",
       "816            50  ['Data Science training — run them effectively...   \n",
       "817            20  ['4 Ways AI Can Improve Your Productivity', 'T...   \n",
       "\n",
       "                                            story_text   claps  responses  \\\n",
       "0    Long story short The difference between Permut...   479.0          5   \n",
       "1    With the demand for “realtime” low latency dat...    16.0          0   \n",
       "2    My foray into understanding, and more importan...    33.0          3   \n",
       "3    Target audience: general. 27 scientists collab...     NaN          0   \n",
       "4    I’ve worked with a number of startups and ther...    83.0          0   \n",
       "..                                                 ...     ...        ...   \n",
       "813  How machine learning and artificial intelligen...    79.0          1   \n",
       "814  Those people dancing in the photo are similar ...     0.0          0   \n",
       "815  Welcome to part 6 of the workshop. As usual, t...    15.0          0   \n",
       "816  Data Science and Analytics is a very rapidly e...    55.0          0   \n",
       "817  Using Artificial Intelligence or Machine Learn...  2000.0         20   \n",
       "\n",
       "     reading_time           publication        date  year  \n",
       "0               3  Towards Data Science  2017-05-31  2017  \n",
       "1               3  Towards Data Science  2017-05-31  2017  \n",
       "2               7  Towards Data Science  2017-05-31  2017  \n",
       "3               3  Towards Data Science  2017-05-31  2017  \n",
       "4               4  Towards Data Science  2017-05-31  2017  \n",
       "..            ...                   ...         ...   ...  \n",
       "813            13  Towards Data Science  2022-02-28  2022  \n",
       "814             8  Towards Data Science  2022-02-28  2022  \n",
       "815             5  Towards Data Science  2022-02-28  2022  \n",
       "816             8  Towards Data Science  2022-02-28  2022  \n",
       "817             5  Towards Data Science  2022-02-28  2022  \n",
       "\n",
       "[818 rows x 15 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f261fc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the following leaves in place two or more capital letters in a row\n",
    "#will be ignored when using standard stemming\n",
    "def abbr_or_lower(word):\n",
    "    if re.match('([A-Z]+[a-z]*){2,}', word):\n",
    "        return word\n",
    "    else:\n",
    "        return word.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a2aca003",
   "metadata": {},
   "outputs": [],
   "source": [
    "#modular pipeline for stemming, lemmatizing and lowercasing\n",
    "#note this is NOT lemmatizing using grammar pos\n",
    "def tokenize(text, modulation):\n",
    "    tokens = re.split(r'\\W+', text)\n",
    "    stems = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        lowers=abbr_or_lower(token)\n",
    "        if lowers not in stop_words:\n",
    "            if re.search('[a-zA-Z]', lowers):\n",
    "                if modulation==0:\n",
    "                    stems.append(lowers)\n",
    "                if modulation==1:\n",
    "                    stems.append(porter.stem(lowers))\n",
    "                if modulation==2:\n",
    "                    stems.append(lmtzr.lemmatize(lowers))\n",
    "                stems.append(\" \")\n",
    "    return \"\".join(stems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6c48e67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(tokens, vocab):\n",
    "    vector=[]\n",
    "    for w in vocab:\n",
    "        vector.append(tokens.count(w))\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "829e5bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for text in df.title:\n",
    "    processed_text = tokenize(text, 1)\n",
    "    corpus.append(processed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5b1e3b8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'differ': 1, 'permut': 2, 'combin': 3, 'build': 4, 'realtim': 5, 'dashboard': 6, 'flink': 7, 'backend': 8, 'artifici': 9, 'intellig': 10, 'panacea': 11, 'today': 12, 'market': 13, 'challeng': 14, 'span': 15, 'class': 16, 'markup': 17, 'anchor': 18, 'h3': 19, 'data': 20, 'action': 21, 'open': 22, 'inner': 23, 'link': 24, 'valu': 25, 'https': 26, 'www': 27, 'linkedin': 28, 'com': 29, 'puls': 30, 'opportun': 31, 'obstacl': 32, 'deep': 33, 'learn': 34, 'biolog': 35, 'medicin': 36, 'johnni': 37, 'isra': 38, 'singl': 39, 'import': 40, 'thing': 41, 'startup': 42, 'probabl': 43, 'weight': 44, 'coin': 45, 'flip': 46, 'game': 47, 'use': 48, 'python': 49, 'numpi': 50, 'right': 51, 'answer': 52, 'bias': 53, 'statist': 54, 'problem': 55, 'scienc': 56, 'divers': 57, 'linear': 58, 'algebra': 59, 'cheat': 60, 'sheet': 61, 'kalman': 62, 'filter': 63, 'intuit': 64, 'discret': 65, 'case': 66, 'deriv': 67, 'strong': 68, 'explor': 69, 'ed': 70, 'sheeran': 71, 'song': 72, 'found': 73, 'machin': 74, 'stori': 75, 'recogn': 76, 'news': 77, 'time': 78, 'mix': 79, 'effect': 80, 'random': 81, 'forest': 82, 'smarter': 83, 'analyt': 84, 'strategi': 85, 'rust': 86, 'grid': 87, 'introduct': 88, 'program': 89, 'languag': 90, 'retail': 91, 'trend': 92, 'shape': 93, 'industri': 94, 'infograph': 95, 'hold': 96, 'meet': 97, 'product': 98, 'team': 99, 'start': 100, 'up': 101, 'beginn': 102, 'free': 103, 'googl': 104, 'studio': 105, 'good': 106, 'still': 107, 'long': 108, 'way': 109, 'go': 110, 'democrat': 111, 'becom': 112, 'driven': 113, 'level': 114, 'user': 115, 'trust': 116, 'teach': 117, 'general': 118, 'assembl': 119, 'week': 120, 'topic': 121, 'model': 122, 'latent': 123, 'dirichlet': 124, 'alloc': 125, 'lda': 126, 'guid': 127, 'convolut': 128, 'neural': 129, 'network': 130, 'ai': 131, 'artist': 132, 'part': 133, 'privaci': 134, 'updat': 135, 'thank': 136, 'ignor': 137, 'yet': 138, 'toward': 139, 'reproduc': 140, 'balanc': 141, 'public': 142, 'custom': 143, 'churn': 144, 'explicit': 145, 'observ': 146, 'r': 147, 'vehicl': 148, 'detect': 149, 'support': 150, 'vector': 151, 'svm': 152, 'prepar': 153, 'resum': 154, 'portfolio': 155, 'bandit': 156, 'recommend': 157, 'system': 158, 'optim': 159, 'web': 160, 'scrape': 161, 'illustr': 162, 'cia': 163, 'world': 164, 'fact': 165, 'book': 166, 'averag': 167, 'wrong': 168, 'ii': 169, 'crack': 170, 'mors': 171, 'code': 172, 'rnns': 173, 'concurr': 174, 'realli': 175, 'increas': 176, 'perform': 177, 'bayesbal': 178, 'bayesian': 179, 'analysi': 180, 'bat': 181, 'extend': 182, 'can': 183, 'tensorflow': 184, 'estim': 185, 'best': 186, 'deal': 187, 'cloud': 188, 'provid': 189, 'gpu': 190, 'imag': 191, 'process': 192, 'opencl': 193, 'access': 194, 'twitter': 195, 'api': 196, 'find': 197, 'similar': 198, 'quora': 199, 'question': 200, 'word2vec': 201, 'xgboost': 202, 'generat': 203, 'text': 204, 'recurr': 205, 'scale': 206, 'distribut': 207, 'spark': 208, 'empow': 209, 'mlflow': 210, 'segment': 211, 'identifi': 212, 'rooftop': 213, 'low': 214, 'resolut': 215, 'satellit': 216, 'dimension': 217, 'reduct': 218, 'dummi': 219, 'lay': 220, 'brick': 221, 'transfer': 222, 'fastai': 223, 'librari': 224, 'state': 225, 'elder': 226, 'care': 227, 'revolution': 228, 'age': 229, 'solv': 230, 'last': 231, 'mile': 232, 'logist': 233, 'conundrum': 234, 'jupyterlab': 235, 'credenti': 236, 'store': 237, 'spot': 238, 'leakag': 239, 'heat': 240, 'map': 241, 'laptop': 242, 'ml': 243, 'practic': 244, 'cycl': 245, 'experiment': 246, 'success': 247, 'youtub': 248, 'extens': 249, 'exploratori': 250, 'simpl': 251, 'reinforc': 252, 'tempor': 253, 'analyz': 254, 'friez': 255, 'london': 256, 'art': 257, 'fair': 258, 'fudg': 259, 'number': 260, 'step': 261, 'hire': 262, 'scientist': 263, 'stop': 264, 'look': 265, 'bonsai': 266, 'simulink': 267, 'fake': 268, 'respons': 269, 'visual': 270, 'multivari': 271, 'trajectori': 272, 'confid': 273, 'interv': 274, 'plot': 275, 'k': 276, 'mean': 277, 'cluster': 278, 'scikit': 279, 'top': 280, 'skill': 281, 'rockstar': 282, 'wavelet': 283, 'end': 284, 'almost': 285, 'everi': 286, 'purpos': 287, 'method': 288, 'icc': 289, 'cricket': 290, 'cup': 291, 'predict': 292, 'quick': 293, 'speed': 294, 'power': 295, 'bi': 296, 'annot': 297, 'heatmap': 298, 'correl': 299, 'matrix': 300, 'thompson': 301, 'sampl': 302, 'multi': 303, 'arm': 304, 'hous': 305, 'design': 306, 'cool': 307, 'behind': 308, 'beta': 309, 'gem': 310, 'first': 311, 'job': 312, 'evalu': 313, 'root': 314, 'squar': 315, 'error': 316, 'absolut': 317, 'court': 318, 'decis': 319, 'make': 320, 'fight': 321, 'get': 322, 'geograph': 323, 'role': 324, 'redefin': 325, 'bank': 326, 'engin': 327, 'anim': 328, 'legendari': 329, 'pokémon': 330, 'algorithm': 331, 'zero': 332, 'sota': 333, 'small': 334, 'set': 335, 'nano': 336, 'gigantum': 337, 'humeri': 338, 'insident': 339, 'doubl': 340, 'dip': 341, 'natur': 342, 'decomposit': 343, 'norm': 344, 'penalti': 345, 'multitask': 346, 'minut': 347, 'put': 348, 'loss': 349, 'amazon': 350, 'menglin': 351, 'wang': 352, 'interview': 353, 'auria': 354, 'kathi': 355, 'microsoft': 356, 'azur': 357, 'pipelin': 358, 'asynchron': 359, 'workflow': 360, 'relationship': 361, 'financi': 362, 'resourc': 363, 'educ': 364, 'select': 365, 'may': 366, 'ask': 367, 'readmiss': 368, 'interest': 369, 'polici': 370, 'write': 371, 'auto': 372, 'encod': 373, 'face': 374, 'opencv': 375, 'region': 376, 'pool': 377, 'creat': 378, 'voic': 379, 'recognit': 380, 'calcul': 381, 'android': 382, 'app': 383, 'review': 384, 'squeezenet': 385, 'classif': 386, 'new': 387, 'hadoop': 388, 'know': 389, 'various': 390, 'file': 391, 'format': 392, 'easi': 393, 'pro': 394, 'tip': 395, 'defaultdict': 396, 'counter': 397, 'place': 398, 'dictionari': 399, 'explan': 400, 'dispar': 401, 'impact': 402, 'remov': 403, 'modi': 404, 'train': 405, 'ann': 406, 'dump': 407, 'b': 408, 'test': 409, 'bite': 410, 'size': 411, 'recip': 412, 'nlp': 413, 'better': 414, 'emphasi': 415, 'gibb': 416, 'princip': 417, 'compon': 418, 'math': 419, 'post': 420, 'less': 421, 'refocus': 422, 'focus': 423, 'microscopi': 424, 'gender': 425, 'infer': 426, 'quantit': 427, 'deeper': 428, 'descent': 429, 'virtual': 430, 'race': 431, 'hong': 432, 'kong': 433, 'perspect': 434, '2d': 435, 'polygon': 436, 'point': 437, 'brief': 438, 'primer': 439, 'busi': 440, 'watch': 441, 'rise': 442, 'plastic': 443, 'ocean': 444, 'one': 445, 'innoc': 446, 'interpret': 447, 'suspici': 448, 'elect': 449, 'secret': 450, 'unknow': 451, 'understand': 452, 'complex': 453, 'exampl': 454, 'standard': 455, 'normal': 456, 'input': 457, 'tutori': 458, 'fuzzi': 459, 'name': 460, 'match': 461, 'run': 462, 'jupyt': 463, 'notebook': 464, 'remot': 465, 'server': 466, 'real': 467, 'live': 468, 'kera': 469, 'read': 470, 'empir': 471, 'cdf': 472, 'anoth': 473, 'stage': 474, 'reactiv': 475, 'dash': 476, 'robust': 477, 'environ': 478, 'extract': 479, 'must': 480, 'paper': 481, 'gan': 482, 'regress': 483, 'classifi': 484, 'flask': 485, 'automat': 486, 'entiti': 487, 'spaci': 488, 'theori': 489, 'histori': 490, 'overview': 491, 'gold': 492, 'win': 493, 'solut': 494, 'without': 495, 'framework': 496, 'fundament': 497, 'seek': 498, 'advic': 499, 'younger': 500, 'self': 501, 'organ': 502, 'spreadsheet': 503, 'need': 504, 'regular': 505, 'express': 506, 'sql': 507, 'explain': 508, 'manag': 509, 'present': 510, 'script': 511, 'production': 512, 'erlang': 513, 'elixir': 514, 'struggl': 515, 'qualiti': 516, 'march': 517, 'edit': 518, 'sens': 519, 'much': 520, 'clean': 521, 'locat': 522, 'geopi': 523, 'base': 524, 'attack': 525, 'break': 526, 'larg': 527, 'xor': 528, 'puf': 529, 'casm': 530, 'fractal': 531, 'secur': 532, 'structur': 533, 'brain': 534, 'neuron': 535, 'examin': 536, 'rewir': 537, 'defens': 538, 'dark': 539, 'stat': 540, 'plain': 541, 'enigma': 542, 'let': 543, 'object': 544, 'reduc': 545, 'memori': 546, 'life': 547, 'innov': 548, 'research': 549, 'land': 550, 'freedom': 551, 'inform': 552, 'request': 553, 'iii': 554, 'programm': 555, 'persist': 556, 'homolog': 557, 'non': 558, 'mathi': 559, 'lesser': 560, 'known': 561, 'panda': 562, 'trick': 563, 'continu': 564, 'genet': 565, 'scratch': 566, 'smart': 567, 'deploy': 568, 'comput': 569, 'gas': 570, 'price': 571, 'amp': 572, 'transform': 573, 'favorit': 574, 'featur': 575, 'investor': 576, 'emot': 577, 'detector': 578, 'pytorch': 579, 'semant': 580, 'bert': 581, 'elasticsearch': 582, 'popular': 583, 'tool': 584, 'full': 585, 'stack': 586, 'five': 587, 'move': 588, 'funci': 589, 'like': 590, 'jame': 591, 'brown': 592, 'essenti': 593, 'colaboratori': 594, 'drive': 595, 'github': 596, 'made': 597, 'simpler': 598, 'winter': 599, 'vue': 600, 'js': 601, 'vs': 602, 'angular': 603, 'javascript': 604, 'develop': 605, 'back': 606, 'slack': 607, 'bot': 608, 'climat': 609, 'chang': 610, 'food': 611, 'scarciti': 612, 'lie': 613, 'everyon': 614, 'julia': 615, 'under': 616, 'play': 617, 'scotch': 618, 'big': 619, 'ecommerc': 620, 'person': 621, 'ident': 622, 'futur': 623, 'jump': 624, 'puzzl': 625, 'hansard': 626, 'beautifulsoup': 627, 'project': 628, 'actual': 629, 'think': 630, 'peopl': 631, 'danger': 632, 'stem': 633, 'mobil': 634, 'matric': 635, 'fit': 636, 'overfit': 637, 'v': 638, 'underfit': 639, 'dehuman': 640, 'franc': 641, 'confer': 642, 'student': 643, 'view': 644, 'organis': 645, 'readi': 646, 'compani': 647, 'video': 648, 'autoencod': 649, 'might': 650, 'blow': 651, 'plan': 652, 'ultim': 653, 'git': 654, 'command': 655, 'implement': 656, 'naiv': 657, 'bay': 658, 'categor': 659, 'speaker': 660, 'diarize': 661, 'kaldi': 662, 'django': 663, 'postgresql': 664, 'basic': 665, 'q': 666, 'markov': 667, 'chain': 668, 'mont': 669, 'carlo': 670, 'join': 671, 'venn': 672, 'diagram': 673, 'insan': 674, 'awesom': 675, 'optuna': 676, 'mani': 677, 'dreamteam': 678, 'never': 679, 'truli': 680, 'andrew': 681, 'ng': 682, 'want': 683, 'convey': 684, 'technic': 685, 'cours': 686, 'skin': 687, 'cancer': 688, 'surpris': 689, 'document': 690, 'adversari': 691, 'revit': 692, 'old': 693, 'textur': 694, 'pattern': 695, 'conduct': 696, 'popul': 697, 'proport': 698, 'studi': 699, 'binari': 700, 'variabl': 701, 'idea': 702, 'approach': 703, 'show': 704, 'healthcar': 705, 'cost': 706, 'york': 707, 'vari': 708, 'softwar': 709, 'see': 710, 'colour': 711, 'supervis': 712, 'pdf': 713, 'pca': 714, 'tsne': 715, 'umap': 716, 'hyperparamet': 717, 'tune': 718, 'c': 719, 'gamma': 720, 'paramet': 721, 'aw': 722, 'certifi': 723, 'practition': 724, 'day': 725, '__slots__': 726, 'reason': 727, 'websit': 728, 'gpt': 729, 'mighti': 730, 'openai': 731, 'determinist': 732, 'gradient': 733, 'ddpg': 734, 'scatter': 735, 'boxplot': 736, 'graph': 737, 'result': 738, 'matplotlib': 739, 'seaborn': 740, 'line': 741, 'dataset': 742, 'webpag': 743, 'adam': 744, 'taught': 745, 'list': 746, 'packag': 747, 'publish': 748, 'unravel': 749, 'execut': 750, 'apach': 751, 'paraphras': 752, 't5': 753, 'pretrain': 754, 'tell': 755, 'autocorrect': 756, 'embed': 757, 'condit': 758, 'seattl': 759, 'rain': 760, 'tricki': 761, 'friend': 762, 'exchang': 763, 'rate': 764, 'seri': 765, 'forecast': 766, 'arima': 767, 'node': 768, 'node2vec': 769, 'neo4j': 770, 'slacker': 771, 'autom': 772, 'stock': 773, 'trade': 774, 'seven': 775, 'sin': 776, 'quickstart': 777, 'tensorboard': 778, 'consum': 779, 'platform': 780, 'pub': 781, 'sub': 782, 'bigqueri': 783, 'firebas': 784, 'engag': 785, 'attract': 786, 'assess': 787, 'activ': 788, 'function': 789, 'propag': 790, 'epoch': 791, 'imput': 792, 'experi': 793, 'lightn': 794, 'torchtext': 795, 'built': 796, 'selenium': 797, 'human': 798, 'dig': 799, 'enough': 800, 'techniqu': 801, 'benefit': 802, 'insight': 803, 'kendra': 804, 'fargat': 805, 'servic': 806, 'send': 807, 'email': 808, 'task': 809, 'medium': 810, 'unsung': 811, 'hero': 812, 'mask': 813, 'adapt': 814, 'track': 815, 'vision': 816, 'true': 817, 'fals': 818, 'lost': 819, 'search': 820, 'leagu': 821, 'legend': 822, 'pivot': 823, 'tabl': 824, 'oper': 825, 'bigger': 826, 'also': 827, 'releas': 828, 'karl': 829, 'marx': 830, 'varianc': 831, 'weather': 832, 'umbrella': 833, 'remind': 834, 'instagram': 835, 'messag': 836, 'say': 837, 'beauti': 838, 'soup': 839, 'ifici': 840, 'creativ': 841, 'career': 842, 'seeker': 843, 'intro': 844, 'scrap': 845, 'easili': 846, 'ronaldo': 847, 'score': 848, 'goal': 849, 'us': 850, 'reddit': 851, 'flair': 852, 'heroku': 853, 'volum': 854, 'high': 855, 'content': 856, 'screen': 857, 'cellprofil': 858, 'airflow': 859, 'home': 860, 'modern': 861, 'oracl': 862, 'pyramid': 863, 'sale': 864, 'dbscan': 865, 'complet': 866, 'cron': 867, 'launchd': 868, 'maco': 869, 'linux': 870, 'macbook': 871, 'tri': 872, 'infecti': 873, 'diseas': 874, 'coronavirus': 875, 'candlestick': 876, 'chart': 877, 'etl': 878, 'einstein': 879, 'resiz': 880, 'pil': 881, 'covid': 882, 'epidem': 883, 'inpaint': 884, 'batch': 885, 'breakdown': 886, 'index': 887, 'effici': 888, 'equat': 889, 'three': 890, 'java': 891, 'streamlit': 892, 'summar': 893, 'bart': 894, 'kubernet': 895, 'docker': 896, 'multipl': 897, 'label': 898, 'mutabl': 899, 'star': 900, 'war': 901, 'stata': 902, 'creation': 903, 'report': 904, 'work': 905, 'easier': 906, 'subreddit': 907, 'bokeh': 908, 'tire': 909, 'told': 910, 'direct': 911, '2nd': 912, 'firm': 913, 'improv': 914, 'appl': 915, 'includ': 916, 'causal': 917, 'blog': 918, 'month': 919, 'colab': 920, 'roi': 921, 'metric': 922, 'recess': 923, 'hospit': 924, 'capac': 925, 'pandem': 926, 'csvkit': 927, 'swiss': 928, 'armi': 929, 'knife': 930, 'csv': 931, 'across': 932, 'mental': 933, 'earli': 934, 'meta': 935, 'dear': 936, 'fellow': 937, 'white': 938, 'men': 939, 'tech': 940, 'excus': 941, 'venu': 942, 'sydney': 943, 'context': 944, 'ingredi': 945, 'nlp365': 946, 'summari': 947, 'abstract': 948, 'achiev': 949, 'domin': 950, 'swim': 951, 'berlin': 952, 'mlmachin': 953, 'crowd': 954, 'sourc': 955, 'eu': 956, 'interrog': 957, 'excel': 958, 'architectur': 959, 'throughput': 960, 'latenc': 961, 'global': 962, 'ner': 963, 'master': 964, 'date': 965, 'timestamp': 966, 'gather': 967, 'daemon': 968, 'systemd': 969, 'tree': 970, 'depth': 971, 'via': 972, 'cross': 973, 'valid': 974, 'four': 975, 'except': 976, 'sloppi': 977, 'interact': 978, 'worri': 979, 'blender': 980, 'queue': 981, 'represent': 982, 'everyth': 983, 'faker': 984, 'genom': 985, 'holi': 986, 'grail': 987, 'domain': 988, 'knowledg': 989, 'second': 990, 'spotifi': 991, 'playlist': 992, 'album': 993, 'quantum': 994, 'applic': 995, 'bit': 996, 'rent': 997, 'singapor': 998, 'hdb': 999, 'resal': 1000, 'stuck': 1001, 'europ': 1002, 'worst': 1003, 'outbreak': 1004, 'warehous': 1005, 'anti': 1006, 'matur': 1007, 'em': 1008, 'preprocess': 1009, 'dask': 1010, 'xresnet': 1011, 'saa': 1012, 'group2vec': 1013, 'advanc': 1014, 'ventur': 1015, 'capit': 1016, 'sagemak': 1017, 'unsupervis': 1018, 'benchmark': 1019, 'stanfordnlp': 1020, 'ibm': 1021, 'dialogflow': 1022, 'textspac': 1023, 'manipul': 1024, 'path': 1025, '128m': 1026, 'repositori': 1027, 'malwar': 1028, 'bulk': 1029, 'arxiv': 1030, 'preprint': 1031, 'bordeaux': 1032, 'onlin': 1033, 'hackathon': 1034, 'devic': 1035, 'mayb': 1036, 'someth': 1037, 'russian': 1038, 'stt': 1039, 'tts': 1040, 'economi': 1041, 'bengali': 1042, 'handwritten': 1043, 'graphem': 1044, 'final': 1045, 'epiphani': 1046, 'enhanc': 1047, 'progress': 1048, 'america': 1049, 'peak': 1050, 'keyword': 1051, 'sparksess': 1052, 'sparkcontext': 1053, 'sqlcontext': 1054, 'hivecontext': 1055, 'surrog': 1056, 'mysql': 1057, 'workbench': 1058, 'degre': 1059, '3d': 1060, 'polygen': 1061, 'hedg': 1062, 'powerbi': 1063, 'shini': 1064, 'two': 1065, 'altern': 1066, 'compar': 1067, 'collabor': 1068, 'approxim': 1069, 'attent': 1070, 'ensembl': 1071, 'host': 1072, 'offlin': 1073, 'measur': 1074, 'z': 1075, 'load': 1076, 'citi': 1077, 'qgis': 1078, 'keyboard': 1079, 'shortcut': 1080, 'vscode': 1081, 'orient': 1082, 'diverg': 1083, 'bar': 1084, 'beyond': 1085, 'movi': 1086, 'compact': 1087, 'pitfal': 1088, 'employe': 1089, 'attrit': 1090, 'catboost': 1091, 'shap': 1092, 'help': 1093, 'nba': 1094, 'nutshel': 1095, 'version': 1096, 'pre': 1097, 'acoust': 1098, 'pet': 1099, 'least': 1100, 'risk': 1101, 'minim': 1102, 'eda': 1103, 'behaviour': 1104, 'block': 1105, 'nearest': 1106, 'neighbor': 1107, 'databas': 1108, 'facebook': 1109, 'mileston': 1110, 'translat': 1111, 'spooki': 1112, 'scari': 1113, 'stuff': 1114, 'string': 1115, 'super': 1116, 'regex': 1117, 'texa': 1118, 'poker': 1119, 'align': 1120, 'suppli': 1121, 'demand': 1122, 'latest': 1123, 'pick': 1124, 'common': 1125, 'pros': 1126, 'con': 1127, 'ground': 1128, 'truth': 1129, 'warn': 1130, 'numba': 1131, 'hypothesi': 1132, 'simpli': 1133, 'possibl': 1134, 'tackl': 1135, 'choos': 1136, 'restyl': 1137, 'fashion': 1138, 'law': 1139, 'total': 1140, 'expect': 1141, 'covari': 1142, 'even': 1143, 'golang': 1144, 'premier': 1145, 'player': 1146, 'boost': 1147, 'xai': 1148, 'convex': 1149, 'hull': 1150, 'gift': 1151, 'wrap': 1152, 'easiest': 1153, 'relat': 1154, 'pytorch3d': 1155, 'main': 1156, 'heurist': 1157, 'construct': 1158, 'dimens': 1159, 'wine': 1160, 'lover': 1161, 'tini': 1162, 'glitter': 1163, 'polit': 1164, 'sentiment': 1165, 'british': 1166, 'media': 1167, 'chanc': 1168, 'raini': 1169, 'hand': 1170, 'group': 1171, 'scenario': 1172, 'god': 1173, 'defenc': 1174, 'principl': 1175, 'visualis': 1176, 'hangar': 1177, 'ad': 1178, 'closer': 1179, 'neuralink': 1180, 'connect': 1181, 'internet': 1182, 'short': 1183, 'note': 1184, 'avoid': 1185, 'extinct': 1186, 'year': 1187, 'mermaid': 1188, 'effortless': 1189, 'syntax': 1190, 'correct': 1191, 'overload': 1192, 'breakpoint': 1193, 'scala': 1194, 'cnns': 1195, 'salienc': 1196, 'univers': 1197, 'summer': 1198, 'school': 1199, 'fpgas': 1200, 'kafka': 1201, 'hidden': 1202, 'pdfs': 1203, 'scan': 1204, 'beam': 1205, 'stream': 1206, 'dataflow': 1207, 'eleg': 1208, 'emmett': 1209, 'ultrafast': 1210, 'pecanpi': 1211, 'auc': 1212, 'roc': 1213, 'academ': 1214, 'biomedicin': 1215, 'word': 1216, 'openstreetmap': 1217, 'great': 1218, 'moment': 1219, 'competit': 1220, 'ongo': 1221, 'upcom': 1222, 'detectron2': 1223, 'etf': 1224, 'ark': 1225, 'fund': 1226, 'highlight': 1227, 'summit': 1228, 'na': 1229, 'fanci': 1230, 'debug': 1231, 'fan': 1232, 'photor': 1233, 'accur': 1234, 'accuraci': 1235, 'breast': 1236, 'cat': 1237, 'happi': 1238, 'travel': 1239, 'accessviz': 1240, 'mapper': 1241, 'helsinki': 1242, 'magic': 1243, 'ball': 1244, 'maxim': 1245, 'softmax': 1246, 'entropi': 1247, 'anomali': 1248, 'pycaret': 1249, 'heikin': 1250, 'ashi': 1251, 'candl': 1252, 'parallel': 1253, 'wordcloud': 1254, 'monad': 1255, 'len': 1256, 'imper': 1257, 'storag': 1258, 'gcs': 1259, 'intermedi': 1260, 'ethereum': 1261, 'genr': 1262, 'rigor': 1263, 'mathemat': 1264, 'formul': 1265, 'underutil': 1266, 'event': 1267, 'render': 1268, 'mentor': 1269, 'lesson': 1270, 'op': 1271, 'mine': 1272, 'interfac': 1273, 'dataload': 1274, 'polynot': 1275, 'plus': 1276, 'malaria': 1277, 'bird': 1278, 'term': 1279, 'anywher': 1280, 'forc': 1281, 'comprehens': 1282, 'ditch': 1283, 'ipython': 1284, 'kernel': 1285, 'playbook': 1286, 'monitor': 1287, 'taskflow': 1288, 'decrypt': 1289, 'tumour': 1290, 'exploit': 1291, 'discov': 1292, 'treasur': 1293, 'control': 1294, 'gym': 1295, 'elegantrl': 1296, 'lightweight': 1297, 'stabl': 1298, 'retrain': 1299, 'mention': 1300, 'fail': 1301, 'fix': 1302, 'goe': 1303, 'glanc': 1304, 'excit': 1305, 'paradigm': 1306, 'shift': 1307, 'traffic': 1308, 'intersect': 1309, 'simul': 1310, 'pygam': 1311, 'groupbi': 1312, 'love': 1313, 'reliabl': 1314, 'hierarchi': 1315, 'influenc': 1316, 'smartwatch': 1317, 'stand': 1318, 'agronom': 1319, 'sustain': 1320, 'agricultur': 1321, 'bottom': 1322, 'sudoku': 1323, 'facial': 1324, 'patent': 1325, 'ace': 1326, 'phd': 1327, 'dice': 1328, 'exact': 1329, 'probabilist': 1330, 'hria': 1331, 'supercharg': 1332, 'welcom': 1333, 'stay': 1334, 'neurosci': 1335, 'februari': 1336, 'eeoc': 1337, 'discrimin': 1338, 'investig': 1339, 'drug': 1340, 'util': 1341, 'sweden': 1342, 'denmark': 1343, 'english': 1344, 'major': 1345, 'm1': 1346, 'max': 1347, 'close': 1348, 'form': 1349, 'fibonacci': 1350, 'sequenc': 1351, 'take': 1352, 'stanford': 1353, 'key': 1354, 'netflix': 1355, 'metaflow': 1356, 'blockchain': 1357, 'nfts': 1358, 'literatur': 1359, 'font': 1360, 'curriculum': 1361, 'caret': 1362, 'tidymodel': 1363, 'reusabl': 1364, 'reus': 1365, 'hindsight': 1366, 'replay': 1367, 'her': 1368, 'smooth': 1369, 'integr': 1370, 'discontinu': 1371, 'euler': 1372, 'constant': 1373, 'sne': 1374, 'callback': 1375, 'feder': 1376, 'openfl': 1377, 'tableau': 1378, 'hack': 1379, 'demo': 1380, 'delay': 1381, 'ms': 1382, 'dayolo': 1383, 'multiscal': 1384, 'yolo': 1385, 'sar': 1386, 'cov': 1387, 'raman': 1388, 'spectroscopi': 1389, '300x': 1390, 'iter': 1391, 'yes': 1392, 'pleas': 1393, 'blocker': 1394, 'turn': 1395, 'agil': 1396, 'walk': 1397, 'baptism': 1398, 'fire': 1399, 'coffe': 1400, 'expand': 1401, 'water': 1402, 'fine': 1403, 'aspect': 1404, 'textblob': 1405, 'ae': 1406, 'vae': 1407, 'cyclegan': 1408, 'devop': 1409, 'toolkit': 1410, 'undeni': 1411, 'doc': 1412, 'wish': 1413, 'knew': 1414, 'ago': 1415, 'ridg': 1416, 'lasso': 1417, 'sklearn': 1418, 'multiprocess': 1419, 'adaboost': 1420, 'nest': 1421, 'field': 1422, 'anymor': 1423, 'minimum': 1424, 'wage': 1425, 'could': 1426, 'appli': 1427, 'contribut': 1428, 'well': 1429, 'log': 1430, 'survey': 1431, 'improp': 1432, 'serverless': 1433, 'lambda': 1434, 'demystifi': 1435, 'written': 1436, 'communic': 1437, 'novic': 1438, 'journey': 1439, 'kaggl': 1440, 'navig': 1441, 'subway': 1442, 'flood': 1443, 'emerg': 1444, 'threat': 1445, 'coastlin': 1446, 'often': 1447, 'miss': 1448, 'meaning': 1449, 'acceler': 1450, 'come': 1451, 'mac': 1452, 'compos': 1453, 'voxel': 1454, 'mesh': 1455, 'debias': 1456, 'pymood': 1457, 'differenti': 1458, 'evolut': 1459, 'mlop': 1460, 'seldon': 1461, 'core': 1462, 'lineapi': 1463, 'fourier': 1464, '1024x1024': 1465, 'larger': 1466, 'morethansenti': 1467, 'next': 1468, 'expert': 1469, 'datafram': 1470, 'bodi': 1471, 'orbit': 1472, 'cope': 1473, 'overcom': 1474, 'bottleneck': 1475, 'grpc': 1476, 'simpi': 1477, 'pointgan': 1478, 'simplest': 1479, 'ehr': 1480, 'hierarch': 1481, 'mechan': 1482, 'lean': 1483, 'proof': 1484, 'concept': 1485, 'semi': 1486, 'rapid': 1487, 'famili': 1488, 'check': 1489, 'mindset': 1490, 'amateur': 1491, 'profession': 1492, 'analyst': 1493, 'prompt': 1494, 'statsmodel': 1495, 'uncertainti': 1496, 'seed': 1497, 'naca': 1498, 'airfoil': 1499, 'aerodynam': 1500, 'simplifi': 1501, 'metadata': 1502, 'foundat': 1503, 'pythagorean': 1504, 'sport': 1505, 'happen': 1506, 'omit': 1507, 'intergener': 1508, 'scipi': 1509, 'style': 1510, 'relev': 1511, 'knn': 1512, 'optic': 1513, 'hdbscan': 1514, 'smote': 1515, 'wrangl': 1516, 'produc': 1517, 'wandb': 1518, 'bee': 1519, 'coloni': 1520, 'diabet': 1521, 'multidimension': 1522, 'login': 1523, 'restrict': 1524, 'page': 1525, 'rl': 1526, 'lifecycl': 1527, 'consid': 1528, 'speech': 1529, 'pos': 1530, 'tag': 1531, 'bottl': 1532, 'wastewat': 1533, 'merg': 1534, 'loop': 1535, 'mislead': 1536, 'audienc': 1537, 'green': 1538, 'initi': 1539, 'fool': 1540, 'signific': 1541, 'hot': 1542, 'rainbow': 1543, 'push': 1544, 'era': 1545, 'mortal': 1546, 'triag': 1547, 'receiv': 1548, 'notif': 1549, 'phone': 1550, 'rank': 1551, 'crud': 1552, 'usag': 1553, 'dagshub': 1554, 'supplement': 1555, 'kolmogorov': 1556, 'smirnov': 1557, 'ks': 1558, 'dynam': 1559, 'transcrib': 1560, 'audio': 1561, 'geospati': 1562, 'leafmap': 1563, 'pyneuralog': 1564, 'jinja': 1565, 'fast': 1566, 'statsforecast': 1567, 'reservoir': 1568, 'augment': 1569, 'marianmt': 1570, 'dive': 1571, 'contain': 1572, 'syntact': 1573, 'asterisk': 1574, 'jaro': 1575, 'winkler': 1576, 'pagerank': 1577, 'mcnemar': 1578, 'chatbot': 1579, 'soccer': 1580, 'scene': 1581, 'siames': 1582, 'establish': 1583, 'ecosystem': 1584, 'sign': 1585, 'invest': 1586, 'kmean': 1587, 'scientif': 1588, 'postgi': 1589, 'seamless': 1590, 'share': 1591, 'colleagu': 1592, 'binder': 1593, 'sinc': 1594, 'cybersecur': 1595, 'compress': 1596, 'gis': 1597, 'forget': 1598, 'copi': 1599, 'raw': 1600, 'ingest': 1601, 'background': 1602, 'workshop': 1603}\n",
      " \n",
      "Total size of stemmed vocabulary: 1603\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 445 ms, sys: 16.9 ms, total: 462 ms\n",
      "Wall time: 455 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1593</th>\n",
       "      <th>1594</th>\n",
       "      <th>1595</th>\n",
       "      <th>1596</th>\n",
       "      <th>1597</th>\n",
       "      <th>1598</th>\n",
       "      <th>1599</th>\n",
       "      <th>1600</th>\n",
       "      <th>1601</th>\n",
       "      <th>1602</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>813</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>814</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>815</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>816</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>817</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>818 rows × 1603 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0     1     2     3     4     5     6     7     8     9     ...  1593  \\\n",
       "0       1     1     1     0     0     0     0     0     0     0  ...     0   \n",
       "1       0     0     0     1     1     1     1     1     0     0  ...     0   \n",
       "2       0     0     0     0     0     0     0     0     1     1  ...     0   \n",
       "3       0     0     0     0     0     0     0     0     0     0  ...     0   \n",
       "4       0     0     0     0     0     0     0     0     0     0  ...     0   \n",
       "..    ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  ...   ...   \n",
       "813     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
       "814     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
       "815     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
       "816     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
       "817     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
       "\n",
       "     1594  1595  1596  1597  1598  1599  1600  1601  1602  \n",
       "0       0     0     0     0     0     0     0     0     0  \n",
       "1       0     0     0     0     0     0     0     0     0  \n",
       "2       0     0     0     0     0     0     0     0     0  \n",
       "3       0     0     0     0     0     0     0     0     0  \n",
       "4       0     0     0     0     0     0     0     0     0  \n",
       "..    ...   ...   ...   ...   ...   ...   ...   ...   ...  \n",
       "813     0     0     0     0     0     0     0     0     0  \n",
       "814     0     0     0     0     0     0     0     1     0  \n",
       "815     0     0     0     0     0     0     0     0     1  \n",
       "816     0     0     0     0     0     0     0     0     0  \n",
       "817     0     0     0     0     0     0     0     0     0  \n",
       "\n",
       "[818 rows x 1603 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "#build vocabulary\n",
    "vocab, index = {}, 1  # start indexing from 1\n",
    "for doc in corpus:\n",
    "    #I added this line from lecture to add tokenization\n",
    "    tokens = doc.split()\n",
    "    for token in tokens:\n",
    "      if token not in vocab:\n",
    "        vocab[token] = index\n",
    "        index += 1\n",
    "        \n",
    "vocab_stem=vocab\n",
    "vocab_stem_size = len(vocab)\n",
    "print(vocab_stem)\n",
    "print(\" \")\n",
    "print(\"Total size of stemmed vocabulary:\", vocab_stem_size)\n",
    "\n",
    "vectors=[]\n",
    "for doc in corpus:\n",
    "    vectors.append(vectorize(doc, vocab_stem))\n",
    "\n",
    "df = pd.DataFrame(vectors)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c82867a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ead444c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
